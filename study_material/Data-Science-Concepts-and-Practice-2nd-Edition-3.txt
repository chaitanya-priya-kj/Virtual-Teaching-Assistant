Data ScienceData Science
Concepts and Practice
Second Edition
Vijay Kotu
Bala DeshpandeMorgan Kaufmann is an imprint of Elsevier
50 Hampshire Street, 5th Floor, Cambridge, MA 02139, United States
Copyright r2019 Elsevier Inc. All rights reserved.
No part of this publication may be reproduced or transmitted in any form or by any means, electronic or mechanical,
including photocopying, recording, or any information storage and retrieval system, without permission in writing
from the publisher. Details on how to seek permission, further information about the Publisher ’s permissions policies
and our arrangements with organizations such as the Copyright Clearance Center and the Copyright Licensing Agency,
can be found at our website: www.elsevier.com/permissions .
This book and the individual contributions contained in it are protected under copyright by the Publisher (other than
as may be noted herein).
Notices
Knowledge and best practice in this field are constantly changing. As new research and experience broaden our
understanding, changes in research methods, professional practices, or medical treatment may become necessary.
Practitioners and researchers must always rely on their own experience and knowledge in evaluating and using any
information, methods, compounds, or experiments described herein. In using such information or methods they
should be mindful of their own safety and the safety of others, including parties for whom they have a professional
responsibility.
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume any liability for
any injury and/or damage to persons or property as a matter of products liability, negligence or otherwise, or from any
use or operation of any methods, products, instructions, or ideas contained in the material herein.
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library
Library of Congress Cataloging-in-Publication Data
A catalog record for this book is available from the Library of Congress
ISBN: 978-0-12-814761-0
For Information on all Morgan Kaufmann publications
visit our website at https://www.elsevier.com/books-and-journals
Publisher: Jonathan Simpson
Acquisition Editor: Glyn Jones
Editorial Project Manager: Ana Claudia Abad Garcia
Production Project Manager: Sreejith Viswanathan
Cover Designer: Greg Harris
Typeset by MPS Limited, Chennai, IndiaDedication
To all the mothers in our livesForeword
A lot has happened since the first edition of this book was published in
2014. There is hardly a day where there is no news on data science, machine
learning, or artificial intelligence in the media. It is interesting that many ofthose news articles have a skeptical, if not an even negative tone. All this
underlines two things: data science and machine learning are finally becom-
ing mainstream. And people know shockingly little about it. Readers of thisbook will certainly do better in this regard. It continues to be a valuable
resource to not only educate about how to use data science in practice, but
also how the fundamental concepts work.
Data science and machine learning are fast-moving fields which is why this
second edition reflects a lot of the changes in our field. While we used to
talk a lot about “data mining ”and “predictive analytics ”only a couple of
years ago, we have now settled on the term “data science ”for the broader
field. And even more importantly: it is now commonly understood that
machine learning is at the core of many current technological breakthroughs.These are truly exciting times for all the people working in our field then!
I have seen situations where data science and machine learning had an
incredible impact. But I have also see n situations where this was not the
case. What was the difference? In most c ases where organizations fail with
data science and machine learning is, they had used those techniques inthe wrong context. Data science models are not very helpful if you only
have one big decision you need to make. Analytics can still help you in
such cases by giving you easier acce ss to the data you need to make this
decision. Or by presenting the data in a consumable fashion. But at the
end of the day, those single big decisi ons are often strategic. Building a
machine learning model to help you make this decision is not worthdoing. And often they also do not yield b etter results than just making the
decision on your own.
xiHere is where data science and machine learning can truly help: these
advanced models deliver the most value whenever you need to make lots of
similar decisions quickly. Good examples for this are:
GDefining the price of a product in markets with rapidly changing
demands.
GMaking offers for cross-selling in an E-Commerce platform.
GApproving credit or not.
GDetecting customers with a high risk for churn.
GStopping fraudulent transactions.
GAnd many others.
You can see that a human being who would have access to all relevant data
could make those decisions in a matter of seconds or minutes. Only that
they can ’t without data science, since they would need to make this type of
decision millions of times, every day. Consider sifting through your customerbase of 50 million clients every day to identify those with a high churn risk.
Impossible for any human being. But no problem at all for a machine learn-
ing model.
So, the biggest value of artificial intelligence and machine learning is not to
support us with those big strategic decisions. Machine learning delivers mostvalue when we operationalize models and automate millions of decisions.
One of the shortest descriptions of this phenomenon comes from Andrew
Ng, who is a well-known researcher in the field of AI. Andrew describes whatAI can do as follows: “If a typical person can do a mental task with less than
one second of thought, we can probably automate it using AI either now or
in the near future. ”
I agree with him on this characterization. And I like that Andrew puts the
emphasis on automation and operationalization of those models —because
this is where the biggest value is. The only thing I disagree with is the timeunit he chose. It is safe to already go with a minute instead of a second.
However, the quick pace of changes as well as the ubiquity of data science
also underlines the importance of laying the right foundations. Keep in
mind that machine learning is not completely new. It has been an active
field of research since the 1950s. Some of the algorithms used today haveeven been around for more than 200 years now. And the first deep learning
models were developed i n the 1960s with the term “deep learning ”being
coined in 1984. Those algorithms are well understood now. And under-
standing their basic concepts will he lp you to pick the right algorithm for
the right task.
To support you with this, some additional chapters on deep learning and rec-
ommendation systems have been added to the book. Another focus area isxii Forewordusing text analytics and natural language processing. It became clear in the
past years that the most successful predictive models have been using
unstructured input data in addition to the more traditional tabular formats.Finally, expansion of Time Series Forecasting should get you started on one
of the most widely applied data science techniques in the business.
More algorithms could mean that there is a risk of increased complexity. But
thanks to the simplicity of the RapidMiner platform and the many practical
examples throughout the book this is not the case here. We continue our
journey towards the democratization of data science and machine learning.
This journey continues until data science and machine learning are as ubiqui-
tous as data visualization or Excel. Of course, we cannot magically transformeverybody into a data scientist overnight, but we can give people the tools to
help them on their personal path of development. This book is the only tour
guide you need on this journey.
Ingo Mierswa
Founder
RapidMiner Inc.
Massachusetts, USAForeword xiiiPreface
Our goal is to introduce you to Data Science .
We will provide you with a survey of the fundamental data science concepts
as well as step-by-step guidance on practical implementations —enough to
get you started on this exciting journey.
WHY DATA SCIENCE?
We have run out of adjectives and superlatives to describe the growth trends
of data. The technology revolution has brought about the need to process,
store, analyze, and comprehend large volumes of diverse data in meaningfulways. However, the value of the stored data is zero unless it is acted upon . The scale
of data volume and variety places new demands on organizations to quickly
uncover hidden relationships and patterns. This is where data science techni-ques have proven to be extremely useful. They are increasingly finding their
way into the everyday activities of many business and government functions,
whether in identifying which customers are likely to take their business else-where, or mapping flu pandemic using social media signals.
Data science is a compilation of techniques that extract value from data.
Some of the techniques used in data science have a long history and trace
their roots to applied statistics, machine learning, visualization, logic, and
computer science. Some techniques have just reached the popularity itdeserves. Most emerging technologies go through what is termed the “hype
cycle.”This is a way of contrasting the amount of hyperbole or hype versus
the productivity that is engendered by the emerging technology. The hypecycle has three main phases: peak of inflated expectation, trough of disillu-
sionment, and plateau of productivity. The third phase refers to the mature
and value-generating phase of any technology. The hype cycle for data sci-ence indicates that it is in this mature phase. Does this imply that data sci-
ence has stopped growing or has reached a saturation point? Not at all. On
the contrary, this discipline has grown beyond the scope of its initialxvapplications in marketing and has advanced to applications in technology,
internet-based fields, health care, government, finance, and manufacturing.
WHY THIS BOOK?
The objective of this book is two-fold: to help clarify the basic concepts
behind many data science techniques in an easy-to-follow manner; and to
prepare anyone with a basic grasp of mathematics to implement these techni-
ques in their organizations without the need to write any lines of program-
ming code.
Beyond its practical value, we wanted to show you that the data science
learning algorithms are elegant, beautiful, and incredibly effective. You will
never look at data the same way once you learn the concepts of the learning
algorithms.
To make the concepts stick, you will have to build data science models.
While there are many data science tools available to execute algorithms anddevelop applications, the approaches to solving a data science problem are
similar among these tools. We wanted to pick a fully functional, open source,
free to use, graphical user interface-based data science tool so readers can fol-low the concepts and implement the data science algorithms. RapidMiner, a
leading data science platform, fit the bill and, thus, we used it as a compan-
ion tool to implement the data science algorithms introduced in everychapter.
WHO CAN USE THIS BOOK?
The concepts and implementations described in this book are geared towardsbusiness, analytics, and technical professionals who use data everyday. You,
the reader of the book will get a comprehensive understanding of the differ-
ent data science techniques that can be used for prediction and for discover-ing patterns, be prepared to select the right technique for a given data
problem, and you will be able to create a general-purpose analytics process.
We have tried to follow a process to describe this body of knowledge. Our
focus has been on introducing about 30 key algorithms that are in wide-
spread use today. We present these algorithms in the framework of:
1.A high-level practical use case for each algorithm.
2.An explanation of how the algorithm works in plain language. Many
algorithms have a strong foundation in statistics and/or computerscience. In our descriptions, we have tried to strike a balance between
being accessible to a wider audience and being academically rigorous.xvi Preface3.A detailed review of implementation using RapidMiner, by describing
the commonly used setup and parameter options using a sample data
set. You can download the processes from the companion website
www.IntroDataScience.com and we recommend you follow-along by
building an actual data science process.
Analysts, finance, engineering, marketing, and business professionals, or any-
one who analyzes data, most likely will use data science techniques in their
job either now or in the near future. For business managers who are one step
removed from the actual data science process, it is important to know what
is possible and not possible with these techniques so they can ask the right
questions and set proper expectations. While basic spreadsheet analysis, slic-
ing, and dicing of data through standard business intelligence tools will con-
tinue to form the foundations of data exploration in business, data science
techniques are necessary to establish the full edifice of analytics in the
organizations.
Vijay Kotu
California, USA
Bala Deshpande, PhD
Michigan, USAPreface xviiAcknowledgments
Writing a book is one of the most interesting and challenging endeavors one
can embark on. We grossly underestimated the effort it would take and the
fulfillment it brings. This book would not have been possible without thesupport of our families, who granted us enough leeway in this time-
consuming activity. We would like to thank the team at RapidMiner, who
provided great help with everything, ranging from technical support toreviewing the chapters to answering questions on the features of the product.
Our special thanks to Ingo Mierswa for setting the stage for the book through
the foreword. We greatly appreciate the thoughtful and insightful commentsfrom our technical reviewers: Doug Schrimager from Slalom Consulting,
Steven Reagan from L&L Products, and Philipp Schlunder, Tobias Malbrecht
and Ingo Mierswa from RapidMiner. Thanks to Mike Skinner of Intel and toDr. Jacob Cybulski of Deakin University in Australia. We had great support
and stewardship from the Elsevier and Morgan Kaufmann team: Glyn Jones,
Ana Claudia Abad Garcia, and Sreejith Viswanathan. Thanks to our collea-gues and friends for all the productive discussions and suggestions regarding
this project.
xixCHAPTER 1
Introduction
Data science is a collection of techniques used to extract value from data. It
has become an essential tool for any organization that collects, stores, and
processes data as part of its operations. Data science techniques rely on find-
ing useful patterns, connections, and relationships within data. Being a buzz-
word, there is a wide variety of definitions and criteria for what constitutes
data science. Data science is also commonly referred to as knowledge discov-
ery, machine learning, predictive analytics, and data mining. However, each
term has a slightly different connotation depending on the context. In this
chapter, we attempt to provide a general overview of data science and point
out its important features, purpose, taxonomy, and methods.
In spite of the present growth and popularity, the underlying methods of
data science are decades if not centuries old. Engineers and scientists have
been using predictive models since the beginning of nineteenth century.
Humans have always been forward-looking creatures and predictive sciences
are manifestations of this curiosity. So, who uses data science today? Almost
every organization and business. Sure, we didn ’t call the methods that are
now under data science as “Data Science. ”The use of the term science in data
science indicates that the methods are evidence based, and are built on
empirical knowledge, more specifically historical observations.
As the ability to collect, store, and process data has increased, in line with
Moore ’s Law - which implies that computing hardware capabilities double
every two years, data science has found increasing applications in many
diverse fields. Just decades ago, building a production quality regression
model took about several dozen hours (Parr Rud, 2001). Technology has
come a long way. Today, sophisticated machine learning models can be run,
involving hundreds of predictors with millions of records in a matter of a
few seconds on a laptop computer.
The process involved in data science, however, has not changed since those
early days and is not likely to change much in the foreseeable future. To get
meaningful results from any data, a major effort preparing, cleaning,
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00001-0
©2019 Elsevier Inc. All rights reserved.1scrubbing, or standardizing the data is still required, before the learning algo-
rithms can begin to crunch them. But what may change is the automation
available to do this. While today this process is iterative and requires ana-
lysts’awareness of the best practices, soon smart automation may be
deployed. This will allow the focus to be put on the most important aspect
of data science: interpreting the results of the analysis in order to make deci-
sions. This will also increase the reach of data science to a wider audience.
When it comes to the data science techniques, are there a core set of procedures
and principles one must master? It turns out that a vast majority of data science
practitioners today use a handful of very powerful techniques to accomplish
their objectives: decision trees, regression models, deep learning, and clustering
(Rexer, 2013). A majority of the data science activity can be accomplished
using relatively few techniques. However, as with all 80/20 rules, the long tail,
which is made up of a large number of specialized techniques, is where the
value lies, and depending on what is needed, the best approach may be a rela-
tively obscure technique or a combination of several not so commonly used
procedures. Thus, it will pay off to learn data science and its methods in a sys-
tematic way, and that is what is covered in these chapters. But, first, how are
the often-used terms artificial intelligence (AI), machine learning, and data sci-
ence explained?
1.1 AI, MACHINE LEARNING, AND DATA SCIENCE
Artificial intelligence, Machine learning, and data science are all related to
each other. Unsurprisingly, they are often used interchangeably and conflated
with each other in popular media and business communication. However,
all of these three fields are distinct depending on the context. Fig. 1.1 shows
the relationship between artificial intelligence, machine learning, and
data science.
Artificial intelligence is about giving machines the capability of mimicking
human behavior, particularly cognitive functions. Examples would be: facial
recognition, automated driving, sor ting mail based on postal code. In some
cases, machines have far exceeded hum an capabilities (sorting thousands of
postal mails in seconds) and in other cases we have barely scratched the
surface (search “artificial stupidity ”) .T h e r ea r eq u i t ear a n g eo ft e c h n i q u e s
that fall under artificial intelligence: linguistics, natural language processing,
decision science, bias, vision, roboti cs, planning, etc. Learning is an impor-
tant part of human capability. In fact, many other living organisms can
learn.
Machine learning can either be considered a sub-field or one of the tools of
artificial intelligence, is providing machines with the capability of learning2CHAPTER 1: Introductionfrom experience. Experience for machines comes in the form of data. Data
that is used to teach machines is called training data. Machine learning turns
the traditional programing model upside down ( Fig. 1.2 ). A program, a set
of instructions to a computer, transforms input signals into output signals
using predetermined rules and relationships. Machine learning algorithms,
Artificial intelligence
Machine learning
Data scienceLinguistics Vision
RoboticsLanguage
synthesis
PlanningSensor
Support
vector
machineskNN
Decision
treesBayesian
learning
Deep
learning Recommendation
enginesText
mining
Statistics
Visualization
ExperimentationProcessing
paradigmsProcess
miningData
preparation
Time series
forecasting
FIGURE 1.1
Artificial intelligence, machine learning, and data science.
FIGURE 1.2
Traditional program and machine learning.1.1 AI, Machine learning, and Data Science 3also called “learners ”, take both the known input and output (training data)
to figure out a model for the program which converts input to output. For
example, many organizations like social media platforms, review sites, or for-
ums are required to moderate posts and remove abusive content. How can
machines be taught to automate the removal of abusive content? The
machines need to be shown examples of both abusive and non-abusive posts
with a clear indication of which one is abusive. The learners will generalize a
pattern based on certain words or sequences of words in order to conclude
whether the overall post is abusive or not. The model can take the form of a
set of “if/C0-then ”rules. Once the data science rules or model is developed,
machines can start categorizing the disposition of any new posts.
Data science is the business application of machine learning, artificial intelli-
gence, and other quantitative fields like statistics, visualization, and mathe-
matics. It is an interdisciplinary field that extracts value from data. In the
context of how data science is used today, it relies heavily on machine learn-
ing and is sometimes called data mining. Examples of data science user cases
are: recommendation engines that can recommend movies for a particular
user, a fraud alert model that detects fraudulent credit card transactions, find
customers who will most likely churn next month, or predict revenue for the
next quarter.
1.2 WHAT IS DATA SCIENCE?
Data science starts with data, which can range from a simple array of a few
numeric observations to a complex matrix of millions of observations with
thousands of variables. Data science utilizes certain specialized computational
methods in order to discover meaningful and useful structures within a dataset.
The discipline of data science coexists and is closely associated with a number
of related areas such as database systems, data engineering, visualization, data
analysis, experimentation, and business intelligence (BI). We can further define
data science by investigating some of its key features and motivations.
1.2.1 Extracting Meaningful Patterns
Knowledge discovery in databases is the nontrivial process of identifying
valid, novel, potentially useful, and ultimately understandable patterns or
relationships within a dataset in order to make important decisions ( Fayyad,
Piatetsky-shapiro, & Smyth, 1996 ). Data science involves inference and itera-
tion of many different hypotheses. One of the key aspects of data science is
the process of generalization of patterns from a dataset. The generalization
should be valid, not just for the dataset used to observe the pattern, but also
for new unseen data. Data science is also a process with defined steps, each4CHAPTER 1: Introductionwith a set of tasks. The term novel indicates that data science is usually
involved in finding previously unknown patterns in data. The ultimate objec-
tive of data science is to find potentially useful conclusions that can be acted
upon by the users of the analysis.
1.2.2 Building Representative Models
In statistics, a model is the representation of a relationship between variables
in a dataset. It describes how one or more variables in the data are related to
other variables. Modeling is a process in which a representative abstraction is
built from the observed dataset. For example, based on credit score, income
level, and requested loan amount, a model can be developed to determine
the interest rate of a loan. For this task, previously known observational data
including credit score, income level, loan amount, and interest rate are
needed. Fig. 1.3 shows the process of generating a model. Once the represen-
tative model is created, it can be used to predict the value of the interest rate,
based on all the input variables.
Data science is the process of building a representative model that fits the
observational data. This model serves two purposes: on the one hand, it pre-
dicts the output (interest rate) based on the new and unseen set of input
variables (credit score, income level, and loan amount), and on the other
hand, the model can be used to understand the relationship between the
output variable and all the input variables. For example, does income level
really matter in determining the interest rate of a loan? Does income level
matter more than credit score? What happens when income levels double or
if credit score drops by 10 points? A Model can be used for both predictive
and explanatory applications.
FIGURE 1.3
Data science models.1.2 What is Data Science? 51.2.3 Combination of Statistics, Machine Learning, and
Computing
In the pursuit of extracting useful and relevant information from large data-
sets, data science borrows computational techniques from the disciplines of
statistics, machine learning, experimentation, and database theories. The
algorithms used in data science originate from these disciplines but havesince evolved to adopt more diverse techniques such as parallel computing,
evolutionary computing, linguistics, and behavioral studies. One of the key
ingredients of successful data science is substantial prior knowledge aboutthe data and the business processes that generate the data, known as subject
matter expertise. Like many quantitative frameworks, data science is an itera-
tive process in which the practitioner gains more information about the pat-
terns and relationships from data in each cycle. Data science also typically
operates on large datasets that need to be stored, processed, and computed.This is where database techniques along with parallel and distributed com-
puting techniques play an important role in data science.
1.2.4 Learning Algorithms
We can also define data science as a process of discovering previously
unknown patterns in data using automatic iterative methods . The application of
sophisticated learning algorithms for extracting useful patterns from data dif-
ferentiates data science from traditional data analysis techniques. Many of
these algorithms were developed in the past few decades and are a part of
machine learning and artificial intelligence. Some algorithms are based on
the foundations of Bayesian probabilistic theories and regression analysis,originating from hundreds of years ago. These iterative algorithms automate
the process of searching for an optimal solution for a given data problem.
Based on the problem, data science is classified into tasks such as classifica-
tion, association analysis, clustering, and regression. Each data science task
uses specific learning algorithms like decision trees, neural networks, k-near-
est neighbors ( k-NN), and k-means clustering, among others. With increased
research on data science, such algorithms are increasing, but a few classic
algorithms remain foundational to many data science applications.
1.2.5 Associated Fields
While data science covers a wide set of techniques, applications, and disci-
plines, there a few associated fields that data science heavily relies on. The
techniques used in the steps of a data science process and in conjunction
with the term “data science ”are:
GDescriptive statistics : Computing mean, standard deviation, correlation,
and other descriptive statistics, quantify the aggregate structure of a6CHAPTER 1: Introductiondataset. This is essential information for understanding any dataset in
order to understand the structure of the data and the relationships
within the dataset. They are used in the exploration stage of the datascience process.
GExploratory visualization : The process of expressing data in visual
coordinates enables users to find patterns and relationships in the dataand to comprehend large datasets. Similar to descriptive statistics, they
are integral in the pre- and post-processing steps in data science.
GDimensional slicing : Online analytical processing (OLAP) applications,
which are prevalent in organizations, mainly provide information on
the data through dimensional slicing, filtering, and pivoting. OLAP
analysis is enabled by a unique database schema design where the dataare organized as dimensions (e.g., products, regions, dates) and
quantitative facts or measures (e.g., revenue, quantity). With a well-
defined database structure, it is easy to slice the yearly revenue byproducts or combination of region and products. These techniques are
extremely useful and may unveil patterns in data (e.g., candy sales
decline after Halloween in the United States).
GHypothesis testing : In confirmatory data analysis, experimental data are
collected to evaluate whether a hypothesis has enough evidence to be
supported or not. There are many types of statistical testing and theyhave a wide variety of business applications (e.g., A/B testing in
marketing). In general, data science is a process where many hypotheses
are generated and tested based on observational data. Since the datascience algorithms are iterative, solutions can be refined in each step.
GData engineering : Data engineering is the process of sourcing,
organizing, assembling, storing, and distributing data for effectiveanalysis and usage. Database engineering, distributed storage, and
computing frameworks (e.g., Apache Hadoop, Spark, Kafka), parallel
computing, extraction transformation and loading processing, and datawarehousing constitute data engineering techniques. Data engineering
helps source and prepare for data science learning algorithms.
GBusiness intelligence : Business intelligence helps organizations consume
data effectively. It helps query the ad hoc data without the need to
write the technical query command or use dashboards or visualizations
to communicate the facts and trends. Business intelligence specializes
in the secure delivery of information to right roles and the distribution
of information at scale. Historical trends are usually reported, but incombination with data science, both the past and the predicted future
data can be combined. BI can hold and distribute the results of data
science.1.2 What is Data Science? 71.3 CASE FOR DATA SCIENCE
In the past few decades, a massive accumulation of data has been seen with
the advancement of information technology, connected networks, and the
businesses it enables. This trend is also coupled with a steep decline in data
storage and data processing costs. The applications built on these advance-
ments like online businesses, social networking, and mobile technologies
unleash a large amount of complex, heterogeneous data that are waiting to
be analyzed. Traditional analysis techniques like dimensional slicing, hypoth-
esis testing, and descriptive statistics can only go so far in information dis-
covery. A paradigm is needed to manage the massive volume of data, explore
the inter-relationships of thousands of variables, and deploy machine learn-
ing algorithms to deduce optimal insights from datasets. A set of frameworks,
tools, and techniques are needed to intelligently assist humans to process all
these data and extract valuable information ( Piatetsky-Shapiro, Brachman,
Khabaza, Kloesgen, & Simoudis, 1996 ). Data science is one such paradigm
that can handle large volumes with multiple attributes and deploy complex
algorithms to search for patterns from data. Each key motivation for using
data science techniques are explored here.
1.3.1 Volume
The sheer volume of data captured by organizations is exponentially increas-
ing. The rapid decline in storage costs and advancements in capturing every
transaction and event, combined with the business need to extract as much
leverage as possible using data, creates a strong motivation to store more
data than ever. As data become more granular, the need to use large volume
data to extract information increases. A rapid increase in the volume of data
exposes the limitations of current analysis methodologies. In a few imple-
mentations, the time to create generalization models is critical and data vol-
ume plays a major part in determining the time frame of development and
deployment.
1.3.2 Dimensions
The three characteristics of the Big Data phenomenon are high volume, high
velocity, and high variety. The variety of data relates to the multiple types of
values (numerical, categorical), formats of data (audio files, video files), and
the application of the data (location coordinates, graph data). Every single
record or data point contains multiple attributes or variables to provide con-
text for the record. For example, every user record of an ecommerce site can
contain attributes such as products viewed, products purchased, user demo-
graphics, frequency of purchase, clickstream, etc. Determining the most effec-
tive offer for an ecommerce user can involve computing information across8CHAPTER 1: Introductionthese attributes. Each attribute can be thought of as a dimension in the data
space. The user record has multiple attributes and can be visualized in multi-
dimensional space. The addition of each dimension increases the complexity
of analysis techniques.
A simple linear regression model that has one input dimension is relatively
easy to build compared to multiple linear regression models with multiple
dimensions. As the dimensional space of data increase, a scalable framework
that can work well with multiple data types and multiple attributes is
needed. In the case of text mining, a document or article becomes a data
point with each unique word as a dimension. Text mining yields a dataset
where the number of attributes can range from a few hundred to hundreds
of thousands of attributes.
1.3.3 Complex Questions
As more complex data are available for analysis, the complexity of informa-
tion that needs to get extracted from data is increasing as well. If the natural
clusters in a dataset, with hundreds of dimensions, need to be found, then
traditional analysis like hypothesis testing techniques cannot be used in a
scalable fashion. The machine-learning algorithms need to be leveraged in
order to automate searching in the vast search space.
Traditional statistical analysis approaches the data analysis problem by
assuming a stochastic model, in order to predict a response variable based
on a set of input variables. A linear regression is a classic example of this
technique where the parameters of the model are estimated from the data.
These hypothesis-driven techniques were highly successful in modeling sim-
ple relationships between response and input variables. However, there is a
significant need to extract nuggets of information from large, complex data-
sets, where the use of traditional statistical data analysis techniques is limited
(Breiman, 2001 )
Machine learning approaches the problem of modeling by trying to find an
algorithmic model that can better predict the output from input variables.
The algorithms are usually recursive and, in each cycle, estimate the output
and “learn ”from the predictive errors of the previous steps. This route of
modeling greatly assists in exploratory analysis since the approach here is not
validating a hypothesis but generating a multitude of hypotheses for a given
problem. In the context of the data problems faced today, both techniques
need to be deployed. John Tuckey, in his article “We need both exploratory
and confirmatory, ”stresses the importance of both exploratory and confir-
matory analysis techniques ( Tuckey, 1980 ). In this book, a range of data sci-
ence techniques, from traditional statistical modeling techniques like
regressions to the modern machine learning algorithms are discussed.1.3 Case for Data Science 91.4 DATA SCIENCE CLASSIFICATION
Data science problems can be broadly categorized into supervised orunsuper-
vised learning models. Supervised or directed data science tries to infer a func-
tion or relationship based on labeled training data and uses this function to
map new unlabeled data. Supervised techniques predict the value of the out-
put variables based on a set of input variables. To do this, a model is devel-
oped from a training dataset where the values of input and output are
previously known. The model generalizes the relationship between the input
and output variables and uses it to predict for a dataset where only input
variables are known. The output variable that is being predicted is also called
a class label or target variable. Supervised data science needs a sufficient
number of labeled records to learn the model from the data. Unsupervised
or undirected data science uncovers hidden patterns in unlabeled data. In
unsupervised data science, there are no output variables to predict. The
objective of this class of data science techniques, is to find patterns in data
based on the relationship between data points themselves. An application
can employ both supervised and unsupervised learners.
Data science problems can also be classified into tasks such as: classification,
regression, association analysis, clustering, anomaly detection, recommenda-
tion engines, feature selection, time series forecasting, deep learning, and text
mining ( Fig. 1.4 ). This book is organized around these data science tasks. An
overview is presented in this chapter and an in-depth discussion of the
FIGURE 1.4
Data science tasks.10 CHAPTER 1: Introductionconcepts and step-by-step implementations of many important techniques
will be provided in the upcoming chapters.
Classification and regression techniques predict a target variable based on input
variables. The prediction is based on a generalized model built from a previ-
ously known dataset. In regression tasks, the output variable is numeric (e.g.,
the mortgage interest rate on a loan). Classification tasks predict output vari-
ables, which are categorical or polynomial (e.g., the yes or no decision to
approve a loan). Deep learning is a more sophisticated artificial neural net-
work that is increasingly used for classification and regression problems.
Clustering is the process of identifying the natural groupings in a dataset. For
example, clustering is helpful in finding natural clusters in customer datasets,
which can be used for market segmentation. Since this is unsupervised data
science, it is up to the end user to investigate why these clusters are formed
in the data and generalize the uniqueness of each cluster. In retail analytics,
it is common to identify pairs of items that are purchased together, so that
specific items can be bundled or placed next to each other. This task is called
market basket analysis or association analysis , which is commonly used in
cross selling. Recommendation engines are the systems that recommend items
to the users based on individual user preference.
Anomaly or outlier detection identifies the data points that are significantly dif-
ferent from other data points in a dataset. Credit card transaction fraud detec-
tion is one of the most prolific applications of anomaly detection. Time series
forecasting is the process of predicting the future value of a variable (e.g., tem-
perature) based on past historical values that may exhibit a trend and season-
ality. Text mining is a data science application where the input data is text,
which can be in the form of documents, messages, emails, or web pages. To
aid the data science on text data, the text files are first converted into docu-
ment vectors where each unique word is an attribute. Once the text file is con-
verted to document vectors, standard data science tasks such as classification,
clustering, etc., can be applied. Feature selection is a process in which attributes
in a dataset are reduced to a few attributes that really matter.
A complete data science application can contain elements of both supervised
and unsupervised techniques ( Tan et al., 2005 ). Unsupervised techniques
provide an increased understanding of the dataset and hence, are sometimes
called descriptive data science. As an example of how both unsupervised and
supervised data science can be combined in an application, consider the fol-
lowing scenario. In marketing analytics, clustering can be used to find the
natural clusters in customer records. Each customer is assigned a cluster label
at the end of the clustering process. A labeled customer dataset can now be
used to develop a model that assigns a cluster label for any new customer
record with a supervised classification technique.1.4 Data Science Classification 111.5 DATA SCIENCE ALGORITHMS
An algorithm is a logical step-by-step procedure for solving a problem. In
data science, it is the blueprint for how a particular data problem is solved.
Many of the learning algorithms are recursive, where a set of steps are
repeated many times until a limiting condition is met. Some algorithms also
contain a random variable as an input and are aptly called randomized algo-
rithms . A classification task can be solved using many different learning algo-
rithms such as decision trees, artificial neural networks, k-NN, and even
some regression algorithms. The choice of which algorithm to use depends
on the type of dataset, objective, structure of the data, presence of outliers,
available computational power, number of records, number of attributes,
and so on. It is up to the data science practitioner to decide which algorithm
(s) to use by evaluating the performance of multiple algorithms. There have
been hundreds of algorithms developed in the last few decades to solve data
science problems.
Data science algorithms can be implemented by custom-developed computer
programs in almost any computer language. This obviously is a time-
consuming task. In order to focus the appropriate amount of time on data
and algorithms, data science tools or statistical programing tools, like R,
RapidMiner, Python, SAS Enterprise Miner, etc., which can implement these
algorithms with ease, can be leveraged. These data science tools offer a library
of algorithms as functions, which can be interfaced through programming
code or configurated through graphical user interfaces. Table 1.1 provides a
summary of data science tasks with commonly used algorithmic techniques
and example cases.
1.6 ROADMAP FOR THIS BOOK
It’s time to explore data science techniques in more detail. The main body of
this book presents: the concepts behind each data science algorithm and a
practical implementation (or two) for each. The chapters do not have to be
read in a sequence. For each algorithm, a general overview is first provided,
and then the concepts and logic of the learning algorithm and how it works
in plain language are presented. Later, how the algorithm can be implemen-
ted using RapidMiner is shown. RapidMiner is a widely known and used
software tool for data science ( Piatetsky, 2018 ) and it has been chosen partic-
ularly for ease of implementation using GUI, and because it is available to
use free of charge, as an open source data science tool. Each chapter is12 CHAPTER 1: Introductionconcluded with some closing thoughts and further reading materials and
references are listed. Here is a roadmap of the book.
1.6.1 Getting Started With Data Science
Successfully uncovering patterns in a dataset is an iterative process.
Chapter 2, Data Science Process, provides a framework to solve the data sci-ence problems. A five-step process outlined in this chapter provides guide-
lines on gathering subject matter expertise; exploring the data with statistics
and visualization; building a model using data science algorithms; testingTable 1.1 Data Science Tasks and Examples
Tasks Description Algorithms Examples
Classification Predict if a data point belongs to
one of the predefined classes.
The prediction will be based on
learning from a known datasetDecision trees, neural
networks, Bayesian
models, induction rules,
k-nearest neighborsAssigning voters into known
buckets by political parties,
e.g., soccer moms
Bucketing new customers into
one of the known customergroups
Regression Predict the numeric target label of
a data point. The prediction will be
based on learning from a knowndatasetLinear regression, logistic
regressionPredicting the unemployment
rate for the next year
Estimating insurance premium
Anomaly detection Predict if a data point is an outlier
compared to other data points in
the datasetDistance-based,
density-based, LOFDetecting fraudulent credit
card transactions and
network intrusion
Time series
forecastingPredict the value of the target
variable for a future timeframe
based on historical valuesExponential smoothing,
ARIMA, regressionSales forecasting, production
forecasting, virtually any
growth phenomenon that
needs to be extrapolated
Clustering Identify natural clusters within the
dataset based on inheritproperties within the datasetk-Means, density-based
clustering (e.g.,DBSCAN)Finding customer segments in
a company based ontransaction, web, and
customer call data
Association
analysisIdentify relationships within an
item set based on transactiondataFP-growth algorithm, a
priori algorithmFinding cross-selling
opportunities for a retailerbased on transaction
purchase history
Recommendation
enginesPredict the preference of an item
for a userCollaborative filtering,
content-based filtering,hybrid recommendersFinding the top recommended
movies for a user
LOF , local outlier factor; ARIMA , autoregressive integrated moving average; DBSCAN , density-based spatial clustering of applications
with noise; FP, frequent pattern.1.6 Roadmap for This Book 13and deploying the model in the production environment; and finally reflect-
ing on new knowledge gained in the cycle.
Simple data exploration either visually or with the help of basic statistical
analysis can sometimes answer seemingly tough questions meant for data
science. Chapter 3, Data Exploration, covers some of the basic tools used in
knowledge discovery before deploying data science techniques. These practi-
cal tools increase one ’s understanding of data and are quite essential in
understanding the results of the data science process.
1.6.2 Practice using RapidMiner
Before delving into the key data science techniques and algorithms, two spe-
cific things should be noted regarding how data science algorithms can be
implemented while reading this book. It is believed that learning the con-
cepts and implementing them enhances the learning experience. First, it is
recommended that the free version of RapidMiner Studio software is down-
loaded from http://www.rapidminer.com and second, the first few sections
of Chapter 15: Getting started with RapidMiner, should be reviewed in order
to become familiar with the features of the tool, its basic operations, and the
user interface functionality. Acclimating with RapidMiner will be helpful
while using the algorithms that are discussed in this book. Chapter 15:
Getting started with RapidMiner, is set at the end of the book because some
of the later sections in the chapter build on the material presented in the
chapters on tasks; however, the first few sections are a good starting point to
become familiar with the tool.
1.6.3 Core Algorithms
Classification is the most widely used data science task in business. The objec-
tive of a classification model is to predict a target variable that is binary (e.g.,
a loan decision) or categorical (e.g., a customer type) when a set of input
variables are given. The model does this by learning the generalized relation-
ship between the predicted target variable with all other input attributesEach chapter has a dataset used to describe the concept
of a particular data science task and in most cases the
same dataset is used for the implementation. The step-
by-step instructions on practicing data science using the
dataset are covered in every algorithm. All the implemen-
tations discussed are available at the companion websiteof the book at www.IntroDataScience.com . Though
not required, it is advisable to access these files to as a
learning aid. The dataset, complete RapidMiner processes
(T.rmp files), and many more relevant electronic files can
be downloaded from this website.14 CHAPTER 1: Introductionfrom a known dataset. There are several ways to skin this cat. Each algorithm
differs by how the relationship is extracted from the known training dataset.
Chapter 4, Classification, on classification addresses several of these methods.
GDecision trees approach the classification problem by partitioning the
data into purer subsets based on the values of the input attributes. The
attributes that help achieve the cleanest levels of such separation areconsidered significant in their influence on the target variable and end
up at the root and closer-to-root levels of the tree. The output model is
a tree framework than can be used for the prediction of new unlabeled
data.
GRule induction is a data science process of deducing “if/C0then”rules from
a dataset or from the decision trees. These symbolic decision rules
explain an inherent relationship between the input attributes and the
target labels in the dataset that can be easily understood by anyone.
GNaïve Bayesian algorithms provide a probabilistic way of building a
model. This approach calculates the probability for each value of the
class variable for given values of input variables. With the help ofconditional probabilities, for a given unseen record, the model
calculates the outcome of all values of target classes and comes up with
a predicted winner.
GWhy go through the trouble of extracting complex relationships from
the data when the entire training dataset can be memorized and the
relationship can appear to have been generalized? This is exactly whatthek-NN algorithm does, and it is, therefore, called a “lazy”learner
where the entire training dataset is memorized as the model.
GNeurons are the nerve cells that connect with each other to form a
biological neural network in our brain. The working of these
interconnected nerve cells inspired the approach of some complex data
problems by the creation of artificial neural networks . The neural
networks section provides a conceptual background of how a simple
neural network works and how to implement one for any general
prediction problem. Later on we extend this to deep neural networkswhich have revolutionized the field of artificial intelligence.
GSupport vector machines (SVMs) were developed to address optical
character recognition problems: how can an algorithm be trained todetect boundaries between different patterns, and thus, identify
characters? SVMs can, therefore, identify if a given data sample belongs
within a boundary (in a particular class) or outside it (not in the class).
GEnsemble learners are“meta ”models where the model is a combination
of several different individual models. If certain conditions are met,
ensemble learners can gain from the wisdom of crowds and greatlyreduce the generalization error in data science.1.6 Roadmap for This Book 15The simple mathematical equation y5ax1bis a linear regression model.
Chapter 5, Regression Methods, describes a class of data science techniques
in which the target variable (e.g., interest rate or a target class) is functionallyrelated to input variables.
GLinear regression : The simplest of all function fitting models is based on
a linear equation, as previously mentioned. Polynomial regression useshigher-order equations. No matter what type of equation is used, the
goal is to represent the variable to be predicted in terms of other
variables or attributes. Further, the predicted variable and the
independent variables all have to be numeric for this to work. The
basics of building regression models will be explored and howpredictions can be made using such models will be shown.
GLogistic regression : Addresses the issue of predicting a target variable that
may be binary or binomial (such as 1 or 0, yes or no) using predictorsor attributes, which may be numeric.
Supervised data science or directed data science predict the value of the target
variables. Two important unsupervised data science tasks will be reviewed:
Association Analysis in Chapter 6 and Clustering in Chapter 7. Ever heard of
the beer and diaper association in supermarkets? Apparently, a supermarket
discovered that customers who buy diapers also tend to buy beer. While thismay have been an urban legend, the observation has become a poster child
for association analysis. Associating an item in a transaction with another
item in the transaction to determine the most frequently occurring patterns istermed association analysis . This technique is about, for example, finding rela-
tionships between products in a supermarket based on purchase data, or
finding related web pages in a website based on clickstream data. It is widelyused in retail, ecommerce, and media to creatively bundle products.
Clustering is the data science task of identifying natural groups in the data. As
an unsupervised task, there is no target class variable to predict. After the
clustering is performed, each record in the dataset is associated with one or
more cluster. Widely used in marketing segmentations and text mining, clus-tering can be performed by a range of algorithms. In Chapter 7, Clustering,
three common algorithms with diverse identification approaches will be dis-
cussed. The k-means clustering technique identifies a cluster based on a central
prototype record. DBSCAN clustering partitions the data based on variation
in the density of records in a dataset. Self-organizing maps create a two-
dimensional grid where all the records related with each other are placednext to each other.
How to determine which algorithms work best for a given dataset? Or for
that matter how to objectively quantify the performance of any algorithm on
a dataset? These questions are addressed in Chapter 8, Model Evaluation,16 CHAPTER 1: Introductionwhich covers performance evaluation. The most commonly used tools for
evaluating classification models such as a confusion matrix, ROC curves, and
lift charts are described.
Chapter 9, Text Mining, provides a detailed look into the area of text mining
and text analytics. It starts with a background on the origins of text mining
and provides the motivation for this fascinating topic using the example ofIBM’s Watson, the Jeopardy —winning computer program that was built using
concepts from text and data mining. The chapter introduces some key concepts
important in the area of text analytics such as term frequency /C0inverse
document frequency scores. Finally, it describes two case studies in which it is
shown how to implement text mining for document clustering and automaticclassification based on text content.
Chapter 10, Deep Learning, describes a set of algorithms to model high level
abstractions in data. They are increasingly applied to image processing,speech recognition, online advertisements, and bioinformatics. This chapter
covers the basic concepts of deep learning, popular use cases, and a sample
classification implementation.
The advent of digital economy exponentially increased the choices of avail-
able products to the customer which can be overwhelming. Personalized rec-ommendation lists help by narrowing the choices to a few items relevant to
a particular user and aid users in making final consumption decisions.
Recommendation engines, covered in Chapter 11, are the most prolific utili-ties of machine learning in everyday experience. Recommendation engines
are a class of machine learning techniques that predict a user preference for
an item. There are a wide range of techniques available to build a recommen-dation engine. This chapter discusses the most common methods starting
with collaborative filtering and content-based filtering concepts and implementa-
tions using a practical dataset.
Forecasting is a common application of time series analysis. Companies use
sales forecasts, budget forecasts, or production forecasts in their planning
cycles. Chapter 12 on Time Series Forecasting starts by pointing out the dis-tinction between standard supervised predictive models and time series fore-
casting models. The chapter covers a few time series forecasting methods,
starting with time series decomposition, moving averages, exponential
smoothing, regression, ARIMA methods, and machine learning based meth-
ods using windowing techniques.
Chapter 13 on Anomaly Detection describes how outliers in data can be
detected by combining multiple data science tasks like classification, regres-
sion, and clustering. The fraud alert received from credit card companies isthe result of an anomaly detection algorithm. The target variable to be1.6 Roadmap for This Book 17predicted is whether a transaction is an outlier or not. Since clustering tasks
identify outliers as a cluster, distance-based and density-based clustering tech-
niques can be used in anomaly detection tasks.
In data science, the objective is to develop a representative model to general-
ize the relationship between input attributes and target attributes, so that we
can predict the value or class of the target variables. Chapter 14, Feature
Selection, introduces a preprocessing step that is often critical for a successful
predictive modeling exercise: feature selection . Feature selection is known by
several alternative terms such as attribute weighting, dimension reduction,
and so on. There are two main styles of feature selection: filtering the key
attributes before modeling (filter style) or selecting the attributes during the
process of modeling (wrapper style). A few filter-based methods such as prin-
cipal component analysis (PCA), information gain, and chi-square, and a
couple of wrapper-type methods like forward selection and backward elimi-
nation will be discussed.
The first few sections of Chapter 15, Getting Started with RapidMiner, should
provide a good overview for getting familiar with RapidMiner, while the lat-
ter sections of this chapter discuss some of the commonly used productivity
tools and techniques such as data transformation, missing value handling,
and process optimizations using RapidMiner.
References
Breiman, L. (2001). Statistical modeling: Two cultures. Statistical Science ,6(3), 199 /C0231.
Fayyad, U., Piatetsky-shapiro, G., & Smyth, P. (1996). From data science to knowledge discovery
in databases. AI Magazine ,17(3), 37 /C054.
Parr Rud, O. (2001). Data science Cookbook . New York: John Wiley and Sons.
Piatetsky, G. (2018). Top Software for Analytics, Data Science, Machine Learning in 2018: Trends
and Analysis. Retrieved July 7, 2018, from https://www.kdnuggets.com/2018/05/poll-tools-
analytics-data-science-machine-learning-results.html .
Piatetsky-Shapiro, G., Brachman, R., Khabaza, T., Kloesgen, W., & Simoudis, E. (1996). An over-
view of issues in developing industrial data science and knowledge discovery applications.
In:KDD-96 conference proceedings. KDD-96 conference proceedings .
Rexer, K. (2013). 2013 Data miner survey summary report . Winchester, MA: Rexer Analytics.
,www.rexeranalytics.com ..
Tan, P.-N., Michael, S., & Kumar, V. (2005). Introduction to data science . Boston, MA:
Addison-Wesley.
Tuckey, J. (1980). We need exploratory and Confirmatory. The American Statistician ,34(1),
23/C025.18 CHAPTER 1: IntroductionCHAPTER 2
Data Science Process
The methodical discovery of useful relationships and patterns in data is
enabled by a set of iterative activities collectively known as the data science
process. The standard data science process involves (1) understanding the
problem, (2) preparing the data samples, (3) developing the model, (4) apply-
ing the model on a dataset to see how the model may work in the real world,
and (5) deploying and maintaining the models. Over the years of evolution of
data science practices, different frameworks for the process have been put
forward by various academic and commercial bodies. The framework put for-
ward in this chapter is synthesized from a few data science frameworks and is
explained using a simple example dataset. This chapter serves as a high-level
roadmap to building deployable data science models, and discusses the chal-
lenges faced in each step and the pitfalls to avoid.
One of the most popular data science process frameworks is Cross Industry
Standard Process for Data Mining (CRISP-DM), which is an acronym for
Cross Industry Standard Process for Data Mining. This framework was devel-
oped by a consortium of companies involved in data mining ( Chapman
et al., 2000 ). The CRISP-DM process is the most widely adopted framework
for developing data science solutions. Fig. 2.1 provides a visual overview of
the CRISP-DM framework. Other data science frameworks are SEMMA, an
acronym for Sample, Explore, Modify, Model, and Assess, developed by the
SAS Institute ( SAS Institute, 2013 ); DMAIC, is an acronym for Define,
Measure, Analyze, Improve, and Control, used in Six Sigma practice ( Kubiak
& Benbow, 2005 ); and the Selection, Preprocessing, Transformation, Data
Mining, Interpretation, and Evaluation framework used in the knowledge dis-
covery in databases process ( Fayyad, Piatetsky-Shapiro, & Smyth, 1996 ). All
these frameworks exhibit common characteristics, and hence, a generic
framework closely resembling the CRISP process will be used. As with any
process framework, a data science process recommends the execution of a
certain set of tasks to achieve optimal output. However, the process of
extracting information and knowledge from the data is iterative . The steps
within the data science process are not linear and have to undergo many
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00002-2
©2019 Elsevier Inc. All rights reserved.19loops, go back and forth between steps, and at times go back to the first step
to redefine the data science problem statement.
The data science process presented in Fig. 2.2 is a generic set of steps that is
problem, algorithm, and, data science tool agnostic. The fundamental objec-
tive of any process that involves data science is to address the analysis ques-
tion. The problem at hand could be a segmentation of customers, a prediction
of climate patterns, or a simple data exploration. The learning algorithm used
to solve the business question could be a decision tree, an artificial neural net-
work, or a scatterplot. The software tool to develop and implement the data
science algorithm used could be custom coding, RapidMiner, R, Weka, SAS,
Oracle Data Miner, Python, etc., ( Piatetsky, 2018 )t om e n t i o naf e w .
Data science, specifically in the context of big data, has gained importance in
the last few years. Perhaps the most visible and discussed part of data science
is the third step: modeling . It is the process of building representative models
that can be inferred from the sample dataset which can be used for either
predicting ( predictive modeling ) or describing the underlying pattern in the
data ( descriptive or explanatory modeling ). Rightfully so, there is plenty of
Business
understanding
Deployment
Data
EvaluationModelingData
understanding
Data
preparation
FIGURE 2.1
CRISP data mining framework.20 CHAPTER 2: Data Science Processacademic and business research in the modeling step. Most of this book has
been dedicated to discussing various algorithms and the quantitative founda-tions that go with it. However, emphasis should be placed on considering
data science as an end-to-end, multi-step, iterative process instead of just a
model building step. Seasoned data science practitioners can attest to the factthat the most time-consuming part of the overall data science process is not
the model building part, but the preparation of data, followed by data and
business understanding. There are many data science tools, both open sourceand commercial, available on the market that can automate the model build-
ing. Asking the right business question, gaining in-depth business understand-
ing, sourcing and preparing the data for the data science task, mitigatingimplementation considerations, integrating the model into the business pro-
cess, and, most useful of all, gaining knowledge from the dataset, remain cru-
cial to the success of the data science process. It ’s time to get started with Step
1: Framing the data science question and understanding the context.
2.1 PRIOR KNOWLEDGE
Prior knowledge refers to information that is already known about a subject.
The data science problem doesn ’t emerge in isolation; it always develops on
FIGURE 2.2
Data science process.2.1 Prior Knowledge 21top of existing subject matter and contextual information that is already
known. The prior knowledge step in the data science process helps to define
what problem is being solved, how it fits in the business context, and what
data is needed in order to solve the problem.
2.1.1 Objective
The data science process starts with a need for analysis, a question, or a busi-
ness objective. This is possibly the most important step in the data science
process ( Shearer, 2000 ). Without a well-defined statement of the problem, it
is impossible to come up with the right dataset and pick the right data science
algorithm. As an iterative process, it is common to go back to previous data
science process steps, revise the assumptions, approach, and tactics. However,
it is imperative to get the first step —the objective of the whole process —right.
The data science process is going to be explained using a hypothetical use
case. Take the consumer loan business for example, where a loan is provi-
sioned for individuals against the collateral of assets like a home or car, that
is, a mortgage or an auto loan. As many homeowners know, an important
component of the loan, for the borrower and the lender, is the interest rate
at which the borrower repays the loan on top of the principal. The interest
rate on a loan depends on a gamut of variables like the current federal funds
rate as determined by the central bank, borrower ’s credit score, income level,
home value, initial down payment amount, current assets and liabilities of
the borrower, etc. The key factor here is whether the lender sees enough
reward (interest on the loan) against the risk of losing the principal (bor-
rower ’s default on the loan). In an individual case, the status of default of a
loan is Boolean; either one defaults or not, during the period of the loan.
But, in a group of tens of thousands of borrowers, the default rate can be
found —a continuous numeric variable that indicates the percentage of bor-
rowers who default on their loans. All the variables related to the borrower
like credit score, income, current liabilities, etc., are used to assess the default
risk in a related group; based on this, the interest rate is determined for a
loan. The business objective of this hypothetical case is: If the interest rate of
past borrowers with a range of credit scores is known, can the interest rate for a new
borrower be predicted?
2.1.2 Subject Area
The process of data science uncovers hidden patterns in the dataset by expos-
ing relationships between attributes. But the problem is that it uncovers a lot
of patterns. The false or spurious signals are a major problem in the data sci-
ence process. It is up to the practitioner to sift through the exposed patterns
and accept the ones that are valid and relevant to the answer of the objective22 CHAPTER 2: Data Science Processquestion. Hence, it is essential to know the subject matter, the context, and
the business process generating the data.
The lending business is one of the oldest, most prevalent, and complex of all
the businesses. If the objective is to predict the lending interest rate, then it is
important to know how the lending business works, why the prediction mat-
ters, what happens after the rate is predicted, what data points can be col-
lected from borrowers, what data points cannot be collected because of the
external regulations and the internal policies, what other external factors can
affect the interest rate, how to verify the validity of the outcome, and so
forth. Understanding current models and business practices lays the founda-
tion and establishes known knowledge. Analysis and mining the data pro-
vides the new knowledge that can be built on top of the existing knowledge
(Lidwell, Holden, & Butler, 2010 ).
2.1.3 Data
Similar to the prior knowledge in the subject area, prior knowledge in the
data can also be gathered. Understanding how the data is collected, stored,
transformed, reported, and used is essential to the data science process. This
part of the step surveys all the data available to answer the business question
and narrows down the new data that need to be sourced. There are quite a
range of factors to consider: quality of the data, quantity of data, availability of
data, gaps in data, does lack of data compel the practitioner to change the busi-
ness question, etc. The objective of this step is to come up with a dataset to
answer the business question through the data science process. It is critical to
recognize that an inferred model is only as good as the data used to create it.
For the lending example, a sample dataset of ten data points with three attri-
butes has been put together: identifier, credit score, and interest rate. First,
some of the terminology used in the data science process are discussed.
GAdataset (example set ) is a collection of data with a defined structure.
Table 2.1 shows a dataset. It has a well-defined structure with 10 rows
and 3 columns along with the column headers. This structure is also
sometimes referred to as a “data frame ”.
GAdata point (record ,object orexample ) is a single instance in the dataset.
Each row in Table 2.1 is a data point. Each instance contains the same
structure as the dataset.
GAnattribute (feature ,input, dimension, variable, orpredictor ) is a single
property of the dataset. Each column in Table 2.1 is an attribute.
Attributes can be numeric, categorical, date-time, text, or Boolean data
types. In this example, both the credit score and the interest rate are
numeric attributes.2.1 Prior Knowledge 23GAlabel (class label ,output ,prediction, target, orresponse ) is the special
attribute to be predicted based on all the input attributes. In Table 2.1 ,
the interest rate is the output variable.
GIdentifiers are special attributes that are used for locating or providing
context to individual records. For example, common attributes like
names, account numbers, and employee ID numbers are identifier
attributes. Identifiers are often used as lookup keys to join multiple
datasets. They bear no information that is suitable for building data
science models and should, thus, be excluded for the actual modeling
step. In Table 2.1 , the attribute ID is the identifier.
2.1.4 Causation Versus Correlation
Suppose the business question is inverted: Based on the data in Table 2.1 ,can the
credit score of the borrower be predicted based on interest rate? The answer is yes —
but it does not make business sense. From the existing domain expertise, it is
known that credit score influences the loan interest rate. Predicting credit score
based on interest rate inverses the direction of the causal relationship. This
question also exposes one of the key aspects of model building. The correlation
between the input and output attributes doesn ’t guarantee causation. Hence, it
is important to frame the data science question correctly using the existing
domain and data knowledge. In this data science example, the interest rate of
the new borrower with an unknown interest rate will be predicted ( Table 2.2 )
based on the pattern learned from known data in Table 2.1 .Table 2.1 Dataset
Borrower ID Credit Score Interest Rate (%)
01 500 7.31
02 600 6.70
03 700 5.95
04 700 6.40
05 800 5.40
06 800 5.70
07 750 5.90
08 550 7.00
09 650 6.50
10 825 5.70
Table 2.2 New Data With Unknown Interest Rate
Borrower ID Credit Score Interest Rate
11 625 ?24 CHAPTER 2: Data Science Process2.2 DATA PREPARATION
Preparing the dataset to suit a data science task is the most time-consuming
part of the process. It is extremely rare that datasets are available in the form
required by the data science algorithms. Most of the data science algorithms
would require data to be structured in a tabular format with records in the
rows and attributes in the columns. If the data is in any other format, the
data would need to be transformed by applying pivot, type conversion, join,
or transpose functions, etc., to condition the data into the required structure.
2.2.1 Data Exploration
Data preparation starts with an in-depth exploration of the data and gaining
a better understanding of the dataset. Data exploration, also known as explor-
atory data analysis , provides a set of simple tools to achieve basic understand-
ing of the data. Data exploration approaches involve computing descriptive
statistics and visualization of data. They can expose the structure of the data,
the distribution of the values, the presence of extreme values, and highlight
the inter-relationships within the dataset. Descriptive statistics like mean,
median, mode, standard deviation, and range for each attribute provide an
easily readable summary of the key characteristics of the distribution of data.
On the other hand, a visual plot of data points provides an instant grasp of
all the data points condensed into one chart. Fig. 2.3 shows the scatterplot of
credit score vs. loan interest rate and it can be observed that as credit score
increases, interest rate decreases.
2.2.2 Data Quality
Data quality is an ongoing concern wherever data is collected, processed, and
stored. In the interest rate dataset ( Table 2.1 ), how does one know if the
credit score and interest rate data are accurate? What if a credit score has a
recorded value of 900 (beyond the theoretical limit) or if there was a data
entry error? Errors in data will impact the representativeness of the model.
Organizations use data alerts, cleansing, and transformation techniques to
improve and manage the quality of the data and store them in companywide
repositories called data warehouses . Data sourced from well-maintained data
warehouses have higher quality, as there are proper controls in place to
ensure a level of data accuracy for new and existing data. The data cleansing
practices include elimination of duplicate records, quarantining outlier
records that exceed the bounds, standardization of attribute values, substitu-
tion of missing values, etc. Regardless, it is critical to check the data using
data exploration techniques in addition to using prior knowledge of the data
and business before building models.2.2 Data Preparation 252.2.3 Missing Values
One of the most common data quality issues is that some records have miss-
ing attribute values. For example, a credit score may be missing in one of the
records. There are several different mitigation methods to deal with thisproblem, but each method has pros and cons. The first step of managing
missing values is to understand the reason behind why the values are miss-
ing. Tracking the data lineage (provenance) of the data source can lead to theidentification of systemic issues during data capture or errors in data transfor-
mation. Knowing the source of a missing value will often guide which miti-
gation methodology to use. The missing value can be substituted with arange of artificial data so that the issue can be managed with marginal
impact on the later steps in the data science process. Missing credit score
values can be replaced with a credit score derived from the dataset (mean,minimum, or maximum value, depending on the characteristics of the attri-
bute). This method is useful if the missing values occur randomly and the
frequency of occurrence is quite rare. Alternatively, to build the representativemodel, all the data records with missing values or records with poor data
quality can be ignored. This method reduces the size of the dataset. Some
data science algorithms are good at handling records with missing values,while others expect the data preparation step to handle it before the model is
7.50%
7.00%6.50%
6.00%
5.50%
5.00%
400 500 600
Credit scoreInterest rate
700 800 900
FIGURE 2.3
Scatterplot for interest rate dataset.26 CHAPTER 2: Data Science Processinferred. For example, k-nearest neighbor (k-NN) algorithm for classification
tasks are often robust with missing values. Neural network models for classi-
fication tasks do not perform well with missing attributes, and thus, the datapreparation step is essential for developing neural network models.
2.2.4 Data Types and Conversion
The attributes in a dataset can be of different types, such as continuousnumeric (interest rate), integer numeric (credit score), or categorical. For
example, the credit score can be expressed as categorical values (poor, good,
excellent) or numeric score. Different data science algorithms impose differ-ent restrictions on the attribute data types. In case of linear regression mod-
els, the input attributes have to be numeric. If the available data are
categorical, they must be converted to continuous numeric attribute. A spe-cific numeric score can be encoded for each category value, such as
poor5400, good 5600, excellent 5700, etc. Similarly, numeric values can
be converted to categorical data types by a technique called binning , where a
range of values are specified for each category, for example, a score between
400 and 500 can be encoded as “low”and so on.
2.2.5 Transformation
In some data science algorithms like k-NN, the input attributes are expected
to be numeric and normalized , because the algorithm compares the values of
different attributes and calculates distance between the data points.
Normalization prevents one attribute dominating the distance results because
of large values. For example, consider income (expressed in USD, in thou-sands) and credit score (in hundreds). The distance calculation will always
be dominated by slight variations in income. One solution is to convert the
range of income and credit score to a more uniform scale from 0 to 1 by nor-malization. This way, a consistent comparison can be made between the two
different attributes with different units.
2.2.6 Outliers
Outliers are anomalies in a given dataset. Outliers may occur because of cor-
rect data capture (few people with income in tens of millions) or erroneousdata capture (human height as 1.73 cm instead of 1.73 m). Regardless, the pres-
ence of outliers needs to be understood and will require special treatments. The
purpose of creating a representative model is to generalize a pattern or a rela-tionship within a dataset and the presence of outliers skews the representative-
ness of the inferred model. Detecting outliers may be the primary purpose of
some data science applications, like fraud or intrusion detection.2.2 Data Preparation 272.2.7 Feature Selection
The example dataset shown in Table 2.1 has one attribute orfeature —the
credit score —and one label—the interest rate. In practice, many data science
problems involve a dataset with hundreds to thousands of attributes. In text
mining applications, every distinct word in a document forms a distinct attri-
bute in the dataset. Not all the attributes are equally important or useful in
predicting the target. The presence of some attributes might be counterpro-
ductive. Some of the attributes may be highly correlated with each other, like
annual income and taxes paid. A large number of attributes in the dataset
significantly increases the complexity of a model and may degrade the per-
formance of the model due to the curse of dimensionality . In general, the pres-
ence of more detailed information is desired in data science because
discovering nuggets of a pattern in the data is one of the attractions of using
data science techniques. But, as the number of dimensions in the data
increase, data becomes sparse in high-dimensional space. This condition
degrades the reliability of the models, especially in the case of clustering and
classification ( Tan, Steinbach, & Kumar, 2005 ).
Reducing the number of attributes, without significant loss in the perfor-
mance of the model, is called feature selection. It leads to a more simplified
model and helps to synthesize a more effective explanation of the model.
2.2.8 Data Sampling
Sampling is a process of selecting a subset of records as a representation of
the original dataset for use in data analysis or modeling. The sample data
serve as a representative of the original dataset with similar properties, such
as a similar mean. Sampling reduces the amount of data that need to be pro-
cessed and speeds up the build process of the modeling. In most cases, to
gain insights, extract the information, and to build representative predictive
models it is sufficient to work with samples. Theoretically, the error intro-
duced by sampling impacts the relevancy of the model, but their benefits far
outweigh the risks.
In the build process for data science applications, it is necessary to segment
the datasets into training and test samples. The training dataset is sampled
from the original dataset using simple sampling or class label specific sam-
pling. Consider the example cases for predicting anomalies in a dataset (e.g.,
predicting fraudulent credit card transactions). The objective of anomaly
detection is to classify the outliers in the data. These are rare events and often
the dataset does not have enough examples of the outlier class. Stratified sam-
pling is a process of sampling where each class is equally represented in the
sample; this allows the model to focus on the difference between the patterns
of each class that is, normal and outlier records. In classification applications,28 CHAPTER 2: Data Science Processsampling is used create multiple base models, each developed using a differ-
ent set of sampled training datasets. These base models are used to build one
meta model, called the ensemble model , where the error rate is improved
when compared to that of the base models.
2.3 MODELING
Am o d e li st h ea b s t r a c tr e p r e s e n t a t i o n of the data and the relationships
in a given dataset. A simple rule of thumb like “mortgage interest rate reduces
with increase in credit score ”is a model; although there is not enough
quantitative information to use in a production scenario, it provides
directional information by abstracting the relationship between credit score
and interest rate.
There are a few hundred data science algorithms in use today, derived from sta-
tistics, machine learning, pattern recognition, and the body of knowledge
related to computer science. Fortunately, there are many viable commercial and
open source data science tools on the market to automate the execution of these
learning algorithms. As a data science practitioner, it is sufficient to an overview
of the learning algorithm, how it works, and determining what parameters need
to be configured based on the understanding of the business and data.
Classification and regression tasks are predictive techniques because they pre-
dict an outcome variable based on one or more input variables. Predictive algo-
rithms require a prior known dataset to learn the model. Fig. 2.4 shows the
steps in the modeling phase of predictive data science. Association analysis and
clustering are descriptive data science techniques where there is no target vari-
able to predict; hence, there is no test dataset. However, both predictive and
descriptive models have an evaluation step.
FIGURE 2.4
Modeling steps.2.3 Modeling 292.3.1 Training and Testing Datasets
The modeling step creates a representative model inferred from the data. The
dataset used to create the model, with known attributes and target, is called
thetraining dataset . The validity of the created model will also need to be
checked with another known dataset called the test dataset orvalidation data-
set. To facilitate this process, the overall known dataset can be split into a
training dataset and a test dataset. A standard rule of thumb is two-thirds of
the data are to be used as training and one-third as a test dataset. Tables 2.3
and 2.4 show the random split of training and test data, based on the exam-
ple dataset shown in Table 2.1 .Fig. 2.5 shows the scatterplot of the entire
example dataset with the training and test datasets marked.
2.3.2 Learning Algorithms
The business question and the availability of data will dictate what data sci-
ence task (association, classification, regression, etc.,) can to be used. The
practitioner determines the appropriate data science algorithm within the
chosen category. For example, within a classification task many algorithms
can be chosen from: decision trees, rule induction, neural networks, Bayesian
models, k-NN, etc. Likewise, within decision tree techniques, there are quite
a number of variations of learning algorithms like classification and regres-
sion tree (CART), CHi-squared Automatic Interaction Detector (CHAID) etc.Table 2.3 Training Dataset
Borrower Credit Score ( X) Interest Rate ( Y) (%)
01 500 7.31
02 600 6.70
03 700 5.95
05 800 5.40
06 800 5.70
08 550 7.00
09 650 6.50
Table 2.4 Test Dataset
Borrower Credit Score ( X) Interest Rate ( Y)
04 700 6.40
07 750 5.90
10 825 5.7030 CHAPTER 2: Data Science ProcessThese algorithms will be reviewed in detail in later chapters. It is not uncom-
mon to use multiple data science tasks and algorithms to solve a business
question.
Interest rate prediction is a regression problem. A simple linear regression
technique will be used to model and generalize the relationship between
credit score and interest rate. The training set of seven records is used to cre-
ate the model and the test set of three records is used to evaluate the validity
of the model.
The objective of simple linear regression can be visualized as fitting a straight
line through the data points in a scatterplot ( Fig. 2.6 ). The line has to be
built in such a way that the sum of the squared distance from the data points
to the line is minimal. The line can be expressed as:
y5a/C3x1b ð2:1Þ
where yis the output or dependent variable, xis the input or independent
variable, bis the y-intercept, and ais the coefficient of x. The values of aand
bcan be found in such a way so as to minimize the sum of the squared resi-
duals of the line.
7.50%
7.00%
6.50%
6.00%
5.50%
5.00%
Credit scoreInterest rate
400 500 600 700 800 900
FIGURE 2.5
Scatterplot of training and test data.2.3 Modeling 31The line shown in Eq. (2.1) serves as a model to predict the outcome of new
unlabeled datasets. For the interest rate dataset, the simple linear regression
for the interest rate ( y) has been calculated as (details in Chapter 5:
Regression):
y50:116
100 ;000x
Interest rate 510263credit score
1000
Using this model, the interest rate for a new borrower with a specific credit
score can be calculated.
2.3.3 Evaluation of the Model
The model generated in the form of an equation is generalized and synthe-
sized from seven training records. The credit score in the equation can be
substituted to see if the model estimates the interest rate for each of the7.50%
7.00%
6.50%
6.00%
5.50%
5.00%
400 500 600
Credit scoreInterest rate
700 800 900
FIGURE 2.6
Regression model.32 CHAPTER 2: Data Science Processseven training records. The estimation may not be exactly the same as the
values in the training records. A model should not memorize and output
the same values that are in the training records. The phenomenon of a model
memorizing the training data is called overfitting . An overfitted model just
memorizes the training records and will underperform on real unlabeled
new data. The model should generalize or learn the relationship between
credit score and interest rate. To evaluate this relationship, the validation or
test dataset, which was not previously used in building the model, is used
for evaluation, as shown in Table 2.5 .
Table 2.5 provides the three testing records where the value of the interest
rate is known; these records were not used to build the model. The actual
value of the interest rate can be compared against the predicted value using
the model, and thus, the prediction error can be calculated. As long as the
error is acceptable, this model is ready for deployment. The error rate can be
used to compare this model with other models developed using different
algorithms like neural networks or Bayesian models, etc.
2.3.4 Ensemble Modeling
Ensemble modeling is a process where multiple diverse base models are used
to predict an outcome. The motivation for using ensemble models is to
reduce the generalization error of the prediction. As long as the base models
are diverse and independent , the prediction error decreases when the ensemble
approach is used. The approach seeks the wisdom of crowds in making a pre-
diction. Even though the ensemble model has multiple base models within
the model, it acts and performs as a single model. Most of the practical data
science applications utilize ensemble modeling techniques.
At the end of the modeling stage of the data science process, one has (1) ana-
lyzed the business question; (2) sourced the data relevant to answer the
question; (3) selected a data science technique to answer the question; (4)
picked a data science algorithm and prepared the data to suit the algorithm;
(5) split the data into training and test datasets; (6) built a generalized
model from the training dataset; and (7) validated the model against the testTable 2.5 Evaluation of Test Dataset
BorrowerCredit
Score ( X)Interest Rate
(Y) (%)Model Predicted
(Y) (%)Model Error
(%)
04 700 6.40 6.11 20.29
07 750 5.90 5.81 20.09
10 825 5.70 5.37 20.332.3 Modeling 33dataset. This model can now be used to predict the interest rate of new bor-
rowers by integrating it in the actual loan approval process.
2.4 APPLICATION
Deployment is the stage at which the model becomes production ready or
live. In business applications, the result s of the data science process have to
be assimilated into the business process —usually in software applications.
The model deployment stage has to deal with: assessing model readiness,
technical integration, response time, model maintenance, and assimilation.
2.4.1 Production Readiness
The production readiness part of the deployment determines the critical
qualities required for the deployment objective. Consider two business use
cases: determining whether a consumer qualifies for a loan and determiningthe groupings of customers for an enterprise by marketing function.
The consumer credit approval process is a real-time endeavor. Either through
a consumer-facing website or through a specialized application for frontlineagents, the credit decisions and terms need to be provided in real-time as
soon as prospective customers provide the relevant information. It is optimal
to provide a quick decision while also proving accurate. The decision-makingmodel has to collect data from the customer, integrate third-party data like
credit history, and make a decision on the loan approval and terms in a mat-
ter of seconds. The critical quality of this model deployment is real-timeprediction.
Segmenting customers based on their relationship with the company is a
thoughtful process where signals fr om various customer interactions are
collected. Based on the patterns, similar customers are put in cohorts and
campaign strategies are devised to best engage the customer. For thisapplication, batch processed, time la gged data would suffice. The critical
quality in this application is the ab ility to find unique patterns amongst
customers, not the response time of the model. The business applicationinforms the choices that need to be made in the data preparation and
modeling steps.
2.4.2 Technical Integration
Currently, it is quite common to use data science automation tools or coding
using R or Python to develop models. Data science tools save time as theydo not require the writing of custom codes to execute the algorithm. This
allows the analyst to focus on the data, business logic, and exploring patterns34 CHAPTER 2: Data Science Processfrom the data. The models created by data science tools can be ported to pro-
duction applications by utilizing the Predictive Model Markup Language
(PMML) ( Guazzelli, Zeller, Lin, & Williams, 2009 ) or by invoking data sci-
ence tools in the production application. PMML provides a portable and con-
sistent format of model description which can be read by most data science
tools. This allows the flexibility for practitioners to develop the model with
one tool (e.g., RapidMiner) and deploy it in another tool or application.
Some models such as simple regression, decision trees, and induction rules
for predictive analytics can be incorporated directly into business applica-
tions and business intelligence systems easily. These models are represented
by simple equations and the “if-then ”rule, hence, they can be ported easily
to most programming languages.
2.4.3 Response Time
Data science algorithms, like k-NN, are easy to build, but quite slow at pre-
dicting the unlabeled records. Algorithms such as the decision tree take time
to build but are fast at prediction. There are trade-offs to be made between
production responsiveness and modeling build time. The quality of predic-
tion, accessibility of input data, and the response time of the prediction
remain the critical quality factors in business application.
2.4.4 Model Refresh
The key criterion for the ongoing relevance of the model is the representative-
ness of the dataset it is processing. It is quite normal that the conditions in
which the model is built change after the model is sent to deployment. For
example, the relationship between the credit score and interest rate change
frequently based on the prevailing macroeconomic conditions. Hence, the
model will have to be refreshed frequently. The validity of the model can be
routinely tested by using the new known test dataset and calculating the pre-
diction error rate. If the error rate exceeds a particular threshold, then the
model has to be refreshed and redeployed. Creating a maintenance schedule
is a key part of a deployment plan that will sustain a relevant model.
2.4.5 Assimilation
In the descriptive data science applications, deploying a model to live sys-
tems may not be the end objective. The objective may be to assimilate the
knowledge gained from the data science analysis to the organization. For
example, the objective may be finding logical clusters in the customer data-
base so that separate marketing approaches can be developed for each cus-
tomer cluster. Then the next step may be a classification task for new
customers to bucket them in one of known clusters. The association analysis2.4 Application 35provides a solution for the market basket problem, where the task is to find
which two products are purchased together most often. The challenge for the
data science practitioner is to articulate these findings, establish relevance to
the original business question, quantify the risks in the model, and quantify
the business impact. This is indeed a challenging task for data science practi-
tioners. The business user community is an amalgamation of different points
of view, different quantitative mindsets, and skill sets. Not everyone is aware
about the process of data science and what it can and cannot do. Some
aspects of this challenge can be addressed by focusing on the end result, the
impact of knowing the discovered information, and the follow-up actions,
instead of the technical process of extracting the information through data
science.
2.5 KNOWLEDGE
The data science process provides a framework to extract nontrivial informa-
tion from data. With the advent of massive storage, increased data collection,
and advanced computing paradigms, the available datasets to be utilized are
only increasing. To extract knowledge from these massive data assets,
advanced approaches need to be employed, like data science algorithms, in
addition to standard business intelligence reporting or statistical analysis.
Though many of these algorithms can provide valuable knowledge, it is up
to the practitioner to skillfully transform a business problem to a data prob-
lem and apply the right algorithm. Data science, like any other technology,
provides various options in terms of algorithms and parameters within the
algorithms. Using these options to extract the right information from data is
a bit of an art and can be developed with practice.
The data science process starts with prior knowledge and ends with posterior
knowledge, which is the incremental insight gained. As with any quantitative
technique, the data science process can bring up spurious irrelevant patterns
from the dataset. Not all discovered patterns lead to incremental knowledge.
Again, it is up to the practitioner to invalidate the irrelevant patterns and
identify the meaningful information. The impact of the information gained
through data science can be measured in an application. It is the difference
between gaining the information through the data science process and the
insights from basic data analysis. Finally, the whole data science process is a
framework to invoke the right questions ( Chapman et al., 2000 ) and provide
guidance, through the right approaches, to solve a problem. It is not meant
to be used as a set of rigid rules, but as a set of iterative, distinct steps that
aid in knowledge discovery.36 CHAPTER 2: Data Science ProcessIn the upcoming chapters, the details of key data science concepts along with
their implementation will be explored. Exploring data using basic statistical
and visual techniques are an important first step in preparing the data for
data science. The next chapter on data exploration provides a practical tool
kit to explore and understand data. The techniques of data preparation are
explained in the context of individual data science algorithms in the chapters
on classification, association analysis, clustering, text mining, time series, and
anomaly detection.
References
Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C., & Wirth, R. (2000).
CRISP-DM 1.0: Step-by-step data mining guide . SPSS Inc. Retrieved from ,ftp://ftp.software.
ibm.com/software/analytics/spss/support/Modeler/Documentation/14/UserManual/CRISP-DM.
pdf..
Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. (1996). From data mining to knowledge discovery
in databases. AI Magazine ,17(3), 37 /C054.
Guazzelli, A., Zeller, M., Lin, W., & Williams, G. (2009). PMML: An open standard for sharing
models. The R Journal ,1(1), 60 /C065.
Kubiak, T., & Benbow, D. W. (2005). The certified six sigma black belt handbook . Milwaukee, WI:
ASQQuality Press.
Lidwell, W., Holden, K., & Butler, J. (2010). Universal principles of design, revised and updated: 125
ways to enhance usability, influence perception, increase appeal, make better design decisions, and
teach through design . Beverly, MA: Rockport Publishers.
Piatetsky, G. (2018). Top software for analytics, data science, machine learning in 2018: Trends and
analysis . Retrieved from ,https://www.kdnuggets.com/2018/05/poll-tools-analytics-data-sci-
ence-machine-learning-results.html .Accessed 07.07.18.
SAS Institute. (2013). Getting started with SAS enterprise miner 12.3 .
Shearer, C. (2000). The CRISP-DM model: The new blueprint for data mining. Journal of Data
Warehousing ,5(4), 13 /C022.
Tan, P.-N., Steinbach, M., & Kumar, V. (2005). Introduction to data mining. Journal of School
Psychology ,19,5 1/C056. Available from https://doi.org/10.1016/0022-4405(81)90007-8 .
Weisstein, E. W. (2013). Retrieved from ,http://mathworld.wolfram.com/LeastSquaresFitting.
html.Least squares fitting . Champaign, Illinois: MathWorld —Wolfram Research, Inc.References 37CHAPTER 3
Data Exploration
The word “data”is derived from the Latin word dare, which means “some-
thing given ”—an observation or a fact about a subject. (Interestingly, the
Sanskrit word dAta also means “given ”). Data science helps decipher the hid-
den useful relationships within data. Before venturing into any advanced
analysis of data using statistical, machine learning, and algorithmic techni-
ques, it is essential to perform basic data exploration to study the basic char-
acteristics of a dataset. Data exploration helps with understanding data
better, to prepare the data in a way that makes advanced analysis possible,
and sometimes to get the necessary insights from the data faster than using
advanced analytical techniques.
Simple pivot table functions, computing statistics like mean and deviation,
and plotting data as a line, bar, and scatter charts are part of data exploration
techniques that are used in everyday business settings. Data exploration, also
known as exploratory data analysis, provides a set of tools to obtain funda-
mental understanding of a dataset. The results of data exploration can be
extremely powerful in grasping the structure of the data, the distribution of
the values, and the presence of extreme values and the interrelationships
between the attributes in the dataset. Data exploration also provides guid-
ance on applying the right kind of further statistical and data science
treatment.
Data exploration can be broadly classified into two types —descriptive sta-
tistics and data visualization. Descriptive statistics is the process of con-
densing key characteristics of the dataset into simple numeric metrics.
Some of the common quantitative metrics used are mean, standard devia-
tion, and correlation. Visualization is the process of projecting the data, or
parts of it, into multi-dimensional space or abstract images. All the useful
(and adorable) charts fall under this category. Data exploration in the con-
text of data science uses both descrip tive statistics and visualization
techniques.
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00003-4
©2019 Elsevier Inc. All rights reserved.393.1 OBJECTIVES OF DATA EXPLORATION
In the data science process, data exploration is leveraged in many different
steps including preprocessing or data preparation, modeling, and interpreta-
tion of the modeling results.
1.Data understanding : Data exploration provides a high-level overview of
each attribute (also called variable) in the dataset and the interaction
between the attributes. Data exploration helps answers the questions
like what is the typical value of an attribute or how much do the data
points differ from the typical value, or presence of extreme values.
2.Data preparation : Before applying the data science algorithm, the dataset
has to be prepared for handling any of the anomalies that may be
present in the data. These anomalies include outliers, missing values,
or highly correlated attributes. Some data science algorithms do not
work well when input attributes are correlated with each other. Thus,
correlated attributes need to be identified and removed.
3.Data science tasks : Basic data exploration can sometimes substitute the
entire data science process. For example, scatterplots can identify
clusters in low-dimensional data or can help develop regression or
classification models with simple visual rules.
4.Interpreting the results : Finally, data exploration is used in understanding
the prediction, classification, and clustering of the results of the data
science process. Histograms help to comprehend the distribution of the
attribute and can also be useful for visualizing numeric prediction,
error rate estimation, etc.
3.2 DATASETS
Throughout the rest of this chapter (and the book) a few classic datasets,
which are simple to understand, easy to explain, and can be used commonly
across many different data science techniques, will be introduced. The most
popular datasets used to learn data science is probably the Iris dataset , intro-
duced by Ronald Fisher, in his seminal work on discriminant analysis, “The
use of multiple measurements in taxonomic problems ”(Fisher, 1936 ). Iris is
a flowering plant that is found widely, across the world. The genus of Iris
contains more than 300 different species. Each species exhibits different
physical characteristics like shape and size of the flowers and leaves. The Iris
dataset contains 150 observations of three different species, Iris setosa ,Iris vir-
ginica , and I. versicolor , with 50 observations each. Each observation consists
of four attributes: sepal length, sepal width, petal length, and petal width.
The fifth attribute, the label, is the name of the species observed, which takes
the values I. setosa ,I. virginica , and I. versicolor . The petals are the brightly40 CHAPTER 3: Data Explorationcolored inner part of the flowers and the sepals form the outer part of the
flower and are usually green in color. However, in an Iris flower, both sepals
and petals are bright purple in color, but can be distinguished from each
other by differences in the shape ( Fig. 3.1 ).
All four attributes in the Iris dataset are numeric continuous values measured in
centimeters. One of the species, I. setosa , can be easily distinguished from the
other two using simple rules like the petal length is less than 2.5 cm. Separating
thevirginica andversicolor classes requires more complex rules that involve more
attributes. The dataset is available in all standard data science tools, such as
RapidMiner, or can be downloaded from public websites such as the University
of California Irvine —Machine Learning repository2(Bache & Lichman, 2013 ).
This dataset and other datasets used in this book can be accessed from the book
companion website: www.IntroDataScience.com .
FIGURE 3.1
Iris versicolor . Source : Photo by Danielle Langlois. July 2005 (Image modified from original by marking parts.
“Iris versicolor 3.”Licensed under Creative Commons Attribution-Share Alike 3.0 via Wikimedia Commons.1)
1http://commons.wikimedia.org/wiki/File:Iris_versicolor_3.jpg#mediaviewer/File:Iris_versicolor_3.jpg .
2https://archive.ics.uci.edu/ml/datasets.html .3.2 Datasets 41The Iris dataset is used for learning data science mainly because it is simple
to understand, explore, and can be used to illustrate how different data sci-
ence algorithms approach the problem on the same standard dataset. Thedataset extends beyond two dimensions, with three class labels, of which one
class is easily separable ( I. setosa ) just by visual exploration, while classifying
the other two classes is slightly more challenging. It helps to reaffirm the clas-sification results that can be derived based on visual rules, and at the same
time sets the stage for data science to build new rules beyond the limits of
visual exploration.
3.2.1 Types of Data
Data come in different formats and types. Understanding the properties ofeach attribute or feature provides information about what kind of operationscan be performed on that attribute. For example, the temperature in weather
data can be expressed as any of the following formats:
GNumeric centigrade (31/C14C, 33.3/C14C) or Fahrenheit (100/C14F, 101.45/C14F) or
on the Kelvin scale
GOrdered labels as in hot, mild, or cold
GNumber of days within a year below 0/C14C (10 days in a year below freezing)
All of these attributes indicate temperature in a region, but each have differ-
ent data types. A few of these data types can be converted from one toanother.
Numeric or Continuous
Temperature expressed in Centigrade or Fahrenheit is numeric and continu-
ous because it can be denoted by numbers and take an infinite number of
values between digits. Values are ordered and calculating the differencebetween the values makes sense. Hence, additive and subtractive mathemati-
cal operations and logical comparison operators like greater than, less than,
and equal to, operations can be applied.
An integer is a special form of the numeric data type which does not have
decimals in the value or more precisely does not have infinite values between
consecutive numbers. Usually, they denote a count of something, number ofdays with temperature less than 0
/C14C, number of orders, number of children
in a family, etc.
If a zero point is defined, numeric data become a ratio orrealdata type.
Examples include temperature in Kelvin scale, bank account balance, and
income. Along with additive and logical operations, ratio operations can beperformed with this data type. Both integer and ratio data types are catego-
rized as a numeric data type in most data science tools.42 CHAPTER 3: Data ExplorationCategorical or Nominal
Categorical data types are attribute s treated as distinct symbols or just
names. The color of the iris of the human eye is a categorical data type
because it takes a value like black, gr een, blue, gray, etc. There is no direct
relationship among the data values, and hence, mathematical operators
except the logical or “is equal ”operator cannot be app lied. They are also
called a nominal or polynominal data type, derived from the Latin word
forname .
An ordered nominal data type is a special case of a categorical data type
where there is some kind of order among the values. An example of an
ordered data type is temperature expressed as hot, mild, cold.
Not all data science tasks can be performed on all data types. For example,
the neural network algorithm does not work with categorical data. However,
one data type can be converted to another using a type conversion process,
but this may be accompanied with possible loss of information. For exam-
ple, credit scores expressed in poor, average, good, and excellent categories
can be converted to either 1, 2, 3, and 4 or average underlying numerical
scores like 400, 500, 600, and 700 (scores here are just an example). In this
type conversion, there is no loss of information. However, conversion from
numeric credit score to categories (poor, average, good, and excellent) does
incur loss of information.
3.3 DESCRIPTIVE STATISTICS
Descriptive statistics refers to the study of the aggregate quantities of a
dataset. These measures are some of the commonly used notations in
everyday life. Some examples of descr iptive statistics include average
annual income, median home price in a neighborhood, range of credit
scores of a population, etc. In general, descriptive analysis covers the fol-
lowing characteristics of the sample or population dataset ( Kubiak &
Benbow, 2006 ):
Characteristics of the Dataset Measurement Technique
Center of the dataset Mean, median, and mode
Spread of the dataset Range, variance, and standard deviation
Shape of the distribution of the dataset Symmetry, skewness, and kurtosis
The definition of these metrics will be explored shortly. Descriptive statistics
can be broadly classified into univariate and multivariate exploration
depending on the number of attributes under analysis.3.3 Descriptive Statistics 433.3.1 Univariate Exploration
Univariate data exploration denotes analysis of one attribute at a time. The
example Iris dataset for one species, I. setosa , has 50 observations and 4 attri-
butes, as shown in Table 3.1 . Here some of the descriptive statistics for sepal
length attribute are explored.
Measure of Central Tendency
The objective of finding the central location of an attribute is to quantify the
dataset with one central or most common number.
GMean : The mean is the arithmetic average of all observations in the
dataset. It is calculated by summing all the data points and dividing by
the number of data points. The mean for sepal length in centimeters is
5.0060.
GMedian : The median is the value of the central point in the distribution.
The median is calculated by sorting all the observations from small to
large and selecting the mid-point observation in the sorted list. If the
number of data points is even, then the average of the middle two data
points is used as the median. The median for sepal length is in
centimeters is 5.0000.
GMode : The mode is the most frequently occurring observation. In the
dataset, data points may be repetitive, and the most repetitive data
point is the mode of the dataset. In this example, the mode in
centimeters is 5.1000.
Table 3.1 Iris Dataset and Descriptive Statistics ( Fisher, 1936 )
Observation Sepal Length Sepal Width Petal Length Petal Width
1 5.1 3.5 1.4 0.2
2 4.9 3.1 1.5 0.1
... ... ... ... ...
49 5 3.4 1.5 0.2
50 4.4 2.9 1.4 0.2
Statistics Sepal Length Sepal Width Petal Length Petal Width
Mean 5.006 3.418 1.464 0.244
Median 5.000 3.400 1.500 0.200
Mode 5.100 3.400 1.500 0.200
Range 1.500 2.100 0.900 0.500
Standard deviation 0.352 0.381 0.174 0.107
Variance 0.124 0.145 0.030 0.01144 CHAPTER 3: Data ExplorationIn an attribute, the mean, median, and mode may be different numbers, and
this indicates the shape of the distribution. If the dataset has outliers, the
mean will get affected while in most cases the median will not. The mode of
the distribution can be different from the mean or median, if the underlying
dataset has more than one natural normal distribution.
Measure of Spread
In desert regions, it is common for the temperature to cross above 110/C14F dur-
ing the day and drop below 30/C14F during the night while the average tempera-
ture for a 24-hour period is around 70/C14F. Obviously, the experience of living
in the desert is not the same as living in a tropical region with the same aver-
age daily temperature around 70/C14F, where the temperature within the day is
between 60/C14F and 80/C14F. What matters here is not just the central location of
the temperature, but the spread of the temperature. There are two common
metrics to quantify spread.
GRange: The range is the difference between the maximum value and the
minimum value of the attribute. The range is simple to calculate and
articulate but has shortcomings as it is severely impacted by the
presence of outliers and fails to consider the distribution of all other
data points in the attributes. In the example, the range for the
temperature in the desert is 80/C14F and the range for the tropics is 20/C14F.
The desert region experiences larger temperature swings as indicated by
the range.
GDeviation: The variance and standard deviation measures the spread, by
considering all the values of the attribute. Deviation is simply
measured as the difference between any given value ( xi) and the mean
of the sample ( μ). The variance is the sum of the squared deviations of
all data points divided by the number of data points. For a dataset with
Nobservations, the variance is given by the following equation:
Variance 5s251
NXN
i51xi2μðÞ2ð3:1Þ
Standard deviation is the square root of the variance. Since the standard devi-
ation is measured in the same units as the attribute, it is easy to understand
the magnitude of the metric. High standard deviation means the data points
are spread widely around the central point. Low standard deviation means
data points are closer to the central point. If the distribution of the data
aligns with the normal distribution , then 68% of the data points lie within one
standard deviation from the mean. Fig. 3.2 provides the univariate summary
of the Iris dataset with all 150 observations, for each of the four numeric
attributes.3.3 Descriptive Statistics 453.3.2 Multivariate Exploration
Multivariate exploration is the study of more than one attribute in the data-
set simultaneously. This technique is critical to understanding the relation-
ship between the attributes, which is central to data science methods. Similar
to univariate explorations, the measure of central tendency and variance in
the data will be discussed.
Central Data Point
In the Iris dataset, each data point as a set of all the four attributes can be
expressed:
observation i: {sepal length, sepal width, petal length, petal width}
For example, observation one: {5.1, 3.5, 1.4, 0.2}. This observation point
can also be expressed in four-dimensional Cartesian coordinates and can be
plotted in a graph (although plotting more than three dimensions in a visual
graph can be challenging). In this way, all 150 observations can be expressed
in Cartesian coordinates. If the objective is to find the most “typical ”obser-
vation point, it would be a data point made up of the mean of each attribute
in the dataset independently. For the Iris dataset shown in Table 3.1 , the cen-
tral mean point is {5.006, 3.418, 1.464, 0.244}. This data point may not be
an actual observation. It will be a hypothetical data point with the most typi-
cal attribute values.
FIGURE 3.2
Descriptive statistics for the Iris dataset.46 CHAPTER 3: Data ExplorationCorrelation
Correlation measures the statistical relationship between two attributes,
particularly dependence of one attribute on another attribute. When two
attributes are highly correlated with each other, they both vary at the same
rate with each other either in the same or in opposite directions. For
example, consider average temperature of the day and ice cream sales.
Statistically, the two attributes that are correlated are dependent on each
other and one may be used to predict the other. If there are sufficient
d a t a ,f u t u r es a l e so fi c ec r e a mc a nb ep r e d i c t e di ft h et e m p e r a t u r ef o r e c a s t
is known. However, correlation between two attributes does not imply
causation, that is, one doesn ’t necessarily cause the other. The ice cream
sales and the shark attacks are correlated, however there is no causation.
Both ice cream sales and shark attacks are influenced by the third attri-
bute—the summer season. Generally, ice cream sales spikes as tempera-
tures rise. As more people go to beaches during summer, encounters with
sharks become more probable.
Correlation between two attributes is commonly measured by the Pearson
correlation coefficient ( r), which measures the strength of linear dependence
(Fig. 3.3 ). Correlation coefficients take a value from 21#r#1. A value
closer to 1 or 21 indicates the two attributes are highly correlated, with per-
fect correlation at 1 or 21. Perfect correlation also exists when the attributes
are governed by formulas and laws. For example, observations of the values
of gravitational force and the mass of the object (Newton ’s second law) or
the quantity of the products sold and total revenue (price*
volume 5revenue). A correlation value of 0 means there is no linear relation-
ship between two attributes.
FIGURE 3.3
Correlation of attributes.3.3 Descriptive Statistics 47The Pearson correlation coefficient between two attributes xand yis calcu-
lated with the formula:
rxy5Pn
i51xi2x ðÞ yi2y ðÞﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPn
i51xi2xðÞ2Pn
i51yi2yðÞ2q
5PN
i51xi2x ðÞ yi2y ðÞ
N3sx3syð3:2Þ
where sxand syare the standard deviations of random variables xand y,
respectively. The Pearson correlation coefficient has some limitations in
quantifying the strength of correlation. When datasets have more complex
nonlinear relationships like quadratic functions, only the effects on linear
relationships are considered and quantified using correlation coefficient. The
presence of outliers can also skew the measure of correlation. Visually, corre-
lation can be observed using scatterplots with the attributes in each Cartesian
coordinate ( Fig. 3.3 ). In fact, visualization should be the first step in under-
standing correlation because it can identify nonlinear relationships and show
any outliers in the dataset. Anscombe ’s quartet ( Anscombe, 1973 ) clearly
illustrates the limitations of relying only on the correlation coefficient to
understand the data ( Fig. 3.4 ). The quartet consists of four different datasets,
with two attributes ( x,y). All four datasets have the same mean, the same
variance for xand y, and the same correlation coefficient between xand y,
but look drastically different when plotted on a chart. This evidence illus-
trates the necessity of visualizing the attributes instead of just calculating sta-
tistical metrics.
3.4 DATA VISUALIZATION
Visualizing data is one of the most important techniques of data discovery
and exploration. Though visualization is not considered a data science tech-
nique, terms like visual mining or pattern discovery based on visuals are
increasingly used in the context of data science, particularly in the business
world. The discipline of data visualization encompasses the methods of
expressing data in an abstract visual form. The visual representation of data
provides easy comprehension of complex data with multiple attributes and
their underlying relationships. The motivation for using data visualization
includes:
GComprehension of dense information : A simple visual chart can easily
include thousands of data points. By using visuals, the user can see the
big picture, as well as longer term trends that are extremely difficult to
interpret purely by expressing data in numbers.48 CHAPTER 3: Data ExplorationGRelationships : Visualizing data in Cartesian coordinates enables
exploration of the relationships between the attributes. Although
representing more than three attributes on the x,y, and z-axes is not
feasible in Cartesian coordinates, there are a few creative solutions
available by changing properties like the size, color, and shape of data
markers or using flow maps ( Tufte, 2001 ), where more than two
attributes are used in a two-dimensional medium.
Vision is one of the most powerful senses in the human body. As such, it is
intimately connected with cognitive thinking ( Few, 2006 ). Human vision is
trained to discover patterns and anomalies even in the presence of a large
volume of data. However, the effectiveness of the pattern detection depends
on how effectively the information is visually presented. Hence, selecting
suitable visuals to explore data is critically important in discovering and
comprehending hidden patterns in the data ( Ware, 2004 ). As with descriptive
statistics, visualization techniques are also categorized into: univariate visuali-
zation, multivariate visualization and visualization of a large number of attri-
butes using parallel dimensions.
FIGURE 3.4
Anscombe ’s Quartet: descriptive statistics versus visualization. Source : Adapted from: Anscombe, F. J., 1973. Graphs in statistical
analysis, American Statistician ,27(1), pp. 19 /C020.3.4 Data Visualization 493.4.1 Univariate Visualization
Visual exploration starts with investigating one attribute at a time using uni-
variate charts. The techniques discussed in this section give an idea of how
the attribute values are distributed and the shape of the distribution.
Histogram
A histogram is one of the most basic v isualization techniques to under-
stand the frequency of the occurrence of values. It shows the distribution
o ft h ed a t ab yp l o t t i n gt h ef r e q u e n c yo fo c c u r r e n c ei nar a n g e .I nah i s t o -
gram, the attribute under inquiry is s hown on the horizontal axis and the
frequency of occurrence is on the vertical axis. For a continuous numeric
data type, the range or binning value to group a range of values need to be
specified. For example, in the case of human height in centimeters, all the
occurrences between 152.00 and 152.99 are grouped under 152. There is
no optimal number of bins or bin width that works for all the distribu-
tions. If the bin width is too small, the distribution becomes more precise
but reveals the noise due to sampling. A general rule of thumb is to have
a number of bins equal to the square root or cube root of the number of
data points.
Histograms are used to find the central location, range, and shape of distri-
bution. In the case of the petal length attribute in the Iris dataset, the data is
multimodal ( Fig. 3.5 ), where the distribution does not follow the bell curve
pattern. Instead, there are two peaks in the distribution. This is due to the
fact that there are 150 observations of three different species (hence, distribu-
tions) in the dataset. A histogram can be stratified to include different classes
in order to gain more insight. The enhanced histogram with class labels
shows the dataset is made of three different distributions ( Fig. 3.6 ).I. setosa ’s
distribution stands out with a mean around 1.25 cm and ranges from
1/C02 cm. I. versicolor and I. virginica ’sdistributions overlap I. setosa ’shave sep-
arate means.
Quartile
Abox whisker plot is a simple visual way of showing the distribution of a con-
tinuous variable with information such as quartiles, median, and outliers,Some of the common data visualization techniques
used to analyze data will be reviewed. Most of these
visualization techniques are available in commercial
spreadsheet software like MS Excel. RapidMiner, like
any other data science tool, offers a wide range ofvisualization tools. To maintain consistency with rest of
the book, all further visualizations are output from
RapidMiner using the Iris d ataset. Please review
Chapter 15, Getting Started With RapidMiner, to
become familiar with RapidMiner.50 CHAPTER 3: Data Explorationoverlaid by mean and standard deviation. The main attraction of box whisker
or quartile charts is that distributions of multiple attributes can be compared
side by side and the overlap between them can be deduced. The quartiles are
denoted by Q1, Q2, and Q3 points, which indicate the data points with a
25% bin size. In a distribution, 25% of the data points will be below Q1,
50% will be below Q2, and 75% will be below Q3.
The Q1 and Q3 points in a box whisker plot are denoted by the edges of the
box. The Q2 point, the median of the distribution, is indicated by a cross
line within the box. The outliers are denoted by circles at the end of the
whisker line. In some cases, the mean point is denoted by a solid dot overlay
followed by standard deviation as a line overlay.
Fig. 3.7 shows that the quartile charts for all four attributes of the Iris dataset
are plotted side by side. Petal length can be observed as having the broadest
range and the sepal width has a narrow range, out of all of the four attributes.
One attribute can also be selected —petal length —and explored further using
quartile charts by introducing a class label. In the plot in Fig. 3.8 ,w ec a ns e et h e
distribution of three species for the petal length measurement. Similar to the
previous comparison, the distribution of multiple species can be compared.
FIGURE 3.5
Histogram of petal length in Iris dataset.3.4 Data Visualization 51Distribution Chart
For continuous numeric attributes like petal length, instead of visualizing the
actual data in the sample, its normal distribution function can be visualized
instead. The normal distribution function of a continuous random variable
is given by the formula:
fxðÞ51ﬃﬃﬃﬃﬃﬃﬃﬃﬃ2πσp ex2μðÞ2=2σ2ð3:3Þ
where μis the mean of the distribution and σis the standard deviation of
the distribution. Here an inherent as sumption is being made that the mea-
surements of petal length (or any co ntinuous variable) follow the normal
distribution, and hence, its distrib ution can be visualized instead of the
actual values. The normal distribution is also called the Gaussian distribu-
tionor“bell curve ”due to its bell shape. The nor mal distribution function
FIGURE 3.6
Class-stratified histogram of petal length in Iris dataset.52 CHAPTER 3: Data Explorationshows the probability of occurrence of a data point within a range of
values. If a dataset exhibits normal dis tribution, then 68.2% of data points
will fall within one standard deviation from the mean; 95.4% of the points
will fall within 2 σand 99.7% within 3 σof the mean. When the normal
distribution curves are stratified by c lass type, more insight into the data
can be gained. Fig. 3.9 shows the normal distribution curves for petal
length measurement for each Iris specie s type. From the distribution chart,
it can be inferred that the petal length for the I. setosa sample is more dis-
tinct and cohesive than I. versicolor and I. virginica. If there is an unlabeled
measurement with a petal length of 1.5 cm, it can be predicted that the
species is I. setosa . However, if the petal length measurement is 5.0 cm,
there is no clear prediction, as the species could be either Iris versicolor and
I. virginica .
3.4.2 Multivariate Visualization
The multivariate visual exploration considers more than one attribute in the
same visual. The techniques discussed in this section focus on the relation-
ship of one attribute with another attribute. These visualizations examine
two to four attributes simultaneously.
7.90E0
Sepal length
Sepal widthPetal length
Petal width6.60E0
5.30E0
4.00E0
2.70E0
1.40E0
1.00E–1
FIGURE 3.7
Quartile plot of Iris dataset.3.4 Data Visualization 53Scatterplot
A scatterplot is one of the most powerful yet simple visual plots available. In a
scatterplot, the data points are marked in Cartesian space with attributes of the
dataset aligned with the coordinates. The attributes are usually of continuous
data type. One of the key observations that can be concluded from a scatterplot
is the existence of a relationship between two attributes under inquiry. If the
attributes are linearly correlated, then the data points align closer to an imagi-
nary straight line; if they are not correlated, the data points are scattered. Apart
from basic correlation, scatterplots can also indicate the existence of patterns or
groups of clusters in the data and identify outliers in the data. This is particularly
useful for low-dimensional datasets. Chapter 13: Anomaly detection, provides
techniques for finding outliers in high-dimensional space.
Fig. 3.10 shows the scatterplot between petal length ( x-axis) and petal width
(y-axis). These two attributes are slightly correlated, because this is a measure-
ment of the same part of the flower. When the data markers are colored to
FIGURE 3.8
Class-stratified quartile plot of petal length in Iris dataset.54 CHAPTER 3: Data Explorationindicate different species using class labels, more patterns can be observed.
There is a cluster of data points, all belonging to species I. setosa , on the
lower left side of the plot. I. setosa has much smaller petals. This feature can
be used as a rule to predict the species of unlabeled observations. One of the
limitations of scatterplots is that only two attributes can be used at a time,
with an additional attribute possibly shown in the color of the data marker.
However, the colors are usually reserved for class labels.
Scatter Multiple
Ascatter multiple is an enhanced form of a simple scatterplot where more
than two dimensions can be included in the chart and studied simulta-
neously. The primary attribute is used for the x-axis coordinate. The sec-
ondary axis is shared with more attributes or dimensions. In this example
(Fig. 3.11 ), the values on the y-axis are shared between sepal length, sepal
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5
Petal length5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5Iris-setosa
Iris setosaIris-versicolor
Iris versicolorIris-virginica
Iris virginica2.4
2.3
2.2
2.1
2.0
1.9
1.8
1.7
1.6
1.5
1.4
1.3
1.2Density1.1
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
FIGURE 3.9
Distribution of petal length in Iris dataset.3.4 Data Visualization 55width, and petal width. The name of the attribute is conveyed by colors
used in data markers. Here, sepal length is represented by data points occu-
pying the topmost part of the chart, se pal width occupies the middle por-
tion, and petal width is in the bottom portion. Note that the data points
areduplicated for each attribute in the y-axis . Data points are color-coded for
each dimension in y-axis while the x-axis is anchored with one attribute —
petal length. All the attributes shar ing the y-axis should be of the same unit
or normalized.
Scatter Matrix
If the dataset has more than two attributes, it is important to look at combi-nations of all the attributes through a scatterplot. A scatter matrix solves this
need by comparing all combinations of attributes with individual scatterplots
and arranging these plots in a matrix.
FIGURE 3.10
Scatterplot of Iris dataset.56 CHAPTER 3: Data ExplorationA scatter matrix for all four attributes in the Iris dataset is shown in Fig. 3.12 .
The color of the data point is used to indicate the species of the flower. Since
there are four attributes, there are four rows and four columns, for a total of
16 scatter charts. Charts in the diagonal are a comparison of the attribute
with itself; hence, they are eliminated. Also, the charts below the diagonal
are mirror images of the charts above the diagonal. In effect, there are six dis-
tinct comparisons in scatter multiples of four attributes. Scatter matrices pro-
vide an effective visualization of comparative, multivariate, and high-density
data displayed in small multiples of the similar scatterplots ( Tufte, 2001 ).
Bubble Chart
Abubble chart is a variation of a simple scatterplot with the addition of one
more attribute, which is used to determine the size of the data point. In the
Iris dataset, petal length and petal width are used for xand y-axis, respectively
FIGURE 3.11
Scatter multiple plot of Iris dataset.3.4 Data Visualization 57and sepal width is used for the size of the data point. The color of the data
point represents a species class label ( Fig. 3.13 ).
Density Chart
Density charts are similar to the scatterplots, with one more dimension
included as a background color. The data point can also be colored to visual-
ize one dimension, and hence, a total of four dimensions can be visualized
in a density chart. In the example in Fig. 3.14 , petal length is used for the
x-axis, sepal length for the y-axis, sepal width for the background color, and
class label for the data point color.
FIGURE 3.12
Scatter matrix plot of Iris dataset.58 CHAPTER 3: Data ExplorationFIGURE 3.13
Bubble chart of Iris dataset.
FIGURE 3.14
Density chart of a few attributes in the Iris dataset.3.4 Data Visualization 593.4.3 Visualizing High-Dimensional Data
Visualizing more than three attributes on a two-dimensional medium (like a
paper or screen) is challenging. This limitation can be overcome by using
transformation techniques to project the high-dimensional data points into
parallel axis space. In this approach, a Cartesian axis is shared by more than
one attribute.
Parallel Chart
Aparallel chart visualizes a data point quite innovatively by transforming or
projecting multi-dimensional data into a two-dimensional chart medium. In
this chart, every attribute or dimension is linearly arranged in one coordinate
(x-axis) and all the measures are arranged in the other coordinate ( y-axis).
Since the x-axis is multivariate, each data point is represented as a linein a
parallel space.
In the case of the Iris dataset, all four attributes are arranged along the x-axis.
The y-axis represents a generic distance and it is “shared ”by all these attri-
butes on the x-axis. Hence, parallel charts work only when attributes share a
common unit of numerical measure or when the attributes are normalized.
This visualization is called a parallel axis because all four attributes are repre-
sented in four parallel axes parallel to the y-axis.
In a parallel chart, a class label is used to color each data lineso that one
more dimension is introduced into the picture. By observing this parallel
chart in Fig. 3.15 , it can be noted that there is overlap between the three spe-
cies on the sepal width attribute. So, sepal width cannot be the metric used
to differentiate these three species. However, there is clear separation of spe-
cies in petal length. No observation of I. setosa species has a petal length
above 2.5 cm and there is little overlap between the I. virginica and I. versico-
lorspecies. Visually, just by knowing the petal length of an unlabeled obser-
vation, the species of Iris flower can be predicted. The relevance of this rule
as a predictor will be discussed in the later chapter on Classification.
Deviation Chart
Adeviation chart is very similar to a parallel chart , as it has parallel axes for all
the attributes on the x-axis. Data points are extended across the dimensions
as lines and there is one common y-axis. Instead of plotting all data lines,
deviation charts only show the mean and standard deviation statistics. For
each class, deviation charts show the mean line connecting the mean of each
attribute; the standard deviation is shown as the band above and below the
mean line. The mean line does not have to correspond to a data point (line).
With this method, information is elegantly displayed, and the essence of a
parallel chart is maintained.60 CHAPTER 3: Data ExplorationInFig. 3.16 , a deviation chart for the Iris dataset stratified by species is
shown. It can be observed that the petal length is a good predictor to classify
the species because the mean line and the standard deviation bands for the
species are well separated.
Andrews Curves
AnAndrews plot belongs to a family of visualization techniques where the
high-dimensional data are projected into a vector space so that each data
point takes the form of a line or curve. In an Andrews plot, each data point
Xwith ddimensions, X5(x1,x2,x3,...,xd), takes the form of a Fourier
series:
fxtðÞ5x1ﬃﬃﬃ
2p1x2sintðÞ1x3costðÞ1x4sin 2 tðÞ1x5cos 2 tðÞ1? ð3:4Þ
This function is plotted for 2π,t,πfor each data point. Andrews plots
are useful to determine if there are any outliers in the data and to identify
potential patterns within the data points ( Fig. 3.17 ). If two data points are
similar, then the curves for the data points are closer to each other. If curves
are far apart and belong to different classes, then this information can be
used classify the data ( Garcia-Osorio & Fyfe, 2005 ).
FIGURE 3.15
Parallel chart of Iris dataset.3.4 Data Visualization 61FIGURE 3.16
Deviation chart of Iris dataset.
FIGURE 3.17
Andrews curves of Iris dataset.62 CHAPTER 3: Data ExplorationMany of the charts and visuals discussed in this chapter explore the multivar-
iate relationships within the dataset. They form the set of classic data visuali-
zations used for data exploration, postprocessing, and understanding data
science models. Some new developments in the area of visualization deals
with networks and connections within the data objects ( Lima, 2011 ). To bet-
ter analyze data extracted from graph data, social networks, and integrated
applications, connectivity charts are often used. Interactive exploration of
data using visualization software provides an essential tool to observe multi-
ple attributes at the same time but has limitations on the number of attri-
butes used in visualizations. Hence, dimensional reduction using techniques
discussed in Chapter 14, Feature Selection, can help in visualizing higher-
dimensional data by reducing the dimensions to a critical few.
3.5 ROADMAP FOR DATA EXPLORATION
If there is a new dataset that has not been investigated before, having a struc-
tured way to explore and analyze the data will be helpful. Here is a roadmap
to inquire a new dataset. Not all steps may be relevant for every dataset and
the order may need to be adjusted for some sets, so this roadmap is intended
as a guideline.
1.Organize the dataset : Structure the dataset with standard rows and
columns. Organizing the dataset to have objects or instances in rows
and dimensions or attributes in columns will be helpful for many data
analysis tools. Identify the target or “class label ”attribute, if applicable.
2.Find the central point for each attribute : Calculate mean ,median , and mode
for each attribute and the class label. If all three values are very
different, it may indicate the presence of an outlier, or a multimodal or
nonnormal distribution for an attribute.
3.Understand the spread of each attribute : Calculate the standard deviation
and range for an attribute. Compare the standard deviation with the
mean to understand the spread of the data, along with the max and
min data points.
4.Visualize the distribution of each attribute : Develop the histogram and
distribution plots for each attribute. Repeat the same for class-stratified
histograms and distribution plots, where the plots are either repeated
or color-coded for each class.
5.Pivot the data : Sometimes called dimensional slicing, a pivot is helpful
to comprehend different values of the attributes. This technique can
stratify by class and drill down to the details of any of the attributes.
Microsoft Excel and Business Intelligence tools popularized this
technique of data analysis for a wider audience.3.5 Roadmap for Data Exploration 636.Watch out for outliers : Use a scatterplot or quartiles to find outliers. The
presence of outliers skews some measures like mean, variance, and
range. Exclude outliers and rerun the analysis. Notice if the results
change.
7.Understand the relationship between attributes : Measure the correlation
between attributes and develop a correlation matrix. Notice what
attributes are dependent on each other and investigate why they are
dependent.
8.Visualize the relationship between attributes : Plot a quick scatter matrix to
discover the relationship between multiple attributes at once. Zoom in
on the attribute pairs with simple two-dimensional scatterplots
stratified by class.
9.Visualize high-dimensional datasets : Create parallel charts and Andrews
curves to observe the class differences exhibited by each attribute.
Deviation charts provide a quick assessment of the spread of each class
for each attribute.
References
Anscombe, F. J. (1973). Graphs in statistical analysis. American Statistician ,27(1), 17 /C021.
Bache, K., & Lichman, M. (2013) University of California, School of Information and Computer
Science . Retrieved from UCI Machine Learning Repository ,http://archive.ics.uci.edu/ml ..
Few, S. (2006). Information dashboard design: The effective visual communication of data . Sebastopol,
CA: O ’Reilly Media.
Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Human
Genetics ,7, 179 /C0188, 10.1111/j.1469-1809.1936.tb02137.x.
Garcia-Osorio, C., & Fyfe, C. (2005). Visualization of high-dimensional data via orthogonal
curves. Journal of Universal Computer Science ,11(11), 1806 /C01819.
Kubiak, T., & Benbow, D. W. (2006). The certified six sigma black belt handbook . Milwaukee, WI:
ASQ Quality Press.
Lima, M. (2011). Visual complexity: Mapping patterns of information . New York: Princeton
Architectural Press.
Tufte, E. R. (2001). The visual display of quantitative information . Cheshire, CT: Graphics Press.
Ware, C. (2004). Information visualization: Perception for design . Waltham, MA: Morgan Kaufmann.64 CHAPTER 3: Data ExplorationCHAPTER 4
Classification
Enter the realm of data science —the process in which historical records are
used to make a prediction about an uncertain future. At a fundamental level,
most data science problems can be categorized into either class or numeric
prediction problems. In classification or class prediction, one should try to
use the information from the predictors or independent variables to sort the
data samples into two or more distinct classes orbuckets . In the case of
numeric prediction, one would try to predict the numeric value of a depen-
dent variable using the values assumed by the independent variables.
Here the classification process will be described with a simple example. Most
golfers enjoy playing if the weather and outlook conditions meet certain
requirements: too hot or too humid conditions, even if the outlook is sunny,
are not preferred. On the other hand, overcast skies are no problem for play-
ing even if the temperatures are somewhat cool. Based on the historic fic-
tional records of these conditions and preferences, and information about a
day’s temperature, humidity level, and outlook, classification will allow one
to predict if someone prefers to play golf or not. The outcome of classifica-
tion is to categorize the weather conditions when golf is likely to be played
or not, quite simply: Play orNot Play (two classes). The predictors can be
continuous (temperature, humidity) or categorical (sunny, cloudy, windy,
etc.). Those beginning to explore data science are confused by the dozens of
techniques that are available to address these types of classification pro-
blems. In this chapter, several commonly used data science techniques will
be described where the idea is to develop rules, relationships, and models
based on predictor information that can be applied to classify outcomes
from new and unseen data.
To begin with, fairly simple schemes will be used with a progression to more
sophisticated techniques. Each section contains essential algorithmic details
about the technique, describes how it is developed using simple examples,
and finally closes with implementation details.
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00004-6
©2019 Elsevier Inc. All rights reserved.654.1 DECISION TREES
Decision trees (also known as classifica tion trees) are probably one of the most
intuitive and frequently used data science techniques. From an analyst ’s point
of view, they are easy to set up and from a business user ’s point of view they
are easy to interpret. Classification trees, as the name implies, are used to sepa-
rate a dataset into classes belonging to the response variable. Usually the
response variable has two classes: Yes or No (1 or 0). If the response variable
hasmore than two categories, then variants of the decision tree algorithm have
been developed that may be applied ( Quinlan, 1986 ). In either case, classifica-
tion trees are used when the response or target variable is categorical in nature.
Regression trees (Brieman, 1984 ) are similar in function to classification trees
and are used for numeric prediction problems, when the response variable is
numeric or continuous: for example, predicting the price of a consumer good
based on several input factors. Keep in mind that in either case, the predic-
tors or independent variables may be either categorical or numeric. It is the
target variable that determines the type of decision tree needed.
4.1.1 How It Works
A decision tree model takes a form of decision flowchart (or an inverted
tree) where an attribute is tested in each node. At end of the decision tree
path is a leaf node where a prediction is made about the target variable
based on conditions set forth by the decision path. The nodes split the data-
set into subsets. In a decision tree, the idea is to splitthe dataset based on the
homogeneity of data. Say for example, there are two variables, age and
weight, that predict if a person is likely to sign up for a gym membership or
not. In the training data if it was seen that 90% of the people who are older
than 40 signed up, the data can be split into two parts: one part consisting of
people older than 40 and the other part consisting of people under 40. The
first part is now “90% pure ”from the standpoint of which class they belong
to. However, a rigorous measure of impurity is needed, which meets certain
criteria, based on computing a proportion of the data that belong to a class.
These criteria are simple:
1.The measure of impurity of a dataset must be at a maximum when all
possible classes are equally represented. In the gym membership
example, in the initial dataset, if 50% of samples belonged to “not
signed up ”and 50% of the samples belonged to “signed up, ”then this
non-partitioned raw data would have maximum impurity.
2.The measure of impurity of a dataset must be zero when only one class
is represented. For example, if a group is formed of only those people
who signed up for the membership (only one class 5members), then
this subset has “100% purity ”or“0% impurity. ”66 CHAPTER 4: ClassificationMeasures such as entropy orGini index easily meet these criteria and are used
to build decision trees as described in the following sections. Different crite-
ria will build different trees through different biases, for example, information
gain favors tree splits that contain many cases, while information gain ratio
attempts to balance this.
HOW DATA SCIENCE CAN REDUCE UNCERTAINTY
Imagine a box that can contain one of three colored
balls inside —red, yellow, and blue, see Fig. 4.1 . Without
opening the box, if one had to “predict ”which colored
ball is inside, then they are basically dealing with a lack
of information or uncertainty. Now what is the highest
number of “yes/no ”questions that can be asked to
reduce this uncertainty and, thus, increase our
information?
1.Is it red? No.
2.Is it yellow? No.
Then it must be blue.
That is twoquestions. If there were a fourth color, green,
then the highest number of yes/no questions is three .B y
extending this reasoning, it can be mathematically shown
that the maximum number of binary questions needed to
reduce uncertainty is essentially log( T), where the log is
taken to base 2 and Tis the number of possible outcomes
(Meagher, 2005 ) [e.g., if there was only one color, that is,one outcome, then log(1) 50, which means there is no
uncertainty!].
Many real-world business problems can be thought of as
extensions to this “uncertainty reduction ”example. For
example, knowing only a handful of characteristics such
as the length of a loan, borrower ’s occupation, annual
income, and previous credit behavior, several available
data science techniques can be used to rank the riskiness
of a potential loan, and by extension, the interest rate of
the loan. This is nothing but a more sophisticated uncer-
tainty reduction exercise, similar in spirit to the ball-in-a-
box problem. Decision trees embody this problem-solving
technique by systematically examining the available attri-
butes and their impact on the eventual class or category
of a sample. Later in this section, how to predict the credit
ratings of a bank ’s customers using their demographic
and other behavioral data will be examined in detail using
a decision tree, which is a practical implementation of the
entropy principle for decision-making under uncertainty.
FIGURE 4.1
Playing 20 questions with entropy.4.1 Decision Trees 67Continuing with the example in the box, if there are Tevents with equal
probability of occurrence P, then T51/P. Claude Shannon, who developed
the mathematical underpinnings for information theory ( Shannon, 1948 ),
defined entropy as log 2(1/p)o r2log2pwhere pis the probability of an event
occurring. If the probability for all events is not identical, a weighted expres-
sion is needed and, thus, entropy, H, is adjusted as follows:
H52Xm
k51pklog2ðpkÞð 4:1Þ
where k51, 2, 3, ...,mrepresents the mclasses of the target variable. pk
represents the proportion of samples that belong to class k. For the gym
membership example from earlier, there are two classes: member or non-
member. If the dataset had 100 samples with 50% of each, then the entropy
of the dataset is given by H52 [(0.5 log 20.5)1(0.5 log 20.5)]52 log2
0.552 (21)51. On the other hand, if the data can be partitioned into
two sets of 50 samples each that exclusively contain all members and all
nonmembers, the entropy of either of these two partitioned sets is given by
H52 1 log 2150. Any other proportion of samples within a dataset will
yield entropy values between 0 and 1 (which is the maximum). The Gini
index ( G) is similar to the entropy measure in its characteristics and is
defined as
G512Xm
k51p2
k ð4:2Þ
The value of Granges between 0 and a maximum value of 0.5, but otherwise
has properties identical to H, and either of these formulations can be used to
create partitions in the data ( Cover, 1991 ).
the golf example, introduced earlier, is used in this section to explain the
application of entropy concepts for creating a decision tree. This was the
same dataset used by J. Ross Quinlan to introduce one of the original deci-
sion tree algorithms, the Iterative Dichotomizer 3, or ID3 ( Quinlan, 1986 ). The
full data are shown in Table 4.1 .
There are essentially two questions that need to be answered at each step of
the tree building process: where to split the data and when to stop splitting .
Step 1: Where to Split Data?
There are 14 examples, with four attributes —Outlook, Temperature,
Humidity, and Wind. The target attribute that needs to be predicted is Play
with two classes: Yes and No. It ’s important to understand how to build a
decision tree using this simple dataset.68 CHAPTER 4: ClassificationStart by partitioning the data on each of the four regular attributes. Let us
start with Outlook. There are three categories for this variable: sunny, over-
cast, and rain. We see that when it is overcast, there are four examples where
the outcome was Play 5yes for all four cases (see Fig. 4.2 ) and so the pro-
portion of examples in this case is 100% or 1.0. Thus, if we split the dataset
here, the resulting four sample partition will be 100% pure for Play 5yes.
Mathematically for this partition, the entropy can be calculated using
Eq.(4.1) as:
Houtlook :overcast52 ð0=4Þlog2ð0=4Þ2ð4=4Þlog2ð4=4Þ50:0
Similarly, the entropy in the other two situations for Outlook can be
calculated:
Houtlook :sunny52 ð2=5Þlog2ð2=5Þ2ð3=5Þlog2ð3=5Þ50:971
Houtlook :rain52 ð3=5Þlog2ð3=5Þ2ð2=5Þlog2ð2=5Þ50:971
For the attribute on the whole, the total information I is calculated as the
weighted sum of these component entropies. There are four instances
of Outlook 5overcast, thus, the proportion for overcast is given by
poutlook:overcast 54/14. The other proportions (for Outlook 5sunny and
rain) are 5/14 each:
Ioutlook5Poutlook :overcast3Houtlook :overcast1Poutlook :sunny3Houtlook :sunny
1Poutlook :rain3Houtlook :rain
Ioutlook5ð4=14Þ301ð5=14Þ30:9711ð5=14Þ30:97150:693Table 4.1 The Classic Golf Dataset
Outlook Temperature Humidity Windy Play
Sunny 85 85 false no
Sunny 80 90 true no
Overcast 83 78 false yes
Rain 70 96 false yes
Rain 68 80 false yes
Rain 65 70 true no
Overcast 64 65 true yes
Sunny 72 95 false no
Sunny 69 70 false yes
Rain 75 80 false yes
Sunny 75 70 true yes
Overcast 72 90 true yes
Overcast 81 75 false yes
Rain 71 80 true no4.1 Decision Trees 69Had the data notbeen partitioned along the three values for Outlook, the
total information would have been simply the weighted average of the
respective entropies for the two classes whose overall proportions were 5/14
(Play5no) and 9/14 (Play 5yes):
Ioutlook ;no partition 52 ð5=14Þlog2ð5=14Þ2ð9=14Þlog2ð9=14Þ50:940
By creating these splits or partitions, some entropy has been reduced (and,
thus, some information has been gained). This is called, aptly enough, infor-
mation gain . In the case of Outlook, this is given simply by:
Ioutlook ;no partition 2Ioutlook50:94020:69350:247
Similar information gain values for the other three attributes can now be
computed, as shown in Table 4.2 .
For numeric variables, possible split points to examine are essentially
averages of available values. For example, the first potential split point for
Humidity could be Average [65,70], which is 67.5, the next potential split
point could be Average [70,75], which is 72.5, and so on. Similar logic can
FIGURE 4.2
Splitting the data on the Outlook attribute.70 CHAPTER 4: Classificationbe used for the other numeric attribute, Temperature. The algorithm com-
putes the information gain at each of these potential split points and chooses
the one which maximizes it. Another way to approach this would be to dis-
cretize the numerical ranges, for example, Temperature .5 80 could be con-
sidered “Hot,”between 70 and 79 “Mild, ”and less than 70 “Cool. ”
From Table 4.2 , it is clear that if the dataset is partitioned into three sets
along the three values of Outlook, the largest information gain would be
experienced. This gives the first node of the decision tree as shown in
Fig. 4.3 . As noted earlier, the terminal node for the Outlook 5overcast
branch consists of four samples, all of which belong to the class Play 5yes.
The other two branches contain a mix of classes. The Outlook 5rain branch
has three yes results and the Outlook 5sunny branch has three no results.
Thus, not all the final partitions are 100% homogenous. This means that the
same process can be applied for each of these subsets till purer results are
obtained. So, back to the first question once again —where to split the data?
Fortunately, this was already answered for when the information gain for all
attributes was computed. The other attributes, that yielded the highest gains,
are used. Following that logic, the Outlook 5sunny branch can be split along
Humidity (which yielded the second highest information gain) and the
Outlook 5rain branch can be split along Wind (which yielded the third high-
est gain). The fully grown tree shown in Fig. 4.4 does precisely that.
Step 2: When to Stop Splitting Data?
In real-world datasets, it is very unlikely that to get terminal nodes that are
100% homogeneous as was just seen in the golf dataset. In this case, the
algorithm would need to be instructed when to stop. There are several situa-
tions where the process can be terminated:
1.No attribute satisfies a minimum information gain threshold (such as
the one computed in Table 4.2 ).
2.A maximal depth is reached: as the tree grows larger, not only does
interpretation get harder, but a situation called “overfitting ”is induced.
3.There are less than a certain number of examples in the current
subtree: again, a mechanism to prevent overfitting .Table 4.2 Computing the Information Gain for All
Attributes
Attribute Information Gain
Temperature 0.029
Humidity 0.102
Wind 0.048
Outlook 0.2474.1 Decision Trees 71So, what exactly is overfitting? Overfitting occurs when a model tries to
memorize the training data instead of generalizing the relationship between
inputs and output variables. Overfitting often has the effect of performingwell on the training dataset but performing poorly on any new data previ-
ously unseen by the model. As mentioned, overfitting by a decision tree
results not only in difficulty interpreting the model, but also provides quite auseless model for unseen data. To prevent overfitting, tree growth may need
FIGURE 4.3
Splitting the Golf dataset on the Outlook attribute yields three subsets or branches. The middle and right
branches may be split further.
FIGURE 4.4
Decision Tree for the Golf data.72 CHAPTER 4: Classificationto be restricted or reduced, using a process called pruning . All three stopping
techniques mentioned constitute what is known of as pre-pruning the decision
tree, because the pruning occurs before or during the growth of the tree.There are also methods that will not restrict the number of branches and
allow the tree to grow as deep as the data will allow, and then trim or prune
those branches that do not effectively change the classification error rates.This is called post-pruning . Post-pruning may sometimes be a better option
because one will not miss any small but potentially significant relationships
between attribute values and classes if the tree is allowed to reach its maxi-mum depth. However, one drawback with post-pruning is that it requires
additional computations, which may be wasted when the tree needs to be
trimmed back.
Now the application of the decision tree algorithm can be summarized with
this simple five-step process:
1.Using Shannon entropy, sort the dataset into homogenous (by class)
and non-homogeneous variables. Homogeneous variables have
low information entropy and non-homogeneous variables havehigh information entropy. This was done in the calculation of
I
outlook, no partition .
2.Weight the influence of each independent variable on the target variable
using the entropy weighted averages (sometimes called joint entropy).
This was done during the calculation of Ioutlook in the example.
3.Compute the information gain, which is essentially the reduction in
the entropy of the target variable due to its relationship with each
independent variable. This is simply the difference between the
information entropy found in step 1 minus the joint entropycalculated in step 2. This was done during the calculation of
I
outlook, no partition 2Ioutlook .
4.The independent variable with the highest information gain will
become the root or the first node on which the dataset is divided. This
was done using the calculation of the information gain table.
5.Repeat this process for each variable for which the Shannon entropy is
nonzero. If the entropy of a variable is zero, then that variable
becomes a “leaf”node.
4.1.2 How to Implement
Before jumping into a business use case of decision trees, a simple decision
tree model will be implemented using the concepts discussed in the earlier
section. The first implementation gives an idea about key building blocks ina data science implementation process. The second implementation provides
a deep-dive into a business application. This is the first implementation of a4.1 Decision Trees 73data science technique, so some extra effort will be spent going into detail
on many of the preliminary steps and also on introducing several additional
tools and concepts that will be required throughout the rest of this chapter
and other chapters that focus on supervised learning methods. These are the
concepts of splitting data into testing and training samples and applying the
trained model on testing. It may also be useful to first review Section 15.1
(Introduction to the GUI) and Section 15.2 (Data Import and Export) from
Chapter 15, Getting started with RapidMiner before working through the rest
of this implementation. As a final note, the ways and means to improve the
performance of a classification model using RapidMiner will not be discussed
in this section, but this very important part of data science will be revisited
in several later chapters, particularly in the section on using optimization.
Implementation 1: To Play Golf or Not?
The complete RapidMiner process for implementing the decision tree model
discussed in the earlier section is shown in Fig. 4.5 . The key building blocks
for this process are: training dataset, test dataset, model building, predicting
using the model, predicted dataset, model representation, and performance
vector.
The decision tree process has two input datasets. The training dataset , shown
inTable 4.1 , is used to build the decision tree with default parameter
options. Fig. 4.6 shows the test dataset . The test dataset shares the same struc-
ture as the training dataset but with different records. These two operators
constitute the inputs to the data science process.
The modeling block builds the decision tree using the training dataset. The
Apply model block predicts the class label of the test dataset using the devel-
oped model and appends the predicted label to the dataset. The predicted
dataset is one of the three outputs of the process and is shown in Fig. 4.7 .
FIGURE 4.5
Building blocks of the Decision Tree process.74 CHAPTER 4: ClassificationFIGURE 4.6
Test data.
FIGURE 4.7
Results of applying the simple Decision Tree model.4.1 Decision Trees 75Note the prediction test dataset has both the predicted and the original class
label. The model has predicted correct class for nine of the records, but not
for all. The five incorrect predictions are highlighted in Fig. 4.7 .
The decision tree model developed using the training dataset is shown in
Fig. 4.8 . This is a simple decision tree with only three nodes. The leaf nodes
are pure with a clean split of data. In practical applications, the tree will have
dozens of nodes and the split will have mixed classes in the leaf nodes.
The performance evaluation block compares the predicted class label and the
original class label in the test dataset to compute the performance metrics
like accuracy, recall, etc. Fig. 4.9 shows the accuracy results of the model and
the confusion matrix. It is evident that the model has been able to get 9 of
the 14 class predictions correct and 5 of the 14 (in boxes) wrong, which
translates to about 64% accuracy.
FIGURE 4.8
Decision Tree for Golf dataset.
FIGURE 4.9
Performance vector.76 CHAPTER 4: ClassificationImplementation 2: Prospect Filtering
A more involved business application will be examined to better understand
how to apply decision trees for real-world problems. Credit scoring is a fairly
common data science problem. Some types of situations where credit scoring
could be applied are:
1.Prospect filtering: Identify which prospects to extend credit to and
determine how much credit would be an acceptable risk.
2.Default risk detection: Decide if a particular customer is likely to
default on a loan.
3.Bad debt collection: Sort out those debtors who will yield a good cost
(of collection) to benefit (of receiving payment) performance.
The German Credit dataset from the University of California-Irvine Machine
Learning (UCI-ML) data repository1will be used with RapidMiner to build a
decision tree for addressing a prospect filtering problem . There are four main
steps in setting up any supervised learning algorithm for a predictive model-
ing exercise:
1.Read in the cleaned and prepared data typically from a database or a
spreadsheet, but the data can be from any source.
2.Split data into training and testing samples.
3.Train the decision tree using the training portion of the dataset.
4.Apply the model on the testing portion of the dataset to evaluate the
performance of the model.
Step 1 may seem rather elementary but can confuse many beginners and,
thus, sometime will be spent explaining this in somewhat more detail.
Step 1: Data Preparation
The raw data is in the format shown in Table 4.3 . It consists of 1000 samples
and a total of 20 attributes and 1 label or target attribute. There are seven
numeric attributes and the rest are categorical or qualitative, including the
label, which is a binominal variable. The label attribute is called Credit
Rating and can take the value of 1 (good) or 2 (bad). In the data 70% of the
samples fall into the good credit rating class. The descriptions for the data
are shown in Table 4.3 . Most of the attributes are self-explanatory, but the
raw data has encodings for the values of the qualitative variables. For exam-
ple, attribute 4 is the purpose of the loan and can assume any of 10 values
(A40 for new car, A41 for used car, and so on). The full details of these
encodings are provided under the dataset description on the UCI-ML
website.
1http://archive.ics.uci.edu/ml/datasets/ All datasets used in this book are available at the companion
website.4.1 Decision Trees 77RapidMiner ’s easy interface allows quick importing of spreadsheets. A useful
feature of the interface is the panel on the left, called the Operators . Simply
typing in text in the box provided automatically pulls up all available
RapidMiner operators that match the text. In this case, an operator to needs
to read an Excel spreadsheet, and so one would simply type excel in the box.
Either double-click on the Read Excel operator or drag and drop it into the
Main Process panel —the effect is the same. Once the Read Excel operator
appears in the main process window as shown in Fig. 4.10 , the data import
process needs to be configured. What this means is telling RapidMiner which
columns to import, what is contained in the columns, and if any of the col-
umns need special treatment.
This is probably the most cumbersome part about this step. RapidMiner has
a feature to automatically detect the type of values in each attribute (Guess
Value types). But it is a good exercise for the analyst to make sure that the
right columns are picked (or excluded) and the value types are correctly
guessed. If not, as seen in Fig. 4.11 , the value type can be changed to the cor-
rect setting by clicking on the button below the attribute name.
Once the data is imported, the target variable must be assigned for analysis,
also known as a label. In this case, it is the Credit Rating. Finally, it is a good
idea to run RapidMiner and generate results to ensure that all columns are
read correctly.
An optional step is to convert the values from A121, A143, etc., to more
meaningful qualitative descriptions. This is accomplished by the use ofTable 4.3 A View of the Raw German Credit Data
Checking
Account
StatusDuration
in MonthsCredit
History PurposeCredit
AmountSavings
Account/
BondsPresent
Employment
sinceCredit
Rating
A11 6 A34 A43 1169 A65 A75 1
A12 48 A32 A43 5951 A61 A73 2
A14 12 A34 A46 2096 A61 A74 1
A11 42 A32 A42 7882 A61 A74 1
A11 24 A33 A40 4870 A61 A73 2
A14 36 A32 A46 9055 A65 A73 1
A14 24 A32 A42 2835 A63 A75 1
A12 36 A32 A41 6948 A61 A73 1
A14 12 A32 A43 3059 A64 A74 1
A12 30 A34 A40 5234 A61 A71 2
A12 12 A32 A40 1295 A61 A72 2
A11 48 A32 A49 4308 A61 A72 278 CHAPTER 4: Classificationanother operator called Replace (Dictionary) , which will replace the values
with bland encodings such as A121 and so on with more descriptive values.
A dictionary will need to be created and supplied to RapidMiner as a
comma-separated value (csv) file to enable this. Such a dictionary is easy to
create and is shown in Fig. 4.12 ; Note that RapidMiner needs to be informed
which column in the dictionary contains old values and which contain new
values.
The last pre-processing step shown here is converting the numeric label into
a binomial one by connecting the example output of Replace (Dictionary) to a
Numerical to Binominal operator. To configure the Numerical to Binominal
operator.
Finally, change the name of the label variable from Credit Rating to Credit
Rating5Good so that it makes more sense when the integer values get con-
verted to true or false after passing through the Numerical to Binomial opera-
tor. This can be done using the Rename operator. When this setup is run, the
dataset shown in Fig. 4.13 will be generated. Comparing to Fig. 4.11 , see
FIGURE 4.10
Using the Read Excel operator.
FIGURE 4.11
Verifying data read-in and adjusting attribute value types if necessary.4.1 Decision Trees 79that the label attribute is the first one shown and the values are trueorfalse.
The statistics tab of the results can be examined to get more informationabout the distributions of individual attributes and also to check for missing
values and outliers. In other words, one must make sure that the data prepa-
ration step is properly executed before proceeding. In this implementation,there is no to worry about this because the dataset is relatively clean (for
FIGURE 4.12
Attribute value replacement using a dictionary.
FIGURE 4.13
Data transformed for Decision Tree analysis.80 CHAPTER 4: Classificationinstance, there are no missing values), and one could proceed directly to the
model development phase.
Step 2: Divide dataset Into Training and Testing Samples
As with all supervised model building, data must be separated into two sets:
one for training or developing an acceptable model, and the other for testing
or ensuring that the model would work equally well on a different dataset.
The standard practice is to split the available data into a training set and a
testing set. Typically, the training set contains 70% /C090% of the original data.
The remainder is set aside for testing. The Split Validation operator sets up
splitting, modeling, andthe validation check in one operator. Choose strati-
fied sampling with a split ratio of 0.9 (90% training). Stratified sampling2will
ensure that both training and testing samples have equal distributions of
class values. The final sub step here is to connect the output from the
Numerical to Binominal operator output to the Split Validation operator input
(see Fig. 4.14 ).
Step 3: Modeling Operator and Parameters
A demonstration of how to build a decision tree model on this data will
now be given. The Validation operator allows one to build a model and apply
it on validation data in the same step. This means that two operations —
model building and model evaluation —must be configured using the same
operator. This is accomplished by double-clicking on the Validation operator,
which is what is called a nested operator. When this operator is opened, note
that there are two parts inside (see Fig. 4.15 ). The left box is where the
Decision Tree operator has to be placed and the model will be built using the
90% of training data samples. The right box is for applying this trained
model on the remaining 10% of the testing data samples using the Apply
Model operator and evaluating the performance of the model using the
Performance operator.
Step 4: Configuring the Decision Tree Model
The main parameters to pay attention to are the Criterion pull-down menu
and the minimal gain box. This is essentially a partitioning criterion and
offers information gain, Gini index, and gain ratio as choices. The first two
criteria were covered earlier, and gain ratio will be briefly explained in the
next section.
As discussed earlier in this chapter, decision trees are built up in a simple
five-step process by increasing the information contained in the reduced
2Although not necessary, it is sometimes useful to check the use local random seed option, so that it is
possible to compare models between different iterations. Fixing the random seed ensures that the same
examples are chosen for training (and testing) subsets each time the process is run.4.1 Decision Trees 81FIGURE 4.14
Decision Tree process.
FIGURE 4.15
Setting up the split validation process.82 CHAPTER 4: Classificationdataset following each split. Data by its nature contains uncertainties.
Uncertainties could possibly be systematically reduced, thus, increasing infor-
mation by activities like sorting or classifying. When data have been sortedor classified to achieve the greatest reduction in uncertainty, basically, the
greatest increase in information has been achieved. It has already been
shown that entropy is a good measure of uncertainty and how keeping trackof it allows information to be quantified. So, back to the options that are
available within RapidMiner for splitting decision trees:
1.Information gain: This is computed as the information before the split
minus the information after the split. It works fine for most cases,
unless there are a few variables that have a large number of values (orclasses). Information gain is biased toward choosing attributes with a
large number of values as root nodes. This is not a problem, except in
extreme cases. For example, each customer ID is unique and, thus, thevariable has too many values (each ID is a unique value). A tree that is
split along these lines has no predictive value.
2.Gain ratio (default): This is a modification of information gain that
reduces its bias and is usually the best option. Gain ratio overcomes
the problem with information gain by taking into account the number
of branches that would result before making the split. It correctsinformation gain by taking the intrinsic information of a split into
account. Intrinsic information can be explained using the golf example.
Suppose each of the 14 examples had a unique ID attribute associatedwith them. Then the intrinsic information for the ID attribute is given
by 143(21/143log (1/14)) 53.807. The gain ratio is obtained by
dividing the information gain for an attribute by its intrinsicinformation. Clearly attributes that have high intrinsic information
(high uncertainty) tend to offer low gains upon splitting and, hence,
would not be preferred in the selection process.
3.Gini index: This is also used sometimes but does not have too many
advantages over gain ratio.
4.Accuracy: This is also used to improve performance. The best way to
select values for these parameters is by using many of the optimizing
operators.
The other important parameter is the minimal gain value. Theoretically this
can take any range from 0 upwards. The default is 0.1. It has been set as 0.01
for this example.
The other parameters minimal size for a split ,minimal leaf size ,maximal depth
are determined by the size of the dataset. In this case, the values have been
set as 4,5, and 5 respectively. The model is ready for training. Next, twomore operators are added, Apply Model and Performance (Binominal4.1 Decision Trees 83Classification) , and the analysis is ready to be run. Configure the Performance
(Binominal Classification) operator by selecting the accuracy , area under ROC
(receiver operator characteristic) curve ( AUC ),precision , and recall options.3
Remember to connect the ports correctly as this can be a source of
confusion:
1.“mod ”el port of the Testing window to “mod ”onApply Model
2.“tes”ting port of the Testing window to “unl”abeled on Apply Model
3.“lab”eled port of Apply Model to“lab”eled on Performance
4.“per”formance port on the Performance operator to “ave”rageable port
on the output side of the testing box
The final step before running the model is to go back to the main process
view (see Fig. 4.15 ) and connect the output ports model and “ave”of the
Validation operator to the main process outputs.
Step 5: Process Execution and Interpretation
When the model is setup and run as explained, RapidMiner generates two
tabs in the Results perspective. The Performance Vector (Performance) tab
shows a confusion matrix that lists the model accuracy on the testing data,
along with the other options selected above for the Performance (Binominal
Classification) operator in step 3. The Tree (Decision Tree) tab shows a
graphic of the tree that was built on the training data (see Fig. 4.16 ).
Fig. 4.17 shows the tree model in the form of rules. Several important points
must be highlighted before the performance of this model is discussed:
1.The root node —Checking Account Status —is the most important
predictor in the dataset.
2.If the Checking Account Status 5No account , a prediction can be made
without the influence of other attributes.
3.For rest of the Checking Account Status values, other parameters come
into effect and play an increasingly important role in deciding if
someone is likely to have a “good”or“bad”credit rating.
4.Watch out for overfitting. Overfitting refers to the process of building a
model specific to the training data that achieves close to full accuracy
on the training data. However, when this model is applied to new data
or if the training data changes somewhat, then there is a significant
degradation in its performance. Overfitting is a potential issue with all
supervised models, not just decision trees. One way this situation
could be avoided is by changing the decision tree criterion “Minimal
leaf size ”to something like 10. But doing so, the classification influence
of all the other parameters is also lost, except the root node.
3Performance criteria such as these are explained in more detail in Chapter 8, Model Evaluation.84 CHAPTER 4: ClassificationFIGURE 4.16
Decision Tree.
FIGURE 4.17
Decision Tree rules.4.1 Decision Trees 85Now look at the Performance result. As seen in Fig. 4.18 , the model ’s overall
accuracy on the testing data is 67%. The model has a class recall of 92.86%
for the “true”class implying that it is able to pick out customers with good
credit rating with good accuracy. However, its class recall for the “false”class
is an abysmal 6.67%! That is, the model can only pick out a potential
defaulter in 1 out of 15 cases!
One way to improve this performance is by penalizing false negatives by
applying a cost for every such instance. This is handled by another operator
called MetaCost , which is described in detail in the next chapter on logistic
regression. When a parameter search optimization is performed by iterating
through three of the decision tree parameters, splitting criterion, minimum
gain ratio, and maximal tree depth, significantly improved performance is
gained. More details on how to set this type of optimization are provided in
Chapter 15, Getting started with RapidMiner.
In addition to assessing the model ’s performance by aggregate measures such
as accuracy, one can also use gain/lift charts, ROC charts, and AUC charts.
An explanation of how these charts are constructed and interpreted is given
in Chapter 8, Model Evaluation.
4.1.3 Conclusion
Decision trees are one of the most commonly used predictive modeling algo-
rithms in practice. The reasons for this are numerous. Some of the distinct
advantages of using decision trees in many classification and prediction
applications will be explained below along with some common pitfalls.
1.Easy to interpret and explain to non-technical users: As seen in the few
examples discussed so far, decision trees are intuitive and easy to
explain to non-technical people, who are typically the consumers of
analytics.
FIGURE 4.18
Performance vector.
The RapidMiner process for a decision tree covered in the
implementation section can be accessed from the com-
panion site of the book at www.IntroDataScience.com . The
RapidMiner process ( T.rmp files) can be downloadedto one ’s computer and imported to RapidMiner through
File.Import Process. Additionally, all the datasets used
in this book can be downloaded from http://www.
IntroDataScience.com .86 CHAPTER 4: Classification2.Decision trees require relatively little effort from users for data
preparation: If one has a dataset consisting of widely ranging attributes,
for example, revenues recorded in millions and loan age recorded inyears, many algorithms require scale normalization before model
building and application. Such variable transformations are not
required with decision trees because the tree structure will remain thesame with or without the transformation. Another feature that saves
data preparation time: missing values in training data will not impede
partitioning the data for building trees. Decision trees are also notsensitive to outliers since the partitioning happens based on the
proportion of samples within the split ranges and not on absolute
values.
3.Nonlinear relationships between parameters do not affect tree
performance. As described in Chapter 5, Regression Methods, highly
nonlinear relationships between variables will result in failing checksfor simple regression models, and thus, rendering such models invalid.
However, decision trees do not require any assumptions of linearity in
the data. Thus, one can use them in scenarios where one knows theparameters are nonlinearly related.
4.Decision trees implicitly perform variable screening or featureselection. When a decision tree is fitted to a training dataset, the topfew nodes on which the tree is split are essentially the most important
variables within the dataset and feature selection is completed
automatically. In fact, RapidMiner has an operator for performingvariable screening or feature selection using the information gain ratio.
In Chapter 12, Time Series Forecasting, the importance of feature
selection in data science will be discussed. A few common techniquesfor performing feature selection or variable screening will be
introduced in that chapter.
However, all these advantages need to be tempered with the one key disad-
vantage of decision trees: without proper pruning or limiting tree growth,
they tend to overfit the training data, making them somewhat poorpredictors.
4.2 RULE INDUCTION
Rule induction is a data science process of deducing if-then rules from a data-set. These symbolic decision rules explain an inherent relationship between
the attributes and class labels in a dataset. Many real-life experiences are
based on intuitive rule induction. For example, one could come up with arule that states that “if it is 8:00 a.m. on a weekday, then highway traffic will
be heavy ”and “if it is 8:00 p.m. on a Sunday, then the traffic will be light. ”4.2 Rule Induction 87These rules are not necessarily right all the time. The 8:00 a.m. weekday traf-
fic may be light during a holiday season. But, in general, these rules hold
true and are deduced from real-life experience based on our everyday obser-
vations. The rule induction provides a powerful classification approach that
can be easily understood by the general audience. Apart from its use in data
science by classification of unknown data, rule induction is also used to
describe the patterns in the data. The description is in the form of simple if-
then rules that can be easily understood by general users.
The easiest way to extract rules from a dataset is from a decision tree that
is developed on the same dataset. A d ecision tree splits data on every
node and leads to the leaf where the class is identified. If one traces back
from the leaf to the root node, they can combine all the split conditions
to form a distinct rule. For example, in the Golf dataset ( Table 4.1 ), based
on four weather conditions, a rule set can be generalized to determine
when a player prefers to play golf or not. Fig. 4.19 shows the decision tree
developed from the Golf data with five leaf nodes and two levels. A rule
can be extracted if one traces back the first leaf from the left: If Outlook is
overcast, then Play 5yes. Similarly, rules can be extracted from all the five
leaves:
Rule 1: if (Outlook 5overcast) then Play 5yes
Rule 2: if (Outlook 5rain) and (Wind 5false) then Play 5yes
Rule 3: if (Outlook 5rain) and (Wind 5true) then Play 5no
Rule 4: if (Outlook 5sunny) and (Humidity .77.5) then Play 5no
Rule 5: if (Outlook 5sunny) and (Humidity #77.5) then Play 5yes
Outlook
Windovercast
rain
false
true> 77.500≤ 77.500sunny
Humidityyes
yesyes
no
no
FIGURE 4.19
Decision tree model for Golf dataset88 CHAPTER 4: ClassificationThe set of all the five rules is called a rule set . Each individual rule riis called
adisjunct or classification rule. The entire rule set can be represented as:
R5fr1-r2-r3- ...rkg
where kis the number of disjuncts in a rule set. Individual disjuncts can be
represented as:
ri5ðantecedent or condition Þthen ðconsequent Þ
For example, Rule 2 is r2: if (Outlook 5rain) and (Wind 5false) then
Play5yes.
In r 2, (Outlook 5rain) and (Wind 5false) is the antecedent orcondition of the
rule. The antecedent of the rule can have many attributes and values each
separated by a logical AND operator. Each attribute and value test is called
theconjunct of the rule. An example of a conjunct is (Outlook 5rain). The
antecedent is a group of conjuncts with the AND operator. Each conjunct is a
node in the equivalent decision tree.
In the Golf dataset, one can observe a couple of properties of the rule set in
relation with the dataset. First, the rule set R is mutually exclusive. This
means that no example record will trigger more than one rule and hence the
outcome of the prediction is definite. However, there can be rule sets that are
not mutually exclusive. If a record activates more than one rule in a rule set
and all the class predictions are the same, then there is no conflict in the pre-
diction. If the class predictions differ, ambiguity exists on which class is the
prediction of the induction rule model. There are a couple of techniques
used to resolve conflicting class prediction by more than one rule. One tech-
nique is to develop an ordered list of rules where if a record activates many
rules, the first rule in the order will take precedence. A second technique is
where each active rule can “vote”for a prediction class. The predicted class
with the highest vote is the prediction of the rule set R. The rule set discussed
is also exhaustive. This means the rule set R is activated for all the combina-
tions of the attribute values in the record set, not just limited to training
records. If the rule set is not exhaustive, then a final catch all bucket rule
“else Class 5Default Class Value ”can be introduced to make the rule set
exhaustive.
Approaches to Developing a Rule Set
Rules can be directly extracted from the dataset or derived from previously
built decision trees from the same dataset. Fig. 4.20 shows the approaches to
generate rules from the dataset. The former approach is called the direct
method, which is built on leveraging the relationship between the attribute
and class label in the dataset. Deriving a rule set from a previously built clas-
sifier decision tree model is a passive or indirect approach. Since building a4.2 Rule Induction 89decision tree is covered in the previous section and the derivation of rules
from the decision tree model is straightforward, the rest of the discussion
will be focused on direct rule generation based on the relationships from the
data. Specifically, the focus will be placed on the sequential covering technique
used to build the rule set.
FIGURE 4.20
Approaches to rule generation
PREDICTING AND PREVENTING MACHINE BREAKDOWNS
A machine breakdown in the field almost always results in
the disruption of a manufacturing process. In a large-
scale process like an oil refinery, chemical plants, etc., it
causes serious financial damage to the company and
manufacturers of the machines. Rather than waiting for
the machine to breakdown and react, it is much prefera-
ble to diagnose the problem and prevent the breakdown
before a problem occurs. Industrial operations track thou-
sands of real-time readings from multiple systems or
sub-systems. (Such machines connected to networks that
can gather readings and act based on smart logic or sta-
tistical learning constitute the Internet of Things.) One of
the solutions is to leverage how these readings are trend-
ing and develop a rule base which says, for example, if the
cylinder temperature continues to report more than 852/C14C,
then the machine will breakdown in the near future . These
types of the rules are simple to interpret, do not requirean expert to be around to take further action, and can be
deployed by automated systems.
Developing learned rules requires historical analysis of all
the readings that lead up to a machine failure ( Langley &
Simon, 1995 ). These learned rules are different, and in
many cases, supersede the rule of thumb assumed by the
machine operators. Based on the historic readings of fail-
ure and non-failure events, the learned rule set can pre-
dict the failure of the machine and, hence, alert the
operator of imminent future breakdowns. Since these
rules are simple to understand, these preventive mea-
sures can be easily deployed to line workers. This use
case demonstrates the need of not only a predictive data
model, but also a descriptive model where the inner work-
ing of the model can be easily understood by the users. A
similar approach can be developed to prevent customer
churn, or loan default, for example.90 CHAPTER 4: Classification4.2.1 How It Works
Sequential covering is an iterative procedure of extracting rules from a data-
set. The sequential covering approach attempts to find all the rules in the
dataset class by class. One specific implementation of the sequential covering
approach is called the RIPPER, which stands for Repeated Incremental
Pruning to Produce Error Reduction ( Cohen, 1995 ). Consider the dataset
shown in Fig. 4.21 , which has two attributes (dimensions) on the Xand Y
axis and two-class labels marked by “1”and “2.”The steps in sequential
covering rules generation approach are provided here ( Tan, Michael, &
Kumar, 2005 ).
Step 1: Class Selection
The algorithm starts with the selection of class labels one by one. The rule set
is class-ordered where all the rules for a class are developed before moving
on to the next class. The first class is usually the least-frequent class label.
From Fig. 4.21 , the least-frequent class is “1”and the algorithm focuses on
generating all the rules for “1”class.
Step 2: Rule Development
The first rule r1will need to be developed. The objective in this step is to
cover all “1”data points using a rectilinear box with none or as few “2”as
possible. For example, in Fig. 4.22 , rule r1identifies the area of four “1”in
FIGURE 4.21
Dataset with two classes and two dimensions.4.2 Rule Induction 91the top left corner. Since this rule is based on simple logic operators in con-
juncts, the boundary is rectilinear. Once rule r1is formed, the entire data
points covered by r1are eliminated and the next best rule is found from the
datasets. The algorithm grows in a greedy fashion using a technique calledLearn-One-Rule which is described in the next section. One of the conse-
quences of the greedy algorithms that start with initial configuration is that
they yield local optima instead of a global optimum . A local optimum is a solu-
tion that is optimal in the neighborhood of potential solutions, but worse
than the global optimum.
Step 3: Learn-One-Rule
Each disjunct rule riis grown by the learn-one-rule approach. Each rule starts
with an empty rule set and conjuncts are added one by one to increase therule accuracy. Rule accuracy is the ratio of the number of “1”covered by
the rule to all records covered by the rule:
Rule accuracy AðriÞ5Correct records covered by rule
All records covered by the rule
Learn-one-rule starts with an empty rule set: if {} then class 5“1.”
Obviously, the accuracy of this rule is the same as the proportion of “1”
data points in the dataset. Then the algorithm greedily adds conjuncts until
the accuracy reaches 100%. If the addition of a conjunct decreases the
FIGURE 4.22
Generation of rule r1.92 CHAPTER 4: Classificationaccuracy, then the algorithm looks for other conjuncts or stops and starts the
iteration of the next rule.
Step 4: Next Rule
After a rule is developed, then all the data points covered by the rule are
eliminated from the dataset. All these steps are repeated for the next rule to
cover the rest of the “1”data points. In Fig. 4.23 , rule r2is developed after
the data points covered by r1are eliminated.
Step 5: Development of Rule Set
After the rule set is developed to identify all “1”data points, the rule model
is evaluated with a test dataset used for pruning to reduce generalization
errors. The metric used to evaluate the need for pruning is ( p2n)/(p1n),
where pis the number of positive records covered by the rule and nis the
number of negative records covered by the rule. The conjunct is iteratively
removed if it improves the metric. All rules that identify “1”data points are
aggregated to form a rule group. In multi-class problems, the previous steps
are repeated with for next class label. Since this is a two-class problem, any
data points not covered by the rule set for identifying “1”are predicted to
be“2.”The outcome of the sequential covering or RIPPER algorithm is a
set of optimized rules that can describe the relationship between attributes
and the class label ( Saian & Ku-Mahamud, 2011 ).
FIGURE 4.23
Elimination of r1data points and next rule.4.2 Rule Induction 934.2.2 How to Implement
Rules remain the most common expression to communicate the inherent
relationship in the data. There are a few different ways to generate rules fromthe data using RapidMiner. The modeling operators for rule induction are
available in the Modeling .Classification and Regression .Rule Induction
folder. These modeling operators available:
1.Rule Induction: Commonly used generic rule induction modeler based
on the RIPPER algorithm.
2.Single Rule Induction (Attribute): Uses only one attribute in antecedent,
usually the attribute with the most predictive power.
3.Single Rule Induction: Generates only one rule with an if/else statement.
4.Tree to Rule: Indirect method of rule generation that is based on
underlying decision tree.
Single rule induction is used for quick discovery of the most dominant rule.
Because of its simplicity, single rule modeling operators are used to establish
a baseline performance for other classification models. The implementation
will be reviewed using the Rule Induction and Tree to Rule modeling operators
in RapidMiner.
Step 1: Data Preparation
The dataset used in the implementation is the standard Iris dataset (SeeTable 3.1 and Fig. 3.1) with four attributes, sepal length, sepal width, petallength, and petal width, and a class label to identify the species of flower,
viz., Iris setosa ,Iris versicolor, and Iris virginica . The Iris dataset is available in
the RapidMiner repository under Sample .Data. Since the original dataset
refers to the four attributes as a1 to a4, the Rename operator was used to
change the name of the attributes (not values) so they can be more descrip-
tive. The Rename operator is available in Data Transformation .Name and
Role modification. Similar to a decision tree, rule induction can accept
both numeric and polynomial data types. The Iris dataset is split into twoequal sets for training and testing, using the Split Data operator (Data
Transformation .Filtering .Sampling). The split ratio used in this imple-
mentation is 50% /C050% for training and test data.
Step 2: Modeling Operator and Parameters
The Rule Induction modeling operator accepts the training data and pro-
vides the rule set as the model output. The rule set is the text output of if-
then rules, along with the accuracy and coverage statistics. These parameters94 CHAPTER 4: Classificationare available in the model operator and can be configured for desired
modeling behavior:
1.Criterion: Since the algorithm takes the greedy strategy, it needs an
evaluation criterion to indicate whether adding a new conjunct helps
in a rule. Information gain is commonly used for RIPPER and is
similar to information gain for decision trees. Another easy-to-use
criterion is accuracy, which was discussed in the sequential covering
algorithm.
2.Sample ratio: This is the ratio of data used for training in the example
set. The rest of the data is used for pruning. This sample ratio is
different from the training/test split ratio that is used in the data
preparation stage.
3.Pureness: This is the minimal ratio of accuracy desired in the
classification rule.
4.Minimal prune benefit: This is the percentage increase in the prune
metric required at the minimum.
The output of the model is connected to the Apply Model operator to apply
the developed rule base against the test dataset. The test dataset from the
Split Data operator is connected to the Apply Model operator. The
Performance operator for classification is then used to create the perfor-
mance vector from the labeled dataset generated from the Apply Model
operator. The process can be saved and executed after the output ports are
c o n n e c t e dt ot h er e s u l tp o r t s . Fig. 4.24 shows the complete RapidMiner
process for rule induction. The completed process and the dataset can be
downloaded from the companion website of the book at: www.
IntroDataScience.com .
Step 3: Results Interpretation
The results screen consists of the Rule Model window, the labeled test data-
set, and the performance vector. The performance vector is similar to the
decision tree performance vector. The Rule Model window, shown in
Fig. 4.25 , consists of a sequence of if-then rules with antecedents and conse-
quents. The parentheses next to each classification rule indicate the class dis-
tribution of the rule covered from the training dataset. Note that these
statistics are based on the training dataset, not the test dataset.
The Performance Vector window provides the accuracy statistics of the pre-
diction based on the rules model applied to the test dataset. For the Iris
dataset and the RapidMiner process shown in this example, the accuracy of4.2 Rule Induction 95prediction is 92%. 69 out of 75 test records are predicted accurately based
on simple rules developed by the rule induction model. Not bad for a
quick, easy-to-use and easy-to-understand model!
Alternative Approach: Tree-to-Rules
An indirect but easy way to generate a mutually exclusive and exhaustive ruleset is to convert a decision tree to an induction rule set. Each classification
FIGURE 4.24
RapidMiner process for rule induction.
FIGURE 4.25
Rule output for rule induction.96 CHAPTER 4: Classificationrule can be traced from the leaf node to the root node, where each node
becomes a conjunct and the class label of the leaf becomes the consequent.
Even though tree-to-rules may be simple to implement, the resulting rule set
may not be the most optimal to understand, as there are many repetitive
nodes in the rule path.
In the rule induction process developed, the previous Rule Induction operator
can simply be replaced with the Tree to Rules operator. This modeling opera-
tor does not have any parameters as it simply converts the tree to rules.
However, the decision tree will have to be specified in the inner sub-process
of the Tree to Rules operator. On double-clicking the Tree to Rules operator,
the inner process can be seen, where a Decision Tree modeling operator has
to be inserted as shown in Figs. 4.26 and 4.27 .
FIGURE 4.26
RapidMiner process for Tree to Rules operator.
FIGURE 4.27
Decision Tree operator inside the sub-process for Tree to Rules.4.2 Rule Induction 97The parameters for a decision tree are the same as reviewed in Section 4.1 of
this chapter. The RapidMiner process can be saved and executed. The result
set consists of a set of rule model, usually with repetitive conjuncts in antece-
dents, a fingerprint of rules derived from trees. Note the difference between
the rules that are developed for the Rule Induction operator and the rules
developed from Tree to Rules operator. The rules generated from Tree to Rules
are shown in Fig. 4.28 .
4.2.3 Conclusion
Classification using rules provides a simple framework to identify a relation-
ship between attributes and the class label that is not only used as a predictive
model, but also a descriptive model. Rules are closely associated to decision
trees. They split the data space in a rectilinear fashion and generate a mutually
exclusive and exhaustive rule set. When the rule set is not mutually exclusive,
then the data space can be divided by complex and curved decision bound-
aries. Single rule learners are the simplest form of data science model and indi-
cate the most powerful predictor in the given set of attributes. Since rule
induction is a greedy algorithm, the result may not be the most globally opti-
mal solution and like decision trees, rules can overlearn the example set. This
scenario can be mitigated by pruning. Given the wide reach of rules, rule
induction is commonly used as a tool to express the results of data science,
even if other data science algorithms are used to create the model.
4.3 k-NEAREST NEIGHBORS
The predictive data science using decision trees and rule induction techniques
were built by generalizing the relationship within a dataset and using it to
predict the outcome of new unseen data. If one needs to predict the loan
FIGURE 4.28
Rules based on Decision Tree.98 CHAPTER 4: Classificationinterest rate based on credit score, income level, and loan amount,
one approach is to develop a mathematical relationship such as an equation
y5f(X) based on the known data and then using the equation to predict the
interest rate for new unseen data points. These approaches are called eager
learners because they attempt to find the best approximation of the actual
relationship between the input and target variables. But there is also a simple
alternative to this approach. One can “predict ”the interest rate for a poten-
tial borrower with a known credit score, income level, and loan amount by
looking up the interest rate of other customer loan records with a similar
credit score, a closely matching income level and loan amount from the
training dataset. This alternative class of learners adopts a blunt approach,
where no “learning ”is performed from the training dataset; rather the train-
ing dataset is used as a lookup table to match the input variables and find
the outcome. These approaches that memorize the training set are called lazy
learners .
The underlying idea here is somewhat similar to the old adage, “birds of a
feather flock together. ”Similar records congregate in a neighborhood in
n-dimensional space, with the same target class labels. This is the central
logic behind the approach used by the k-nearest neighbor algorithm, or
simply referred k-NN. The entire training dataset is “memorized ”and when
unlabeled example records need to be cla ssified, the input attributes of the
new unlabeled records are compared against the entire training set to find
the closest match. The class label of the closest training record is the pre-
dicted class label for the unseen test record. This is a nonparametric
method, where no generalization or att empt to find the distribution of the
dataset is made ( Altman, 1992 ). Once the training records are in memory,
the classification of the test record is straightforward. The closest training
record needs to be found for each test record. Even though no mathematical
generalization or rule generation is in volved, finding the closet training
record for a new unlabeled record can b e a tricky problem to solve, particu-
larly when there is no exact match of training data available for a given test
data record.
PREDICTING THE TYPE OF FOREST
Satellite imaging and digit al image processing have
provided a wealth of data about almost every part of
the earth ’s landscape. There is a strong motivation for
forestry departments, government agencies, universi-
ties, and research bodies to understand the makeup
of forests, species of trees and their health, biodiver-
sity, density, and forest condition. Field studies fordeveloping forest databases a nd classification projects
are quite tedious and expensive tasks. However, this
process can be aided with the leveraging of satellite
imagery, limited field data, elevation models, aerial
photographs, and survey data ( McInerney, 2005 ). The
objective is to classify whether the particular
(Continued )4.3 k-Nearest Neighbors 994.3.1 How It Works
Any record in a dataset can be visualized as a point in an n-dimensional
space, where nis the number of attributes. While it is hard for us to visu-
alize in more than three dimensions, mathematical functions are scalable
to any dimension and, hence, all the operations that can be done in two-
dimensional spaces and be performed in n-dimensional space. Consider
the standard Iris dataset (150 exampl e s ,f o u ra t t r i b u t e s ,o n ec l a s sl a b e l .
See Fig. 3.1 and Table 3.1) and focus on two petal attributes, petal length
and petal width. The scatterplot of these two dimensions is shown in
Fig. 4.29 . The colors indicate the species of Iris, the target class variable.
F o ra nu n s e e nt e s tr e c o r dA ,w i t h( p e t a ll e n g t h ,p e t a lw i d t h )v a l u e s( 2 . 1 ,
0.5), one can visually deduce that the predicted species for the values of
data point A would be I. setosa . This is based on the fact that test data
point A is in the neighborhood of other data points that belong to the
species I. setosa . Similarly, unseen test data point B has values (5.7, 1.9)
and is in the neighborhood of I. virginica ,h e n c e ,t h et e s td a t ap o i n tc a n
be classified as I. virginica . However, if the data points are in between the
boundaries of two species, for data points such as (5.0, 1.8), then the clas-
sification can be tricky because the neighborhood has more than one spe-
cies in the vicinity. An efficient algorithm is needed in order to resolve
these corner cases and measure the nearness of data points with more
than two dimensions. One technique is to find the nearest training data
point from an unseen test data point in multi-dimensional space. That is
how the k-NN algorithm works.
The kin the k-NN algorithm indicates the number of close training record(s)
that need to be considered when making the prediction for an unlabeled test
record. When k51, the model tries to find the firstnearest record and adopts(Continued )
landscape is a forest or not and further predict the
type of trees and species.
The approach to classifying a landscape involves dividing
the area into land units (e.g., a pixel in a satellite image)
and creating a vector of all the measurements for the
land unit. Each unit ’s measurements are then compared
against the measurements of known preclassified units.
For every new unclassified pixel, one can find a pixel in
the pre-classified catalog, which has measurementssimilar to the measurement of the pixel to be predicted.
Say the pre-classified pixel with the closest measurement
corresponds to birch trees. Thus, the pixel area can be
predicted to be a birch forest. Each pixel ’s measurement
is compared to measurements of the preclassified dataset
to determine the like pixels and, hence, the same forest
types. This is the core concept of the k-NN algorithm that
is used to classify landscape areas ( Haapanen, Lehtinen,
Miettinen, Bauer, & Ek, 2001 ).100 CHAPTER 4: Classificationthe class label of the first nearest training record as the predicted target class
value. Fig. 4.30 provides an example a training set with two dimensions and
the target class values as circles and triangles. The unlabeled test record is the
dark square in the center of the scatterplot. When k51, the predicted target
class value of an unlabeled test record is triangle because the closest training
record is a triangle. But, what if the closest training record is an outlier with
the incorrect class in the training set? Then, all the unlabeled test records
near the outlier will get wrongly classified. To prevent this misclassification,
the value of kcan be increased to, say, 3. When k53, the nearest three train-
ing records are considered instead of one. From Fig. 4.30 , based on the
majority class of the nearest three training records, the predicted class of the
test record can be concluded as circle . Since the class of the target record is
evaluated by voting, kis usually assigned an odd number for a two-class
problem ( Peterson, 2009 ).
Iris-setosa Iris-versicolor Iris-virginica
FIGURE 4.29
Two-dimensional plot of Iris dataset: petal length and petal width. Classes are stratified with colors.4.3 k-Nearest Neighbors 101The key task in the k-NN algorithm is determination of the nearest training
record from the unlabeled test record using a measure of proximity. Once
the nearest training record(s) are determined, the subsequent class voting of
the nearest training records is straightforward. The various techniques used to
measure proximity are discussed here.
Measure of Proximity
The effectiveness of the k-NN algorithm hinges on the determination of how
similar or dissimilar a test record is when compared with the memorized
training record. A measure of proximity between two records is a measure of
the proximity of its attributes. To quantify similarity between two records,
there is a range of techniques available such as calculating distance, correla-
tion, Jaccard similarity, and cosine similarity ( Tan et al., 2005 ).
Distance
The distance between two points X(x1,x2) and Y(y1,y2) in two-dimensional
space can be calculated by Eq. (4.3) :
Distance d5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx12y1Þ21ðx22y2Þ2q
ð4:3Þ
One can generalize the two-dimensional distance formula shown
inEq. (4.3) for datasets with nattributes, where Xis (x1,x2,...,xn) and Yis
(y1,y2,...,yn), as:
Distance d5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx12y1Þ21ðx22y2Þ21?1ðxn2ynÞ2q
ð4:4Þ
For example, the first two records of a four-dimensional Iris dataset is X5
(4.9, 3.0, 1.4, 0.2) and Y5(4.6, 3.1, 1.5, 0.2). The distance between Xand Y
is:d5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð0:3Þ21ð0:1Þ21ð0:1Þ21ð0Þ2p
50:33 centimeters.
FIGURE 4.30
(A) Dataset with a record of unknown class. (B) Decision boundary with k51 around unknown class
record. (C) Decision boundary with k53 around unknown test record.102 CHAPTER 4: ClassificationAll the attributes in the Iris dataset are homogenous in terms of measure-
ments (size of the flower parts) and units (centimeters). However, in a typi-
cal practical dataset, it is common to see attributes in different measures
(e.g., credit score, income) and varied units. One problem with the distance
approach is that it depends on the scale and units of the attributes. For exam-
ple, the difference in credit score between two records could be a few hun-
dred points, which is minor in magnitude compared to the difference in
income, which could be in the thousands. Consider two pairs of data points
with credit score and annual income in USD. Pair A is (500, $40,000) and
(600, $40,000). Pair B is (500, $40,000) and (500, $39,800). The first data
point in both the pairs is same. The second data point is different from the
first data point, with only one attribute changed. In Pair A, credit score is
600, which is significantly different to 500, while the income is the same. In
Pair B, the income is down by $200 when compared to the first data point,
which is only a 0.5% change. One can rightfully conclude that the data
points in Pair B are more similar than the data points in Pair A. However,
the distance [ Eq. (4.4) ] between data points in Pair A is 100 and the distance
between Pair B is 200! The variation in income overpowers the variation in
credit score. The same phenomenon can be observed when attributes are
measured in different units, scales, etc. To mitigate the problem caused by
different measures and units, all the inputs of k-NN are normalized, where
the data values are rescaled to fit a particular range. Normalizing all the attri-
butes provides a fair comparison between them. Normalization can be per-
formed using a few different methods. Range transformation rescales all the
values of the attributes to specified min and max values, usually 0 to 1. Z-
transformation attempts to rescale all the values by subtracting the mean
from each value and dividing the result by the standard deviation, resulting
in a transformed set of values that have a mean of 0 and a standard devia-
tion of 1. For example, when the Iris dataset is normalized using Z-
transformation, sepal length, which takes values between 4.3 and 7.9 cm,
and has a standard deviation of 0.828, is transformed to values between
21.86 and 2.84, with a standard deviation of 1. The distance measurement
discussed so far is also called Euclidean distance , which is the most common
distance measure for numeric attributes. In addition to the Euclidean,
Manhattan , and Chebyshev distance measures are sometimes used to calculate
the distance between two numeric data points. Consider two data points X
(1,2) and Y(3,1), as shown in Fig. 4.31 . The Euclidean distance between X
and Yis the straight-line distance between Xand Y, which is 2.7. The
Manhattan distance is the sum of the difference between individual attri-
butes, rather than the root of squared difference. The Manhattan distance
between Xand Yis (321)1(221)53. The Manhattan distance is also
called the taxicab distance, due to the similarities of the visual path traversed
by a vehicle around city blocks (In Fig. 4.31 , the total distance that is covered4.3 k-Nearest Neighbors 103by a cab that has to travel from XtoYin terms of number of city blocks is
two blocks to the right and one block down). The Chebyshev distance is the
maximum difference between all attributes in the dataset. In this example,
the Chebyshev distance is the max of [(3 21), (122)]52. If Fig. 4.31 were
a chess board, the Chebyshev distance would be the minimum number of
moves required by the king to go from one position to another and the
Manhattan distance is the minimum number of squares covered by the move
of a rook from one position to another. All three aforementioned distance
measures can be further generalized with one formula, the Minkowski dis-
tance measure. The distance between two points X(x1,x2,...,xn) and Y(y1,
y2,...,yn)i nn-dimensional space is given by Eq. (4.5) :
d5Xn
i51xi2yi/C12/C12/C12/C12p/C16/C17 1
pð4:5Þ
When p51, the distance measure is the Manhattan distance, when p52 the
distance measure is the Euclidean distance, and when p5Nthe distance
measure is the Chebyshev distance. pis also called the norm and Eq. (4.5) is
called the p-norm distance. The choice of distance measure depends on the
data ( Grabusts, 2011 ). The Euclidean measure is the most commonly used
distance measure for numeric data. The Manhattan distance is used for
binary attributes. For a new dataset with no prior knowledge, there is no
FIGURE 4.31
Distance measures.104 CHAPTER 4: Classificationrule-of-thumb for an ideal distance measure. Euclidean distance would be a
good start and the model can be tested with a selection of other distance
measures and the corresponding performance.
Once the nearest kneighbors are determined, the process of determining the
predicted target class is straightforward. The predicted target class is the
majority class of the nearest kneighbors. Eq. (4.6) provides the prediction of
thek-NN algorithm:
y05majority class y1;y2;y3; :::yk ðÞ ð 4:6Þ
where y0is the predicted target class of the test data point and yiis the class
ofithneighbor ni.
Weights
The premise of the k-NN algorithm is that data points closer to each other are
similar and, hence, have the same target class labels. When kis more than one,
it can be argued that the closest neighbors should have more say in the outcome
of the predicted target class than the farther neighbors ( Hill & Lewicki, 2007 ).
The farther away neighbors should have less influence in determining the final
class outcome. This can be accomplished by assigned weights for all the neigh-
bors, with the weights increasing as the neighbors get closer to the test data
point. The weights are included in the final multi-voting step, where the pre-
dicted class is calculated. Weights ( wi) should satisfy two conditions: they
should be proportional to the distance of the test data point from the neighbor
and the sum of all weights should be equal to one. One of the calculations for
weights shown in Eq. (4.7) follows an exponential decay based on distance:
wi5e2dðx;niÞ
Pk
i51e2dðx;niÞð4:7Þ
where wiis the weight of ithneighbor ni,kthe is total number of neighbors,
and xis the test data point. The weight is used in predicting target class y0:
y05majority class w1/C3y1;w2/C3y2;w3/C3y3;...;wk/C3yk ðÞ ð 4:8Þ
where yiis the class outcome of neighbor ni.
The distance measure works well for numeric attributes. However, if the attri-
bute is categorical, the distance between two points is either 0 or 1. If the
attribute values are the same, the distance is 0 and if the attribute values are
different, the distance is 1. For example, distance between (overcast,
sunny) 51 and distance between (sunny, sunny) 50. If the attribute is ordi-
nal with more than two values, then the ordinal values can be converted to
an integer data type with values 0, 1, 2, ...,n21 and the converted attribute
can be treated as a numeric attribute for distance calculation. Obviously,4.3 k-Nearest Neighbors 105converting ordinal into numeric retains more information than using it as a
categorical data type, where the distance value is either 0 or 1.
Correlation similarity
The correlation between two data points Xand Yis the measure of the linear
relationship between the attributes Xand Y. Pearson correlation takes a value
from21 (perfect negative correlation) to 11 (perfect positive correlation)
with the value of zero being no correlation between Xand Y. Since correla-
tion is a measure of linear relationship, a zero value does not mean there is
no relationship. It just means that there is no linear relationship, but theremay be a quadratic or any other higher degree relationship between the data
points. Also, the correlation between one data point and another will now
be explored. This is quite different from correlation between variables . Pearson
correlation between two data points Xand Yis given by:
Correlation ðX;YÞ5sxy
sx3syð4:9Þ
where sxyis the covariance of Xand Y, which is calculated as:
sxy51
n21Xn
i51ðxi2xÞðyi2yÞ
and sxand syare the standard deviation of Xand Y, respectively. For example,
the Pearson correlation of two data points X(1,2,3,4,5) and Y
(10,15,35,40,55) is 0.98.
Simple matching coefficient
The simple matching coefficient is used when datasets have binary attributes.
For example, let Xbe (1,1,0,0,1,1,0) and Ybe (1,0,0,1,1,0,0). One can mea-
sure the similarity between these two data points based on the simultaneousoccurrence of 0 or 1 with respect to total occurrences. The simple matching
coefficient for Xand Ycan be calculated as:
Simple matching coefficient ðSMC Þ5Matching occurences
Total occurences
5m001m11
m101m011m111m00ð4:10Þ
In this example, m115occurrences where ( X51 and Y51)52;
m105occurrences where ( X51 and Y50)52;m015occurrences where
(X50 and Y51)51;m005occurrences where ( X50 and Y50)52.
Simple matching coefficient is, (2 12) / (2121112)54/7.
Jaccard similarity
IfXand Yrepresent two text documents, each word will be an attribute in a
dataset called a term document matrix or document vector. Each record in106 CHAPTER 4: Classificationthe document dataset corresponds to a separate document or a text blob.
This is explained in greater detail in Chapter 9, Text Mining. In this applica-
tion, the number of attributes would be very large, often in the thousands.However, most of the attribute values will be zero. This means that two
documents do not contain the same rare words. In this instance, what is
interesting is that the comparison of the occurrence of the same word and
non-occurrence of the same word does not convey any information and can
be ignored. The Jaccard similarity measure is similar to the simple matching
similarity but the nonoccurrence frequency is ignored from the calculation.For the same example X(1,1,0,0,1,1,0) and Y(1,0,0,1,1,0,0),
Jaccard coefficient 5Common occurences
Total occurences
5m11
m101m011m1152
5ð4:11Þ
Cosine similarity
Continuing with the example of the document vectors, where attributes rep-
resent either the presence or absence of a word. It is possible to construct amore informational vector with the number of occurrences in the document,
instead of just 1 and 0. Document datasets are usually long vectors with
thousands of variables or attributes. For simplicity, consider the example ofthe vectors with X(1,2,0,0,3,4,0) and Y(5,0,0,6,7,0,0). The cosine similarity
measure for two data points is given by:
Cosine similarity ðjX;YjÞ5xUy
jjxjj jjyjjð4:12Þ
where x/C1yis the dot product of the xand yvectors with, for this example,
xUy5Pn
i51xiyiand jjxjj5ﬃﬃﬃﬃﬃﬃﬃxUxp
xUy5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
135123010301036133714301030p
55:1
jjxjj5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
131123210301030133314341030p
55:5
jjyjj5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
535103010301636173710301030p
510:5
Cosine similarity ðjxUyjÞ5xUy
jjxjj jjyjj55:1
5:5310:550:08
The cosine similarity measure is one of the most used similarity measures,
but the determination of the optimal measure comes down to the data struc-
tures. The choice of distance or similarity measure can also be parameterized,
where multiple models are created with each different measure. The modelwith a distance measure that best fits the data with the smallest generaliza-
tion error can be the appropriate proximity measure for the data.4.3 k-Nearest Neighbors 1074.3.2 How to Implement
Implementation of lazy learners is the most straightforward process amongst
all the data science methods. Since the key functionality is referencing orlooking up the training dataset, one could implement the entire algorithm in
spreadsheet software like MS Excel, using lookup functions. Of course, if the
complexity of the distance calculation or when the number of attributes rises,then one may need to rely on data science tools or programming languages.
In RapidMiner, k-NN implementation is similar to other classification and
regression process, with data preparation, modeling, and performance evalu-ation operators. The modeling step memorizes all the training records and
accepts input in the form of real and nominal values. The output of this
modeling step is just the dataset of all the training records.
Step 1: Data Preparation
The dataset used in this example is the standard Iris dataset with 150 exam-ples and four numeric attributes. First, all attributes will need to be normal-
ized using the Normalize operator, from the Data Transformation .Value
Modification .Numerical Value Modification folder. The Normalize operator
accepts numeric attributes and generates transformed numeric attributes. The
user can specify one of four normalization methods in the parameter config-
urations: Z-transformation (most commonly used), range transformation,proportion transformation, and interquartile range. In this example, Z-
transformation is used because all the attributes are being standardized.
The dataset is then split into two equal exclusive datasets using the Split Data
operator. Split data (from Data Transformation .Filtering .Sampling) is
used to partition the test and training datasets. The proportion of the parti-
tion and the sampling method can be specified in the parameter configura-tion of the split operator. For this example, the data is split equally between
the training and test sets using shuffled sampling. One half of the dataset is
used as training data for developing the k-NN model and the other half of
the dataset is used to test the validity of the model.
Step 2: Modeling Operator and Parameters
Thek-NN modeling operator is available in Modeling .Classification .Lazy
Modeling. These parameters can be configured in the operator settings:
1.k:The value of kink-NN can be configured. This defaults to one
nearest neighbor. This example uses k53.108 CHAPTER 4: Classification2.Weighted vote: In the case of k.1, this setting determines if the
algorithm needs to take into consideration the distance value for the
weights, while predicting the class value of the test record.
3.Measure types: There are more than two dozen distance measures
available in RapidMiner. These measures are grouped in Measure
Types. The selection of Measure Types drives the options for the next
parameter (Measure).
4.Measure: This parameter selects the actual measure like Euclidean
distance, Manhattan distance, and so on. The selection of the measure
will put restrictions on the type of input the model receives.
Depending on the weighting measure, the input data type choices
will be limited and, hence, a data t ype conversion is required if the
input data contains attributes that are not compatible with that
measure.
Similar to other classification model implementations, the model will need
to be applied to test the dataset, so the effectiveness of the model can be
evaluated. Fig. 4.32 shows the RapidMiner process where the initial Iris
dataset is split using a split operator. A random 75 of the initial 150 records
are used to build the k-NN model and the rest of the data is the test dataset.
The Apply Model operator accepts the test data and applies the k-NN model
t op r e d i c tt h ec l a s st y p eo ft h es p e c i e s .T h e Performance operator is then
used to compare the predicted class with the labeled class for all of the test
records.
FIGURE 4.32
Data mining process for k-NN algorithm. k-NN, k-Nearest Neighbor.4.3 k-Nearest Neighbors 109Step 3: Execution and Interpretation
After the output from the Performance operator is connected to the result
ports, as shown in Fig. 4.32 , the model can be executed. The result output is
observed as:
1.k-NN model: The model for k-NN is just the set of training records.
Hence, no additional information is provided in this view, apart from
the statistics of training records. Fig. 4.33 shows the output model.
2.Performance vector: The output of the Performance operator provides the
confusion matrix with correct and incorrect predictions for all of the
test dataset. The test set had 75 records. Fig. 4.34 shows the accurate
prediction of 71 records (sum of diagonal cells in the matrix) and 4
incorrect predictions.
3.Labeled test dataset: The prediction can be examined at the record
level.
FIGURE 4.33
k-NN model output. k-NN, k-Nearest Neighbor.
FIGURE 4.34
Performance vector for k-NN model. k-NN, k-Nearest Neighbor.110 CHAPTER 4: Classification4.3.3 Conclusion
The k-NN model requires normalization to avoid bias by any attribute that
has large or small units in the scale. The model is quite robust when thereare any missing attribute values in the test record. If a value in the test record
is missing, the entire attribute is ignored in the model, and the model can
still function with reasonable accuracy. In this implementation example, ifthe sepal length of a test record is not known, then the sepal length is
ignored in the model. k-NN becomes a three-dimensional model instead of
the original four dimensions.
As a lazy learner, the relationship between input and output cannot be
explained, as the model is just a memorized set of all training records. There
is no generalization or abstraction of the relationship. Eager learners are bet-ter at explaining the relationship and providing a description of the model.
Model building in k-NN is just memorizing and does not require much time.
But, when a new unlabeled record is to be classified, the algorithm needs to
find the distance between the unseen record and allthe training records. This
process can get expensive, depending on the size of training set and the num-ber of attributes. A few sophisticated k-NN implementations index the
records so that it is easy to search and calculate the distance. One can also
convert the actual numbers to ranges so as to make it easy to index and com-pare it against the test record. However, k-NN is difficult to be used in time-
sensitive applications like serving an online advertisement or real-time fraud
detection.
k-NN models can handle categorical inputs but the distance measure will be
either 1 or 0. Ordinal values can be converted to integers so that one can bet-ter leverage the distance function. Although the k-NN model is not good at
generalizing the input-output relationship, it is still quite an effective model
and leverages the existing relationships between attributes and class label inthe training records. For good quality outcome, it requires a significant num-
ber of training records with the maximum possible permutations values in
input attributes.
4.4 NAÏVE BAYESIAN
The data science algorithms used for classification tasks are quite diverse. Theobjective of all these algorithms is the same —prediction of a target variable.
The method of prediction is drawn from a range of multidisciplinary techni-
ques. The naïve Bayes algorithm finds its roots in statistics and probabilitytheory. In general, classification techniques try to predict class labels by best
approximating the relationship between the attributes and the class label.4.4 Naïve Bayesian 111Every day, we mentally estimate a myriad of outcomes based on past evi-
dence. Consider the process of guessing commuting time to work. First, com-
mute time depends heavily on when one is leaving home or work. If one is
traveling during peak hours, the commute is going to be longer. Weather
conditions like rain, snow, or dense fog will slow down the commute. If the
day is a school holiday, like summer break, then the commute will be lighter
than on school days. If there is any road work, the commute usually takes
longer. When more than one adverse factor is at play, then the commute will
be even longer than if it is just one isolated factor is in the play. This if-then
knowledge is based on previous experience of commuting when one or more
factors come into play. Our experience creates a model in our brain and we
mentally run the model before pulling out of the driveway!
Take the case of defaults in home mortgages and assume the average overall
default rate is 2%. The likelihood of an average person defaulting on their
mortgage loan is 2%. However, if a given individual ’s credit history is above
average (or excellent), then the likelihood of their default would be less than
average. Furthermore, if one knows that the person ’s annual income is above
average with respect to loan value, then the likelihood of default falls further.
As more evidence is obtained on the factors that impact the outcome,
improved guesses can be made about the outcome using probability theory.
The naïve Bayesian algorithm leverages the probabilistic relationship between
the attributes and the class label. The algorithm makes a strong and some-
times naïve assumption of independence between the attributes, thus, its
name. The independence assumption between attributes may not always
hold true. It can be assumed that annual income and home value are inde-
pendent of each other. However, homeowners with high income tend to buy
more expensive houses. Although the independence assumption doesn ’t
always hold true, the simplicity and robustness of the algorithm offsets the
limitation introduced by the assumption.
PREDICTING AND FILTERING SPAM EMAIL
Spam is unsolicited bulk email sent to a wide number of
email users. At best it is an annoyance to recipients but
many of the spam emails hide a malicious intent by host-
ing false advertisements or redirecting clicks to phishing
sites. Filtering spam email is one of the essential features
provided by email service providers and administrators.
The key challenge is balance between incorrectly flagging
a legitimate email as spam (false positive) versus not
catching all the spam messages. There is no perfect
spam filtering solution and spam detection is a catch-up
game. The spammers always try to deceive and outsmartthe spam filters and email administrators fortify the filters
for various new spam scenarios. Automated spam filtering
based on algorithms provides a promising solution in con-
taining spam and in learning frameworks to update the
changing filtering solutions ( Process Software, 2013 ).
Some words occur in spam emails more often than in
legitimate email messages. For example, the probability
of the occurrence for words like free, mortgage, credit,
sale, Viagra, etc., is higher in spam mails than in normal
emails. The exact probabilities can be calculated if one
(Continued )112 CHAPTER 4: Classification4.4.1 How It Works
The naïve Bayesian algorithm is built on the Bayes ’theorem, named after
Reverend Thomas Bayes. Bayes ’work is described in “Essay Towards Solving
a Problem in the Doctrine of Chances ”(1763), published posthumously in
thePhilosophical Transactions of the Royal Society of London by Richard Price.
The Bayes ’theorem is one of the most influential and important concepts in
statistics and probability theory. It provides a mathematical expression for
how a degree of subjective belief changes to account for new evidence. First,
the terminology used in Bayes ’theorem need to be discussed.
Assume Xis the evidence (attribute set) and Yis the outcome (class label).
Here Xis a set, not individual attributes, hence, X5{X1,X2,X3,...,Xn},
where X iis an individual attribute, such as credit rating. The probability of
outcome P(Y) is called prior probability , which can be calculated from the
training dataset. Prior probability shows the likelihood of an outcome in a
given dataset. For example, in the mortgage case, P(Y) is the default rate on a
home mortgage, which is 2%. P(Y|X) is called the conditional probability ,
which provides the probability of an outcome given the evidence, that is,
when the value of Xis known. Again, using the mortgage example, P(Y|X)i s
the average rate of default given that an individual ’s credit history is known.
If the credit history is excellent, then the probability of default is likely to be
less than 2%. P(Y|X) is also called posterior probability . Calculating posterior
probability is the objective of data science using Bayes ’theorem. This is the
likelihood of an outcome as the conditions are learnt.
Bayes ’theorem states that:
PðYjXÞ5PðYÞ3PðXjYÞ
PðXÞð4:13Þ
P(X|Y) is another conditional probability, called the class conditional probability .
P(X|Y) is the probability of the existence of conditions given an outcome. Like
P(Y),P(X|Y) can be calculated from the training dataset as well. If the training
set of loan defaults is known, the probability of an “excellent ”credit rating
can be calculated given that the default is a “yes.”As indicated in the Bayes ’(Continued )
has a sample of previously known spam emails and regu-
lar emails. Based on the known word probabilities, the
overall probability of an email being spam can be com-
puted based on all the words in the email and the proba-
bility of each word being in spam versus regular emails.
This is the foundation of Bayesian spam filtering systems
(Zdziarski, 2005 ). Any misclassified spam messages thatare subsequently reclassified by the user (by marking it as
spam) are an opportunity to refine the model, making
spam filtering adaptive to new spam techniques. Though
current spam reduction methods use a combination of dif-
ferent algorithms, Bayesian-based spam filtering remains
one of the foundational elements of spam prediction sys-
tems ( Sahami, Dumais, Heckerman, & Horvitz, 1998 ).4.4 Naïve Bayesian 113theorem, class conditional probability is crucial to calculating posterior proba-
bility. P(X) is basically the probability of the evidence. In the mortgage exam-
ple, this is simply the proportion of individuals with a given credit rating. To
classify a new record, one can compute P(Y|X)f o r each class of Y and see which
probability “wins. ”Class label Ywith the highest value of P(Y|X)w i n sf o r
given condition X.S i n c e P(X) is the same for every class value of the outcome,
one does not have to calculate this and it can be assumed as a constant. More
generally, for an example set with nattributes X5{X1,X2,X3...Xn},
PðYjXÞ5PðYÞ3Ln
i51PðXijYÞ
PðXÞð4:14Þ
If one knows how to calculate class conditional probability P(X|Y)
orLn
i51PðXijYÞ, then it is easy to calculate posterior probability P(Y|X). Since
P(X) is constant for every value of Y, it is enough to calculate the numerator
of the equation PðYÞ3Ln
i51PðXijYÞfor every class value.
To further explain how the naïve Bayesian algorithm works, the modified
Golf dataset shown in Table 4.4 will be used. The Golf table is an artificial
dataset with four attributes and one class label. Note that the categorical data
type is being used for ease of explanation (temperature and humidity have
been converted from the numeric type). In Bayesian terms, weather condition
is the evidence and decision to play or not play is the belief . Altogether there
are 14 examples with 5 examples of Play 5no and nine examples of
Play5yes. The objective is to predict if the player will Play (yes or no), given
the weather condition, based on learning from the dataset in Table 4.4 . Here
is the step-by-step explanation of how the Bayesian model works.
Table 4.4 Golf Dataset With Modified Temperature and Humidity Attributes
No. Temperature X1 Humidity X2 Outlook X3 Wind X4 Play (Class Label) Y
1 High Med Sunny false no
2 High High Sunny true no
3 Low Low Rain true no
4 Med High Sunny false no
5 Low Med Rain true no
6 High Med Overcast false yes
7 Low High Rain false yes
8 Low Med Rain false yes
9 Low Low Overcast true yes
10 Low Low Sunny false yes
11 Med Med Rain false yes
12 Med Low Sunny true yes
13 Med High Overcast true yes
14 High Low Overcast false yes114 CHAPTER 4: ClassificationStep 1: Calculating Prior Probability P(Y)
Prior probability P(Y) is the probability of an outcome. In this example set
there are two possible outcomes: Play 5yes and Play 5no. From Table 4.4 ,5
out of 14 records with the “no”class and 9 records with the “Yes”class. The
probability of outcome is
P(Y5no)55/14
P(Y5yes)59/14
Since the probability of an outcome is calculated from the dataset, it is impor-
tant that the dataset used for data science is representative of the population, if
sampling is used. The class-stratified sampling of data from the population
will be ideal for naïve Bayesian modeling. The class-stratified sampling ensures
the class distribution in the sample is the same as the population.
Step 2: Calculating Class Conditional Probability PðXijYÞ
Class conditional probability is the probability of each attribute value for an
attribute, for each outcome value. This calculation is repeated for all the attri-
butes: Temperature ( X1), Humidity ( X2), Outlook ( X3), and Wind ( X4), and
for every distinct outcome value. Here is a calculation of the class conditional
probability of Temperature ( X1). For each value of the Temperature attribute,
P(X1|Y5no) and P(X1|Y5yes) can be calculated by constructing a class con-
ditional probability table as shown in Table 4.5 . From the dataset there are
five Y5no records and nine Y5yes records. Out of the five Y5no records,
the probability of occurrence can be calculated for when the temperature is
high, medium, and low. The values will be 2/5, 1/5, and 2/5, respectively.
The same process can be repeated when the outcome is Y5yes.
Similarly, the calculation can be repeated to find the class conditional proba-
bility for the other three attributes: Humidity ( X2), Outlook ( X3), and Wind
(X4). This class conditional probability table is shown in Table 4.6 .
Step 3: Predicting the Outcome Using Bayes ’Theorem
With the class conditional probability tables all prepared they can now be
used in the prediction task. If a new, unlabeled test record ( Table 4.7 ) has
the conditions Temperature 5high, Humidity 5low, Outlook 5sunny, and
Table 4.5 Class Conditional Probability of Temperature
Temperature ( X1) P(X1|Y5no) P(X1|Y5yes)
High 2/5 2/9
Med 1/5 3/9
Low 2/5 4/94.4 Naïve Bayesian 115Wind5false, what would the class label prediction be? Play 5yes or no? The
outcome class can be predicted based on Bayes ’theorem by calculating
the posterior probability P(Y|X) for both values of Y. Once P(Y5yes|X) and
P(Y5no|X) are calculated, one can determine which outcome has higher
probability and the predicted outcome is the one that has the highest proba-
bility. While calculating both conditional probabilities using Eq. (4.14) ,i ti s
sufficient to just calculate the numerator portion, as P(X) is going to be the
same for both the outcome classes ( Table 4.7 ).
PðY5yesjXÞ5PðYÞ/C3Ln
i51pðXijYÞ
PðXÞ
5PðY5yesÞ/C3fPðTemp5high jY5yesÞ/C3PðHumidity 5lowjY5yesÞ/C3
PðOutlook 5sunny jY5yesÞ/C3PðWind5false jY5yesÞg
PðXÞ
59=14/C3f2=9/C34=9/C32=9/C36=9g
PðXÞ
50:0094
PðXÞ
PðY5nojXÞ55=14/C3f2=5/C34=5/C33=5/C32=5g
50:0274
PðXÞTable 4.6 Conditional Probability of Humidity, Outlook, and Wind
Humidity ( X2) P(X2|Y5no) P(X2|Y5yes)
High 2/5 2/9
Low 1/5 4/9
Med 2/5 3/9
Outlook ( X3) P(X3|Y5no) P(X3|Y5yes)
Overcast 0/5 4/9
Rain 2/5 3/9
Sunny 3/5 2/9
Wind ( X4) P(X4|Y5no) P(X4|Y5yes)
False 2/5 6/9
True 3/5 3/9
Table 4.7 Test Record
No. Temperature X1 Humidity X2 Outlook X3 Wind X4 Play (Class Label) Y
Unlabeled test high Low Sunny False ?116 CHAPTER 4: ClassificationBoth the estimates can be normalized by dividing both conditional probabil-
ity by (0.0094 10.027) to get:
Likelihood of (Play 5yes)50:0094
0:027410:0094526%
Likelihood of (Play 5no)50:0094
0:027410:0094574%
In this case P(Y5yes|X),P(Y5no|X), hence, the prediction for the unla-
beled test record will be Play 5no.
The Bayesian modeling is relatively simple to understand, once one gets past
the probabilistic concepts, and is easy to implement in practically any pro-
graming language. The computation for model building is quite simple and
involves the creation of a lookup table of probabilities. Bayesian modeling is
quite robust in handling missing values. If the test example set does not con-
tain a value, suppose temperature is not available, the Bayesian model sim-
ply omits the corresponding class conditional probability for all the
outcomes. Having missing values in the test set would be difficult to handle
in decision trees and regression algorithms, particularly when the missing
attribute is used higher up in the node of the decision tree or has more
weight in regression. Even though the naïve Bayes algorithm is quite robust
to missing attributes, it does have a few limitations. Here are couple of the
most significant limitations and methods of mitigation.
Issue 1: Incomplete Training Set
Problems arise when an attribute value in the testing record has no example
in the training record. In the Golf dataset ( Table 4.4 ), if an unseen test
example consists of the attribute value Outlook 5overcast, the probability of
P(Outlook 5overcast| Y5no) is zero. Even if one of the attribute ’s class
conditional probabilities is zero, by nature of the Bayesian equation, the
entire posterior probability will be zero.
PðY5nojXÞ5PðY5NoÞ/C3fPðTeam5high jY5noÞ/C3PðHumidity 5lowj
Y5noÞ/C3PðOutlook 5overcast jY5noÞ/C3PðWind5false jY5noÞg
PðXÞ
55=14/C3f2=5/C31=5/C30/C32=5g
PðXÞ
50
In this case P(Y5yes|X).P(Y5no|X), and the test example will be classi-
fied as Play 5yes. If there are no training records for any other attribute
value, like Temperature 5low for outcome yes, then probability of both out-
comes, P(Y5no|X) and P(Y5yes|X), will also be zero and an arbitrary pre-
diction shall be made because of the dilemma.4.4 Naïve Bayesian 117To mitigate this problem, one can assign small default probabilities for the
missing records instead of zero. With this approach, the absence of an attri-
bute value doesn ’t wipe out the value of P(X|Y), albeit it will reduce the
probability to a small number. This technique is called Laplace correction .
Laplace correction adds a controlled error in all class conditional probabili-
ties. If the training set contains Outlook 5overcast, then P(X|Y5no)50.
The class conditional probability for al l the three values for Outlook is 0/5,
2/5, and 3/5, Y5no. Controlled error can be added by adding 1 to all
numerators and 3 for all denominators, so the class conditional probabili-
ties are 1/8, 3/8, and 4/8. The sum of all the class conditional probabilities
is still 1. Generically, the Laplace correction is given by corrected
probability:
PðXijYÞ501μp3
51μ;21μp2
51μ;31μp2
51μð4:15Þ
where p11p21p351 and μis the correction.
Issue 2: Continuous Attributes
If an attribute has continuous numeric values instead of nominal values, this
solution will not work. The continuous values can always be converted to
nominal values by discretization and the same approach as discussed can be
used. But discretization requires exercising subjective judgment on the buck-
eting range, leading to loss of information. Instead, the continuous values
can be preserved as such and the probability density function can used. One
assumes the probability distribution for a numerical attribute follows a nor-
mal or Gaussian distribution. If the attribute value is known to follow some
other distribution, such as Poisson, the equivalent probability density func-
tion can be used. The probability density function for a normal distribution
is given by:
fðxÞ51ﬃﬃﬃﬃﬃﬃﬃﬃﬃ2πσp eðx2μÞ2
2σ2 ð4:16Þ
where μis the mean and σis the standard deviation of the sample.
In the Golf dataset shown in Table 4.8 , temperature and humidity are contin-
uous attributes. In such a situation, the mean and standard deviation can be
computed for both class labels (Play 5yes and Play 5no) for temperature
and humidity ( Table 4.9 ).
If an unlabeled test record has a Humidity value of 78, the probability den-
sity can be computed using Eq. (4.16) , for both outcomes. For outcome
Play5yes, if the values x578,μ573, and σ56.16 are plugged in to the
probability density function, the equation renders the value 0.04. Similarly,118 CHAPTER 4: Classificationfor outcome Play 5no,x578,µ574.6, σ57.89 can be plugged in and the
probability density is computed to obtain 0.05:
P(temperature 578|Y5yes)50.04
P(temperature 578|Y5no)50.05
These values are probability densities and notprobabilities. In a continuous
scale, the probability of temperature being exactly at a particular value is
zero. Instead, the probability is computed for a range, such as temperatures
from 77.5 to 78.5 units. Since the same range is used for computing the
probability density for both the outcomes, Play 5yes and Play 5no, it is not
necessary to compute the actual probability. Hence, these temperature values
can be substituted in the Bayesian Eq. 4.14 for calculating class conditional
probability P(X|Y).Table 4.9 Mean and Deviation for Continuous Attributes
Play Value Humidity X2 Temperature X3
Y5no Mean 74.60 84.00
Deviation 7.89 9.62
Y5yes Mean 73.00 78.22
Deviation 6.16 9.88Table 4.8 Golf Dataset with Continuous Attributes
No. Outlook X1 Humidity X2 Temperature X3 Wind X4 PlayY
1 Sunny 85 85 false no
2 Sunny 80 90 true no
6 Rain 65 70 true no
8 Sunny 72 95 false no
14 Rain 71 80 true no
3 Overcast 83 78 false yes
4 Rain 70 96 false yes
5 Rain 68 80 false yes
7 Overcast 64 65 true yes
9 Sunny 69 70 false yes
10 Rain 75 80 false yes
11 Sunny 75 70 true yes
12 Overcast 72 90 true yes
13 Overcast 81 75 false yes4.4 Naïve Bayesian 119Issue 3: Attribute Independence
One of the fundamental assumptions in the naïve Bayesian model is attribute
independence . Bayes ’theorem is guaranteed only for independent attributes.
In many real-life cases, this is quite a stringent condition to deal with. This is
why the technique is called “naïve ”Bayesian, because it assumes an attri-
bute’s independence. In practice the naïve Bayesian model works fine with
slightly correlated features ( Rish, 2001 ). This problem can be handled by
pre-processing the data. Before applying the naïve Bayesian algorithm, it
makes sense to remove strongly correlated attributes. In the case of all
numeric attributes, this can be achieved by computing a weighted correlation
matrix. An advanced application of Bayes ’theorem, called a Bayesian belief
network, is designed to handle datasets with attribute dependencies.
The independence of categorical attributes can be tested by the chi-square
(χ2) test for independence. The chi-square test is calculated by creating a con-
tingency table of observed frequency like the one shown in Table 4.10 A. A
contingency table is a simple cross tab of two attributes under consideration.
A contingency table of expected frequency ( Table 4.10 B) is created based on
the equation:
Er;c5ðrow total 3column total Þ
ðtable total Þð4:17Þ
The chi-square statistic ( χ2) calculates the sum of the difference between
these two tables. χ2is calculated by Eq. (4.18) . In this equation, O is
observed frequency and E is expected frequency:
x25XðO2EÞ2
Eð4:18Þ
If the chi-square statistic ( χ2) is less than the critical value calculated from
the chi-square distribution for a given confidence level, then one can assume
the two variables under consideration are independent, for practical
purposes.
Table 4.10 Contingency Tables with observed frequency (A) and expected frequency (B)
(A) Wind —Observed Frequency (B) Wind —Expected Frequency
Outlook False True Total Outlook False True Total
Overcast 2 2 4 Overcast 2.29 1.71 4
Rain 3 2 5 Rain 2.86 2.14 5
Sunny 3 2 5 Sunny 2.86 2.14 5
Total 8 6 14 Total 8 6 14120 CHAPTER 4: Classification4.4.2 How to Implement
The naïve Bayesian model is one of the few data science techniques that can
be easily implemented in any programing language. Since the conditional
probability tables can be prepared in the model building phase, the execu-
tion of the model in runtime is quick. Data science tools have dedicated
naïve Bayes classifier functions. In RapidMiner, the Naïve Bayes operator is
available under Modeling .Classification. The process of building a model
and applying it to new data is similar to with decision trees and other classi-
fiers. The naïve Bayesian models can accept both numeric and nominal
attributes.
Step 1: Data Preparation
The Golf dataset shown in Table 4.8 is available in RapidMiner under
Sample .Data in the repository section. The Golf dataset can just be dragged
and dropped in the process area to source all 14 records of the dataset.
Within the same repository folder, there is also a Golf-Test dataset with a set
of 14 records used for testing. Both datasets need to be added in the main
process area. Since the Bayes operator accepts numeric and nominal data
types, no other data transformation process is necessary. Sampling is a com-
mon method to extract the training dataset from a large dataset. It is espe-
cially important for naïve Bayesian modeling for the training dataset to be
representative and proportional to the underlying dataset.
Step 2: Modeling Operator and Parameters
The Naïve Bayes operator can now be connected to the Golf training dataset.
The Naïve Bayesian operator has only one parameter option to set: whether
or not to include Laplace correction. For smaller datasets, Laplace correction
is strongly encouraged, as a dataset may not have all combinations of attri-
bute values for every class value. In fact, by default, Laplace correction is
checked. Outputs of the Naïve Bayes operator are the model and original
training dataset. The model output should be connected to Apply Model
(Model Application folder) to execute the model on the test dataset. The out-
put of the Apply Model operator is the labeled test dataset and the model.
Step 3: Evaluation
The labeled test dataset that one gets after using the Apply Model operator is
then connected to the Performance —Classification operator to evaluate the
performance of the classification model. The Performance —Classification
operator can be found under Evaluation .Performance Measurement .
Performance. Fig. 4.35 shows the complete naïve Bayesian predictive classifi-
cation process. The output ports should be connected to the result ports and
the process can be saved and executed.4.4 Naïve Bayesian 121Step 4: Execution and Interpretation
The process shown in Fig. 4.35 has three result outputs: a model description,
performance vector, and labeled dataset. The labeled dataset contains the test
dataset with the predicted class as an added column. The labeled dataset also
contains the confidence for each label class, which indicates the prediction
strength.
The model description result contains more information on class conditional
probabilities of all the input attributes, derived from the training dataset. The
Charts tab in model description contains probability density functions for
the attributes, as shown in Fig. 4.36 . In the case of continuous attributes, the
decision boundaries can be discerned across the different class labels for the
Humidity attribute. It has been observed that when Humidity exceeds 82,
the likelihood of Play 5no increases. The Distribution Table shown in
Fig. 4.37 contains the familiar class conditional probability table similar to
Tables 4.5 and 4.6 .
The performance vector output is simila r to previously discussed classifica-
tion algorithms. The performance vector provides the confusion matrix
describing accuracy, precision, and recall metrics for the predicted test
dataset.
4.4.3 Conclusion
The Bayesian algorithm provides a probabilistic framework for a classifica-
tion problem. It has a simple and sound foundation for modeling data and
FIGURE 4.35
Data mining process for naïve Bayes algorithm.122 CHAPTER 4: ClassificationFIGURE 4.36
Naïve Bayes model output: probability density function for humidity attribute.
FIGURE 4.37
Naïve Bayes distribution table output.4.4 Naïve Bayesian 123is quite robust to outliers and missing values. This algorithm is deployed
widely in text mining and document classification where the application has
a large set of attributes and attributes values to compute. The naïve Bayesian
classifier is often a great place to start for a data science project as it serves as
a good benchmark for comparison to other models. Implementation of the
Bayesian model in production systems is quite straightforward and the use of
data science tools is optional. One major limitation of the model is the
assumption of independent attributes, which can be mitigated by advanced
modeling or decreasing the dependence across the attributes through pre-
processing. The uniqueness of the technique is that it leverages new informa-
tion as it arrives and tries to make a best prediction considering new evi-
dence. In this way, it is quite similar to how our minds work. Talking about
the mind, the next algorithm mimics the biological process of human
neurons!
4.5 ARTIFICIAL NEURAL NETWORKS
The objective of a supervised learner is to model the relationship between
input and output variables. The neural network technique approaches this
problem by developing a functional relationship between input and output
variables by mimicking the architecture of the biological process of a neuron .
Although the developers of this technique have used many biological terms
to explain the inner workings of neural network modeling process, it has a
simple mathematical foundation. Consider the linear model:
Y5112X113X214X3
where Yis the calculated output and X1,X2, and X3are input attributes. 1 is
the intercept and 2, 3, and 4 are the scaling factors or coefficients for the
input attributes X1,X2, and X3, respectively. This simple linear model can be
represented in a topological form as shown in Fig. 4.38 .
In this topology, X1is the input value and passes through a node, denoted
by a circle. Then the value of X1is multiplied by its weight, which is 2, as
noted in the connector. Similarly, all other attributes ( X2and X3) go through
a node and scaling transformation. The last node is a special case with no
input variable; it just has the intercept. Finally, the values from all the con-
nectors are summarized in an output node that yields the predicted output
Y. The topology shown in Fig. 4.38 represents the simple linear model
Y5112X113X214X3. The topology also represents a simple artificial neu-
ral network (ANN). The neural networks model more complex nonlinear124 CHAPTER 4: Classificationrelationships of data and learn though adaptive adjustments of weights
between the nodes. The ANN is a computational and mathematical model
inspired by the biological nervous system. Hence, some of the terms used in
an ANN are borrowed from biological counterparts.
In neural network terminology, nodes are called units. The first layer of nodes
closest to the input is called the input layer or input nodes. The last layer of
nodes is called the output layer or output nodes. The output layer performs
anaggregation function and can also have a transfer function . The transfer func-
tion scales the output into the desired range. Together with the aggregation
and transfer function, the output layer performs an activation function .T h i s
simple two-layer topology, as shown in Fig. 4.38 , with one input and one
output layer is called a perceptron. It is the most simplistic form of an ANN. A
perceptron is a feed-forward neural network where the input moves in one
direction and there are no loops in the topology.
An ANN is typically used for modeling nonlinear , complicated relationships
between input and output variables. This is made possible by the existence of
more than one layer in the topology, apart from the input and output layers,
FIGURE 4.38
Model topology.
BIOLOGICAL NEURONS
The functional unit of cells in the nervous system is the
neuron. An ANN of nodes and connectors has a close
resemblance to a biological network of neurons and con-
nections, with each node acting as a single neuron. There
are close to 100 billion neurons in the human brain and
they are all interconnected to form this extremely impor-
tant organ of the human body (see Fig. 4.39 ). Neuron cells
are found in most animals; they transmit information
through electrical and chemical signals. Theinterconnection between one neuron with another neuron
happens through a synapse . A neuron consists of a cell
body, a thin structure that forms from the cell body called
a dendrite, and a long linear cellular extension called an
axon. Neurons are composed of a number of dendrites and
one axon. The axon of one neuron is connected to the den-
drite of another neuron through a synapse, and electro-
chemical signals are sent from one neuron to another.
There are about 100 trillion synapses in the human brain.
(Continued )4.5 Artificial Neural Networks 125called hidden layers . A hidden layer contains a layer of nodes that connects input
from previous layers and applies an activation function. The output is now cal-
culated by a more complex combination of input values, as shown in Fig. 4.40 .
Consider the example of the Iris dataset. It has four input variables, sepal length,
sepal width, petal length, and petal width, with three classes ( I. setosa ,I. versicolor ,
I. virginica ). An ANN based on the Iris dataset yields a three-layer structure (the
number of layers can be specified by the user) with three output nodes, one for
each class variable. For a categorical label problem, as in predicting the species for
Iris, the ANN provides output for each class type. A winning class type is picked
based on the maximum value of the output class label. The topology in Fig. 4.40
is a feed-forward ANN with one hidden layer. Of course, depending on the prob-
lem to be solved, one can use a topology with multiple hidden layers and even
with looping where the output of one layer is used as input for preceding layers.
Chapter 10 on Deep Learning introduces more complex topologies to solve
sophisticated use cases. Specifying what topology to use is a challenge in neural
network modeling.
The activation function used in the output node consists of a combination of
an aggregation function, usually summarization, and a transfer function .(Continued )
FIGURE 4.39
Anatomy of a neuron. Modified from original “Neuron Hand-tuned. ”Original uploader: Quasar Jarosz at en.wikipedia.org . Transferred
from en.wikipedia.org to Commons by user Faigl.ladislav using CommonsHelper. Licensed under Creative Commons Attribution-Share
Alike 3.0 via Wikimedia Commons.4126 CHAPTER 4: ClassificationTransfer functions commonly used are: sigmoid, normal bell curve, logistic,
hyperbolic, or linear functions. The purpose of sigmoid and bell curves is to
provide a linear transformation for a particular range of values and a nonlin-
ear transformation for the rest of the values. Because of the transfer function
and the presence of multiple hidden layers, one can model or closely approx-
imate almost any mathematical continuous relationship between input vari-
ables and output variables. Hence, a multilayer ANN is called a universal
approximator . However, the presence of multiple user options such as topol-
ogy, transfer function, and a number of hidden layers makes the search for
an optimal solution quite time consuming.Sepal lengthInput Hidden 1 Output
Sepal width
Petal length
Petal widthIris setosa
Iris virginicaIris versicolo r
FIGURE 4.40
Topology of a Neural Network model.
OPTICAL CHARACTER RECOGNITION
Character recognition is the process of interpreting hand-
written text and converting it into digitized characters. It
has a multitude of practical applications in everyday life,
including converting handwritten notes to standardized
text, automated sorting of postal mail by looking at the zip
codes (postal area codes), automated data entry from
forms and applications, digitizing classic books, license
plate recognition, etc. How does it work?
In its most basic form, character recognition has two
steps: digitization and a predictive model. In the digitiza-
tion step, every individual character is converted to a digi-
tal matrix, say 12 312 pixel, where each cell takes a
value of either 0 or 1 based on the handwritten character
overlay. The input vector now has 144 binary attributes
(12312) indicating the information of the handwritten
characters. Assume the objective is to decipher a
numeric handwritten zip code ( Matan et al., 1990 ).An ANN model can be developed which accepts 144
inputs and has 10 outputs, each indicating a digit from 0
to 9. The model has to be learnt in such a way that when
the input matrix is fed, one of the outputs shows the
highest signal indicating the prediction for the character.
Since a neural network is adaptable and relatively easy to
deploy, it is getting used increasingly in character recog-
nition, image processing, and related applications ( Li,
1994 ). This specific use case is also an example where
the explanatory aspect of the model is less important —
maybe because no one knows exactly how the human
brain does it. So, there is less expectation that the model
should be understandable as long as it works with
acceptable performance. ANN models are not easy to
explain, and, in many situations, this alone will remove
them from consideration of the data science techniques
to use. One can only wish this wasn ’t the case!4.5 Artificial Neural Networks 1274.5.1 How It Works
An ANN learns the relationship between input attributes and the output class
label through a technique called back propagation . For a given network topol-
ogy and activation function, the key training task is to find the weights of the
links. The process is rather intuitive and closely resembles the signal trans-
mission in biological neurons. The model uses every training record to esti-
mate the error between the predicted and the actual output. Then the model
uses the error to adjust the weights to minimize the error for the next train-
ing record and this step is repeated until the error falls within the
acceptable range ( Laine, 2003 ). The rate of correction from one step to
another should be managed properly, so that the model does not overcor-
rect. The steps in developing an ANN from a training dataset include:
Step 1: Determine the Topology and Activation Function
For this example, imagine a dataset with three numeric input attributes ( X1,
X2,X3) and one numeric output ( Y). To model the relationship, a topology
with two layers and a simple aggregation activation function is being used,
as shown in Fig. 4.41 . There is no transfer function used in this example.
Step 2: Initiation
Assume the initial weights for the four links are 1, 2, 3, and 4. Take an exam-
ple model and a training record with all the inputs as 1 and the known out-
put as 15. So, X1,X2,X351 and output Y515.Fig. 4.42 shows initiation of
the first training record.
Step 3: Calculating Error
The predicted output of the record from Fig. 4.42 can be calculated. This is a
simple feed-forward process when the input data passes through the nodes
Inputs Input
LayerOutput
LayerOutput
YX1
X2
X3
FIGURE 4.41
Two-layer topology with summary aggregation.128 CHAPTER 4: Classificationand the output is calculated. The predicted output Yaccording to the current
model is 1 113211331134510. The difference between the
actual output from the training record and the predicted output is the model
error:
e5Y2Y
The error for this example training record is 15 21055.
Step 4: Weight Adjustment
Weight adjustment is the most important part of learning in an ANN. The
error calculated in the previous step is passed back from the output node to
all other nodes in the reverse direction. The weights of the links are adjusted
from their old value by a fraction of the error. The fraction λapplied to the
error is called the learning rate. λtakes values from 0 to 1. A value close to 1
results in a drastic change to the model for each training record and a value
close to 0 results in smaller changes and, hence, less correction. The new
weight of the link ( w) is the sum of the old weight ( w0) and the product of
the learning rate and proportion of the error ( λ3e).
w5w01λ3e
The choice of λcan be tricky in the implementation of an ANN. Some model
processes start with λclose to 1 and reduce the value of λwhile training
each cycle. By this approach any outlier records later in the training cycle will
not degrade the relevance of the model. Fig. 4.43 shows the error propaga-
tion in the topology.
The current weight of the first link is w252 .A s s u m et h el e a r n i n gr a t ei s
0.5. The new weight will be w25210.535/352.83. The error is divided
by 3 because the error is back propagated to three links from the output
node. Similarly, the weight will be adjusted for all the links. In the next
Inputs Input
LayerOutput
LayerOutput
YX1 = 1 w2 = 2
w3 = 3
w4 = 4
w1 = 1X2 = 1
X3 = 1
FIGURE 4.42
Initiation and first training record.4.5 Artificial Neural Networks 129cycle, a new error will be computed for the next training record. This cycle
goes on until all the training records are processed by iterative runs. The
same training example can be repeated until the error rate is less than a
threshold. It was an extremely simple case of an ANN that was reviewed. Inreality, there will be multiple hidden layers and multiple output links —one
for each nominal class value. Because of the numeric calculations, an ANN
model works well with numeric inputs and outputs. If the input contains anominal attribute, a pre-processing step should be included to convert the
nominal attribute into multiple numeric attributes —one for each attribute
value. This process is similar to dummy variable introduction, which willbe further explored in Chapter 12, Time Series Forecasting. This specific pre-
processing increases the number of input links for a neural network in the
case of nominal attributes and, thus, increases the necessary computingresources. Hence, an ANN is more suitable for attributes with a numeric
data type.
4.5.2 How to Implement
An ANN is one of the most popular algorithms available for data science
tools. In RapidMiner, the ANN model operators are available in the
Classification folder. There are three types of models available: A simple
perceptron with one input and one output layer, a flexible ANN modelcalled Neural Net with all the parameters for complete model building,
and an advanced AutoMLP algorithm. AutoMLP (fo r Automatic Multilayer
Perceptron) combines concepts from genetic and stochastic algorithms.
FIGURE 4.43
Neural Network error back propagation.130 CHAPTER 4: ClassificationIt leverages an ensemble group of ANNs with different parameters like hid-
den layers and learning rates. It also o ptimizes by replacing the worst per-
forming models with better ones and maintains an optimal solution.
For the rest of the discussion, the Neural Net model operator will be
focused on.
Step 1: Data Preparation
The Iris dataset is used to demonstrate the implementation of an ANN. All
four attributes for the Iris dataset are numeric and the output has three clas-
ses. Hence the ANN model will have four input nodes and three output
nodes. The ANN model will not work with categorical or nominal data types.If the input has nominal attributes, it should be converted to numeric using
data transformation, see Chapter 15, Getting started with RapidMiner. In this
example, the Rename operator is used to name the four attributes of the Iris
dataset and the Split Data operator to split 150 Iris records equally into the
training and test data.
Step 2: Modeling Operator and Parameters
The training dataset is connected to the Neural Net operator. The Neural Net
operator accepts real data type and normalizes the values. These parameters
are available in ANN for users to change and customize in the model:
1.Hidden layer: Determines the number of layers, size of each hidden
layer, and names of each layer for easy identification in the output
screen. The default size of the node is 21, which is calculated by
(number of attributes 1number of classes )/211. The default node size
can be overwritten by specifying a number, not including a no-input
threshold node per layer.
2.Training cycles: This is the number of times a training cycle is repeated;
it defaults to 500. In a neural network, every time a training record is
considered, the previous weights are quite different, and hence, it is
necessary to repeat the cycle many times.
3.Learning rate: The value of λdetermines the sensitivity of the change
while back propagating the error. It takes a value from 0 to 1. A value
closer to 0 means the new weight would be more based on the
previous weight and less on error correction. A value closer to 1 wouldbe mainly based on error correction.
4.Momentum: This value is used to prevent local maxima and seeks to
obtain globally optimized results by adding a fraction of the previousweight to the current weight.
5.Decay: During the neural network training, ideally the error would be
minimal in the later portion of the training record sequence. One4.5 Artificial Neural Networks 131wouldn ’t want a large error due to any outlier records in the last few
records, as it would thereby impact the performance of the model.
Decay reduces the value of the learning rate and brings it closer to zero
for the last training record.
6.Shuffle: If the training record is sorted, one can randomize the sequence
by shuffling it. The sequence has an impact in the model, particularly
if the group of records exhibiting nonlinear characteristics are all
clustered together in the last segment of the training set.
7.Normalize: Nodes using a sigmoid transfer function expect input in the
range of 21 to 1. Any real value of the input should be normalized in
an ANN model.
8.Error epsilon: The objective of the ANN model should be to minimize
the error but not make it zero, at which the model memorizes the
training set and degrades the performance. The model building process
can be stopped when the error is less than a threshold called the error
epsilon.
The output of the Neural Net operator can be connected to the Apply Model
operator, which is standard in every data science workflow. The Apply Model
operator also gets an input dataset from the Split data operator for the test
dataset. The output of the Apply Model operator is the labeled test dataset and
the ANN model.
Step 3: Evaluation
The labeled dataset output after using the Apply Model operator is then con-
nected to the Performance —Classification operator, to evaluate the perfor-
mance of the classification model. Fig. 4.44 shows the complete ANN
predictive classification process. The output connections should be connected
to the result ports and the process can be saved and executed.
Step 4: Execution and Interpretation
The output results window for the model provides a visual on the topology
of the ANN model. Fig. 4.45 shows the model output topology. With a click
on a node, one can get the weights of the incoming links to the node. The
color of the link indicates relative weights. The description tab of the model
window provides the actual values of the link weights.
The output performance vector can be examined to see the accuracy of the
ANN model built for the Iris dataset. Fig. 4.46 shows the performance vector
for the model. A two-layer ANN model with the default parameter options
and equal splitting of input data and training set yields 93% accuracy. Out
of 75 examples, only 5 were misclassified.132 CHAPTER 4: Classification4.5.3 Conclusion
Neural network models require stringent input constraints and pre-
processing. If the test example has missing attribute values, the model cannot
function, similar to a regression or decision tree model. The missing values
FIGURE 4.44
Data mining process for Neural Network.
FIGURE 4.45
Neural Network model output with three hidden layers and four attributes.4.5 Artificial Neural Networks 133can be replaced with average values or any default values to mitigate the con-
straint. The relationship between input and output cannot be explained
clearly by an ANN. Since there are hidden layers, it is quite complex to
understand the model. In many data science applications explanation of themodel is as important as the prediction itself. Decision trees, induction rules,
and regression do a far better job at explaining the model.
Building a good ANN model with optimized parameters takes time. It
depends on the number of training records and iterations. There are no con-
sistent guidelines on the number of hidden layers and nodes within each hid-den layer. Hence, one would need to try out many parameters to optimize
the selection of parameters. However, once a model is built, it is straightfor-
ward to implement, and a new unseen record gets classified quite fast.
An ANN does not handle categorical input data. If the data has nominal
values, it needs to be converted to binary or real values. This means oneinput attribute explodes into multiple input attributes and exponentially
increases nodes, links, and complexity. Also, converting non-ordinal categori-
cal data, like zip code, to a numeric value provides an opportunity for ANNto make numeric calculations, which doesn ’t make sense. However, having
redundant correlated attributes is not going to be a problem in an ANN
model. If the example set is large, having outliers will not degrade the perfor-mance of the model. However, outliers will impact the normalization of the
signal, which most ANN models require for input attributes. Because model
building is by incremental error correction, ANN can yield local optima asthe final model. This risk can be mitigated by managing a momentum
parameter to weigh the update.
Although model explanation is quite difficult with an ANN, the rapid classifi-
cation of test examples makes an ANN quite useful for anomaly detection
FIGURE 4.46
Performance vector for Artificial Neural Network.134 CHAPTER 4: Classificationand classification problems. An ANN is commonly used in fraud detection, a
scoring situation where the relationship between inputs and output is non-
linear. If there is a need for something that handles a highly nonlinear land-
scape along with fast real-time performance, then the ANN fits the bill.
4.6 SUPPORT VECTOR MACHINES
Support vector algorithms are a relatively new concept, like so many other
machine learning techniques. Cortes (1995) provided one of the first formal
introductions to the concept while investigating algorithms for optical char-
acter recognition at the AT&T Bell Labs.
The term “support vector machine ”(SVM) is a confusing name for a data sci-
ence algorithm . The fact is this term is very much a misnomer: there is really
no specialized hardware. But it is a powerful algorithm that has been quite
successful in applications ranging from pattern recognition to text mining.
SVM emphasizes the interdisciplinary nature of data science by drawing
equally from three major areas: computer science, statistics, and mathemati-
cal optimization theory.
Firstly, the essential terminology and definitions that are unique to SVMs will
be introduced. Then the functioning of the algorithm will be explained for a
simple linear dataset and then for a slightly more complex nonlinear dataset.
A brief mathematical explanation of the workings of the algorithm will be pro-
vided before a case study based demonstration of how to implement SVMs in
practice. Finally, the way SVMs perform better in some situations compared to
other classification techniques will be highlighted and a list of the advantages
and disadvantages of SVMs in general will be described.
Concept and Terminology
At a basic level, a SVM is a classification method. It works on the principle of
fitting a boundary to a region of points that are all alike (that is, belong to
one class). Once a boundary is fitted on the training sample, for any new
points (test sample) that need to be classified, one must simply check
whether they lie inside the boundary or not. The advantage of a SVM is that
once a boundary is established, most of the training data is redundant. All it
needs is a core set of points that can help identify and fix the boundary. These
data points are called support vectors because they “support ”the boundary.
Why are they called vectors? Because each data point (or observation) is a
vector: that is, it is a row of data that contains values for a number of differ-
ent attributes.
This boundary is traditionally called a hyperplane . In a simple example of two
dimensions, this boundary can be a straight line or a curve (as shown in4.6 Support Vector Machines 135Fig. 4.47 ). In three dimensions it can be a plane or an irregular complex sur-
face. Higher dimensions are difficult to visualize, and a hyperplane is, thus, a
generic name for a boundary in more than three dimensions.
As seen in Fig. 4.47 , a number of such hyperplanes can be found for the
same dataset. Which one is the “correct ”one? Clearly a boundary that sepa-
rates the classes with minimal misclassification is the best one. In the
sequence of images shown, the algorithm applied to the third image appears
to have zero misclassifications and may be the best one. Additionally, a
boundary line that ensures that the average geometric distance between the
two regions (or classes) is maximized is even better. This n-dimensional dis-
tance is called a margin . An SVM algorithm, therefore, essentially runs an
optimization scheme to maximize this margin. The points with the “X”
through them are the support vectors.
But it is not always possible to ensure that data can be cleanly separated. It
may be rare to find that the data are linearly separable . When this happens,
there may be many points within the margin. In this case, the best hyper-
plane is the one that has a minimum number of such points within the mar-
gin. To ensure this, a penalty is charged for every “contaminant ”inside the
margin and the hyperplane that has a minimum aggregate penalty cost is
chosen. In Fig. 4.48 ,ξrepresents the penalty that is applied for each error
and the sum of all such errors is minimized to get the best separation.
What would happen if the data are not linearly separable (even without such
contaminating errors)? For example, in Fig. 4.49A , the data points belong to
two main classes: an inner ring and an outer ring. Clearly these two classes
are not “linearly separable. ”In other words, a straight line can be drawn to
split the two classes. However, it is intuitively clear that an elliptical or circu-
lar“hyperplane ”can easily separate the two classes. In fact, if a simple linear
FIGURE 4.47
Three different hyperplanes for the same set of training data. There are two classes in this dataset,
which are shown as filled and open circles.136 CHAPTER 4: ClassificationFIGURE 4.48
Key concepts in SVM construction: boundary, margin, and penalty, ξ.SVM , Support Vector Machine.
FIGURE 4.49
(A) Linearly non-separable classes. (B) Transformation to linearly separable.SVM were to be run on this data, one would get a classification accuracy of
around 46%.
How can such complex feature spaces be classified? In the example, a simple
trick would be to transform the two variables xand yinto a new feature
space involving x(ory) and a new variable zdefined as z5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx21y2Þp
.T h e
representation of zis nothing more than the equation for a circle. When the
data is transformed in this way, the resulting feature space involving xand z
will appear as shown in Fig. 4.49B . The two clusters of data correspond to
the two radii of the rings: the inner one with an average radius of around 5.5
and the outer cluster with an average radius of around 8.0. Clearly this new
problem in the xand zdimensions is now linearly separable and a standard
SVM can be applied to do the classification. When a linear SVM is run on
this transformed data, one would get a classification accuracy of 100%. After
classifying the transformed feature space, the transformation can be inverted
to get back the original feature space.
Kernel functions offer the user the option of transforming nonlinear spaces
into linear ones. Most packages that include SVM will have several nonlinear
kernels ranging from simple polynomial basis functions to sigmoid func-
tions. The user does not have to do the transformation beforehand, but sim-
ply has to select the appropriate kernel function; the software will take care
of transforming the data, classifying it, and retransforming the results back
into the original space.
Unfortunately, with a large number of attributes in a dataset it is difficult to
know which kernel would work best. The most commonly used ones are
polynomial and radial basis functions. From a practical standpoint, it is a
good idea to start with a quadratic polynomial and work ones way up into
some of the more exotic kernel functions until the desired accuracy level is
reached. This flexibility of SVMs does come at the cost of computation.
Now that an intuitive understanding of how SVMs work is reached, the work-
ing of the algorithm can be examined with a more formal mathematical
explanation.
4.6.1 How It Works
Given a training dataset, how does one go about determining the bound-
ary and the hyperplane? The case of a simple linearly separable dataset
will be used consisting of two attributes, x1and x2.U l t i m a t e l yb yu s i n g
proper kernels any complex feature space can be mapped into a linear
space, so this formulation will apply t o any general dataset. Furthermore,
extending the algorithm to more than two attributes is conceptually
straightforward.138 CHAPTER 4: ClassificationThere are three essential tasks involved here: the first step is to find the
boundary of each class. Then the best hyperplane, H, is the one that maxi-
mizes the margin or the distance to each of the class boundaries (see
Fig. 4.48 ). Both of these steps use the training data. The final step is to deter-
mine on which side of this hyperplane a given test example lies in order to
classify it.
Step 1 : Finding the class boundary. When one connects every point in one
class of a dataset to every other in that class, the outline that emerges defines
the boundary of this class. This boundary is also known as the convex hull ,a s
shown in Fig. 4.50 .
Each class will have its own convex hull and because the classes are (assumed
to be) linearly separable, these hulls do not intersect each other.
Step 2: Finding the hyperplane. There are infinitely many available hyper-
planes, two of which are shown in Fig. 4.51 . How does one know which
hyperplane maximizes the margin? Intuitively one knows that H0has a larger
margin than H1, but how can this be determined mathematically?
First of all, any hyperplane can be expressed in terms of the two attributes, x1
and x2, as:
H5b1wUx50 ð4:19Þ
FIGURE 4.50
A convex hull for one class of data.4.6 Support Vector Machines 139where xis (x1,x2), the weight wis (w 1,w2), and b0is an intercept-like term
usually called the bias. Note that this is similar to the standard form of the
equation of a line. An optimal hyperplane, H0, is uniquely defined by ( b01
w0dx50). Once the hyperplane is defined in this fashion, it can be shown
that the margin is given by 2/ O(w0.w0)Cortes (1995) .
Maximizing this quantity requires quadratic programming, which is a well-
established process in mathematical optimization theory ( Fletcher, 1987 ).
Furthermore, the w0can be conveniently expressed in terms of only a few of
the training examples, known as support vectors, as:
w05X
jyixijð 4:20Þ
where the yiare the class labels ( 11o r21 for a binary classification), and
thexiare called the support vectors. The i’s are coefficients that are nonzero
only for these support vectors.
Step 3 : Once the boundary and the hyperplane have been defined, any new
test example can be classified by computing on which side of the hyperplane
the example lies. This is easily found by substituting the test example, x, into
the equation for the hyperplane. If it computes to 11, then it belongs to the
positive class and if it computes to 21 it belongs to the negative class. For
more in depth information refer to Smola (2004) orCortes (1995) for a full
mathematical description of the formulation. Hsu (2003) provides a more
practical demonstration of programming an SVM.
FIGURE 4.51
Both hyperplanes shown can separate data. It is intuitively clear that H0is better.140 CHAPTER 4: Classification4.6.2 How to Implement
How to use RapidMiner to perform classification with SVMs will now be
described using two simple cases.4
Implementation 1: Linearly Separable Dataset
The default SVM implementation in Ra pidMiner is based on the so-called “dot
product ”formulation shown in the equations above. In this first example an
SVM will be built using a two-dimensional dataset that consists of two classes: A
and B ( Fig. 4.52 ). A RapidMiner process reads in t he training dataset, applies the
default SVM model, and then classifies new points based on the model trained.
The dataset consists of 17 rows of data for three attributes: x1,x2, and class .
The attributes x1and x2are numeric and class is a binomial variable consist-
ing of the two classes A and B. Table 4.11 shows the full dataset and
Fig. 4.52 shows the plot of the dataset. The model will be used to classify the
three test examples: (1.50, 1.00), (1.50, 4.00), and (2.00, 7.00).
Step 1: Data Preparation
1.Read simpleSVMdemo.csv into RapidMiner by either using the Read csv
operator or import the data into the repository using Import csv file.
The dataset can be downloaded from the companion website of the
book: www.IntroDataScience.com .
2.Add a Set Role operator to indicate that class is a label attribute and
connect it to the data retriever. See Fig. 4.53 A.
Step 2: Modeling Operator and Parameters
1.In the Operators tab, type in SVM, drag and drop the operator into the
main window, and connect it to Set Role. Leave the parameters of this
operator in their default settings.
2.Connect the “mod ”output port of SVM to an Apply Model operator.
3.Insert a Generate Data by User Specification operator and click on the
Edit List button of the attribute values parameter. When the dialog box
opens up, click on Add Entry twice to create two test attribute names:
x1and x2. Set x152 and x257 under “attribute value. ”Note that the
attribute values will need to be changed for each new test point that
one wants to classify.
4.When this simple process is run, RapidMiner builds an SVM on the
training data and applies the model to classify the test example, which
was manually input using the Generate Data by User Specification
operator.
4A pair of simplistic datasets were deliberately chosen to illustrate how SVMs can be implemented. A
more sophisticated case study will be used to demonstrate how to use SVMs for text mining in
Chapter 9, Text Mining.4.6 Support Vector Machines 1418Class A Class B T est Point 1T est Point 2T est Point 3
7
6
5
4
3
2
1
0
0 0.5 1 1.5 2 2.5 3
FIGURE 4.52
Two-class training data: class A (diamond) and class B (square). Points 1 to 3 are used to test the
capability of the SVM. SVM , Support Vector Machine.
Table 4.11 A Simple Dataset to Demonstrate SVM
x1 x2 Class
1.50 2.50 A
2.00 2.00 A
1.00 2.00 A
0.75 3.00 A2.00 1.00 A1.75 1.75 A2.75 0.75 A2.50 1.50 A0.50 6.00 B1.50 6.00 B
2.00 5.50 B
1.00 5.50 B1.00 6.50 B2.00 4.50 B1.75 5.25 B2.75 4.25 B2.50 5.00 B1.50 1.00 Test point 1
1.50 4.00 Test point 2
2.00 7.00 Test point 3
SVM , Support vector machine.142 CHAPTER 4: ClassificationStep 3: Process Execution and Interpretation
For more practical applications, the Kernel Model output may not be very
interesting, it is shown here ( Fig. 4.53 B) to observe what the hyperplane
for this simplistic example looks like. Note that this is essentially the same
form as Eq. (4.19) with bias b050.051, w150.370, and w251.351.
The more interesting result in this case is the output from the “lab”port
of the Apply Model, which is the result of applying the SVM model on
the test point (2, 7).
As can be seen in Fig. 4.54 , the model has correctly classified this test
point as belonging to class B [see the “prediction(class) ”column].
Furthermore, it says that the confidence that this point belongs in class B
is 92.6%. Looking at the chart, one can see that indeed there is little
ambiguity about the classification of test point (2, 7).
FIGURE 4.53
(A) A simple SVM setup for training and testing. (B) The accompanying SVM model. SVM , Support Vector Machine.4.6 Support Vector Machines 143If one changes the test example input to the point (1.5, 1), it can be seen
that this point would be classified under class A, with 88% confidence.
However, the same cannot be said of test point (1.5, 4); one can run the
process and test for themselves!
In actual practice the labeled test data with prediction confidences are the
most useful results from an SVM application.
Example 2: Linearly Non-Separable Dataset
The first example showed a linearly separable training dataset. Suppose one
wants to now apply the same SVM (dot) kernel to the two-ring problem seen
earlier in this chapter: what would the results look like? From looking at the
dataset it ’s clear that this is a nonlinear problem and the dot kernel would
not work well. This intuitive understanding will be confirmed and how it
can be easily fixed will be demonstrated in the steps described:
Step 1: Data Preparation
1.Start a new process and read in the dataset nonlinearSVMdemodata.csv
using the same procedure as before. This dataset consists of 200
examples in four attributes: x1,x2,y, and ring. The ring is a binomial
attribute with two values: inner and outer.
2.Connect a Set Role operator to the data and select the “ring”variable to
be the label.
3.Connect a Select Attributes operator to this and select a subset of the
attributes: x1,x2, and ring. Make sure that the Include Special Attributes
checkbox is on.
4.Connect a Split Validation operator. Set the “split”to relative, “split
ratio ”to 0.7, and “sampling type ”to stratified.
Step 2: Modeling Operator and Parameters
1.Double-click the Split Validation box and when the nested layer is
entered, add an SVM operator in the training panel and Apply Model
and Performance (Classification) operators in the testing panel.
2.Once again, do not change the parameters for the SVM operator from
its default values.
FIGURE 4.54
Applying the simple SVM to classify test point 1 from Fig. 4.52 .SVM , Support Vector Machine.144 CHAPTER 4: Classification3.Go back to the main level and add another Apply Model operator.
Connect the “mod ”output from the Validation box to the “mod ”
i n p u tp o r to f Apply Model (2) and the “exa”output from the
Validation box to the “unl”input port of Apply Model (2) .A l s o
connect the “ave”output from the Validation box to the “res”port of
the Main Process. Finally, connect the “lab”output from Apply Model
(2)to the “res”port of the Main Process. The final process should
look like Fig. 4.55 .
Step 3: Execution and Interpretation
1.When this model is run, RapidMiner will generate two result tabs:
ExampleSet (Select Attributes) and Performance Vector (Performance) .
Check the performance of the SVM classifier. Recall that 30% of the
initial input examples are now going to be tested for classification
accuracy (which is a total of 60 test samples).
2.As seen in Fig. 4.56 , the linear SVM can barely get 50% of the classes
correct, which is to be expected considering linearly non-separable data
is being used with a linear (dot) kernel SVM.
3.A better way to visualize this result is by means of the Scatter 3D Color
plot. Click on the ExampleSet (Select Attributes) results tab and select Plot
View and set up the Scatter 3D Color plot as shown in Fig. 4.57 .
The red-colored examples in the upper (outer) ring are correctly classified as
belonging to the class outer while the cyan-colored examples have been
incorrectly classified as belonging to class inner. Similarly, the blue-colored
examples in the lower (inner) ring have been correctly classified as belonging
to class inner, whereas, the yellow-colored ones are not. As can be seen, the
classifier roughly gets about half the total number of test examples right.
To fix this situation, all one needs to do is to go back to the SVM operator in
the process and change the kernel type topolynomial (default degree 2.0) and
FIGURE 4.55
Setup for the nonlinear SVM demo model. SVM , Support Vector Machine.4.6 Support Vector Machines 145rerun the analysis. This time the points will be able to be classified with
100% accuracy as seen in Fig. 4.58 . The same result will be obtained if a
radial kernel is tried as well.
The point of this exercise was to demonstrate the flexibility of SVMs and the
ease of performing such trials using RapidMiner. Unfortunately, with more real-
istic datasets, there is no way of knowing beforehand which kernel type would
work best. The solution is to nest the SVM within an Optimization operator
and explore a host of different kernel types and kernel parameters until one is
FIGURE 4.56
Prediction accuracy of a linear (dot) kernel SVM on nonlinear data. SVM , Support Vector Machine.
FIGURE 4.57
Visualizing the prediction from linear SVM. SVM , Support Vector Machine.146 CHAPTER 4: Classificationfound that performs reasonably well. (Optimization using RapidMiner is
described in Chapter 15: Getting started with RapidMiner.)
Parameter Settings
There are many different parameters that can be adjusted depending on thetype of kernel function that is chosen. There is, however, one parameter thatis critical in optimizing SVM performance: this is the SVM complexity con-
stant, C, which sets the penalties for misclassification, as was described in an
earlier section. Most real-world datasets are not cleanly separable and, there-fore, will require the use of this factor. For initial trials, however, it is best to
go with the default settings.
4.6.3 Conclusion
A disadvantage with higher order SVMs is the computational cost. In general,
since SVMs have to compute the dot product for every classification (and
during training), extremely high dimensions or a large number of attributes
can result in slow computation times. However, this disadvantage is offset bythe fact that once an SVM model is built, small changes to the training data
will not result in significant changes to the model coefficients as long as the
support vectors do not change. This overfitting resistance is one of the rea-sons why SVMs have emerged as one of the most versatile among machine
learning algorithms.
In summary, the key advantages of SVM are:
1.Flexibility in application: SVMs have been applied for activities from
image processing to fraud detection to text mining.
2.Robustness: Small changes in data do not require expensive remodeling.
3.Overfitting resistance: The boundary of classes within datasets can be
adequately described usually by only a few support vectors.
These advantages have to be balanced with the somewhat high computa-
tional costs.
FIGURE 4.58
Classifying the two-ring nonlinear problem using a polynomial SVM kernel. SVM , Support Vector Machine.4.6 Support Vector Machines 1474.7 ENSEMBLE LEARNERS
In supervised machine learning, the objective is to build a model that can
explain the relationship between inputs and output. The model can be con-
sidered as a hypothesis that can map new input data to predicted output. For
a given training set, multiple hypotheses can explain the relationship with
varying degrees of accuracy. While it is difficult to find the exact hypothesis
from an infinite hypothesis space, one would like the modeling process to
find the hypothesis that can bestexplain the relationship with least error.
Ensemble methods or learners optimize the hypothesis-finding problem by
employing an array of individual prediction models and then combining
them to form an aggregate hypothesis or model. These methods provide a
technique for generating a better hypothesis by combining multiple hypothe-
ses into one. Since a single hypothesis can be locally optimal or overfit a par-
ticular training set, combining multiple models can improve the accuracy by
forcing a meta-hypothesis solution. It can be shown that in certain condi-
tions, the combined predictive power of the ensemble model is better than
the predictive power of individual models. Since different methods often
capture different features of the solution space as part of any one model, the
model ensembles have emerged as the most important technique for many
practical classification problems.
Wisdom of the Crowd
Ensemble models have a set of base models that accept the same inputs and
predict the outcome independently. Then the outputs from all of these base
models are combined, usually by voting, to form an ensemble output. This
approach is similar to decision-making by a committee or a board. The method
of improving accuracy by drawing together the prediction of multiple models is
called meta learning . A similar decision-making methodology is seen in higher
courts of justice, corporate boards, and various committees. The logic here is:
while individual members of the committee have biases and opinions, collec-
tive decision-making is better than one individual ’s assessment. Ensemble
methods are used to improve the error rate and overcome the modeling bias of
individual models. They can produce one strong learner by combining many
weak learners. Fig. 4.59 provides the framework of ensemble models.
The predicted class with more votes from the base learners is the output of
the combined ensemble model. Base models predict the outcome with varied
degrees of accuracy. Hence, one can weigh the vote by the accuracy rate of
individual models, which causes base models with higher accuracy to have
higher representation in the final aggregation than models with a lower accu-
racy rate ( Dietterich, 2007 ).148 CHAPTER 4: ClassificationFIGURE 4.59
Ensemble model.
PREDICTING DROUGHT
Drought is a period of time where a region experiences
far less than average water supply. With the onset of cli-
mate change, there has been an increase in frequency
and duration of drought conditions in many parts of the
world. Immediate drought is caused by the development of
high-pressure regions, thereby inhibiting the formation of
clouds, which results in low precipitation and lower
humidity. Predicting drought conditions in a region is an
extremely challenging task. There is no clear start and
end point for drought duration. There are too many vari-
ables that impact the climate patterns that lead to drought
conditions. Hence, there is no strong model to predictdrought well ahead of time ( Predicting Drought, 2013 ).
Predicting drought seasons in advance would provide time
for regional administrations to mitigate the consequences
of the drought.
Droughts involve myriad factors including groundwater
level, air stream flow, soil moisture, topology, and large-
scale global weather patterns like El Nino and La Nina
(Patel, 2012 ). With thousands of attributes and many
unknown variables that influence the conditions for
drought, there is no “silver bullet ”massive model for pre-
dicting when drought is going to hit a region with a high
(Continued )4.7 Ensemble Learners 1494.7.1 How It Works
Here ’s an example of a hypothetical corporate boardroom with three board
members. Assume that individually each board member makes wrong deci-
sions about 20% of time. The board needs to make a yes/no decision for amajor project proposal by looking at the merits. If all board members make
consistent unanimous decisions every time, then the error rate of the board
as a whole is 20%. But, if each board member ’s decisions are independent and
if their outcomes are not correlated, the board makes an error only when
more than two board members make an error at the same time. The board
makes an error only when the majority of its members make an error. Theerror rate of the board can be calculated using the binomial distribution.
In binomial distribution, the probability of ksuccesses in nindependent
trials each with a success rate of pis given by a probability mass function:
pðkÞ5n
k0
@1
Apkð12pÞn2k
PðBoard wrong Þ5n
30
@1
Apkð12pÞn2k1n
20
@1
Apkð12pÞn2k
53
30
@1
A0:23ð120:2Þ013
20
@1
A0:22ð120:2Þ1
50:00810:96
50:104
510:4%ð4:21Þ
In this example, the error rate of the board (10.4%) is lessthan the error rate
of the individuals (20%)! One can, therefore, see the impact of collective
decision-making. A generic formula for calculating error rate for the ensem-ble is given by:
Pðensemble wrong Þ5Pðk$round ðn=2ÞÞ5Xn
k5n=2n
k/C16/C17
Pkð12PÞn2k(Continued )
degree of accuracy. What there is, is many different
“weak ”models that use some of the thousands of attri-
butes available, which make predictions marginally betterthan pure chance. These weak models may provide differ-
ent drought predictions for the same region and time,
based on the diverse input variables for each model. Theprediction can be summarized by combining the predic-
tions of individual models. Ensemble models provide a
systematic method to combine many weak models intoone better model. Most of the data science models
deployed in production applications are ensemble models.150 CHAPTER 4: Classificationwhere nis the number of the base models. Some important criteria to
note are:
1.Each member of the ensemble should be independent.
2.The individual model error rate should be less than 50% for binaryclassifiers.
If the error rate of the base classifier is more than 50%, its prediction power
is worse than pure chance and hence, it is not a good model to begin with.
Achieving the first criterion of independence amongst the base classifier isdifficult. However, there are a few techniques available to make base models
as diverse as possible. In the board analogy, having a board with diverse and
independent members makes statistical sense. Of course, they all have tomake the right decision more than half the time.
Achieving the Conditions for Ensemble Modeling
One will be able to take advantage of the combined decision-making powerof the ensemble model only if the base models are good to begin with.While meta learners can form a strong learner from several weak learners,
those weak learners should be better than random guessing. Because all the
models are developed based on the same training set, the diversity and inde-pendence condition of the model is difficult to accomplish. While complete
independence of the base models cannot be achieved, one can take steps to
promote independence by changing the training sets for each base model,varying the input attributes, building different classes of modeling techniques
and algorithms, and changing the modeling parameters to build the base
models. To achieve diversity in the base models, one can alter the conditionsin which the base model is built. The most commonly used conditions are:
1.Different model algorithms : The same training set can be used to build
different classifiers, such as decision trees using multiple algorithms,naïve Bayesian, k-NNs, ANNs, etc. The inherent characteristics of these
models are different, which yield different error rates and a diverse
base model set.
2.Parameters within the models: Changing the parameters like depth of the
tree, gain ratio, and maximum split for the decision tree model canproduce multiple decision trees. The same training set can be used to
build all the base models.
3.Changing the training record set: Since the training data is the key
contributor to the error in a model, changing the training set to build
the base model is one effective method for building multiple
independent base models. A training set can be divided into multiplesets and each set can be used to build one base model. However, this
technique requires a sufficiently large training set and is seldom used.4.7 Ensemble Learners 151Instead, one can sample training data with replacements from a dataset
and repeat the same process for each base model.
4.Changing the attribute set: Similar to changing the training data where a
sample of records is used for the building of each base model, one can
sample the attributes for each base model. This technique works if the
training data have a large number of attributes.
In the next few sections, specific approaches to building ensemble models
will be reviewed based on the mentioned techniques for promoting indepen-
dence among base models. There are some limitations to using ensemble
models. If different algorithms are used for the base models, they impose dif-
ferent restrictions on the type of input data that can be used. Hence, it could
create a superset of restrictions to inputs for an ensemble model.
4.7.2 How to Implement
In data science tools, ensemble modeling operators can be found in meta
learning or ensemble learning groupings. In RapidMiner, since ensemble
modeling is used in the context of predicting, all the operators are located in
Modeling .Classification and Regression .Meta Modeling. The process of
building ensemble models is similar to that of building any classification
models like decision trees or neural networks. Please refer to previous classifi-
cation algorithms for steps to develop individual classification processes and
models in RapidMiner. In the next few pages the implementation of ensem-
ble modeling will be reviewed with simple voting and a couple of other tech-
niques to make the base models independent by altering examples for the
training set.
Ensemble by Voting
Implementing an ensemble classifier starts with building a simple base classi-
fication process. For this example, a decision tree process can be built with
the Iris dataset as shown in Section 4.1 Decision Trees. The standard decision
tree process involves data retrieval and a decision tree model, followed by
applying the model to an unseen test dataset sourced from the Iris dataset
and using a performance evaluation operator. To make it an ensemble
model, the Decision Tree operator has to be replaced with the Vote operator
from the meta learning folder. All other operators will remain the same. The
ensemble process will look similar to the process shown in Fig. 4.60 .
The Vote operator is an ensemble learner that houses multiple base models
in the inner sub-process . The model output from the vote process behaves like152 CHAPTER 4: Classificationany other classification model and it can be applied in any scenario where a
decision tree can be used. In the apply model phase, the predicted classes are
tallied up amongst all the base classifiers and the class with the highest num-
ber of votes is the predicted class for the ensemble model.
On double-clicking the nested Vote meta modeling operator, multiple base
classification models can be added inside the nested operator. All these mod-
els accept the same training set and provide an individual base model as out-
put. In this example three models have been added: decision tree, k-NN, and
naïve Bayes. Fig. 4.61 shows the inner sub-process of the Vote meta modeling
operator. The act of tallying all the predictions of these base learners and pro-
viding the majority prediction is the job of the meta model —theVote model-
ing operator. This is the aggregation step in ensemble modeling and in
RapidMiner it is called a stacking model. A stacking model is built into the
Vote operator and is not visible on the screen.
The ensemble process with the Vote meta model can be saved and executed.
Once the process is executed, the output panel of the performance vector is no
different than a normal performance vector. Since this process has a meta
model, the model panel in the results window shows new information
(Fig. 4.62 ). The model sub-process shows all the individual base models and
one stacking model. The Vote meta model is simple to use wherever an individ-
ual base model could have been used independently. The limitation of the
model is that all the base learners use the same training dataset and different
base models impose restrictions on what data types they can accept.
FIGURE 4.60
Data mining process using ensemble model.4.7 Ensemble Learners 153Bootstrap Aggregating or Bagging
Bagging is a technique where base models are developed by changing the
training set for every base model. In a given training set Tofnrecords, m
training sets are developed each with nrecords, by sampling with replace-
ment. Each training set T1,T2,T3,...,Tmwill have the same record count of
nas the original training set T. Because they are sampled with replacement,
FIGURE 4.61
Sub-process inside the Vote operator.
FIGURE 4.62
Output of ensemble model based on voting.154 CHAPTER 4: Classificationthey can contain duplicate records. This is called bootstrapping . Each sampled
training set is then used for a base model preparation. Through bootstrap-
ping, one has a set of m base models and the prediction of each model is
aggregated for an ensemble model. This combination of bootstrapping and
aggregating is called bagging.
On average, each base training set T icontains about 63% unique training
records as compared to the original training set T. Sampling with replace-
ment of nrecords contains 1 2(121/n)nunique records. When nis suffi-
ciently large one gets 1 21/e563.2% unique records on average. The rest of
the data contains duplicates from already sampled data. The process of bag-
ging improves the stability of unstable models. Unstable models like deci-
sion trees and neural network are highly susceptible even to slight changes in
the training data. Because a bagging ensemble combines multiple hypotheses
of the same data, the new aggregate hypothesis helps neutralize these train-
ing data variations.
Implementation
The Bagging operator is available in the meta learning folder: Modeling .
Classification and Regression .Meta modeling .Bagging. Like the Vote meta
operator, Bagging is a nested operator with an inner sub-process. Unlike the
vote process, bagging has only one model in the inner sub-process. Multiple
base models are generated by changing the training dataset internally. The
Bagging operator has two parameters.
1.Sample ratio: Indicates the fraction of records used for training.
2.Iterations (m): Number of base modes that need to be generated.
Fig. 4.63 shows the RapidMiner process for the Bagging operator. Fig. 4.64
shows the inner sub-process for the Bagging operator with one model specifi-
cation. Internally, multiple base models are generated based on iterations
(m) configured in the Bagging parameter. The RapidMiner process for bag-
ging can be saved and executed. Similar to the Vote meta model, the Bagging
meta model acts as one model with multiple base models inside. The results
window shows the labeled example set, performance vector, and bagging
model description. In the results window, all m(in this case 10) models can
be examined that are developed based on miterations of the training set. The
base model results are aggregated using simple voting. Bagging is particularly
useful when there is an anomaly in the training dataset that impacts the indi-
vidual model significantly. Bagging provides a useful framework where the
same data science algorithm is used for all base learners. However, each base
model differs because the training data used by the base learners are differ-
ent. Fig. 4.65 shows the model output of the Bagging meta model with con-
stituent decision trees.4.7 Ensemble Learners 155Boosting
Boosting offers another approach to building an ensemble model by manipu-
lating training data similar to bagging. As with bagging, it provides a solution
to combine many weak learners into one strong learner, by minimizing bias
or variance due to training records. Unlike bagging, boosting trains the base
models in sequence one by one and assigns weights for all training records.The boosting process concentrates on the training records that are hard to clas-
sify and over-represents them in the training set for the next iteration.
The boosting model is built with an iterative and sequential process where a
base model is built and tested with all of the training data. Based on the out-
come, the next base model is developed. To start with, all training records
FIGURE 4.63
Ensemble process using bagging.
FIGURE 4.64
Bagging sub-process.156 CHAPTER 4: Classificationhave equal weight. The weight of the record is used for the sampling selec-
tion with replacement. A training sample is selected based on the weight and
then used for model building. Then the model is used for testing with the
whole training set. Incorrectly classified records are assigned a higher weight
so hard-to-classify records have a higher propensity of selection for the next
round. The training sample for the next round will be over-represented with
incorrectly classified records from the previous iteration. Hence, the next
model will focus on the hard-to-classify data space.
Boosting assigns the weight for each training record and has to adaptively
change the weight based on the difficulty of classification. This results in an
ensemble of base learners specialized in classifying both easy-to-classify and
hard-to-classify records. When applying the model, all base learners are com-
bined through a simple voting aggregation.
AdaBoost
AdaBoost is one of the most popular implementations of the boosting
ensemble approach. It is adaptive because it assigns weights for base models
(α) based on the accuracy of the model and changes the weights of the train-
ing records ( w) based on the accuracy of the prediction. Here is the frame-
work of the AdaBoost ensemble model with mbase classifiers and ntraining
records (( x1,y1), (x2,y2),...,(xn,yn)). The steps involved in AdaBoost are:
1.Each training record is assigned a uniform weight wi51/n.
2.Training records are sampled and the first base classifier bk(x) is built.
3.The error rate for the base classifier can be calculated by Eq. (4.22) :
ek5Xn
k51wi3IðbkðxiÞ6¼yiÞð 4:22Þ
where I(x)51 when the prediction is right and 0 when the prediction
is incorrect.
FIGURE 4.65
Output of bagging models.4.7 Ensemble Learners 1574.The weight of the classifier can be calculated as αk5ln (12ek)/ek.I f
the model has a low error rate, then the weight of the classifier is high
and vice versa.
5.Next, the weights of all training records are updated by:
wk11ði11Þ5wkðiÞ3eðαkFðbkðxiÞ6¼yiÞ
where F(x)521 if the prediction is right and F(x)51 if the prediction
is wrong.
Hence, the AdaBoost model updates the weights the training records based
on the prediction and the error rate of the base classifier. If the error rate is
more than 50%, the record weight is not updated and reverted back to the
next round.
Implementation
The AdaBoost operator is available in the meta learning folder: Modeling .
Classification and Regression .Meta modeling .AdaBoost. The operator
functions similar to Bagging and has an inner sub-process. The number of
iterations or base models is a configurable parameter for the AdaBoost opera-
tor.Fig. 4.66 shows the AdaBoost data science process. This example uses the
Iris dataset with the Split Data operator for generating training and test data-
sets. The output of the AdaBoost model is applied to the test set and the per-
formance is evaluated by the Performance operator.
The number of iterations used in the AdaBoost is ten, which is specified in
the parameter. In the inner process, the model type can be specified. In this
example the decision tree model is used. The completed RapidMiner process
is saved and executed. The result window has the output ensemble model,
base models, predicted records, and the performance vector. The model
FIGURE 4.66
Data mining process using AdaBoost.158 CHAPTER 4: Classificationwindow shows the decision trees for the base classifiers. Fig. 4.67 shows the
result output for the AdaBoost model.
Random Forest
Recall that in the bagging technique, for every iteration, a sample of training
records is considered for building the model. The random forest technique
uses a concept similar to the one used in bagging. When deciding on split-
ting each node in a decision tree, the random forest only considers a random
subset of the attributes in the training set. To reduce the generalization error,
the algorithm is randomized in two levels, training record selection and attri-
bute selection, in the inner working of each base classifier. The random for-
ests concept was first put forward by Leo Breiman and Adele Cutler
(Breiman, 2001 ).
In general, the model works using the following steps. If there are ntraining
records with mattributes, and knumber of trees in the forest; then for each
tree:
1.Ann-size random sample is selected with replacement. This step is
similar to bagging.
2.A number Dis selected, where D{m.Ddetermines the number of
attributes to be considered for node splitting.
3.A decision tree is started. For each node, instead of considering all m
attributes for the best split, a random number of Dattributes are
considered. This step is repeated for every node.
4.As in any ensemble, the greater the diversity of the base trees, the lower
the error of the ensemble.
Once all the trees in the forest are built, for every new record, all the trees
predict a class and vote for the class with equal weights. The most predicted
FIGURE 4.67
Output of AdaBoost model.4.7 Ensemble Learners 159class by the base trees is the prediction of the forest ( Gashler, Giraud-Carrier,
& Martinez, 2008 ).
Implementation
The Random Forest operator is available in Modeling .Classification and
Regression .Tree Induction .Random Forest. It works similarly to the other
ensemble models where the user can specify the number of base trees. Since
the inner base model is always a decision tree, there is no explicit inner sub-
process specification. Bagging or boosting ensemble models require explicit
inner sub-process specification. All the tree-specific parameters like leaf size,
depth, and split criterion can be specified in the Random Forest operator. The
key parameter that specifies the number of base trees is Number of Trees
parameter. Fig. 4.68 shows the RapidMiner process with the Iris dataset, the
Random Forest modeling operator, and the Apply Model operator. For this
example, the number of base trees is specified as 10. The process looks and
functions similarly to a simple decision tree classifier.
Once the process is executed, the results window shows the model, predicted
output, and performance vector. Similar to other meta model outputs, the
Random Forest model shows the trees for all base classifiers. Fig. 4.69 shows
the model output for the Random Forest operator. Notice that the nodes are
different in each tree. Since the attribute selection for each node is random-
ized, each base tree is different. Thus, the Random Forest models strive to
reduce the generalization error of the decision tree model. The Random
Forest models are extremely useful as a baseline ensemble model for compar-
ative purposes.
FIGURE 4.68
Data mining process using the Random Forest operator.160 CHAPTER 4: Classification4.7.3 Conclusion
Most of the data science models developed for production applications are
built on ensemble models. They are used in a wide range of applications,
including political forecasting ( Montgomery, Hollenbach, & Ward, 2012 ),
weather pattern modeling, media recommendation, web page ranking
(Baradaran Hashemi, Yazdani, Shakery, & Pakdaman Naeini, 2010 ), etc. Since
many algorithms approach the problem of modeling the relationship between
input and output differently, it makes sense to aggregate the predictions of a
diverse set of approaches. Ensemble modeling reduces the generalization error
that arises due to overfitting the training dataset. The four ensemble techni-
ques discussed provide fundamental methods of developing a cohort of base
models by choosing different algorithms, changing parameters, changing train-
ing records, sampling, and changing attributes. All these techniques can be
combined into one ensemble model. There is no one approach for ensemble
modeling; all the techniques discussed in this chapter were proven to perform
better than base models as long as they are diverse ( Polikar, 2006 ). The wis-
dom of crowds makes sense in data science as long as “group thinking ”is con-
trolled by promoting independence amongst base models.
References
Altman, N. S. (1992). An introduction to kernel and nearest-neighbor nonparametric regression.
The American Statistician ,46(3), 175 /C0185.
Baradaran Hashemi, H., Yazdani, N., Shakery, A., & Pakdaman Naeini, M. (2010). Application of
ensemble models in web ranking. In: 2010 5th International symposium on telecommunications
(pp. 726 /C0731). doi:10.1109/ISTEL.2010.5734118.
Breiman, L. (2001). Random forests. Machine Learning ,45,5/C032.
Brieman, L. F. (1984). Classification and regression trees . Chapman and Hall.
Cohen W.W. (1995). Fast effective rule induction. Machine learning. In: Proceedings of the twelfth
international conference .
FIGURE 4.69
Output of Random Forest models.References 161Cortes, C. A. (1995). Support vector networks. Machine Learning , 273 /C0297.
Cover, T. A. (1991). Entropy, relative information, and mutual information. In T. A. Cover (Ed.),
Elements of information theory (pp. 12 /C049). John Wiley and Sons.
Dietterich, T.G. Ensemble methods in machine learning . (2007). Retrieved from ,http://www.eecs.
wsu.edu/ Bholder/courses/CptS570/fall07/papers/Dietterich00.pdf ..
Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Human
Genetics ,7, 179 /C0188. Available from https://doi.org/10.1111/j.1469-1809.1936.tb02137.x .
Fletcher, R. (1987). Practical methods of optimization . New York: John Wiley.
Gashler, M., Giraud-Carrier, C., & Martinez T. (2008) Decision tree ensemble: small heteroge-
neous is better than large homogeneous. In: 2008 Seventh international conference on machine
learning and applications (pp. 900 /C0905). doi:10.1109/ICMLA.2008.154.
Grabusts, P. (2011). The choice of metrics for clustering algorithms. In: Proceedings of the 8th
international scientific and practical conference II (1) (pp. 70 /C076).
Haapanen, R., Lehtinen, K., Miettinen, J., Bauer, M.E., & Ek, A.R. (2001). Progress in adapting k-
NN methods for forest mapping and estimation using the new annual forest inventory and
analysis data. In: Third annual forest inventory and analysis symposium (p. 87).
Hill, T., & Lewicki, P. (2007). Statistics methods and applications methods. StatSoft, Inc. Tulsa,
OK.
Hsu, C.-W., Chang, C.-C., & Lin, C.-J. (2003). A practical guide to support vector classification (4th
ed.). Taipei: Department of Computer Science, National Taiwan University.
Laine, A. (2003). Neural networks .Encyclopedia of computer science (4th ed., pp. 1233 /C01239). John
Wiley and Sons Ltd.
Langley, P., & Simon, H. A. (1995). Applications of machine learning and rule induction.
Communications of the ACM ,38(11), 54 /C064. doi:10.1145/219717.219768.
Li, E. Y. (1994). Artificial neural networks and their business applications. Information &
Management ,27(5), 303 /C0313. Available from https://doi.org/10.1016/0378-7206(94)90024-8 .
Matan, O., et al. (1990). Handwritten character recognition using neural network architectures.
In:4th USPS advanced technology conference (pp. 1003 /C01011).
McInerney, D. (2005). Remote sensing applications k-NN classification. In: Remote sensing
workshop . ,http://www.forestry.gov.uk/pdf/DanielMcInerneyworkshop.pdf/$FILE/
DanielMcInerneyworkshop.pdf .Retrieved on April 27, 2014.
Meagher, P. Calculating entropy for data mining .PHP Dev Center . (2005). ,http://www.onlamp.
com/pub/a/php/2005/01/06/entropy.html?page 51.Retrieved from O ’Reilly OnLamp.com .
Mierswa, I., Wurst, M., Klinkenberg, R., Scholz, M., & Euler, T. (2006). YALE: Rapid prototyping for
complex data mining tasks. In: Proceedings of the ACM SIGKDD international conference on knowl-
edge discovery and data mining (Vol. 2006, pp. 935 /C0940). doi:10.1145/1150402.1150531.
Montgomery, J. M., Hollenbach, F. M., & Ward, M. D. (2012). Improving predictions using
ensemble Bayesian model averaging. Political Analysis ,20(3), 271 /C0291.
Patel, P. Predicting the future of drought prediction .IEEE Spectrum .( 2 0 1 2 ) . ,http://spectrum.ieee.org/
energy/environment/predicting-the-future-of-drought-prediction .Retrieved April 26, 2014.
Peterson, L. k-Nearest neighbors .Scholarpedia . (2009). Retrieved from ,http://www.scholarpedia.
org/article/K-nearest_neighbor ..
Polikar, R. (2006). Ensemble based systems in decision making. IEEE Circuits and Systems
Magazine ,2 1/C045.
National Drought Mitigation Center . Predicting drought. (2013). ,http://drought.unl.edu/
DroughtBasics/PredictingDrought.aspx .Retrieved April 26, 2014.162 CHAPTER 4: ClassificationProcess Software, (2013). Introduction to Bayseian filtering. In: PreciseMail whitepapers (pp.
1/C08). Retrieved from ,www.process.com ..
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning ,1(1), 81 /C0106.
Rish, I. (2001). An empirical study of the naïve Bayes classifier. In: IBM research report .
Sahami, M., Dumais, S., Heckerman, D., & Horvitz, E. (1998). A Bayesian approach to filtering
junk e-mail. Learning for text categorization. In: Papers from the 1998 workshop. vol. 62,
pp. 98 /C0105.
Saian, R., & Ku-Mahamud, K. R. (2011) Hybrid ant colony optimization and simulated anneal-
ing for rule induction. In: 2011 UKSim 5th European symposium on computer modeling and simu-
lation (pp. 70 /C075). doi:10.1109/EMS.2011.17.
Shannon, C. (1948). A mathematical theory of communication. Bell Systems Technical Journal ,
379/C0423.
Smola, A. J., & Schölkopf, B. (2004). A tutorial on support vector regression. Statistics and
Computing ,14(3), 199 /C0222.
Tan, P.-N., Michael, S., & Kumar, V. (2005). Classfication and classification: Alternative techni-
ques. In P.-N. Tan, S. Michael, & V. Kumar (Eds.), Introduction to data mining (pp. 145 /C0315).
Boston, MA: Addison-Wesley.
Zdziarski, J. A. (2005). Ending spam: Bayesian content filtering and the art of statistical language classi-
fication . No Starch Press.References 163CHAPTER 5
Regression Methods
In this chapter, one of the most commonly used data science techniques —fit-
ting data with functions or function fitting will be explored. The basic idea
behind function fitting is to predict the value (or class) of a dependent attri-
bute y, by combining the predictor attributes Xinto a function, y5f(X).
Function fitting involves many different techniques and the most common
ones are linear regression for numeric prediction and logistic regression for clas-
sification. These two, form the majority of the material in this chapter.
Regression models continue to be one of the most common analytics tools
used by practitioners today.1
Regression is a relatively old technique dating back to the Victorian era
(1830s to the early 1900s). Much of the pioneering work was done by Sir
Francis Galton, a distant relative of Charles Darwin, who came up with the
concept of regressing toward the mean while systematically comparing chil-
dren ’s heights against their parents ’heights. He observed there was a strong
tendency for tall parents to have children slightly shorter than themselves,
and for short parents to have children slightly taller than themselves. Even if
the parents ’heights were at the tail ends of a bell curve or normal distribu-
tion, their children ’s heights tended toward the mean of the distribution.
Thus, in the end, all the samples regressed toward a population mean.
Therefore, this trend was called regression by Galton ( Galton, 1888 ) and,
thus, the foundations for linear regression were laid.
In the first section of this chapter, the theoretical framework for the simplest
of function-fitting methods: the linear regression model, will be provided. The
main focus will be on a case study that demonstrates how to build regression
models. Due to the nature of the function-fitting approach, one limitation
1Rexer Analytics survey available from http://www.rexeranalytics.com .
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00005-8
©2019 Elsevier Inc. All rights reserved.165that modelers have to deal with is what is called the curse of dimensionality .A s
the number of predictors X, increases, not only will our ability to obtain a
good model reduce, it also adds computational and interpretational com-plexity. Feature selection methods will be introduced that can reduce the
number of predictors or factors required to a minimum and still obtain a
good model. The mechanics of implementation, will be explored, to dothe data preparation, model building, and validation. Finally, in closing
some checkpoints to ensure that linea r regression is used correctly will
be described.
In the second section of this chapter logistic regression will be discussed.
Strictly speaking, it is a classification technique, closer in its applicationto decision trees or Bayesian methods. But it shares an important characteris-
tic with linear regression in its function-fitting methodology and, thus, merits
inclusion in this chapter, rather than the previous one on classification.
5.1 LINEAR REGRESSION
Linear regression is not only one of the oldest data science methodologies,but it also the most easily explained method for demonstrating function
PREDICTING HOME PRICES
What features would play a role in deciding the value of a
home? For example, locality, the number of rooms, its
age, the quality of schools in the vicinity, its location with
respect to major sources of employment, and its accessi-bility to major highways, are some of the important con-
siderations most potential home buyers would like to
factor in. But which of these are the most significant influ-encers of the price? Is there a way to determine these?
Once these factors are known, can they be incorporated
into a model that can be used for predictions? The casestudy that will be discussed later in this chapteraddresses this problem, using multiple linear regressions
to predict the median home prices in an urban region
given the characteristics of a home.
A common goal that all businesses have to address in
order to be successful is growth, in revenues and profits.
Customers are what will enable this to happen.
Understanding and increasing the likelihood that someonewill buy again from the company is, therefore, critical.
Another question that would help strategically, forexample in customer segmentation, is being able to
predict how much money a customer is likely to spend,
based on data about their previous purchase habits. Two
very important distinctions need to be made here: under-standing why someone purchased from the company will
fall into the realm of explanatory modeling, whereas,
predicting how much someone is likely to spend will fallinto the realm of predictive modeling. Both these types of
models fall under a broader category of surrogate or
empirical models which relies on historical data todevelop rules of behavior as opposed to system models
which use fundamental principles (such as laws of physics
or chemistry) to develop rules. See Fig. 1.2 for a taxonomy
of data science. In this chapter, the predictive capability ofmodels will be focused on as opposed to the explanatory
capabilities. Historically much of applied linear regression
in statistics has been used for explanatory needs. Later onin this chapter with the case of logistic regression, it will
be demonstrated, how both needs can be met with good
analytical interpretation of models.
(Continued )166 CHAPTER 5: Regression Methodsfitting. The basic idea is to come up with a function that explains and pre-
dicts the value of the target variable when given the values of the predictor
variables.
5.1.1 How it Works
A simple example is shown in Fig. 5.1 : if one would like to know the effect
of the number of rooms in a house (predictor) on its median sale price (tar-
get). Each data point on the chart corresponds to a house ( Harrison, 1978 ).
It is evident that on average, increasing the number of rooms tends to also
increase median price. This general statement can be captured by drawing a
straight line through the data. The problem in linear regression is, therefore,
finding a line (or a curve) that best explains this tendency. If there are two
predictors, then the problem is to find a surface (in a three-dimensional
space). With more than two predictors, visualization becomes difficult and
one has to revert to a general statement where the dependent variables are
expressed as a linear combination of independent variables:
y5b01b1x11b2x21?1bnxn ð5:1Þ
Consider the problem with one predictor. Clearly, one can fit an infinite
number of straight lines through a given set of points such as the ones
shown in Fig. 5.1 . How does one know which one is the best? A metric is(Continued )
Empirical
Models
Predictive Explanatory
Unsupervised Supervised
Classification Regression Clustering Association5.1 Linear Regression 167needed, one that helps quantify the different straight line fits through the
data. Once this metric is found, then selecting the best line becomes a matter
of finding the optimum value for this quantity.
A commonly used metric is the concept of an error function. Suppose one
fits a straight line through the data. In a single predictor case, the predicted
value, ŷ, for a value of xthat exists in the dataset is then given by:
^y5b01b1x ð5:2Þ
Then, error is simply the difference between the actual target value and pre-
dicted target value:
e5y2^y5y2ðb01b1xÞð 5:3Þ
This equation defines the error at a single location ( x,y) in the dataset. One
could easily compute the error for all existing points to come up with an
aggregate error. Some errors will be positive, and others will be negative. Thedifference can be squared to eliminate the sign bias and an average error for
a given fit can be calculated as:
J
n5Pe2
n5Pðyi2y_iÞ2
n5Pðyi2b02bixiÞ2
nð5:4Þ
where nrepresents the number of points in the dataset. Jis the total squared
error. For a given dataset, the best combination of ( b0,b1) can then be found,
60
50
40
30Median home price in '000s
Number of rooms20
10
00123456789 10
FIGURE 5.1
A simple regression model.168 CHAPTER 5: Regression Methodswhich minimizes the total error, e. This is a classical minimization problem,
which is handled with methods of calculus. Stigler provides some interesting
historical details on the origins of the method of least squares, as it is known
(Stigler, 1999 ). Using the methods of calculus, the values of bcan be found,
which minimize the total error J. Specifically, one can take partial derivatives
ofJwith respect to b1and b0and set them equal to zero. Chain rule of differ-
ential calculus gives us:
@J=@b15@J=@y^@y^=@b1
.@J=@b152ðΣðyi2b02b1xiÞÞ@y^=@b150
.Σðyi2b02b1xiÞð2xiÞ50
.2ΣðyixiÞ1Σðb0xiÞ1Σðb1x2iÞ50
.ΣðyixiÞ5b0ΣðxiÞ1b1Σðx2iÞð 5:5Þ
Similarly, one can use:
@J=@b052ðΣðyi2b02b1xÞÞ@y^=@b050
.Σðyi2b02b1xiÞð21Þ50
.2ΣðyiÞ1Σðb0:1Þ1Σðb1xiÞ150
.2ΣðyiÞ1b0Σð1Þ1b1ΣðxiÞ50
.ΣðyiÞ5b0N1b1ΣðxiÞð 5:6Þ
Eqs. (5.5) and (5.6) are two equations in two unknowns, b 0and b 1, which
can be further simplified and solved to yield the expressions:
b15ðΣxiyi2yΣxiÞ=ðΣx2
i2x2ΣxiÞð 5:7Þ
b05ðyΣx2
i2xΣxiyiÞ=ðΣx2
i2xΣxiÞð 5:8Þ
b1can also be written as (5.9a):
b15Correlation ðy;xÞ3sy
sxð5:9aÞ
b05ymean2b13xmean ð5:9bÞ
where Correlation( x,y) is the correlation between xand yand sy,sxare the
standard deviations of yand x. Finally, xmean and ymean are the respective
mean values.
Practical linear regression algorithms use an optimization technique known
asgradient descent (Fletcher, 1963; Marquardt, 1963 ) to identify the combina-
tion of b0and b1which will minimize the error function given in Eq. (5.4) .
The advantage of using such methods is that even with several predictors, the
optimization works fairly robustly. When such a process is applied to the
simple example shown, one gets an equation of the form:
Median price 59:13ðnumber of rooms Þ234:7 ð5:10Þ
where b1is 9.1 and b0is234.7. From this equation, it can be calculated that
for a house with six rooms, the value of the median price is about 20 (the
prices are expressed in thousands of US dollars, c. 1970). In Fig. 5.1 it’s5.1 Linear Regression 169evident that for a house with six rooms, the actual price can range between
10.5 and 25. An infinite number of lines could have been fit in this band,
which would have all predicted a median price within this range —but the
algorithm chooses the line that minimizes the average error over the full range
of the independent variable and is, therefore, the best fit for the given dataset.
For some of the points (houses) shown in Fig. 5.1 (at the top of the chart,
where median price 550) the median price appears to be independent of the
number of rooms. This could be because there may be other factors that also
influence the price. Thus, more than one predictor will need to be modeled
and multiple linear regression (MLR) , which is an extension of simple linear
regression, will need to be used. The algorithm to find the coefficients of the
regression Eq. (5.1) can be easily extended to more than one dimension.
The single variable expression for error function in (5.4) can be generalized to
multiple variables quite easily, y^i5b01b1x11...1bDxD.I fw el e t x5[x0,x1,...,xD]
and recognize that the intercept term can be written as b0x0where x051, then
we can write (5.4) as E5PN
i51(yi2BTxi)2where Bis a vector of weights [ b0,
b1,...,bD] for a dataset with Dcolumns or features and Nsamples. Similar to
how we computed (5.7) and (5.8), we can take a derivative of Ewith respect to
each weight Band will end up with Dequations to be solved for Dweights
(one corresponding to each feature). The partial derivative for each weight is
@E=@bj5@E=@^y/C3@^yi=@bj
.@E=@bj52Σyi2BTxi/C0/C1
@^yi=@bj
.@E=@bj52Σyi2BTxi/C0/C1
2xi ðÞ
.Σyi2xi ðÞ2BTΣxiðÞ2xi ðÞ
When we consider all Dweights at the same time, this can be very simply
written in matrix form as follows:
@E=@B52 YTX/C0/C1
1BXTX/C0/C1
ð5:11Þ
Here Bis a 13Dmatrix or vector. As in the case of simple linear regression,
we can set this derivative to zero, to solve for the weights, Band get the
following expression: 2(YTX)1B(XTX)50. In this case, solving for Bnow
becomes a matrix inversion problem and results in B5(XTX)21YTX. The
reader can verify as an exercise that this matrix is dimensionally consistent
(Hint: Use the fact that the matrix shape of XisN3D,YisN31 and Bis
13D). MLR can be applied in any situation where a numeric prediction, for
example “how much will something sell for, ”is required. This is in contrast
to making categorical predictions such as “will someone buy/not buy ”or
“will/will not fail, ”where classification tools such as decision trees or logistic
regression models are used. In order to ensure regression models are not
arbitrarily deployed, several checks must be performed on the model to
ensure that the regression is accurate which will be the focus of a later section
in this chapter.170 CHAPTER 5: Regression MethodsThe housing example can be extended in order to include additional vari-
ables. This comes from a study of urban environments conducted in the late
1970s2(Harrison, 1978 ). The objectives of this are:
1.Identify which of the several attributes are required to accurately
predict the median price of a house.
2.Build a multiple linear regression model to predict the median price
using the most important attributes.
The original data consist of thirteen predictors and one response variable, which
is the variable that needs to be predicted. The predictors include physical charac-
teristics of the house (such as number of rooms, age, tax, and location) and
neighborhood features (schools, industries, zoning) among others. The response
variable is of course the median value (MEDV) of the house in thousands of
dollars. Table 5.1 shows a snapshot of the dataset, which has altogether 506
examples. Table 5.2 describes the features or attributes of the dataset.Table 5.1 Sample View of the Classic Boston Housing Dataset
CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTATTarget
5MEDV
0.00632 18 2.31 0 0.538 6.575 65.2 4.09 1 296 15.3 396.9 4.98 24
0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.9 9.14 21.6
0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 34.7
0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 33.4
0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.9 5.33 36.2
0.02985 0 2.18 0 0.458 6.43 58.7 6.0622 3 222 18.7 394.12 5.21 28.7
0.08829 12.5 7.87 0 0.524 6.012 66.6 5.5605 5 311 15.2 395.6 12.43 22.9
0.14455 12.5 7.87 0 0.524 6.172 96.1 5.9505 5 311 15.2 396.9 19.15 27.1
Table 5.2 Attributes of Boston Housing Dataset
1.CRIM per capita crime rate by town
2.ZN proportion of residential land zoned for lots over 25,000 sq.ft.
3.INDUS proportion of non-retail business acres per town
4.CHAS Charles River dummy variable ( 51 if tract bounds river; 0 otherwise)
5.NOX nitric oxide concentrations (parts per 10 million)
6.RM average number of rooms per dwelling
7.AGE proportion of owner-occupied units built prior to 1940
8.DIS weighted distances to five Boston employment centers
9.RAD index of accessibility to radial highways
10.TAX full-value property-tax rate per $10,000
11.PTRATIO pupil-teacher ratio by town
12.B 1000(Bk 20.63)^2 where Bk is the proportion of blacks by town
13.LSTAT percent lower status of the population
14.MEDV Median value of owner-occupied homes in $1000 ’s
2http://archive.ics.uci.edu/ml/datasets/Housing .5.1 Linear Regression 1715.1.2 How to Implement
In this section, how to set up a RapidMiner process to build a multiple linear
regression model for the Boston Housing dataset will be demonstrated. The
following will be described:
1.Building a linear regression model
2.Measuring the performance of the model
3.Understanding the commonly used options for the Linear Regression
operator
4.Applying the model to predict MEDV prices for unseen data
Step 1: Data Preparation
As a first step, the data is separated into a training set and an unseen test set.
The idea is to build the model with the training data and test its performance
on the unseen data. With the help of the Retrieve operator, import the raw
data (available in the companion website www.IntroDataScience.com ) into
the RapidMiner process. Apply the Shuffle operator to randomize the order of
the data so that when the two partitions are seperated, they are statistically
similar. Next, using the Filter Examples Range operator, divide the data into
two sets as shown in Fig. 5.2 . The raw data has 506 examples, which will be
linearly split into a training set (from row 1 to 450) and a test set (row
451 /C0506) using the two operators.
Insert the Set Role operator, change the role of MEDV to label and connect
the output to a Split Validation operator ’s input training port as shown in
Fig. 5.3 . The training data is now going to be further split into a training
set and a validation set (keep the d efault Split Validation options as
FIGURE 5.2
Separating the data into training and testing samples.172 CHAPTER 5: Regression Methodsis, i.e., relative ,0.7,a n d shuffled ). This will be needed in order to measure
the performance of the linear regression model. It is also a good idea to set
the local random seed (to default the value of 1992), which ensures that
RapidMiner selects the same samples if this process is run at a later time.
After this step, double-click the Validation operator to enter the nested pro-
cess. Inside this process insert the Linear Regression operator on the left win-
dow and Apply Model and Performance (Regression) in the right window as
shown in Fig. 5.4 . Click on the Performance operator and check squared error,
correlation, and squared correlation inside the Parameters options selector
on the right ( Fig. 5.5 ).
FIGURE 5.3
Using the split validation operator.
FIGURE 5.4
Applying the linear regression operator and measuring performance.5.1 Linear Regression 173Step 2: Model Building
Select the Linear Regression operator and change the feature selection option to
none. Keep the default eliminate collinear features checked, which will
remove factors that are linearly correlated from the modeling process. When
two or more attributes are correlated to one another, the resulting model will
tend to have coefficients that cannot be intuitively interpreted and, further-
more, the statistical significance of the coefficients also tends to be quite low.
Also keep the use bias checked to build a model with an intercept [the b0in
Eq.(5.2) ]. Keep the other default options intact ( Fig. 5.4 ).
When this process is run the results shown in Fig. 5.6 will be generated.
Step 3: Execution and Interpretation
There are two views that one can examine in the Linear Regression output tab:
the Description view, which actually shows the function that is fitted
(Fig. 5.6 A) and the more useful Data view, which not only shows the coeffi-
cients of the linear regression function, but also gives information about the
significance of these coefficients ( Fig. 5.6 B). The best way to read this table is
to sort it by double-clicking on the column named Code, which will sort the
different factors according to their decreasing level of significance.
RapidMiner assigns four stars ( TTTT ) to any factor that is highly significant.
In this model, no feature selection method was used and as a result all 13 fac-
tors are in the model, including AGE and INDUS, which have very low sig-
nificance. However, if the same model were to be run by selecting any of the
options that are available in the drop-down menu of the feature selection
parameter, RapidMiner would have removed the least significant factors from
the model. In the next iteration, the greedy feature selection is used, and this
will have removed the least significant factors, INDUS and AGE, from the
function ( Fig. 5.7A & B ).
FIGURE 5.5
Selecting performance criteria for the MLR.174 CHAPTER 5: Regression Methods(A)
(B)
FIGURE 5.6
(A) Description of the linear regression model. (B) Tabular view of the model. Sort the table according to significance by double-clicking
on the Code column.5.1 Linear Regression 175FIGURE 5.7
(A) Model without any feature selection. (B) Model with greedy feature selection.176 CHAPTER 5: Regression MethodsFeature selection in RapidMiner can be done automatically within the Linear
Regression operator as described or by using external wrapper functions such
as forward selection and backward elimination. These will be discussed sepa-
rately in Chapter 14, Feature Selection.
The second output to pay attention to is the Performance: a handy check to
test the goodness of fit in a regression model is the squared correlation .
Conventionally this is the same as the adjusted r2for a model, which can
take values between 0.0 and 1.0, with values closer to 1 indicating a better
model. For either of the models shown above, a value around 0.822 can be
obtained ( Fig. 5.8 ). The squared error output was also requested: the raw
value in itself may not reveal much, but this is useful in comparing two dif-
ferent models. In this case it was around 25.
One additional insight that can be extracted from the modeling process is
ranking of the factors. The easiest way to check this is to rank by p-value. As
seen in Fig. 5.9 , RM, LSTAT, and DIS seem to be the most significant factors.
This is also reflected in their absolute t-stat values. The t-stat and P-values are
the result of the hypothesis tests conducted on the regression coefficients. For
the purposes of predictive analysis, the key takeaway is that a higher t-stat
signals that the null hypothesis —which assumes that the coefficient is zero —
can be safely rejected. The corresponding p-value indicates the probability of
wrongly rejecting the null hypothesis. Its already been noted how the num-
ber of rooms (RM) was a good predictor of the home prices, but it was
unable to explain all of the variations in median price. The r2and squared
FIGURE 5.8
Generating the r2for the model.5.1 Linear Regression 177error for that one-variable model were 0.405 and 45, respectively. This can
be verified by rerunning the model built so far using only one independent
variable, the number of rooms, RM. This is done by using the Select Attributes
operator, which has to be inserted in the process before the Set Role operator.
When this model is run, the equation shown earlier, Eq. (5.10) , will be
obtained in the model Description . By comparing the corresponding values
from the MLR model (0.676 and 25) to the simple linear regression model,
it’s evident that both of these quantities have improved, thus, affirming the
decision to use multiple factors.
One now has a more comprehensive m odel that can account for much of
the variability in the response variable, MEDV. Finally, a word about the
sign of the coefficients: LSTAT refers to the percentage of low-income
households in the neighborhood. A lower LSTAT is correlated with higher
median home price, and this is the reason for the negative coefficient on
LSTAT.
Step 4: Application to Unseen Test Data
This model is now ready to be deployed against the unseen data that was cre-
ated at the beginning of this section using the second Filter Examples operator
(Fig. 5.2 ). A new Set Role operator will need to be added, select MEDV under
parameters and set it to target role prediction from the pull-down menu.
Add another Apply Model operator and connect the output of Set Role to its
unlabeled port; additionally, connect the output model from the Validation
FIGURE 5.9
Ranking variables by their P-values.178 CHAPTER 5: Regression Methodsprocess to the input model port of the new Apply Model . What has been done
is, the attribute MEDV has been changed from the unseen set of 56 examples
to a prediction. When the model is applied to this example set, one will be
able to compare the prediction (MEDV) values to the original MEDV values
(which exist in the set) to test how well the model would behave on new
data. The difference between prediction (MEDV) and MEDV is termed resid-
ual. Fig. 5.10 shows one way to quickly check the residuals for the models
application. A Rename operator would be needed to change the name of
“prediction (MEDV) ”to predictedMEDV to avoid confusing RapidMiner
when the next operator, Generate Attributes , is used to calculate residuals (try
without using the Rename operator to understand this issue as it can pop up
in other instances where Generate Attributes is used). Fig. 5.11 shows the sta-
tistics for the new attribute residuals which indicate that the mean is close to
0(20.27) but the standard deviation (and hence, variance) at 4.350 is not
quite small. The histogram also seems to indicate that the residuals are not
quite normally distributed, which would be another motivation to continue
to improve the model.
FIGURE 5.10
Setting up a process to do the comparison between the unseen data and the model predicted values.5.1 Linear Regression 1795.1.3 Checkpoints
This section on linear regression can be wrapped up with a brief discussion
of several checkpoints to ensure that any models are valid. This is a critical
step in the analytics process because all modeling follows the garbage in, gar-
bage out (GIGO) dictum. It is incumbent upon the data scientist to ensure
these checks are completed.
Checkpoint 1: One of the first checkpoints to consider before accepting any
regression model is to quantify the r2, which is also known as the coefficient
of determination which effectively explains how much variability in the
dependent variable is explained by the independent variables ( Black, 2008 ).
In most cases of linear regression, the r2value lies between 0 and 1. The ideal
range for r2varies across applications; for example, in social and behavioral
science models typically low values are acceptable. Generally, extremely low
values (B,0.2) indicate that the variables in the model do not explain the
outcome satisfactorily. A word of caution about overemphasizing the value
ofr2: when the intercept is set to zero (in RapidMiner, when one unchecks
use bias ,Fig. 5.5 ),r2values tend to be inflated because of the manner in
which they are calculated. In such situations where a zero intercept are
required, it makes sense to use other checks such as the mean and variance
of the residuals.
FIGURE 5.11
Statistics of the residuals for the unseen data show that some model optimization may be necessary.180 CHAPTER 5: Regression MethodsThe most common metric used for measuring how well a regression
model fits the data is by using the coefficient of determination, r2. This is
defined as:
r2512SSE =SSYY ð5:12Þ
SSE is simply the sum of squared errors which is given byPe2in Eq. (5.4) .
SSYY is the aggregate mean deviation defined by:
SSYY5X
ðy2yÞ2ð5:13Þ
Intuitively, it is easy to see that if SSE is close to zero, then r2is close to 1 —a
perfect fit. If SSE is close to SSYY then r2is close to zero —the model is sim-
ply predicting the average value of yfor all values of x. The nice thing about
r2is that because it only depends on yand not the weights or independent
variables, it can be used for any form of regression model: simple or
multiple.
Checkpoint 2: This leads to the next check, which is to ensure that all error
terms in the model are normally distributed. To do this check in
RapidMiner, a new attribute called error can be generated, which is the differ-
ence between the predicted MEDV and the actual MEDV in the test dataset.
This can be done using the Generate Attributes operator. This is what was
done in step 5 in the last section. Passing checks 1 and 2 will ensure that the
independent and dependent variables are related. However, this does not
imply that the independent variable is the cause and the dependent is the
effect. Remember that correlation is not causation !
Checkpoint 3: Highly nonlinear relationships, result in simple regression
models failing these checks. However, this does not mean that the two vari-
ables are not related. In such cases it may become necessary to resort to
somewhat more advanced analytical methods to test the relationship. This is
best described and motivated by Anscombe ’s quartet, presented in Chapter 3,
Data Exploration.
Checkpoint 4: In addition to testing the goodness of a model fit using r2,i ti s
also important to ensure that there is no overfitting of data. Overfitting refers
to the process of developing a model that is so well tuned to represent the
training data that its least squares error is as low as possible, but this error
becomes high when the model is used on unseen or new data. That is, the
model fails to generalize. The following example illustrates this and also
develops intuition for avoiding such behavior by using what is called
Regularization.
Consider some sample data which represent an underlying simple function
y53x11. If the data were to be plotted it would look like Fig. 5.12 .5.1 Linear Regression 181A regression model could be fit to this data and a nice linear fit obtained, as
shown by the line, as well as obtaining the following coefficients: b051.13
and b153.01, which is close to the underlying function. Now suppose that
there is a new data point [3, 30] which is somewhat of an outlier and now
the model has to be refit, the results would be similar to those shown in
Fig. 5.13 .
–2–100102030
–1 0 1 2 3
FIGURE 5.13
Linear regression line with an outlier.
–2–6–4–202468
–1 0 1 2
FIGURE 5.12
Linear regression line.182 CHAPTER 5: Regression MethodsCompared to the previous fit, the outlier tends to pull the fitted line up
toward the outlying point. In other words, as the model tries to minimize
the squared distance between it and every point, a few outlying points tend
to exert a disproportionate influence on the model's characteristics.
Another way to look at this is that the model tries to fit all the training data
points to the best of its ability or causes overfitting . A consequence of overfit-
ting is that the overall error on the training data will be minimized, but if
the same model is tried on new data points (which were not used for train-
ing), the error tends to increase till it ’s out of proportion. One symptom of
overfitting is the generation of large coefficients. In the example above, these
coefficients are now b052.14 and b156.93, that is, they have increased by a
factor of nearly 2 or more in each case
In order to avoid overfitting by ensuring that none of the weights or coeffi-
cients become large, one can add a penalty factor to the cost function that
penalizes large weights. This process is known as Ridge regression or L2-norm
regularization . The penalty includes the sum of the squared magnitude of all
weights, || b||25b2
11b2
21..., that is, L2-Norm of b m, where m is the num-
ber of attributes.
The cost function is modified as shown:
JRIDGE5ΣNi5ðyi2BTxiÞ21λbjj2ð5:14Þ
By following the usual steps and switching to a matrix form shown earlier,
one arrives at the new solution for the weights as:
B5ðλI1XTXÞ21XTY ð5:15Þ
where Iis the identity matrix and λis a penalty factor .0.
Compare this to the standard solution that can be derived from Eq. (5.11) :
B5ðXTXÞ21YTX: ð5:16Þ
When L2-norm is implemented, the fit looks much improved and the coeffi-
cients are b051.5 and b154.02 which are closer to the underlying function
(shown in Fig. 5.14 ).
Ridge regression tends to push all the weights toward zero in order to mini-
mize the cost function.
There is another scenario of overfitting, that involves selecting more than the
optimal number of independent variables (and, thus, weights). Not all fea-
tures exert the same influence on the predicted outcome, some features have
more influence than others. However, as more features are included in a
model, the training error continues to reduce. But the test error may spiral
out of control and result in another form of overfitting.5.1 Linear Regression 183Lasso regression orL1-norm regularization addresses this concern where the goal
is to select the optimal number of features. The formulation is similar to
Ridge, but using the L1-norm: || b||5|b1|1|b2|1/C1/C1/C1
JLASSO5ΣNi5ðyi2BTxiÞ21λbjj ð 5:17Þ
Sometimes the features are correlated with each other —when this happens
the XTX matrix becomes singular and the closed form solution cannot be
used. However, this should not be any concern if gradient descent is used to
approximate the solution. Gradient descent is a technique that allows us toincrementally evaluate the coefficients, bfor which the error Jis minimized,
when obtaining a closed form derivative for dJ/db is not possible. The idea is
to take small computational steps toward the direction of minimum Jby
choosing the fastest path. In this case we can however get the derivative of
the error function in closed form, which turns out to be:
@J=@b52 2XTY12XTXb1λsign ðbÞ50 ð5:18Þ
here sign( b)51i fb.0,21i fb,0, 0 if b50.
Typically for practical situations, such closed form derivatives are seldom
available and the gradient descent formulation is the alternative. The final
gradient descent setup for both types of regularization are:
–2Original
L2-Regularization
–100102030
–1 0 1 2 3
FIGURE 5.14
L2-regularization.184 CHAPTER 5: Regression MethodsLASSO or L1:
bi115bi2ηXTðY2^YÞ1λ3sign ðbiÞð 5:19Þ
RIDGE or L2:
bi115bi2ηXTðY2^YÞ1λ3bi ð5:20Þ
where ηis the same learning rate parameter that crops up in neural networks
(Chapter 4: Classification) and deep learning (Chapter 10). The number of
steps, i is determined by the rate of convergence of the solution or by other
stopping criteria. In RapidMiner, Rigde regression is implemented by provid-
ing a non-zero penalty factor in the box for ‘ridge ’under parameters —see
Fig. 5.4 .
5.2 LOGISTIC REGRESSION
From a historical perspective, there are two main classes of data science tech-
niques: those that evolved ( Cramer, 2002 ) from statistics (such as regression)
and those that emerged from a blend of statistics, computer science, and
mathematics (such as classification trees). Logistic regression arose in the
mid-twentieth century as a result of the simultaneous development of the
concept of the logit in the field of biometrics and the advent of the digital
computer, which made computations of such terms easy. So, to understand
logistic regression, the logit concept first needs to be explored. The chart in
Fig. 5.15 , adapted from data shown in Cramer (2002) shows the evolving
trend from initial acceptance of the logit concept in the mid-1950s to the
surge in references to this concept toward the latter half of the twentieth cen-
tury. The chart is an indicator of how important logistic regression has
become over the last few decades in a variety of scientific and business
applications.
To introduce the logit, a simple example will be used. Recall that linear
regression is the process of finding a function to fit the x ’s that vary linearly
with y with the objective of being able to use the function as a model for
prediction. The key assumption here are that both the predictor and target
variables are continuous, as seen in the chart in Fig. 5.16 . Intuitively, one can
state that when xincreases, yincreases along the slope of the line. For exam-
ple, advertisement spend and sales.
What happens if the target variable is not continuous? Suppose the target
variable is the response to advertisement campaigns —if more than a thresh-
old number of customers buy for example, then the response is considered5.2 Logistic Regression 185000.511.522.5
y = 1.5519x + 0.3443
R2 = 0.8079
0.2 0.4 0.6 0.8 1 1.2Sales
Linear salesSales vs Ad spend
Linear Regression Model. We can make an intuitive
assessment that increase in Ad spend  also increases
Sales . Using the straight line, we may also be able to
predict.
FIGURE 5.16
Goal of linear regression.
350
300Number of journal articles250
200
150
100
50
0
1935–39 1940–44 1945–49 1950–55 1950–59 1960–64 1965–69 1970–74 1975–79 1980–84 1985–90 1990–94
FIGURE 5.15
Growth of logistic regression applications in statistical research.186 CHAPTER 5: Regression Methodsto be 1; if not the response is 0. In this case, the target ( y) variable is discrete
(as in Fig. 5.17 ); the straight line is no longer a fit as seen in the chart.
Although one can still estimate —approximately —that when x(advertising
spend) increases, y(response or no response to a mailing campaign) also
increases, there is no gradual transition; the yvalue abruptly jumps from one
binary outcome to the other. Thus, the straight line is a poor fit for this data.
On the other hand, take a look at the S-shaped curve in Fig. 5.18 . This is cer-
tainly a better fit for the data shown. If the equation to this “sigmoid ”curve
is known, then it can be used as effectively as the straight line was used in
the case of linear regression.
Logistic regression is, thus, the process of obtaining an appropriate nonlinear
curve to fit the data when the target variable is discrete. How is the sigmoid
curve obtained? How does it relate to the predictors?
5.2.1 How It Works
The dependent variable, y, will be re-examined. If it is binomial, that is, it
can take on only two values (yes/no, pass/fail, respond/does not respond,
and so on), then y can be coded to assume only two values: 1 or 0.
00
0.2 0.4 0.6 0.8 1 1.2Response1Response vs Ad spend
Linear Fit for a Binary outcome: Although we can
make an intuitive assessment that increase in Ad spend
increases Response , the switch is abrupt - around 0.6.
Using the straight line, we cannot really predict outcome.
FIGURE 5.17
Fitting a linear model to discrete data.5.2 Logistic Regression 187The challenge is to find an equation that functionally connects the predictors,
x, to the outcome y where y can only take on two values: 0 or 1. However,
the predictors themselves may have no restrictions: they could be continuous
or categorical. Therefore, the functional range of these unrestricted predictors
is likely to also be unrestricted (between 2Nto1N). To overcome this
problem, one must map the continuous function to a discrete function. This
is what the logit helps to achieve.
How Does Logistic Regression Find the Sigmoid Curve?
As observed in Eq. (5.1) , a straight line can be depicted by only two para-
meters: the slope ( b1) and the intercept ( b0). The way in which x’s and yare
related to each other can be easily specified by b0and b1. However, an S-
shaped curve is a much more complex shape and representing it parametri-
cally is not as straightforward. So how does one find the mathematical para-
meters to relate the x’s to the y?
It turns out that if the target variable yis transformed to the logarithm of the
odds of y , then the transformed target variable is linearly related to the predic-
tors, x. In most cases where the use of logistic regression is needed; the y is
usually a yes/no type of response. This is usually interpreted as the
00
0.2 0.4 0.6 0.8 11
1.2ResponseResponse vs Ad spend
Logistic Regression Model. The S-shaped curve is
clearly a better fit for most  of the data. we can state 
spend  increases Sales , and. we may also be able to predict
using this model.Ad
FIGURE 5.18
Fitting a nonlinear curve to discrete data.188 CHAPTER 5: Regression Methodsprobability of an event happening ( y51) or not happening ( y50). This can
be deconstructed as:
GIf y is an event (response, pass/fail, etc.),
Gand pis the probability of the event happening ( y51),
Gthen (1 2p) is the probability of the event nothappening ( y50),
Gand p/(12p) are the odds of the event happening.
The logarithm of the odds, log ( p/(12p)) is linear in the predictors, X, and
log ( p/(12p)) or the log of the odds is called the logit function .
The logit can be expressed as a linear function of the predictors X, similar to
the linear regression model shown in Eq. (5.1) as:
logit5logp=ð12pÞ5b0x1b1 ð5:21Þ
For a more general case, involving multiple independent variables, x, there is:
logit5b01b1x11b2x21?1bnxn ð5:22Þ
The logit can take any value from 2Nto1N. For each row of predictors
in a dataset, the logit an now be computed. From the logit, it is easy to then
compute the probability of the response y (occurring or not occurring) as
seen below:
p5elogit=ð11elogitÞð 5:23Þ
The logistic regression model from Eq. (5.22) ultimately delivers the proba-
bility of y occurring (i.e., y51), given specific value(s) of xvia Eq. (5.23) .I n
that context, a good definition of logistic regression is that it is a mathemati-
cal modeling approach in which a best-fitting, yet least-restrictive model is
selected to describe the relationship between several independent explanatory
variables and a dependent binomial response variable. It is least-restrictive
because the right side of Eq. (5.22) can assume any value from 2Nto
1N.Cramer (2002) provides more background on the history of the logit
function.
From the data given the x’s are known and using Eqs. (5.22) and (5.23) one
can compute the pfor any given x. But to do that, the coefficients first need
to be determined, b, in Eq. (5.22) . How is this done? Assume that one starts
out with a trial of values for b. Given a training data sample, one can com-
pute the quantity:
py3ð12pÞð12yÞ
where yis the original outcome variable (which can take on 0 or 1) and pis
the probability estimated by the logit equation [Eq. (5.23) ]. For a specific
training sample, if the actual outcome was y50 and the model estimate of p
was high (say 0.9), that is, the model was wrong, then this quantity reduces5.2 Logistic Regression 189to 0.1. If the model estimate of probability was low (say 0.1), that is, the
model was good, then this quantity increases to 0.9. Therefore, this quantity,
which is a simplified form of a likelihood function, is maximized for good esti-
mates and minimized for poor estimates . If one computes a summation of the
simplified likelihood function across allthe training data samples, then a
high value indicates a good model (or good fit) and vice versa.
In reality, gradient descent or other nonlinear optimization techniques are
used to search for the coefficients, b, with the objective of maximizing the
likelihood of correct estimation (or py3(12p)(12y), summed over all
training samples). More sophisticated formulations of likelihood estimators
are used in practice ( Eliason, 1993 ).
A Simple but Tragic Example
In the 1912 shipwreck of the HMS Titanic, hundreds of people perished as
the ship struck an iceberg in the North Atlantic ( Hinde, 1998 ). When the
data is dispassionately analyzed, a couple of basic patterns emerge. 75% of
the women and 63% of the first-class passengers survived. If a passenger was
a woman and if she traveled first class, her probability of survival was 97%!
The scatterplot in Fig. 5.19 depicts this in an easy to understand way (see the
bottom left cluster).
A data science competition used the information from this event and chal-
lenged analysts to develop an algorithm that could classify the passenger list
into survivors and non-survivors.3The training dataset provided will be used
as an example to demonstrate how logistic regression could be employed to
make this prediction and also to interpret the coefficients from the model.
Table 5.3 shows part of a reduced dataset consisting only of three vari-
ables: travel class of the passenger (pclass 51st, 2nd, or 3rd), sex of the
passenger (0 for male and 1 for female), and the label variable “survived ”
(true or false). When a logistic regression model is fit to this data consist-
ing of 891 samples, the following equa tion is obtained for predicting the
class survived 5false (the details of a generic setup process will be described
in the next section):
logit52 0:650322:64173sex10:95953pclass ð5:24Þ
Comparing this to Eq. (5.22) ,b0520.6503, b1522.6417, and b250.9595.
How are these coefficients interpreted? In order to do this, Eq. (5.23) will
need to be recalled,
3http://www.kaggle.com/c/titanic-gettingStarted .190 CHAPTER 5: Regression Methodsp5elogit=½11elogit/C138
which indicates that as logit increases to a large positive quantity, the probability
that the passenger did not survive (survived 5false) approaches 1. More specifi-
cally, when logit approaches 2N,papproaches 0 and when logit approaches
1N,papproaches 1. The negative coefficient on variable sexindicates that this
probability reduces for females ( sex51) and the positive coefficient on variable
pindicates that the probability of not surviving ( survived 5false) increases the
higher the number of the travel class. This verifies the intuitive understanding
that was provided by the scatterplot shown in Fig. 5.19 .
The odds form of the logistic regression model can also be examined, which
is given as:
odds ðsurvived 5false Þ5e20:650332:6103pclass30:0712sexð5:25Þ
FIGURE 5.19
Probability of survival in the Titanic wreck based on gender and travel class.5.2 Logistic Regression 191Recall that logit is simply given by log(odds) and essentially the same equa-
tion as Eq. (5.24) is used. A key fact to observe is that a positive coefficient in
the logit model translates into a coefficient higher than 1 in the odds model
(the number 2.6103 in the above equation is e0.9595and 0.0712 is e22.6417)
and a negative coefficient in the logit model translates into coefficients smal-
ler than 1 in the odds model. Again, it is clear that the odds of not surviving
increases with travel class and reduces with gender being female.
Anodds ratio analysis will reveal the value of computing the results in this for-
mat. Consider a female passenger ( sex51). The survivability for this passen-
ger could be calculated if she was in 1st class ( pclass51) versus if she was in
2nd class as an odds ratio:
odds ðsurvived 5false 2nd class Þ=odds ðsurvived 5false 1st class Þ
52:61032=2:6103152:6103ð5:26Þ
Based on the Titanic dataset, the odds that a female passenger would not sur-
vive if she was in 2nd class increases by a factor of 2.6 compared to her odds
if she was in 1st class. Similarly, the odds that a female passenger would not
survive increases by nearly seven times if she was in 3rd class! In the next sec-
tion, the mechanics of logistic regression are discussed as well as the process
of implementing a simple analysis using RapidMiner.Table 5.3 Portion of the Dataset From the Titanic Example
pclass Sex Survived?
3.0 Male 0.0
1.0 Female 1.0
3.0 Female 1.0
1.0 Female 1.0
3.0 Male 0.0
3.0 Male 0.0
1.0 Male 0.0
3.0 Male 0.0
3.0 Female 1.0
2.0 Female 1.0
3.0 Female 1.0
1.0 Female 1.0
3.0 Male 0.0
3.0 Male 0.0
3.0 Female 0.0
2.0 Female 1.0
3.0 Male 0.0192 CHAPTER 5: Regression Methods5.2.2 How to Implement
The data used come from an example for a credit scoring exercise. The objec-
tive is to predict DEFAULT (Y or N) based on two predictors: loan age (busi-
ness age) and number of days of delinquency. There are 100 samples
Table 5.4 .
Step 1: Data Preparation
Load the spreadsheet into RapidMiner. Remember to set the DEFAULT col-
umn as Label. Split the data into training and test samples using the Split
Validation operator.
Step 2: Modeling Operator and Parameters
Add the Logistic Regression operator in the training subprocess of the Split
Validation operator. Add the Apply Model operator in the testing subprocess of
the Split Validation operator. Just use default parameter values. Add the
Performance (Binominal) evaluation operator in the testing subprocess of Split
Validation operator. Check the Accuracy, AUC, Precision, and Recall boxes in
the parameter settings. Connect all ports as shown in Fig. 5.20 .
Step 3: Execution and Interpretation
Run the model and view the results. In particular, check for the kernel model,
which shows the coefficients for the two predictors and the intercept. The
bias (offset) is 21.820 and the coefficients are given by: w[BUSAGE] 50.592
and w[DAYSDELQ] 52.045. Also check the confusion matrix for Accuracy,
Precision, and Recall and finally view the ROC curves and check the area
under the curve or AUC. Chapter 8, Model Evaluation, provides further
details on these important performance measures.Table 5.4 A Sample From the Loan Default Dataset
[Busage] [Daysdelq] [Default]
87.0 2.0 N
89.0 2.0 N
90.0 2.0 N
90.0 2.0 N
101.0 2.0 N
110.0 2.0 N
115.0 2.0 N
115.0 2.0 N
115.0 2.0 N
117.0 2.0 N5.2 Logistic Regression 193The accuracy of the model based on the 30% testing sample is 83%. (The
ROC curves have an AUC of 0.863). The next step would be to review the
kernel model and prepare for deploying this model. Are these numbers
acceptable? In particular, pay attention to the class recall (bottom row of the
confusion matrix in Fig. 5.21 ). The model is quite accurate in predicting if
someone is NOT a defaulter (91.3%), however, its performance when it
comes to identifying if someone IS a defaulter is questionable. For most pre-
dictive applications, the cost of wrong class predictions is not uniform. That
is, a false positive (in the case of identifying someone as a defaulter, when
they are not) may be less expensive than a false negative (in the case of iden-
tifying someone as a non-defaulter, when they actually are). There are ways
to weight the cost of misclassification, and RapidMiner allows this through
the use of the MetaCost operator.
Step 4: Using MetaCost
Nest the Logistic Regression operator inside a MetaCost operator to improve
class recall. The MetaCost operator is now placed inside Split Validation opera-
tor. Configure the MetaCost operator as shown in Fig. 5.22 . Notice that false
FIGURE 5.21
Confusion matrix for the testing sample.
FIGURE 5.20
Setting up the RapidMiner process for a logistic regression model.194 CHAPTER 5: Regression Methodsnegatives have twice the cost of false positives. The actual values of these
costs can be further optimized using an optimization loop —optimization is
discussed for general cases in Chapter 15, Getting Started with RapidMiner.
When this process is run, the new confusion matrix that results is shown in
Fig. 5.23 . The overall accuracy has not changed much. Note that while the
class recall for the Default 5Yes class has increased from 57% to 71%, this
has come at the price of reducing the class recall for Default 5No from 91%
to 87%. Is this acceptable? Again, the answer to this comes from examining
the actual business costs. More details about interpreting the confusion
matrix and evaluating the performance of classification models is provided
in Chapter 8, Model Evaluation.
Step 5: Applying the Model to an Unseen Dataset
In RapidMiner, logistic regression is calculated by creating a support vector
machine (SVM) with a modified loss function ( Fig. 5.24 ). SVMs were intro-
duced in Chapter 4 on Classification. That ’s the reason why support vectors
are seen at all.
FIGURE 5.23
Improved classification performance with the usage of MetaCost operator.
FIGURE 5.22
Configuring the MetaCost operator to improve class recall performance.5.2 Logistic Regression 1955.2.3 Summary Points
GLogistic regression can be considered equivalent to using linear
regression for situations where, when the target (or dependent) variable
is discrete, that is, not continuous. In principle, the response variable
or label is binomial. A binomial response variable has two categories:Yes/No, Accept/Not Accept, Default/Not Default, and so on. Logistic
regression is ideally suited for business analytics applications where
the target variable is a binary decision (fail/pass, response/noresponse, etc.).
GLogistic regression comes from the concept of the logit. The logit is thelogarithm of the odds of the response, y, expressed as a function ofindependent or predictor variables, x, and a constant term. That is, for
example, log (odds of y5Yes)5b
1x1b0.
GThis logit gives the odds of the Yes event, however if one wants
probabilities, the transformed equation will need to be used:
pðy5“Yes”Þ51=ð11eð2b1x2b0ÞÞ
GThe predictors can be either numerical or categorical for standard
logistic regression. However, in RapidMiner, the predictors can only be
numerical, because it is based on the SVM formulation.
5.3 CONCLUSION
This chapter explored two of the most common function-fitting methods.
Function-fitting methods are one of the earliest data science techniques based
on the concept of supervised learning. The multiple linear regression model
FIGURE 5.24
Default logistic regression model in RapidMiner is based on SVM. SVM , Support vector machine.196 CHAPTER 5: Regression Methodsworks with numeric predictors and a numeric label and is, thus, one of the
go-to methods for numeric prediction tasks. The logistic regression model
works with numeric or categorical predictors and a categorical (typically,
binomial) label. How a simple linear regression model is developed was
explained using the methods of calculus and how feature selection impacts
the coefficients of a model was discussed. It was explained, how the signifi-
cance of the coefficients can be interpreted using the t-stat and p-values and
finally several checkpoints that one must follow to build good quality mod-
els were laid down. Logistic regression was then introduced, by comparing
the similar structural nature of the two function-fitting methods. How a sig-
moid curve can better fit predictors to a binomial label was discussed and
the concept of logit was introduced, which enables the transformation of a
complex function into a more recognizable linear form. How the coefficients
of logistic regression can be interpreted was discussed as well as how to mea-
sure and improve the classification performance of the model.
References
Black, K. (2008). Multiple regression analysis. In K. Black (Ed.), Business statistics (pp. 601 /C0610).
Hoboken, NJ: John Wiley and Sons.
Cramer, J. (2002). The origins of logistic regression (pp. 1 /C015). Tinbergen Institute Discussion
Paper.
Eliason, S. (1993). Maximum likelihood estimation: Logic and practice . Newbury Park, CA: Sage
Publishers.
Fletcher, R. A. (1963). A rapidly convergent descent method for minimization. The Computer
Journal ,6(2), 163 /C0168.
Galton, F. (1888). Co-relations and their measurement, chiefly from anthropometric data.
Proceedings of the Royal Society of London ,45(273-279), 135 /C0145.
Harrison, D. A. (1978). Hedonic prices and the demand for clean air. Journal of Environmental
Economics and Management ,5(1), 81 /C0102.
Hinde, P. (1998). Encyclopedia titanica. Retrieved from ,http://www.encyclopedia-titanica.org/ ..
Marquardt, D. (1963). An algorithm for least-squares estimation of nonlinear parameters. Journal
of the Society for Industrial and Applied Mathematics ,11(2), 431 /C0441.
Stigler, S. (1999). Statistics on the table: The history of statistical concepts and methods . Cambridge:
Harvard University Press.References 197CHAPTER 6
Association Analysis
The beer and diaper association story in the analytics circle is (urban) leg-
endary ( Power, 2002 ). There are many variations of this story, but the
basic plot is that a supermarket company discovered that customers who
buy diapers also tend to buy beer. The beer and diaper relationship her-
alded unusual, unknown, and quirky nuggets that could be learned from
the transaction data of a supermarket. How did the supermarket deter-
mine that such a relationship between products existed? The answer: Data
Science (It was called data mining back in the 2000s). Specifically, associ-
ation analysis.
Association analysis measures the strength of co-occurrence between one
item and another. The objective of this class of data science algorithms is not
to predict an occurrence of an item, like classification or regression algo-
rithms do, but to find usable patterns in the co-occurrences of the items.
Association rules learning is a branch of unsupervised learning processes that
discover hidden patterns in data, in the form of easily recognizable rules.
Association algorithms are widely used in retail analysis of transactions, rec-
ommendation engines, and online clickstream analysis across web pages, etc.
One of the popular applications of this technique is called market basket anal-
ysis, which finds co-occurrences of one retail item with another item within
the same retail purchase transaction ( Agrawal, Imieli ´nski, & Swami, 1993 ). If
patterns within transaction data tell us that baby formula and diapers are
usually purchased together in the same transaction, a retailer can take advan-
tage of this association for bundle pricing, product placement, and even shelf
space optimization within the store layout. Similarly, in an online business
setting, this information can be leveraged for real-time cross selling, recom-
mendations, cart offers, and post-purchase marketing strategies. The results
of association analysis are commonly known, for example a burger with fries
or baby formula with diapers; however, uncommon relationships are the
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00006-X
©2019 Elsevier Inc. All rights reserved.199prized discoveries, the ones businesses can take advantage of. The downside
is that association analysis may also yield spurious relationships between
items. When dealing with data containing billions of transactions, transac-tions with all kinds of possibilities with strange combinations of itemsets
(e.g., nicotine patch and cigarettes) can be found. It takes analytical skill and
business knowledge to successfully apply the outcomes of an associationanalysis. The model outcome of an association analysis can be represented as
a set of rules, like the one below:
Item Afg-Item Bfg
This rule indicates that based on the history of all the transactions, when
Item A is found in a transaction or a basket, there is a strong propensity ofthe occurrence of Item B within the same transaction. Item A is the antecedent
orpremise of the rule and Item B is the consequent orconclusion of the rule.
The antecedent and consequent of the rule can contain more than one item,like Item A and Item C. To mine these kinds of rules from the data, previous
customer purchase transactions would need to be analyzed. In a retail busi-
ness, there would be millions of transactions made in a day with thousandsof stock keeping units, which are unique for an item. Hence, two of the key
CROSS SELLING: CUSTOMERS WHO BOUGHT THIS ALSO BOUGHT ...
Consider an e-commerce website that sells a large selec-
tion of products online. One of the objectives of managing
an e-commerce business is to increase the average ordervalue of a visit. Optimizing order size is even more critical
when the businesses pay for acquisition traffic through
search engine marketing, online advertisements, and affil-iate marketing. Businesses attempt to increase averageorder value by cross-selling and up-selling relevant pro-
ducts to the customer, based on what they have pur-
chased or are currently purchasing in the currenttransaction (a common fast-food equivalent: “Do you want
fries with the burger? ”). Businesses need to be careful by
weighing the benefit of suggesting an extremely relevantproduct against the risk of irritating a customer who is
already making a transaction. In a business where
there are limited products (e.g., fast-food industry),cross-selling a product with another product is
straightforward and is quit e embedded in the business.
But, when the number of unique products runs into thethousands or millions, determining a set of affinityproducts when customers are looking at a product is
quite challenging.
To better learn about produc t affinity, understanding
purchase history data is helpful. The information on howone product creates affinity to another product relies on
the fact that both the products appear in the same
transaction. If two products are bought together, then itcan be hypothesized, that the necessity of those productsarise simultaneously for the customer. If the two pro-
ducts are bought together many times, by a large num-
ber of customers, then there is a strong possibility of anaffinity pattern within these products. In a new later
transaction, if a customer picks one of those affinity pro-
ducts, then there is an inc reased likelihood that the
other product will be picked by the customer, in the
same transaction.
The key input for affinity analysis is a list of past transac-
tions with product information. Based on the analysis ofthese transactions, the most frequent product pairs can
(Continued )200 CHAPTER 6: Association Analysisconsiderations of association analysis are computational time and resources.
However, over the last two decades newer and more efficient algorithms
have been developed to mitigate this problem.
6.1 MINING ASSOCIATION RULES
Basic association analysis just deals with the occurrence of one item with
another. More advanced analysis can take into consideration the quantity of
occurrence, price, and sequence of occurrence, etc. The method for finding
association rules through data science involves sequential steps:
Step 1 : Prepare the data in transaction format. An association algorithm
needs input data to be formatted in transaction format tx5{i1,i2,i3}.
Step 2 : Short-list frequently occurring itemsets . Itemsets are combinations
of items. An association algorithm limits the analysis to the most
frequently occurring items, so that the final rule set extracted in the next
step is more meaningful.
Step 3 : Generate relevant association rules from itemsets. Finally, the
algorithm generates and filters the rules based on the interest measure.
To start with, consider a media website, like BBC or Yahoo News, with cate-
gories such as news, politics, finance, entertainment, sports, and arts. A ses-
sion or transaction in this example is one visit for the website, where the
same user accesses content from different categories, within a certain session
period. A new session usually starts after 30 minutes of inactivity. Sessions
are quite similar to transactions in a traditional brick and mortar model and
the pages accessed can be related to items purchased. In online news sites,
items are visits to the categories such as News, Finance, Entertainment,
Sports, and Arts. The data can be collected as shown in Table 6.1 , with a list
of sessions and media categories accessed during a given session. The objec-
tive in this data science task is to find associations between media categories.
For association analysis of these media categories, a dataset in a particular
transaction format would be needed. To get started with association analysis,
it would be helpful to pivot the data in the format shown in Table 6.2 .(Continued)
be determined. A threshold will need to be defined for
“frequent ”because a few appearances of a product pair
doesn ’t qualify as a pattern. The result of an affinity analy-
sis is a rule set that says, “If product A is purchased, there
is an increased likelihood that product B will be purchasedin the same transaction. ”This rule set can be leveraged
to provide cross sell recommendations on the product
page of product A. Affinity analysis is the concept behind
the web widgets which state, “Customers who bought this
also bought ...”6.1 Mining Association Rules 201This binary format indicates the presence or absence of article categories and
ignores qualities such as minutes spent viewing or the sequence of access,
which can be important in certain sequence analysis. For now, the focus is
on basic association analysis and the terminologies used in association rules
will be reviewed.
6.1.1 Itemsets
In the examples of association rules discussed so far, the antecedent and con-sequent of the rules had only one item. But, as mentioned before, they caninvolve multiple items. For example, a rule can be of the following sort:
fNews ;Finance g-fSports g
This rule implies that, if users have accessed news and finance in the same
session, there is a high likelihood that they would also access sports articles,
based on historical transactions. The combination of news and finance itemsis called an itemset . An itemset can occur either in the antecedent or in the
consequent portion of the rule; however, both sets should be disjointed,
which means there should not be any common item on both sides of therule. Obviously, there is no practical relevance for rules like “News and
Finance users are most likely to visit the News and Sports pages. ”Instead,Table 6.1 Clickstream
Session ID List of Media Categories Accessed
1 {News, Finance}
2 {News, Finance}3 {Sports, Finance, News}4 {Arts}
5 {Sports, News, Finance}
6 {News, Arts, Entertainment}
Table 6.2 Clickstream Dataset
Session ID News Finance Entertainment Sports Arts
11 1 0 0 0
21 1 0 0 0
31 1 0 1 0
40 0 0 0 151 1 0 1 061 0 1 0 1202 CHAPTER 6: Association Analysisrules like “If users visited the Finance page they are more likely to visit the
News and Sports pages ”make more sense. The introduction of the itemset
with more than one item greatly increases the permutations of the rules to
be considered and tested for the strength of relationships.
The strength of an association rule is commonly quantified by the support
and confidence measures of a rule. There are a few more quantifications like
liftand conviction measures that can be used in special cases. All these mea-
sures are based on the relative frequency of occurrences of a particular item-
set in the transactions dataset used for training. Hence, it is important that
the training set used for rule generation is unbiased and truly represents a
universe of transactions. Each of these frequency metrics will be elaborated
on in the coming sections.
Support
The support of an item is simply the relative frequency of occurrence of an
itemset in the transaction set. In the dataset shown in Table 6.2 , support of
{News} is five out of six transactions, 5/6 50.83. Similarly, support of an
itemset {News, Finance} is the co-occurrence of both news and finance in a
transaction with respect to all the transactions:
Support({News}) 55/650.83
Support({News, Finance}) 54/650.67
Support({Sports}) 52/650.33
The support of a rule is a measure of how all the items in a rule are repre-
sented in the overall transactions. For example, in the rule {News} -
{Sports}, News and Sports occur in two of six transactions and hence, sup-
port for the rule {News} -{Sports} is 0.33. The support measure for a rule
indicates whether a rule is worth considering. Since the support measure
favors the items where there is high occurrence, it uncovers the patterns that
are worth taking advantage of and investigating. This is particularly interest-
ing for businesses because leveraging patterns in high volume items leads to
incremental revenue. Rules with low support have either infrequently occur-
ring items or an item relationship occurs just by chance, which may yield
spurious rules. In association analysis, a threshold of support is specified to
filter out infrequent rules. Any rule that exceeds the support threshold is then
considered for further analysis.
Confidence
The confidence of a rule measures the likelihood of the occurrence of the con-
sequent of the rule out of all the transactions that contain the antecedent of
the rule. Confidence provides the reliability measure of the rule. Confidence
of the rule ( X-Y) is calculated by6.1 Mining Association Rules 203Confidence X-Y ðÞ 5Support X,Y ðÞ
Support XðÞð6:1Þ
In the case of the rule {News, Finance} -{Sports}, the question that the
confidence measure answers is, if a transaction has both News and Finance,
what is the likelihood of seeing Sports in it?
Confidence fNews ;Finance g-fSports g ðÞ 5Support ðfNews ;Finance ;Sports gÞ
Support fNews ;Finance g ðÞ
52=6
4=6
50:5
Half of the transactions that contain News and Finance also contain Sports.
This means that 50% of the users who visit the news and finance pages also
visit the sports pages.
Lift
Though confidence of the rule is widely used, the frequency of occurrence of
a rule consequent (conclusion) is largely ignored. In some transaction item-sets, this can provide spurious scrupulous rule sets because of the presence of
infrequent items in the rule consequent. To solve this, the support of a con-
sequent can be put in the denominator of a confidence calculation. Thismeasure is called the lift of the rule . The lift of the rule can be calculated by
Life X-Y ðÞ 5Support X,Y ðÞ
Support XðÞ3Support YðÞ
In the case of our example:
Life News ;Finance fg -Sports/C8/C9 /C0/C1
5Support X,Y ðÞ
Support XðÞ3Support YðÞ
50:333
0:66730:3351:5ð6:2Þ
Lift is the ratio of the observed support of {News and Finance} and {Sports}
with what is expected if {News and Finance} and {Sports} usage were
completely independent. Lift values closer to 1 mean the antecedent and
consequent of the rules are independent and the rule is not interesting. Thehigher the value of lift, the more interesting the rules are.
Conviction
The conviction of the rule X -Yis the ratio of the expected frequency of X
occurring in spite of Yand the observed frequency of incorrect predictions.
Conviction takes into account the direction of the rule. The conviction of(X-Y) is not the same as the conviction of ( Y-X). The conviction of a rule
(X-Y) can be calculated by204 CHAPTER 6: Association AnalysisConviction X-Y ðÞ 512Support YðÞ
12Confidence X-Y ðÞ
For our example ;
Conviction News ;Finance fg -Sports/C8/C9 /C0/C1
5120:33
120:551:32ð6:3Þ
A conviction of 1.32 means that the rule ({News, Finance} -{Sports})
would be incorrect 32% more often if the relationship between {News,
Finance} and {Sports} is purely random.
6.1.2 Rule Generation
The process of generating meaningful association rules from the dataset can
be broken down into two basic tasks.
1.Finding all frequent itemsets. For an association analysis of n items it is
possible to find 2n21 itemsets excluding the null itemset. As the
number of items increase, there is an exponential increase in the
number of itemsets. Hence it is critical to set a minimal support
threshold to discard less frequently occurring itemsets in the
transaction universe. All possible itemsets can be expressed in a visual
lattice form like the diagram shown in Fig. 6.1 . In this figure one item
{Arts} is excluded from the itemset generation. It is not uncommon to
exclude items so that the association analysis can be focused on
subsets of important relevant items. In the supermarket example, some
filler items like grocery bags can be excluded from the analysis. An
itemset tree (or lattice) helps demonstrate the methods to easily find
frequent itemsets.
2.Extracting rules from frequent itemsets. For the dataset with n items it is
possible to find 3n22n1111 rules ( Tan, Steinbach, & Kumar, 2005 ).
This step extracts all the rules with a confidence higher than a
minimum confidence threshold.
This two-step process generates hundreds of rules even for a small dataset
with dozens of items. Hence, it is important to set a reasonable support and
confidence threshold to filter out less frequent and less relevant rules in the
search space. The generated rules can also be evaluated with support, confi-
dence, lift, and conviction measures. In terms of computational require-
ments, finding all the frequent itemsets above a support threshold is more
expensive than extracting the rules. Fortunately, there are some algorithmic
approaches to efficiently find the frequent itemsets. The Apriori and Frequent
Pattern (FP)-Growth algorithms are two of the most popular association
analysis algorithms.6.1 Mining Association Rules 2056.2 APRIORI ALGORITHM
All association rule algorithms should efficiently find the frequent itemsets
from the universe of all the possible itemsets. The Apriori algorithm leverages
some simple logical principles on the lattice itemsets to reduce the number
of itemsets to be tested for the support measure ( Agrawal & Srikant, 1994 ).
The Apriori principles states that “If an itemset is frequent, then all its subset
items will be frequent .”(Tan et al, 2005 ). The itemset is “frequent ”if the
support for the itemset is more than that of the support threshold.
For example, if the itemset {News, Finance, Sports} from the dataset shown
inTable 6.2 is a frequent itemset, that is, its support measure (0.33) is higher
than the threshold support measure k (say, 0.25), then all of its subset items
or itemsets will be frequent itemsets. Subset itemsets will have a support
measure higher than or equal to the parent itemset. Fig. 6.2 shows theNull
News Finance Sports Entertainment
Sports
EntertainmentFinance
EntertainmentFinance
SportsNews
EntertainmentNews
SportsNews
Finance
News
Finance
SportsNews
Finance
EntertainmentNews
Sports
EntertainmentFinance
Sports
Entertainment
News
Finance
Sports
Entertainment
FIGURE 6.1
Itemset tree.206 CHAPTER 6: Association Analysisapplication of the Apriori principle in a lattice. The support measures of the
subset itemsets for {News, Finance, Sports} are
Support {News, Finance, Sports} 50.33 (above threshold support)
Support {News, Finance} 50.66
Support {News, Sports} 50.33
Support {News} 50.83
Support {Sports} 50.33
Support {Finance} 50.66
Conversely, if the itemset is infrequent, then all its supersets will be infre-
quent. In this example, support of Entertainment is 0.16, and the support of
all the supersets that contain Entertainment as an item will be less than or
equal to 0.16, which is infrequent when considering the support threshold of
0.25. Superset exclusion of an infrequent item is shown in Fig. 6.3 .
The Apriori principle is helpful because not all itemsets have to be consid-
ered for a support calculation and tested for the support threshold; hence,Null
News Finance Sports Entertainment
Sports
EntertainmentFinance
EntertainmentFinance
SportsNews
EntertainmentNews
SportsNews
Finance
News
Finance
SportsNews
Finance
EntertainmentNews
Sports
EntertainmentFinance
Sports
Entertainment
News
Finance
Sports
EntertainmentSubsets
If {News, Finance, 
Sports} is frequent, 
all subsets will be 
frequent
FIGURE 6.2
Frequent itemsets using Apriori principle.6.2 Apriori Algorithm 207generation of the frequent itemsets can be handled efficiently by eliminating
a bunch of itemsets that have an infrequent item or itemsets ( Bodon, 2005 ).
6.2.1 How it Works
Consider the dataset shown in Table 6.3 , which is the condensed version of
the prior example set discussed. In this dataset there are six transactions. If
the support threshold is assumed to be 0.25, then all items are expected to
appear in at least two out of six transactions.
Frequent Itemset Generation
The support count and support for all itemset(s) can now be calculated. Support
count is the absolute count of the transactions and support is the ratio of sup-
port count to total transaction count. Any one itemset below the threshold sup-
port count (which is 2 in this example) can be eliminated from further
processing. Table 6.4 shows the support count and support calculation for eachNull
News Finance Sports Entertainment
Sports
EntertainmentFinance
EntertainmentFinance
SportsNews
EntertainmentNews
SportsNews
Finance
News
Finance
SportsNews
Finance
EntertainmentNews
Sports
EntertainmentFinance
Sports
Entertainment
News
Finance
Sports
Entertainment
SupersetsIf Entertainment is 
infrequent, all 
supersets will be 
infrequent
FIGURE 6.3
Frequent itemsets using Apriori principle: exclusion.208 CHAPTER 6: Association Analysisitem. Since {Entertainment} has a support count less than the threshold, it can
be eliminated for the next iteration of itemset generation. The next step is gener-
ating possible two-itemset generations for {News}, {Finance}, and {Sports},
which yield three two-itemsets. If the {Entertainment} itemset is not elimi-
nated, six two-itemsets would be obtained. Fig. 6.4 shows the visual representa-
tion of the itemsets with elimination of the {Entertainment} item.
This process is continued until all n-itemsets are considered from previous sets. At
the end, there are seven frequent itemsets passing the support threshold. The total
possible number of itemsets is 15 ( 52421). By eliminating {Entertainment} in
the first step, seven additional itemse ts do not have to be generated that would
not pass the support threshold anyway ( Witten & Frank, 2005 ).
Rule Generation
Once the frequent itemsets are generated, the next step in association analysis
is generating useful rules which have a clear antecedent (premise) and conse-
quent (conclusion), in the format of the rule:Table 6.3 Clickstream Dataset: Condensed Version
Session News Finance Entertainment Sports
11 10 0
21 10 0
31 10 1
40 00 0
51 10 1
61 01 0
Table 6.4 Frequent Itemset Support Calculation
Item Support Count Support
{News} 5 0.83
{Finance} 4 0.67
{Entertainment} 1 0.17
{Sports} 2 0.33
Two-Itemsets Support Count Support
{News, Finance} 4 0.67
{News, Sports} 2 0.33
{Finance, Sports} 2 0.33
Three-Itemsets Support Count Support
{News, Finance, Sports} 2 0.336.2 Apriori Algorithm 209Item Afg-Item Bfg
The usefulness of the rule can be approximated by an objective measure of
interest such as confidence, conviction, or lift. Confidence for the rule is cal-
culated by the support scores of the individual items as given in Eq. (6.1) .
Each frequent itemset of n items can generate 2n22 rules. For example
{News, Sports, Finance} can generate rules with confidence scores as follows:
Rules and confidence scores
{News, Sports} -{Finance} 20.33/0.33 51.0
{News, Finance} -{Sports} 20.33/0.67 50.5
{Sports, Finance} -{News} 20.33/0.33 51.0
{News}-{Sports, Finance} 20.33/0.83 50.4
{Sports}-{News, Finance} 20.33/0.33 51.0
{Finance} -{News, Sports} 20.33/0.67 50.5
Since all the support scores have already been calculated in the itemset gener-
ation step, there is no need for another set of computations for calculatingNull
News
0.83 0.67
0.670.33
0.33 0.33
0.33Finance Sports Entertainment
Sports
EntertainmentFinance
EntertainmentFinance
SportsNews
EntertainmentNews
SportsNews
Finance
News
Finance
SportsNews
Finance
EntertainmentNews
Sports
EntertainmentFinance
Sports
Entertainment
News
Finance
Sports
Entertainment
SupersetsIf Entertainment is 
infrequent, all 
supersets will be 
infrequent
FIGURE 6.4
Frequent itemset with support.210 CHAPTER 6: Association Analysisconfidence. However, it is possible to prune potentially low confidence rules
using the same Apriori method. For a given frequent itemset {News, Finance,
Sports}, if the rule {News, Finance} -{Sports} is a low confidence rule,
then it can be concluded that any rules within the subset of the antecedent
will be a low confidence rule. Hence, all the rules like {News} -{Sports,
Finance} and {Finance} -{News, Sports} can be discarded, which are in the
subsets of the antecedent of the rule. The reason is that all three rules have
the same numerator in the confidence score calculation [Eq. (6.1) ], which is
0.33. The denominator calculation depends on the support of the anteced-
ent. Since the support of a subset is always greater or equal to the set, it can
be concluded that all further rules within a subset of an itemset in the pre-
mises will be a low confidence rule, and hence, can be ignored.
All the rules passing a particular confidence threshold are considered for out-
put along with both support and confidence measures. These rules should be
further evaluated for rational validity to determine if a useful relationship
was uncovered, if there was an occurrence by chance, or if the rule confirms
a known intuitive relationship.
6.3 FREQUENT PATTERN-GROWTH ALGORITHM
The FP-Growth algorithm provides an alternative way of calculating a fre-
quent itemset by compressing the transaction records using a special graph
data structure called FP-Tree . FP-Tree can be thought of as a transformation
of the dataset into graph format. Rather than the generate and test approach
used in the Apriori algorithm, FP-Growth first generates the FP-Tree and uses
this compressed tree to generate the frequent itemsets. The efficiency of the
FP-Growth algorithm depends on how much compression can be achieved
in generating the FP-Tree ( Han, Pei, & Yin, 2000 ).
6.3.1 How it Works
Consider the dataset shown in Table 6.5 containing six transactions of four
items —news, finance, sports, and entertainment. To visually represent this
dataset in a tree diagram ( Fig. 6.6 ), the list of transactions need to be trans-
formed into a tree map, preserving all the information and representing the
frequent paths . Here is a break-down of the FP-Tree for this dataset step by step.
1.The first step is to sort all the items in each transaction in descending
order of frequency (or support count). For example, News is the most
frequent item and Sports is the least frequent item in the transaction,
based on the data in Table 6.5 . The third transaction of {Sports, News,
Finance} has to be rearranged to {News, Finance, Sports}. This will
help to simplify mapping frequent paths in later steps.6.3 Frequent Pattern-Growth Algorithm 2112.Once the items within a transaction are rearranged, the transaction can
now be mapped to the FP-Tree. Starting with a null node, the first
transaction {News, Finance} can be represented by Fig. 6.5 .T h e
number within the parenthesis next to the item name is the number of
transactions following the path.
3.Since the second transaction {News, Finance} is the same as the first
one, it follows the same path as the first one. In this case, the numbers
can simply be incremented.
4.The third transaction contains {News, Finance, Sports}. The tree is now
extended to include Sports and the item path count is incremented
(Fig. 6.6 ).
5.The fourth transaction only contains the {Sports} item. Since Sports is
not preceded by News and Finance, a new path should be created from
the null item and the item count should be noted. This node for
Sports is different from the Sports node next to Finance (the latter co-
occurs with News and Finance). However, since both nodes indicate
the same item, they should be linked by a dotted line.
6.This process is continued until all the transactions are scanned. All of
the transaction records can be now represented by a compact FP-Tree
(Fig. 6.7 ).Table 6.5 Transactions List: Session and Items
Session Items
1 {News, Finance}
2 {News, Finance}
3 {News, Finance, Sports}
4 {Sports}
5 {News, Finance, Sports}
6 {News, Entertainment}
Null
News (1)
Finance (1)
FIGURE 6.5
FP-Tree: Transaction 1.212 CHAPTER 6: Association AnalysisThe compression of the FP-Tree depends on how frequently a path occurs
within a given transaction set. Since the key objective of association analy-
sis is to identify these common paths, the datasets used from this analysis
contain many frequent paths. In the worst case, all transactions containunique itemset paths and there wouldn ’t be any compression. In that case
the rule generation itself would be less meaningful for association
analysis.Null
News (3)
Finance (3)
Sports (1)
FIGURE 6.6
FP-Tree: Transactions 1 /C03.
Null
News (5)
Finance (4)
Sports (2)Entertainment (1)Sports (1)
FIGURE 6.7
FP-Tree: Transactions 1 /C06.6.3 Frequent Pattern-Growth Algorithm 213Frequent Itemset Generation
Once the transaction set is expressed by a compact FP-Tree, the most frequent
itemset can be generated from the FP-Tree effectively. To generate the fre-
quent itemset, the FP-Growth algorithm adopts a bottom-up approach of
generating all the itemsets starting with the least frequent items. Since the
structure of the tree is ordered by the support count, the least frequent items
can be found in the leaves of the tree. In Fig. 6.8 , the least frequent items are
{Entertainment} and {Sports}, because the support count is just one transac-
tion. If {Entertainment} is indeed a frequent item, because the support
exceeds the threshold, the algorithm will find all the itemsets ending with
entertainment, like {Entertainment} and {News, Entertainment}, by follow-
ing the path from the bottom-up. Since the support counts are mapped to
the nodes, calculating the support for {News, Entertainment} will be instant.
If {Entertainment} is not frequent, the algorithm skips the item and goes
with the next item, {Sports}, and finds all possible itemsets ending with
sports: {Sports}, {Finance, Sports}, {News, Finance, Sports}.
Finding the entire itemset ending with a particular item number is actually
made possible by generating a prefix path and conditional FP-Tree for an
item, as shown in Fig. 6.9 . The prefix path of an item is a subtree with only
paths that contain the item of interest. A conditional FP-Tree for an item, say
{Sports}, is similar to the FP-Tree, but with the {Sports} item removed.
Based on the conditional FP-Tree, the algorithm repeats the process of find-
ing leaf nodes. Since leaf nodes of the sports conditional tree co-exist with
{Sports}, the algorithm finds the association with finance and generates
{Finance, Sports}.
Null
News (5)
Finance (4)
Sports (2)Sports (1)
FIGURE 6.8
Trimmed FP-Tree.214 CHAPTER 6: Association AnalysisRule generation in the FP-Growth algorithm is very similar to the Apriori
algorithm. Since the intent is to find frequently occurring items, by defini-
tion, many of the transactions should have essentially the same path. Hence,
in many practical applications the compaction ratio is very high. In those sce-
narios, the FP-Growth algorithm provides efficient results. Since the FP-
Growth algorithm uses graphs to map the relationship between frequent
items, it has found applications beyond association analysis. It is now
applied in research as a preprocessing phase for document clustering, text
mining, and sentiment analysis ( Akbar & Angryk, 2008 ). However, in spite
of the execution differences, both the FP-Growth and Apriori algorithms
yield similar results. Rule generation from the frequent itemsets is similar to
the Apriori algorithm. Even though the concepts and explanations include
analyzing graphs and subgraphs, FP-Growth algorithms can be easily ported
to programming languages, particularly to SQL and PL/SQL programs on top
of relational databases, where the transactions are usually stored ( Shang,
Sattler, & Geist, 2004 ).
6.3.2 How to Implement
The retrieval of association rules from a dataset is implemented through the
FP-Growth algorithm in RapidMiner. Since the modeling parameters and the
results for most of the association algorithms are the same, the FP-Growth
algorithm will be used to observe the inputs, process, and the result of an
association analysis implementation.
Step 1: Data Preparation
The Association analysis process expects transactions to be in a particular for-
mat. The input grid should have binominal (true or false) data with items in
the columns and each transaction as a row. If the dataset contains transaction
IDs or session IDs, they can either be ignored or tagged as a special attribute
in RapidMiner. Datasets in any other format have to be converted to this
Null
News (2)
Finance (2)
FIGURE 6.9
Conditional FP-Tree.6.3 Frequent Pattern-Growth Algorithm 215transactional format using data transformation operators. In this example,
the data shown in Table 6.3 has been used, with a session ID on each row
and content accessed in the columns, indicated by 1 and 0. This integer for-
mat has to be converted to a binomial format by a numerical to binominal
operator. The output of numerical to binominal is then connected to the FP-
Growth operator to generate frequent itemsets. The dataset and RapidMiner
process for association analysis can be accessed from the companion site of
the book at www.IntroDataScience.com .Fig. 6.10 shows the RapidMiner pro-
cess of association analysis with the FP-Growth algorithm.
Step 2: Modeling Operator and Parameters
The FP-Growth operator in RapidMiner generates all the frequent itemsets
from the input dataset meeting a certain parameter criterion. The modeling
operator is available at Modeling .Association and itemset Mining folder.
This operator can work in two modes, one with a specified number of high
support itemsets (default) and the other with minimum support criteria.
These parameters can be set in this operator, thereby, affecting the behavior
of the model:
GMin Support: Threshold for support measure. All the frequent itemsets
passing this threshold will be provided in the output.
GMax Items: Maximum number of items in an itemset. Specifying this
parameter limits too many items in an itemset.
GMust Contain: Regular expression to filter itemsets to contain specified
items. Use this option to filter out items.
FIGURE 6.10
Data science process for FP-Growth algorithm.216 CHAPTER 6: Association AnalysisGFind Minimum Number of Itemsets: This option allows the FP-Growth
operator to lower the support threshold, if fewer itemsets are generated
with the given threshold. The support threshold is decreased by 20% in
each retry.
GMin Number of Itemsets: Value of minimum number of itemsets to be
generated.
GMax Number of Retries: Number of retries allowed in achieving
minimum itemsets
In this example, Min Support is set to 0.25. The result of the FP-Growth opera-
tor is the set of itemsets generated, which can be viewed in the results page.
The reporting options include filtering based on the number of items and
sorting based on the support threshold. Fig. 6.11 shows the output of fre-
quent itemsets operator where all possible itemsets with support higher than
the threshold can be seen.
Step 3: Create Association Rules
The next step in association analysis is generation of the most interesting
rules from the frequent itemsets created from the FP-Growth operator. The
Create Association Rules operator generates relevant rules from frequent item-
sets. The interest measure of the rule can be specified by providing the correct
interest criterion based on the dataset under investigation. The input of the
Create Association Rules operator are frequent itemsets from the FP-Growth
operator and the output generates all the association rules meeting the inter-
est criterion. These parameters govern the functionality of this operator:
GCriterion: Used to select the interest measure to filter the association
rules. All other parameters change based on the criterion selection.
Confidence, lift, and conviction are commonly used interest criterion.
FIGURE 6.11
Frequent itemset output.6.3 Frequent Pattern-Growth Algorithm 217GMin Criterion Value: Specifies the threshold. Rules not meeting the
thresholds are discarded.
GTheGain theta and Laplace parameters are the values specified when
using Gain and Laplace parameters for the interest measure.
In this example process, we are using confidence as the criterion and a confi-
dence value of 0.5. Fig. 6.10 shows the completed RapidMiner process for
association analysis. The process can be saved and executed.
Step 4: Interpreting the Results
The filtered association analysis rules extracted from the input transactions
can be viewed in the results window ( Fig. 6.12 ). The listed association rules
are in a table with columns including the premise and conclusion of the
rule, as well as the support, confidence, gain, lift, and conviction of the rule.
The interactive control window on the left-hand side of the screen allows the
users to filter the processed rules to contain the selected item and there is a
slide bar to increase the confidence or criterion threshold, thereby, showing
fewer rules.
The main purpose of association analysis is to understand the relationship
between items. Since the items take the role of both premise and conclusion,
a visual representation of relationships between all the items, through a rule,
FIGURE 6.12
Association rules output.218 CHAPTER 6: Association Analysiscan help comprehend the analysis. Fig. 6.13 shows the rules in text format
and by interconnected graph format through the results window, for selected
items. Fig. 6.13 B shows the items selected, connected with the rules through
arrows. The incoming item to a rule is the premise of the rule and the outgo-
ing item is the conclusion of the association rule.
FIGURE 6.13
Association rules output (A) text view and (B) graph view.6.3 Frequent Pattern-Growth Algorithm 2196.4 CONCLUSION
Association rules analysis has gained popularity in the last two decades par-
ticularly in retail, online cross selling, recommendation engines, text analysis,
document analysis, and web analysis. Typically, a commercial data science
tool offers association analysis in its tool package. Though there may be a
variation in how the algorithm is implemented in each commercial package,
the framework of generating a frequent itemset using a support threshold
and generating rules from the itemsets using an interest criterion is the same.
Applications that involve large amount of items and real-time decision mak-
ing, demand new approaches with efficient and scalable association analysis
(Zaki, 2000 ). Association analysis is also one of the prevalent algorithms that
is applied to information stored using big data technologies, data streams,
and large databases ( Tanbeer, Ahmed, Jeong, & Lee, 2008 ).
References
Agrawal, R., Imieli ´nski, T., & Swami, A. (1993). Mining association rules between sets of items in
large databases. In SIGMOD ‘93 proceedings of the 1993 ACM SIGMOD international conference
on management of data (pp. 207 /C0216).
Agrawal, R., & Srikant, R. (1994). Fast algorithms for mining association rules. The international
conference on very large databases , 487 /C0499.
Akbar, M., & Angryk, R. (2008). Frequent pattern-growth approach for document organization. In
Proceeding of the 2nd international workshop on Ontologies and information systems for the semantic
web, ACM (pp. 77 /C082). Available from http://dl.acm.org/citation.cfm?id 51458496 .
Bodon, F. (2005). A trie-based APRIORI implementation for mining frequent item sequences. In
Proceedings of the 1st international workshop on open source data science frequent pattern mining
implementations /C0OSDM ‘05(pp. 56 /C065). http://dx.doi.org/10.1145/1133905.1133913 .
Han, J., Pei, J., & Yin, Y. (2000). Mining frequent patterns without candidate generation. In
SIGMOD ‘00 proceedings of the 2000 ACM SIGMOD international conference on management of
data (pp. 1 /C012).
Power, D.J. (2002, Nov 10). DSS News . Retrieved Jan 21, 2014, from Desicion Support Systems
(DSS). Retrieved Jan 21, 2014, from Desicion Support Systems (DSS), http://www.dssre-
sources.com/newsletters/66.php .
Shang, X., Sattler, K.U., Geist, I. (2004). SQL based frequent pattern mining without candidate
generation. In 2004 ACM symposium on applied computing /C0Poster Abstract (pp. 618 /C0619).
Tan, P.-N., Steinbach, M., & Kumar, V. (2005). Association analysis: Basic concepts and algorithms .
Introduction to data mining (pp. 327 /C0404). Boston, MA: Addison Wesley.
Tanbeer, S.K., Ahmed, C.F., Jeong, B.-S., Lee, Y.-K. (2008). Efficient frequent pattern mining over
data streams. In Proceeding of the 17th ACM conference on information and knowledge mining /C0
CIKM ‘08(Vol. 1, pp. 1447 /C01448). http://dx.doi.org/10.1145/1458082.1458326 .
Witten, I. H., & Frank, E. (2005). Algorithms: The basic methods: Mining association rules .Data sci-
ence: Practical machine learning tools and techniques (pp. 112 /C0118). San Francisco, CA: Morgan
Kaufmann.
Zaki, M. Jk (2000). Scalable algorithms for association mining. IEEE Transactions on Knowledge
and Data Engineering ,12(3), 372 /C0390. Available from https://doi.org/10.1109/69.846291 .220 CHAPTER 6: Association AnalysisCHAPTER 7
Clustering
Clustering is the process of finding meaningful groups in data. In clustering,
the objective is not to predict a target class variable, but to simply capture
the possible natural groupings in the data. For example, the customers of a
company can be grouped based on purchase behavior. In the past few years,
clustering has even found a use in political elections ( Pearson & Cooper,
2012 ). The prospective electoral voters can be clustered into different groups
so that candidates can tailor the messages to resonate within each group. the
difference between classification and clustering can be illustrated with an
example. Categorizing a given voter as a “soccer mom ”(a known user group)
or not, based on previously available labeled data, is supervised learning —
Classification. Segregating a population of electorates into different groups,
based on similar demographics is unsupervised learning —Clustering. The
process of identifying whether a data point belongs to a particular known
group is classification. The process of dividing the dataset into meaningful
groups is clustering. In mostcases, one would not know ahead what groups
to look for and, thus, the inferred groups might be difficult to explain. The
task of clustering can be used in two different classes of applications: to
describe a given dataset and as a preprocessing step for other data science
algorithms.
CLUSTERING TO DESCRIBE THE DATA
The most common application of clust ering is to explore the data and find
all the possible meaningful groups in the data. Clustering a company ’sc u s -
tomer records can yield a few groups in such a way that customers within a
group are more like each other than cu stomers belonging to a different
group. Depending on the clustering technique used, the number of groups
or clusters is either user-defined or automatically determined by the algo-
rithm from the dataset. Since clustering is not about predicting the mem-
bership of a customer in a well-defined meaningful group (e.g., frequent
high-volume purchaser), the similarities of customers within a group need
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00007-1
©2019 Elsevier Inc. All rights reserved.221to be carefully investigated to make sense of the group as a whole. Some of
the common applications of clustering to describe the underlying natural
structure of data are:
1.Marketing: Finding the common groups of customers based on all past
customer behaviors, and/or purchase patterns. This task is helpful to
segment the customers, identify prototype customers (description of atypical customer of a group), and to tailor a marketing message to the
customers in a group.
2.Document clustering: One common text mining task is to automatically
group documents (or text blobs) into groups of similar topics.
Document clustering provides a way of identifying key topics,comprehending and summarizing these clustered groups rather than
having to read through whole documents. Document clustering is used
for routing customer support incidents, online content sites, forensicinvestigations, etc.
3.Session grouping: In web analytics, clustering is helpful to understand
common groups of clickstream patterns and to discover different kindsof clickstream profiles. One clickstream profile may be that of a
customer who knows what they want and proceeds straight to
checkout. Another profile may be that of a customer who hasresearched the products, read through customer reviews, and then
makes a purchase during a later session. Clustering the web sessions by
profile helps the e-commerce company provide features fitting eachcustomer profile.
CLUSTERING FOR PREPROCESSING
Since a clustering process considers all the attributes of the dataset andreduces the information to a cluster, which is really another attribute (i.e., the
ID of the cluster to which a record would belong to), clustering can be used
as a data compression technique. The output of clustering is the cluster IDfor each record and it can be used as an input variable for other data science
tasks. Hence, clustering can be employed as a preprocessing technique for
other data science processes. In general, clustering can be used for two typesof preprocessing:
1.Clustering to reduce dimensionality: In an n-dimensional dataset (n
number of attributes), the computational complexity is proportional to
the number of dimensions or “n.”With clustering, n-dimensional
attributes can be converted or reduced to one categorical attribute —
“Cluster ID. ”This reduces the complexity, although there will be some
loss of information because of the dimensionality reduction to one222 CHAPTER 7: Clusteringsingle attribute. Chapter 14, Feature Selection provides an in-depth
look at feature selection techniques.
2.Clustering for object reduction: Assume that the number of customers
for an organization is in the millions and the number of cluster
groups is 100. For each of these 100 cluster groups, one “poster
child ”customer can be identified that represents the typical
characteristics of all the customers in that cluster group. The poster
child customer can be an actual customer or a fictional customer.
The prototype of a cluster is the most common representation of all
the customers in a group. Reducing millions of customer records to
100 prototype records provides an obvious benefit. In some
applications, instead of processi ng millions of records, just the
prototypes can be processed for fur ther classification or regression
tasks. This greatly reduces the record count and the dataset can be
made appropriate for classification by algorithms like k-nearest
neighbor ( k-NN) where computation complexity depends on the
number of records.
TYPES OF CLUSTERING TECHNIQUES
Regardless of the types of clustering applications, the clustering process seeks
to find groupings in data, in such a way that data points within a cluster are
more similar to each other than to data points in the other clusters ( Witten &
Frank, 2005 ). One common way of measuring similarity is the Euclidean dis-
tance measurement in n-dimensional space. In Fig. 7.1 all data points in
Cluster 2 are closer to other data points in Cluster 2 than to other data
points in Cluster 1. Before explaining the different ways to implement clus-
tering, the different types of clusters have to be defined. Based on a data
point ’s membership to an identified group, a cluster can be:
GExclusive or strict partitioning clusters : Each data object belongs to one
exclusive cluster, like the example shown in Fig. 7.1 . This is the most
common type of cluster.
GOverlapping clusters : The cluster groups are not exclusive, and each data
object may belong to more than one cluster. These are also known as
multi-view clusters. For example, a customer of a company can be
grouped in a high-profit customer cluster and a high-volume customer
cluster at the same time.
GHierarchical clusters : Each child cluster can be merged to form a parent
cluster. For example, the most profitable customer cluster can be
further divided into a long-term customer cluster and a cluster with
new customers with high-value purchases.Types of Clustering Techniques 223GFuzzy or probabilistic clusters : Each data point belongs to all cluster
groups with varying degrees of membership from 0 to 1. For example,in a dataset with clusters A, B, C, and D, a data point can be associated
with all the clusters with degree A 50.5, B50.1, C50.4, and D 50.
Instead of a definite association of a data point with one cluster, fuzzyclustering associates a probability of membership to all the clusters.
Clustering techniques can also be classi fied based on the algorithmic approach
used to find clusters in the dataset. Each o f these classes of clustering algorithms
differ based on what relationship they leverage between the data objects.
GPrototype-based clustering : In the prototype-based clustering, each
cluster is represented by a central data object, also called a prototype.
The prototype of each cluster is usually the center of the cluster, hence,
this clustering is also called centroid clustering or center-basedclustering. For example, in clustering customer segments, each customer
FIGURE 7.1
Example of a clustering of the Iris dataset without class labels.224 CHAPTER 7: Clusteringcluster will have a central prototype customer and customers with
similar properties are associated with the prototype customer of a
cluster.
GDensity clustering :InFig. 7.1 , it can be observed that clusters occupy
the area where there are more data points per unit space and are
separated by sparse space. A cluster can also be defined as a dense
region where data objects are concentrated surrounded by a low-density
area where data objects are sparse. Each dense area can be assigned a
cluster and the low-density area can be discarded as noise. In this form
of clustering not all data objects are clustered since noise objects are
unassigned to any cluster.
GHierarchical clustering :Hierarchical clustering is a process where a
cluster hierarchy is created based on the distance between data points.
T h eo u t p u to fah i e r a r c h i c a lc l u s t e r i n gi sa dendrogram :at r e ed i a g r a m
that shows different clusters at any point of precision which is
specified by the user. There are two approaches to create a hierarchy
of clusters. A bottom-up approach is where each data point is
considered a cluster, and the clusters are merged to finally form one
massive cluster. The top-down approach is where the dataset is
considered one cluster and they are re cursively divided into different
sub-clusters until individual data objects are defined as separate
clusters. Hierarchical clustering i s useful when the data size is limited.
A level of interactive feedback is required to cut the dendrogram tree
to a given level of precision.
GModel-based clustering :Model-based clustering gets its foundation from
statistics and probability distribut ion models; this technique is also
called distribution-based cluster ing. A cluster can be thought of as a
grouping that has the data points belonging to the same probability
distribution. Hence, each cluster can be represented by a distribution
model (like Gaussian or Poisson), where the parameter of the
distribution can be iteratively optimized between the cluster data and
the model. With this approach, the entire dataset can be represented
by a mixture of distribution models. Mixture of Gaussians is one of the
model-based clustering techniq ues used where a fixed number of
distributions are initialized, and parameters are optimized to fit the
cluster data.
In the rest of the chapter, the common implementations of clustering will be
discussed. First, k-means clustering will be covered, which is a kind of
prototype-clustering technique. This is followed by the Density-Based Spatial
Clustering of Applications with Noise (DBSCAN), which provides a view
into density clustering, and the chapter is concluded with an explanation of
a novel approach called self-organizing maps (SOMs).Types of Clustering Techniques 225SEGMENTING CUSTOMER RECORDS
All business entities record most of their interactions with
customers, including but not limited to monetary transac-
tions, customer service, customer location and details,
online interactions, product usage, and warranty and ser-
vice information. Take the telecommunications industry as
an example. Telecommunications companies have now
evolved to provide multiple services to different types of
customers by packaging products like phones, wireless,
internet, data communications, corporate backbones,
entertainment content, home security, etc. To better
understand customers, whose numbers often range in the
millions, it is necessary to combine multiple datasets
about the customers and their interactions with each
product. The vastness of the number and variety of attri-
butes in the dataset provides both an opportunity and
challenge to better know their customers ( Berry & Linoff,
2000a,b ). One logical way to understand the customer
beyond straightforward classifications like customer type
(residential, corporate, government, etc.) or revenue vol-
ume (high-, medium-, and low-revenue customers), is to
segment the customer based on usage patterns, demo-
graphics, geography, and behavior patterns for product
usage.
For a customer segmentation task, the data need to be
prepared in such a way that each record (row) is associ-
ated with each customer and the columns contain all the
attributes about the customer, including demographics,address, products used, revenue details, usage details of
the product, call volume, type of calls, call duration, time
of calls, etc. Table 7.1 shows an example structure of a
denormalized customer dataset. Preparing this dataset is
going to be a time-consuming task. One of the obvious
methods of segmentation is stratifying based on any of the
existing attributes. For example, one can segment the
data based on a customer ’s geographical location.
A clustering algorithm consumes this data and groups the
customers with similar patterns into clusters based on all
the attributes. Based on the da ta, clustering could be based
on a combination of call usage, data patterns, and monthly
bills. The resulting clusters could be a group of customers
who have low data usage but with high bills at a location
where there is weak cellular coverage, which may indicate
dissatisfied customers.
The clustering algorithm doesn ’t explicitly provide the rea-
son for clustering and doesn ’t intuitively label the cluster
groups. While clustering can be performed using a large
number of attributes, it is up to the practitioner to care-
fully select the attributes that will be relevant for cluster-
ing. Automated feature selection methods can reduce the
dimensions for a clustering exercise. Clustering could be
iteratively developed further by selecting or ignoring other
attributes in the customer dataset.
7.1 K-MEANS CLUSTERING
k-Means clustering is a prototype-based clustering method where the data-
set is divided into k-clusters. k-Means clustering is one of the simplest and
most commonly used clustering algorithms. In this technique, the user spe-
cifies the number of clusters ( k) that need to be grouped in the dataset.Table 7.1 Dataset for Customer Segmentation
Customer ID Location Demographics Call Usage Data Usage (MB) Monthly Bill ($)
01 San Jose, CA Male 1400 200 75.23
02 Miami, FL Female 2103 5000 125.78
03 Los Angeles, CA Male 292 2000 89.90
04 San Jose, CA Female 50 40 59.34226 CHAPTER 7: ClusteringThe objective of k-means clustering is to find a prototype data point for each
cluster; all the data points are then assigned to the nearest prototype,
which then forms a cluster. The prototype is called as the centroid, the cen-
ter of the cluster. The center of the cluster can be the mean of all data
objects in the cluster, as in k-means, or the most represented data object, as
ink-medoid clustering. The cluster centroid or mean data object does not
have to be a real data point in the dataset and can be an imaginary data
point that represents the characteristics of all the data points within the
cluster.
The k-means clustering algorithm is based on the works of Stuart Lloyd and
E.W. Forgy ( Lloyd, 1982 ) and is sometimes referred to as the Lloyd /C0Forgy
algorithm or Lloyd ’s algorithm. Visually, the k-means algorithm divides the
data space into kpartitions or boundaries, where the centroid in each parti-
tion is the prototype of the clusters. The data objects inside a partition
belong to the cluster. These partitions are also called Voronoi partitions , and
each prototype is a seed in a Voronoi partition. A Voronoi partition is a pro-
cess of segmenting a space into regions, around a set of points called seeds.
All other points are then associated to the nearest seed and the points associ-
ated with the seed from a unique partition. Fig. 7.2 shows a sample Voronoi
partition around seeds marked as black dots.
k-Means clustering creates kpartitions in n-dimensional space, where nis the
number of attributes in a given dataset. To partition the dataset, a proximity
measure has to be defined. The most commonly used measure for a numeric
attribute is the Euclidean distance. Fig. 7.3 illustrates the clustering of the Iris
dataset with only the petal length and petal width attributes. This Iris dataset
is two-dimensional (selected for easy visual explanation), with numeric attri-
butes and kspecified as 3. The outcome of k-means clustering provides a
clear partition space for Cluster 1 and a narrow space for the other two clus-
ters, Cluster 2 and Cluster 3.
7.1.1 How It Works
The logic of finding k-clusters within a given dataset is rather simple and
always converges to a solution. However, the final result in most cases will
be locally optimal where the solution will not converge to the best global
solution. The process of k-means clustering is similar to Voronoi iteration,
where the objective is to divide a space into cells around points. The differ-
ence is Voronoi iteration partitions the space, whereas, k-means clustering
partitions the points in data space. Take the example of a two-dimensional
dataset ( Fig. 7.4 ). The step-by-step process of finding three clusters is pro-
vided below ( Tan, Michael, & Kumar, 2005 ).7.1k-Means Clustering 227Step 1: Initiate Centroids
The first step in k-means algorithm is to initiate krandom centroids. The num-
ber of clusters kshould be specified by the user. In this case, three centroids
are initiated in a given data space. In Fig. 7.5 , each initial centroid is given a
shape (with a circle to differentiate centroids from other data points) so that
data points assigned to a centroid can be indicated by the same shape.
Step 2: Assign Data Points
Once centroids have been initiated, all the data points are now assigned to
the nearest centroid to form a cluster. In this context the “nearest ”is calcu-
lated by a proximity measure. Euclidean distance measurement is the most
common proximity measure, though other measures like the Manhattan
measure and Jaccard coefficient can be used. The Euclidean distance between
two data points X(x1,x2,...,xn) and C(c1,c2,...,cn) with nattributes is given
by(7.1)
FIGURE 7.2
Voronoi partition “Euclidean Voronoi Diagram ”by Raincomplex —personal work. Licensed under Creative
Commons Zero, Public Domain Dedication via Wikimedia Commons. http://commons.wikimedia.org/wiki/
File:Euclidean_Voronoi_Diagram.png#mediaviewer/File:Euclidean_Voronoi_Diagram.png .1)228 CHAPTER 7: ClusteringFIGURE 7.4
Dataset with two dimensions.
FIGURE 7.3
Prototype-based clustering and boundaries.7.1k-Means Clustering 229Distance d 5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x12c1 ðÞ21x22c2 ðÞ21...1xn2cn ðÞ2q
ð7:1Þ
All the data points associated to a centroid now have the same shape as their
corresponding centroid as shown in Fig. 7.6 . This step also leads to partition-
ing of data space into Voronoi partitions, with lines shown as boundaries.
FIGURE 7.5
Initial random centroids.
FIGURE 7.6
Assignment of data points to nearest centroids.230 CHAPTER 7: ClusteringStep 3: Calculate New Centroids
For each cluster, a new centroid can now be calculated, which is also the pro-
totype of each cluster group. This new centroid is the most representative
data point of all data points in the cluster. Mathematically, this step can be
expressed as minimizing the sum of squared errors (SSEs) of all data points
in a cluster to the centroid of the cluster. The overall objective of the step is
to minimize the SSEs of individual clusters. The SSE of a cluster can be calcu-
lated using Eq. (7.2) .
SSE5Xk
i51X
xjACi:xj2μi:2ð7:2Þ
where Ciis the ithcluster, jare the data points in a given cluster, μiis the cen-
troid for ithcluster, and xjis a specific data object. The centroid with minimal
SSE for the given cluster iis the new mean of the cluster. The mean of the
cluster can be calculated using (7.3)
μi51
jiX
xAciX ð7:3Þ
where Xis the data object vector ( x1,x2,...,xn). In the case of k-means clus-
tering, the new centroid will be the mean of all the data points. k-Medoid
clustering is a variation of k-means clustering, where the median is calculated
instead of the mean. Fig. 7.7 shows the location of the new centroids.
FIGURE 7.7
New centroids.7.1k-Means Clustering 231Step 4: Repeat Assignment and Calculate New Centroids
Once the new centroids have been identified, assigning data points to the
nearest centroid is repeated until all the data points are reassigned to new
centroids. In Fig. 7.8 , note the change in assignment of three data points that
belonged to different clusters in the previous step.
Step 5: Termination
Step 3 —calculating new centroids, and step 4 —assigning data points to new
centroids, are repeated until no further change in assignment of data points
happens. In other words, no significant change in centroids are noted. The
final centroids are declared the prototypes of the clusters and they are used
to describe the whole clustering model. Each data point in the dataset is now
tied with a new clustering ID attribute that identifies the cluster.
Special Cases
Even though k-means clustering is simple and easy to implement, one of the
key drawbacks of k-means clustering is that the algorithm seeks to find a local
optimum , which may not yield globally optimal clustering. In this approach,
the algorithm starts with an initial configuration (centroids) and continu-
ously improves to find the best solution possible for that initial configura-
tion. Since the solution is optimal to the initial configuration (locally
optimal), there might be a better optimal solution if the initial configuration
changes. The locally optimal solution may not be the most optimal solution
for the given clustering problem. Hence, the success of a k-means algorithm
much depends on the initiation of centroids. This limitation can be
FIGURE 7.8
Assignment of data points to new centroids.232 CHAPTER 7: Clusteringaddressed by having multiple random initiations; in each run one could
measure the cohesiveness of the clusters by a performance criterion . The clus-
tering run with the best performance metric can be chosen as the final run.Evaluation of clustering is discussed in the next section. Some key issues to
be considered in k-means clustering are:
GInitiation: The final clustering grouping depends on the random
initiator and the nature of the dataset. When random initiation is used,
one can run the entire clustering process (also called “runs”) with a
different set of random initiators and find the clustering process that
has minimal total SSE. Another technique is hierarchical clustering,
where each cluster is in turn split into multiple clusters and, thereby,minimal SSE is achieved. Hierarchical clustering is further divided into
agglomerative or bottom-up clustering and divisive or top-down
clustering, depending on how the clustering is initiated. Agglomerativeclustering starts with each data point as an individual cluster and
proceeds to combine data points into clusters. Divisive clustering starts
with the whole dataset as one big cluster and proceeds to split that intomultiple clusters.
GEmpty clusters: One possibility in k-means clustering is the formation of
empty clusters in which no data objects are associated. If empty clustersare formed, a new centroid can be introduced in the cluster that has the
highest SSE, thereby, splitting the cluster that contributes to the highest
SSE or selecting a new centroid that is at the farthest point away fromany other centroid.
GOutliers: Since SSE is used as an objective function, k-means clustering
is susceptible to outliers; they drift the centroid away from therepresentative data points in a cluster. Hence, the prototype is no
longer the best representative of the clusters it represents. While outliers
can be eliminated with preprocessing techniques, in some applicationsfinding outliers or a group of outliers is the objective of clustering,
similar to identifying fraudulent transactions.
GPost-processing: Since k-means clustering seeks to be locally optimal, a
few post-processing techniques can be introduced to force a new
solution that has less SSE. One could always increase the number of
clusters, k, and reduce SSE. But, this technique can start overfitting the
dataset and yield less useful information. There are a few approaches
that can be deployed, such as bisecting the cluster that has the highest
SSE and merging two clusters into one even if SSE increases slightly.
Evaluation of Clusters
Evaluation of k-means clustering is different from regression and classifica-
tion algorithms because in clustering there are no known external labels for7.1k-Means Clustering 233comparison. The evaluation parameter will have to be developed from the
very dataset that is being evaluated. This is called unsupervised or internal
evaluation. Evaluation of clustering can be as simple as computing total SSE.
Good models will have low SSE within the cluster and low overall SSE
among all clusters. SSE can also be referred to as the average within-cluster
distance and can be calculated for each cluster and then averaged for all the
clusters.
Another commonly used evaluation measure is the Davies /C0Bouldin index
(Davies & Bouldin, 1979 ). The Davies /C0Bouldin index is a measure of
uniqueness of the clusters and takes into consideration both cohesiveness of
the cluster (distance between the data points and center of the cluster) and
separation between the clusters. It is the function of the ratio of within-
cluster separation to the separation between the clusters. The lower the value
of the Davies /C0Bouldin index, the better the clustering. However, both SSE
and the Davies /C0Bouldin index have the limitation of not guaranteeing better
clustering when they have lower scores.
7.1.2 How to Implement
k-Means clustering implementation in RapidMiner is simple and straightfor-
ward with one operator for modeling and one for unsupervised evaluation.
In the modeling step, the parameter for the number of clusters, k, is specified
as desired. The output model is a list of centroids for each cluster and a new
attribute is attached to the original input dataset with the cluster ID. The
cluster label is appended to the original dataset for each data point and can
be visually evaluated after the clustering. A model evaluation step is required
to calculate the average cluster distance and Davies /C0Bouldin index.
For this implementation, the Iris dataset will be used with four attributes
and 150 data objects ( Fisher, 1936 ). Even though a class label is not needed
for clustering, it was kept for later explanation to see if identified clusters
from an unlabeled dataset are similar to natural clusters of species in the
dataset.
Step 1: Data Preparation
k-Means clustering accepts both numeric and polynominal data types; how-
ever, the distance measures are more effective with numeric data types. The
number of attributes increases the dimension space for clustering. In this
example the number of attributes has been limited to two by selecting petal
width (a3) and petal length (a4) using the Select attribute operator as shown
inFig. 7.9 . It is easy to visualize the mechanics of k-means algorithm by
looking at two-dimensional plots for clustering. In practical implementa-
tions, clustering datasets will have more attributes.234 CHAPTER 7: ClusteringStep 2: Clustering Operator and Parameters
The k-means modeling operator is available in the Modeling .Clustering
and Segmentation folder of RapidMiner. These parameters can be configured
in the model operator:
Gk:kis the desired number of clusters.
GAdd cluster as attribute : Append cluster labels (IDs) into the original
dataset. Turning on this option is recommended for later analysis.
GMax runs : Since the effectiveness of k-means clustering is dependent on
random initial centroids, multiple runs are required to select theclustering with the lowest SSE. The number of such runs can be
specified here.
GMeasure type : The proximity measure can be specified in this parameter.
The default and most common measure ment is Euclidean distance (L2).
Other options here are Manhattan dist ance (L1), Jaccard coefficient, and
cosine similarity for document da ta. Please refer to Chapter 4,
Classification section k-NN for description on distance measures.
GMax optimization steps : This parameter specifies the number of iterations
of assigning data objects to centroids and calculating new centroids.
The output of the modeling step includes the cluster model with kcentroid
data objects and the initial dataset appended with cluster labels. Cluster
labels are named generically such as cluster_0, cluster_1, ..., cluster_ k/C01.
FIGURE 7.9
Data science process for k-means clustering.7.1k-Means Clustering 235Step 3: Evaluation
Since the attributes used in the dataset are numeric, the effectiveness of clus-
tering groups need to be evaluated using SSE and the Davies /C0Bouldin index.
In RapidMiner, the Cluster Model Visualizer operator under Modeling .
Segmentation is available for a performance evaluation of cluster groups and
visualization. Cluster Model Visualizer operator needs both inputs from the
modeling step: cluster centroid vector (model) and the labeled dataset. The
two measurement outputs of the evaluation are average cluster distance and
the Davies /C0Bouldin index.
Step 4: Execution and Interpretation
After the outputs from the performance operator have been connected to the
result ports, the data science process can be executed. These outputs can be
observed from results window:
GCluster Model (Clustering): The model output contains the centroid for
each of the k-clusters, along with their attribute values. As shown in
Fig. 7.10 , in the text view and folder view sections, all the data objects
associated with the each cluster can be seen. The centroid plot view
provides the parallel chart view (Chapter 3: Data Exploration) of the
centroids. A large separation between centroids is desirable, because
well-separated clusters divide the dataset cleanly.
GLabeled example set: The appended dataset has some of the most
important information on clustering. The generic Iris dataset of 150
observations is clustered in three groups. The cluster value is appended
as a new special polynominal attribute and takes a generic label format.
In the scatterplot view of this output dataset, the x- and y-axes can be
configured to be attributes of the original dataset, petal length and
petal width. In the plot in Fig. 7.11 , it can be noted how the algorithm
identified clusters. This output can be compared against the original
label (Iris species). and only five data points in the border of
I.versicolor and I. virginica are mis-clustered! The k-means clustering
process identified the different species in the dataset almost exactly.
GVisualizer and Performance vector: The output of the Cluster Model
Visualizer shows the centroid charts, table, scatter plots, heatmaps, and
performance evaluation metrics like average distance measured and the
Davies /C0Bouldin index ( Fig. 7.12 ). This step can be used to compare
multiple clustering processes with different parameters. In advanced
implementations, it is possible to determine the value of “k”using
Optimize Parameters operator, based on the number of looped clustering
runs with various values of k. Amongst multiple clustering runs each with
a distinct value for k,t h e kvalue with the lowest average-within-centroid
distance or Davies /C0Bouldin index is selected as the most optimal.236 CHAPTER 7: ClusteringThe k-means clustering algorithm is simple, easy to implement, and easy to
interpret. Although the algorithm can effectively handle an n-dimensional
dataset, the operation will be expensive with a higher number of iterations
and runs. One of the key limitations of k-means is that it relies on the user
to assign the value of k(Berry & Linoff, 2000a,2000b ). The number of clus-
ters in the dataset will be unknown to begin with and an arbitrary number
can limit the ability to find the right number of natural clusters in the data-
set. There are a variety of methods to estimate the right number for k, ranging
from the Bayesian Information Criterion to hierarchical methods that
increase the value of kuntil the data points assigned to the cluster are
Gaussian ( Hamerly & Elkan, 2003 ). For a start, it is recommended to use a
FIGURE 7.10
k-Means clustering centroids output.7.1k-Means Clustering 237value of kin the low single digits and increasing it until it fits. Clustering
using density methods will help provide an idea into the number of clusters
and could be used as a value of kink-means clustering.
Since the centroid prototype approach is used, k-means tends to find globu-
lar clusters in the dataset. However, natural clusters can be of all shapes andsizes. The presence of outliers possesses a challenge in the modeling of k-
means clustering. The simplicity of the k-means clustering technique makes it
a great choice for quick evaluation of globular clusters and as a preprocessingtechnique for data science modeling and for dimensionality reduction.
7.2 DBSCAN CLUSTERING
A cluster can also be defined as an area of high concentration (or density) ofdata objects surrounded by areas of low concentration (or density) of dataobjects. A density-clustering algorithm identifies clusters in the data based on
the measurement of the density distribution in n-dimensional space. Unlike
FIGURE 7.11
k-Means clustering visual output.238 CHAPTER 7: Clusteringcentroid methods, specifying the number of the cluster parameters ( k) is not
necessary for density-based algorithms. Thus, density-based clustering can
serve as an important data exploration technique. DBSCAN is one of the
most commonly used density-clustering algorithms ( Ester, Kriegel, Sander, &
Xu,1996 ). To understand how the algorithm works, the concept of density in
a data space first needs to be defined.
Density can be defined as the number of data points in a unit n-dimensional
space. The number of dimensions nis the number of attributes in a dataset. To
simplify the visualization and to furthe r understand how the model works, con-
sider a two-dimensional space or a dataset with two numeric attributes. From
looking at the dataset represented in Fig. 7.13 , it can be visually concluded that
the density in the top-left section is higher than the density in top-right,
bottom-left, and bottom-ri ght sections. Technically, density relates to the num-
ber of points in unit space, in this case a quadrant. Wherever there is high-
density space amongst relatively low-density spaces, there is a cluster.
One can also measure density within a circular space around a point as in
Fig. 7.14 . The number of points within a circular space with radius ε
FIGURE 7.12
Performance measures of k-means clustering.7.2 DBSCAN Clustering 239(epsilon) around a data point A is six. This measure is called center-based
density since the space considered is globular with the center being the point
that is considered.
7.2.1 How It Works
The DBSCAN algorithm creates clusters by identifying high-density and low-density space within the dataset. Similar to k-means clustering, it is preferred
FIGURE 7.14
Density of a data point within radius ε.
FIGURE 7.13
Dataset with two attributes.240 CHAPTER 7: Clusteringthat the attributes are numeric because distance calculation is still used. The
algorithm can be reduced to three steps: defining threshold density, classifica-
tion of data points, and clustering ( Tan et al., 2005 ).
Step 1: Defining Epsilon and MinPoints
The DBSCAN algorithm starts with calculation of a density for all data points
in a dataset, with a given fixed radius ε(epsilon). To determine whether a
neighborhood is high-density or low-density, a threshold of data points
(MinPoints) will have to be defined, above which the neighborhood is con-
sidered high-density. In Fig. 7.14 , the number of data points inside the space
is defined by radius ε. If MinPoints is defined as 5, the space εsurrounding
data point A is considered a high-density region. Both εand MinPoints are
user-defined parameters and can be altered for a dataset.
Step 2: Classification of Data Points
In a dataset, with a given εand MinPoints, all data points can be defined
into three buckets ( Fig. 7.15 ):
GCore points : All the data points inside the high-density region of at least
one data point are considered a core point. A high-density region is a
space where there are at least MinPoints data points within a radius of ε
for any data point.
GBorder points : Border points sit on the circumference of radius εfrom a
data point. A border point is the boundary between high-density and
FIGURE 7.15
Core, border, and density points.7.2 DBSCAN Clustering 241low-density space. Border points are counted within the high-density
space calculation.
GNoise points : Any point that is neither a core point nor border point is
called a noise point. They form a low-density region around the high-
density region.
Step 3: Clustering
Once all data points in the dataset are classified into density points, cluster-
ing is a straightforward task. Groups of core points form distinct clusters. If
two core points are within εof each other, then both core points are within
the same cluster. All these clustered core points form a cluster, which is sur-
rounded by low-density noise points. All noise points form low-density
regions around the high-density cluster, and noise points are not classified in
any cluster. Since DBSCAN is a partial clustering algorithm, a few data points
are left unlabeled or associated to a default noise cluster.
Optimizing Parameters
One of the key advantages of using a density algorithm is that there is no
need for specifying the number of clusters ( k). Clusters are automatically
found in the dataset. However, there is an issue with selecting the distance
parameter εand a minimum threshold (MinPoints) to identify the dense
region. One technique used to estimate optimal parameters for the DBSCAN
clustering algorithm relates to the k-NN algorithm. The initial values of the
parameter can be estimated by building a k-distribution graph. For a user-
specified value of k(say, four data points), the distance of the k-th NN can
be calculated for a data point. If the data point is a core point in a high-
density region, then the distance of the k-th NN will be smaller. For a noise
point, the distance will be larger. Similarly, the k-distance can be calculated
for all data points in a dataset. A k-distance distribution graph can be built
by arranging all the k-distance values of individual data points in descending
order, as shown in Fig. 7.16 . This arrangement is similar to Pareto charts.
Points on the right-hand side of the chart will belong to data points inside a
cluster, because the distance is smaller. In most datasets, the value of k-dis-
tance sharply rises after a particular value. The distance at which the chart
rises will be the optimal value ε(epsilon) and the value of kcan be used for
MinPoints.
Special Cases: Varying Densities
The DBSCAN algorithm partitions data based on a certain threshold density.
This approach creates an issue when a dataset contains areas of varying data
density. The dataset in Fig. 7.17 has four distinct regions numbered from
1/C04. Region 1 is the high-density area A, regions 2 and 4 are of medium-
density B, and between them is region 3, which is extremely low-density C. If242 CHAPTER 7: Clusteringthe density threshold parameters are tuned in such a way as to partition and
identify region 1, then regions 2 and 4 (with density B) will be considered
noise, along with region 3. Even though region 4 with density B is next to anextremely low-density area and clearly identifiable visually, the DBSCAN
algorithm will classify regions 2 through 4 as noise. The k-means clustering
algorithm is better at partitioning datasets with varying densities.
k–Distance value
1.10E0
1.00E0
9.00E–1
8.00E–1
7.00E–1
6.00E–1
5.00E–1
4.00E–1
3.00E–1
2.00E–1
1.00E–1Distance of fourth nearest neighbor
Data points sorted by distanceSorted k–distancesεε
FIGURE 7.16
k-Distribution chart for the Iris dataset with k54.
FIGURE 7.17
Dataset with varying densities.7.2 DBSCAN Clustering 2437.2.2 How to Implement
The implementation of the DBSCAN algorithm is supported in RapidMiner
through the DBSCAN modeling operator. The DBSCAN operator acceptsnumeric and polynominal datasets with provisions for user-specified ε(epsi-
lon) and MinPoints parameters. Here are the implementation steps.
Step 1: Data Preparation
As with the k-means section, the number of attributes in the dataset will be
limited to a3 and a4 (petal length and petal width) using the Select Attribute
operator, so that the cluster can be visualized and the clustering process bet-
ter understood.
Step 2: Clustering Operator and Parameters
The modeling operator is available in the Modeling .Clustering and
Segmentation folder and is labeled DBSCAN. These parameters can be con-
figured in the model operator:
GEpsilon ( ε): Size of the high-density neighborhood. The default value is 1.
GMinPoints : Minimum number of data objects within the epsilon
neighborhood to qualify as a cluster.
GDistance measure : The proximity measure can be specified in this
parameter. The default and most common measurement is Euclideandistance. Other options here are Manhattan distance, Jaccard
coefficient, and cosine similarity for document data. Please refer to
Chapter 4, Classification for a summary of different distance measures.
GAdd cluster as attributes : To append cluster labels into the original
dataset. This option is recommended for later analysis.
Step 3: Evaluation
Similar to k-means clustering implementation, the effectiveness of clustering
groups can be evaluated using average within-cluster distance. InRapidMiner, the Cluster Density Performance operator under Evaluation .
Clustering is available for performance evaluation of cluster groups generated
by Density algorithms. The clustering model and labeled dataset are con-nected to performance operator for cluster evaluation. Additionally, to aid the
calculation, performance operator expects Similarity Measure objects. A simi-
larity measure vector is a distance measure of every example data object withthe other data object. The similarity measure can be calculated by using Data
to Similarity Operator on the example dataset.244 CHAPTER 7: ClusteringStep 4: Execution and Interpretation
After the outputs from the performance operator have been connected to the
result ports, as shown in Fig. 7.18 , the model can be executed. The result out-
put can be observed as:
1.Model : The cluster model output contains information on the number
of clusters found in the dataset (Cluster 1, Cluster 2, etc.) and data
objects identified as noise points (Cluster 0). If no noise points are
found, then Cluster 0 is an empty cluster. As shown in Fig. 7.19 , the
Folder view and Graph view from the output window provide the
visualization of data points classified under different clusters.
2.Clustered example set : The example set now has a clustering label that
can be used for further analysis and visualization. In the scatterplot
view of this dataset ( Fig. 7.20 ),x- and y-axes can be configured to be
attributes of the original dataset, petal length and petal width. The
Color Column can be configured to be the new cluster label. In the
plot, it can be noted how the algorithm found two clusters within the
example set. The I. setosa species data objects have clear high-density
areas but there is a density bridge between the I. versicolor and
I.virginica species data points. There is no clear low-density area to
FIGURE 7.18
Data science process with density clustering.7.2 DBSCAN Clustering 245FIGURE 7.19
Density-clustering model output.
FIGURE 7.20
Density-clustering visual output.246 CHAPTER 7: Clusteringpartition these two species of data points. Hence, I. versicolor and I.
virginica natural clusters are combined to one artificial predicted
cluster. The epsilon and MinPoints parameters can be adjusted to find
different results for the clustering.
3.Performance vector : The performance vector window shows the average
distance within each cluster and the average of all clusters. The average
distance is the distance between all the data points within the cluster
divided by number of data points. These measures can be used to
compare the performance of multiple model runs.
The main attraction of using DBSCAN clustering is that one does not have to
specify the value of k, the number of clusters to be identified. In many practi-
cal applications, the number of clusters to be discovered will be unknown,
like finding unique customers or electoral segments. DBSCAN uses variations
in the density of the data distribution to find the concentration of structures
in the data. These clusters can be of any shape and they are not confined to
globular structures as in the k-means approach. But the density algorithms
run into the risk of finding bridges between two natural clusters and merging
them into one cluster.
Since the density-clustering technique yields partial clustering, DBSCAN
ignores noise and outlier data points and they are not clustered in the final
results. The inability to identify varying densities within a dataset is one of
the major limitations of the DBSCAN clustering technique. Centroid meth-
ods are more successful at finding varying density patterns in a dataset. A
dataset with a high number of attributes will have processing challenges with
density-clustering methods. Given the complementary pros and cons of the
k-means and DBSCAN methods, it is advisable to cluster the dataset by both
methods and understand the patterns of both result sets.
7.3 SELF-ORGANIZING MAPS
A self-organizing map (SOM) is a powerful visual clustering technique that
evolved from a combination of neural networks and prototype-based cluster-
ing. A SOM is a form of neural network where the output is an organized
visual matrix, usually a two-dimensional grid with rows and columns. The
objective of this neural network is to transfer all input data objects with n
attributes ( ndimensions) to the output lattice in such a way that objects next
to each other are closely related to each other. Two examples of SOM layouts
are provided in Fig. 7.21 . This two-dimensional matrix becomes an explora-
tion tool in identifying the clusters of objects related to each other by visual
examination. A key distinction in this neural network is the absence of an
output target function to optimize or predict, hence, it is an unsupervised
learning algorithm. SOMs effectively arrange the data points in a lower7.3 Self-Organizing Maps 247dimensional space, thereby, aiding in the visualization of high-dimensional
data through a low-dimensional space.
SOMs are relevant to clustering because the most common SOM output is a
two-dimensional grid with data objects placed next to each other based on
their similarity to one another. Objects related to each other are placed in
close proximity. SOMs differ from other clustering techniques because there
is no explicit clustering labels assigned to data objects. Data objects are
arranged based on their attribute proximity and the task of clustering is left
to visual analysis by the user. Hence, SOM is used as a visual clustering and
data exploration technique ( Germano, 1999 ).
SOMs were first proposed by Kohonen (1982) and, hence, this technique is
also known as Kohonen networks; it is sometimes also referred to by a more
specific name, self-organizing feature maps. SOM methodology is used to proj-
ect data objects from data space ,m o s t l yi n ndimensions, to grid space ,u s u a l l y
resulting in two dimensions. Though o ther output formats are possible, the
most common output formats for SOMs are: (1) a hexagonal lattice or a (2)
rectangular grid as shown in Fig. 7.21 . Each data point from the dataset occu-
pies a cell or a node in the output lattice, with arrangement constraints depend-
ing on the similarity of data points. Each cell in the SOM grid, called a neuron ,
corresponds to one or a group of data points. In a hexagonal grid, each neuron
has six neighbors while a rectangular lattice has four neighbors.
SOMs are commonly used in comparing data points with a large number of
numeric attributes. The objective of this kind of analysis is to compare the
FIGURE 7.21
Self-organizing maps of countries by GDP data using a (A) hexagonal grid and (B) rectangular lattice. GDP, Gross domestic product.248 CHAPTER 7: Clusteringrelative features of data objects in a simple two-dimensional setting where
the placement of objects is related to each other. In Fig. 7.21 A, the SOM
compares relative gross domestic product (GDP) data from different coun-
tries where countries with similar GDP profiles are placed either in the same
cells or next to each other. All similar countries around a particular cell can
be considered a grouping. Although the individual data objects (countries)
do not have a cluster membership, the placement of objects together aides in
visual data analysis. This application is also called competitive SOMs.
7.3.1 How It Works
The algorithm for a SOM is similar to centroid-based clustering but with a
neural network foundation. Since a SOM is essentially a neural network, the
model accepts only numerical attributes. However, there is no target variable
in SOM because it is an unsupervised learning model. The objective of the
algorithm is to find a set of centroids (neurons) to represent the cluster but
with topological constraints. The topology refers to an arrangement of cen-
troids in the output grid. All the data objects from the dataset are assigned to
each centroid. Centroids closer to each other in the grid are more closely
“related ”to each other than to centroids further away in the grid. A SOM
converts numbers from the data space to a grid space with additional inter-
topology relationships.
Step 1: Topology Specification
The first step for a SOM is specifying the topology of the output. Even
though multi-dimensional output is possible, two-dimensional rows and col-
umns with either a rectangular lattice or a hexagonal lattice are commonly
used in SOMs to aid in the visual discovery of clustering. One advantage in
using a hexagonal lattice is that each node or centroid can have six neigh-
bors, two more than in a rectangular lattice. Hence, in a hexagonal lattice,
the association of a data point with another data point can be more precise
than for a rectangular grid. The number of centroids can be specified by pro-
viding the number of rows and columns in the grid. The number of centroids
is the product of the number of rows and columns in the grid. Fig. 7.22
shows a hexagonal lattice SOM.
Step 2: Initialize Centroids
A SOM starts the process by initializing the centroids. The initial centroids
are values of random data objects from the dataset. This is similar to initializ-
ing centroids in k-means clustering.7.3 Self-Organizing Maps 249Step 3: Assignment of Data Objects
After centroids are selected and placed on the grid in the intersection of rows
and columns, data objects are selected one by one and assigned to the near-
est centroid. The nearest centroid can be calculated using a distance functionlike Euclidean distance for numeric data or a cosine measure for document
or binary data.
Step 4: Centroid Update
The centroid update is the most significant and distinct step in the SOM
algorithm and is repeated for every data object. The centroid update has tworelated sub-steps.
The first sub-step is to update the closest centroid. The objective of the
method is to update the data values of the nearest centroid of the dataobject, proportional to the difference between the centroid and the data
object. This is similar to updating weights in the back-propagation algorithm
of neural networks. In the neural network section of Chapter 4,Classification, how the weights of neurons are updated based on the error
difference between the predicted and actual values was discussed. Similarly,
in the context of a SOM, the values of centroids are updated. In fact, cen-troids are considered neurons in a SOM. Through this update, the closest
centroid moves closer to the data object in the data space.
FIGURE 7.22
Weight of the centroid is updated.250 CHAPTER 7: ClusteringThe centroid update step is repeated for a number of iterations, usually in
the thousands. Denote tas the tthiteration of the update where the data
point d(t) is picked up. Let w1,w2,w3,...,wkrepresent all the centroids in
the grid space. Fig. 7.22 shows the lattice with centroid weight. Let rand cbe
the number of rows and columns in the grid. Then kwill be equal to r*c.
Letwibe the nearest centroid for a data object d(t). During iteration t, the
nearest centroid wiis updated using Eq. (7.4) .
wiðt11Þ5wiðtÞ1fiðtÞ3½dðtÞ2wiðtÞ/C138 ð 7:4Þ
The effect of the update is determined by the difference between the centroid and
the data point in the data space and the neighborhood function fi(t). The neigh-
borhood function decreases for every iter ation so that there are no drastic changes
made in the final iteration. In addition to updating the nearest primary centroid,
all other centroids in the grid space neighborhood of the primary centroid are
updated as well. This will be reviewed in more detail in the next sub-step.
The second sub-step is to update all centroids in the grid space neighbor-
hood as shown in Fig. 7.23 . The neighborhood update step has to be propor-
tional to the distance from the closest centroid to the centroid that is being
updated. The update function has to be stronger when the distance is closer.
Taking into account time decay and distance between neighborhood cen-
troids, a Gaussian function is commonly used for:
FIGURE 7.23
The weight of the neighborhood centroids are updated.7.3 Self-Organizing Maps 251fitðÞ5λitðÞe2gi2gjðÞ2
2σ2/C18/C19
ð7:5Þ
where λi(t) is the learning rate function that takes a value between 0 and 1
and decays for every iteration. Usually it is either a linear rate function or aninverse of the time function. The variable in the exponential parameter g
i2gj
is the distance between the centroid being updated and the nearest centroid
of the data point in the grid space. The variable σdetermines the radius of
the centroid update or the neighborhood effect. By updating the entire neigh-
borhood of centroids in the grid, the SOM self-organizes the centroid lattice.
Effectively, the SOM converts data from the data space to a location-constrained grid space.
Step 5: Termination
The entire algorithm is continued until no significant centroid updates take
place in each run or until the specified number of run count is reached. The
selection of the data object can be repeated if the dataset size is small. Likewith many data science algorithms, a SOM tends to converge to a solution in
most cases but doesn ’t guarantee an optimal solution. To tackle this prob-
lem, it is necessary to have multiple runs with various initiation measuresand to compare the results.
Step 6: Mapping a New Data Object
A SOM model itself is a valuable visualization tool that can describe the rela-
tionship between data objects and be used for visual clustering. After the
grids with the desired number of centroids have been built, any new dataobject can be quickly given a location on the grid space, based on its proxim-
ity to the centroids. The characteristics of new data objects can be further
understood by studying the neighbors.
7.3.2 How to Implement
SOM can be implemented in a few different ways in RapidMiner, with variedfunctionality and resulting output.
GData exploration chart : In Chapter 3, Data Exploration, the SOM chart
was reviewed as one of the data exploration techniques. In
RapidMiner, any dataset connected to a result port has a SOM chart
feature under the Chart tab. This is a quick and easy method wherethe number of rows and columns can be specified, and a SOM chart
can be rendered.
GData Transformation .Attribute set reduction .SOM Operator : The SOM
operator available under the Data Transformation folder is used to
reduce the number of dimensions in the dataset. It is similar to the252 CHAPTER 7: Clusteringapplication of principal component analysis where the dataset is
reduced to a lower dimensionality. In theory, a SOM can help reduce
the data to any number of dimensions below the dimension count of
the dataset. In this operator, the number of desired output dimensions
can be specified in the parameters, as shown in Fig. 7.24 . The net size
parameter indicates the unique values in each of the SOM dimensions.
There is no visual output for this operator and in two dimensions, only
a square topography can be achieved through the SOM data
transformation operator.
GRapidMiner Extension .SOM Modeling Operator : The SOM modeling
operator is available in the SOM extension ( Motl, 2012 ) and offers rich
SOM visuals. SOM extensions will be used for the rest of this
implementation section, so installing the SOM extension is
recommended before proceeding further.
RapidMiner provides a marketplace platform called
Extensions for specialized data science tasks which has
operators, data science algorithms, data transformation
operators, and visual exploration tools. The extensions canbe installed and uninstalled easily from Help .Updates
and Extensions. SOM is one of the extensions, along with
text mining, the R extension, Recommenders, Weka exten-
sion, etc. Extensions will be used in the upcoming chapters.
The dataset used in this section is the relative GDP information by country
(IMF, 2012 ) from the World Economic Outlook Database October 2012 by the
International Monetary Fund. The dataset has 186 records, one for each
country, and four attributes in percentage of GDP: relative GDP invested, rel-
ative GDP saved, government revenue, and current account balance. Fig. 7.25
shows the actual raw data for a few rows and Fig. 7.26 shows the quartile
plot for all the attributes.
FIGURE 7.24
SOM in data transformation. SOM , Self-organizing map.7.3 Self-Organizing Maps 253FIGURE 7.25
GDP by country dataset. GDP, Gross domestic product.
7.52E1
5.52E1
3.91E1
2.10E1
2.97E0
–1.51E1
–3.32E1Current
AccountBalanceGeneral
Government
Revenue
Gross National
SavingsTotal
Investment
FIGURE 7.26
GDP by country: box-whisker (quartile) plot for all four attributes. GDP, Gross domestic product.254 CHAPTER 7: ClusteringThe objective of the clustering is to compare and contrast countries based on
their percentage of GDP invested and saved, government revenue, and cur-
rent account balance. Note that they are not compared using the size of the
economy through absolute GDP but by the size of investment, national sav-
ings, current account, and the size of government relative to the country ’s
GDP. The goal of this modeling exercise is to arrange countries in a grid so
that countries with similar characteristics of investing, savings, size of govern-
ment, and current accounts are placed next to each other. The four-
dimensional information is being compressed into a two-dimensional map
or grid. The dataset and RapidMiner process can be accessed from the com-
panion site of the book at www.IntroDataScience.com .
Step 1: Data Preparation
As a neural network, a SOM cannot accept polynominal or categorical attri-
butes because centroid updates and dis tance calculations work only with
numeric values. Polynominal data can be either ignored with information
loss or converted to a numeric attribute using the Nominal to Numerical type
conversion operator available in RapidMiner. The country attribute is set as
a label using Set role operator. In this da taset, there are records (each record
is a country) where there is no data in few attributes. Neural networks can-
not handle missing values and, hence, they need to be replaced by either
zero or the minimum or average value for the attribute using the Replace
Missing Value operator. In this example the average was chosen as the
default missing value.
Step 2: SOM Modeling Operator and Parameters
The SOM modeling extension operator is available in the Self-Organizing Map
folder, labeled with the same name. Please note that the SOM folder is visi-
ble only when the SOM extension has been installed. The coming parameters
can be configured in the model operator. The modeling operator accepts the
example set with numeric data and a label attribute if applicable. In this
example set, the country name is a label attribute. Fig. 7.27 shows the
RapidMiner process for developing a SOM model.
GTraining Rounds : Defaults to 1000. This value indicates the number of
training rounds for the data object selection process.
GNet Size : Indicates whether the grid size should be specified by the user
or automatically approximated by the system. In this exercise select
user input for Xand Y.
GNet Size X : Specifies the number of columns in the grid (horizontal
direction). This is the same as the possible values for the SOM_0
attribute in the output. In this example this value will be set to 10.7.3 Self-Organizing Maps 255GNet Size Y : Specifies the number of rows in the grid (vertical direction).
This also indicates the values for the SOM_1 attribute in the outputgrid. In this example this value will be set to 10.
GLearning Rate : The neural network learning parameter ( λ), which takes
a value from 0 to 1. The value of λdetermines how sensitive the
change in weight is to the previous weights. A value closer to 0 means
the new weight would be mostly based on previous weight and an λ
closer to 1 means that the weight would be mainly based on errorcorrection. The initial λwill be assigned as 0.9 (see Section 4.5 on
Neural Networks).
GLearning Rate function : The learning rate function in a neural network is
a time decay function of the learning process. The default and most
commonly used time decay function is the inverse of time.
Step 3: Execution and Interpretation
The RapidMiner process can be saved and executed. The output of the SOM
modeling operator consists of a visual model and a grid dataset. The visual
model is a lattice with centroids and mapped data points. The grid dataset
output is the example set labeled with location coordinates for each record
in the grid lattice.
FIGURE 7.27
Clustering with SOM. SOM , Self-organizing map.256 CHAPTER 7: ClusteringVisual Model
The visual model of the SOM displays the most recognizable form of SOM
in a hexagonal grid format. The size of the grid is configured by the input
parameters that set the net size of Xand Y. There are several advanced visual-
ization styles available in the Visualization results window. The SOM visual
output can be customized using these parameters:
GVisualization Style : This selection controls the visual layout and
background color of the SOM hexagons. The value of the selected
measure is represented as a background gradient color. The default,
U-Matrix, presents a background gradient color proportional to the
distance of the central data points in adjacent hexagons. The P-Matrix
option shows the number of example data points through the
background gradient color. The selection of an individual attribute
name for the visualization style renders the background gradient
proportional to the value of the selected attribute. The visualization
style selection does not rearrange the data points assigned to hexagons.
GLabel : Selection shows the attribute value selected in the hexagons.
GColor Schema : Selection of monochrome or color scheme.
Fig. 7.28 shows a SOM with the default selection of the label as Country and
the visualization style as U-Matrix. It can be observed how countries are
FIGURE 7.28
SOM output in the hexagonal grid. SOM , Self-organizing map.7.3 Self-Organizing Maps 257placed on the grid based on their relationship with each other, as evaluated
by the four economic metrics in relation to GDP. Countries with similar
characteristics are placed closer to each other than to others in the grid. If
more than one country belongs to a centroid (hexagon), then the label of
one country closer to the centroid is displayed on the grid. The grid locations
of all the counties are listed in the location coordinates section of the results
window.
A few interesting patterns in the data can be observed by changing the visual-
ization style as a metric in the dataset. In Fig. 7.29 , government revenue as
percentage of GDP is used and visually, countries with high government rev-
enue as a percentage of GDP is displayed on the top-left side of the grid
(e.g., Belgium with 48%) and countries with low government revenue are at
bottom of the grid (Bangladesh 11%).
Fig. 7.30 shows the national savings rate visualization in the SOM; countries
with a high savings rate (Algeria 49%) are concentrated on the left side and
countries with a low savings rate (Maldives 2%) are concentrated on the right
side of the SOM.
FIGURE 7.29
SOM output with color overlay related to government revenue. SOM , Self-organizing map.258 CHAPTER 7: ClusteringLocation Coordinates
The second output of the SOM operator contains the location coordinates of
thex- and y-axes of grid with labels SOM_0 and SOM_1. The coordinate
values of location range from 0 to net size —1, as specified in the model
parameters, since all data objects, in this case countries, are assigned to a spe-
cific location in the grid. This output, as shown in Fig. 7.31 , can be further
used for post-processing such as distance calculation of locations between
countries in the grid space.
Conclusion
The methodology of SOMs is derived from the foundations of both neural
network and prototype-clustering approaches. SOMs are an effective visual
clustering tool to understand high-dimensional numeric data. They reduce
the features of the dataset to two or three features, which is used to specify
the topology of the layout. Hence, SOMs are predominantly used as a visual
discovery and data exploration technique. Some of the applications of SOMs
include the methods that are used in conjunction with other data science
techniques such as graph mining ( Resta, 2012 ), text mining ( Liu, Liu, &
Wang, 2012 ), speech recognition ( Kohonen, 1988 ), etc.
FIGURE 7.30
SOM output with color overlay related to national savings rate. SOM , Self-organizing map.7.3 Self-Organizing Maps 259References
Bache, K., & Lichman, M. (2013). UCI machine learning repository . University of California, School
of Information and Computer Science. Retrieved from ,http://archive.ics.uci.edu/ml ..
Berry, M. J., & Linoff, G. (2000a). Converging on the customer: Understanding the customer
behavior in the telecommunications industry. In M. J. Berry, & G. Linoff (Eds.), Mastering
data science: The art and science of customer relationship management (pp. 357 /C0394). John Wiley
& Sons, Inc.
Berry, M. J., & Linoff, G. (2000b). Data science techniques and algorithms. In M. J. Berry, & G.
Linoff (Eds.), Mastering data science: The art and science of customer relationship management
(pp. 103 /C0107). John Wiley & Sons, Inc.
Davies, D. L., & Bouldin, D. W. (1979). A cluster separation measure. IEEE Transactions on Pattern
Analysis and Machine Intelligence ,1(2), 224 /C0227.
Ester, M., Kriegel, H. -P., Sander, J., & Xu X. (1996). A density-based algorithm for discovering
clusters in large spatial databases with noise. In AAAI Press proceedings of 2nd international con-
ference on knowledge discovery and data science KDD-96 (Vol. 96, pp. 226 /C0231). AAAI Press.
Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of
Human Genetics ,7, 179 /C0188. Retrieved from ,https://doi.org/10.1111/j.1469-1809.1936.
tb02137.x ..
Germano, T. (March 23, 1999) Self-organizing maps. Retrieved from ,http://davis.wpi.edu/
Bmatt/courses/soms/ .Accessed 10.12.13.
FIGURE 7.31
SOM output with location coordinates. SOM , Self-organizing map.260 CHAPTER 7: ClusteringHamerly, G., & Elkan, C. (2003). Learning the kink-means. Advances in Neural Information
Processing Systems ,17,1/C08. Available from http://dx.doi.org/10.1.1.9.3574 .
IMF, (2012, October). World economic outlook database . International Monetary Fund. Retrieved
from ,http://www.imf.org/external/pubs/ft/weo/2012/02/weodata/index.aspx .Accessed
15.03.13.
Kohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biological
Cybernetics ,43,5 9 /C069.
Kohonen, T. (1988). The “neural ”phonetic typewriter. Computer, IEEE ,21(3), 11 /C022. Available
from https://doi.org/10.1109/2.28 .
Liu, Y., Liu, M., & Wang, X. (2012). Application of self-organizing maps in text clustering: A
review. In M. Johnsson (Ed.), Applications of self-organizing maps (pp. 205 /C0220). InTech.
Lloyd, S. (1982). Least squares quantization in PCM. IEEE Transactions on Information Theory ,28,
129 /C0137.
Motl, J. (2012). SOM extension for rapid miner . Prague: Czech Technical University.
Pearson, P., & Cooper, C. (2012). Using self organizing maps to analyze demographics and
swing state voting in the 2008 U.S. presidential election. In N. Mana, F. Schwenker, & E.
Trentin (Eds.), Artificial neural networks in pattern recognition ANNPR ’12 Proceedings of the 5th
INNS IAPR TC 3 GIRPR conference (pp. 201 /C0212). Heidelberg: Springer Berlin Heidelberg,
Berlin10.1007/978-3-642-33212-8.
Resta, M. (2012). Graph mining based SOM: A tool to analyze economic stability. In M.
Johnsson (Ed.), Applications of self-organizing maps (pp. 1 /C026). InTech. Retrieved from
,http://www.intechopen.com/books/applications-of-self-organizing-maps ..
Tan, P.-N., Michael, S., & Kumar, V. (2005). Clustering analysis: Basic concepts and algorithms.
In P.-N. Tan, S. Michael, & V. Kumar (Eds.), Introduction to data science (pp. 487 /C0555).
Boston, MA: Addison-Wesley.
Witten, I. H., & Frank, E. (2005). Algorithms: The basic methods .Data science: Practical machine
learning tools and techniques (pp. 136 /C0139). San Francisco, CA: Morgan Kaufmann.References 261CHAPTER 8
Model Evaluation
In this chapter, the most commonly used methods for testing the quality of a
data science model will be formally introduced. Throughout this book, vari-
ous validation techniques have been used to split the available data into a
training set and a testing set. In the implementation sections, different types
of performance operators in conjunction with validation have been used
without an in detail explanation of how these operators really function.
Several ways in which predictive data science models are evaluated for their
performance will now be discussed.
There are a few main tools that are available to test a classification model ’s
quality: confusion matrices (or truth tables), lift charts, ROC (receiver operator
characteristic) curves, area under the curve (AUC). How these tools are con-
structed will be defined in detail and how to implement performance eva-
luations will be described. To evaluate a numeric prediction from a
regression model, there are many conven tional statistical tests that may be
applied ( Black, 2008 ) a few of which were discussed in Chapter 5,
Regression Methods.
DIRECT MARKETING
Direct marketing (DM) companies, which send out postal
mail (or in the days before do-not-call lists, they called
prospects) were one of the early pioneers in applying data
science techniques ( Berry, 1999 ). A key performance indi-
cator for their marketing activities is of course the
improvement in their bottom line as a result of their utili-
zation of predictive models.
Assume that a typical average response rate for a direct
mail campaign is 10%. Further assume that: cost per
mail sent 5$1 and potential revenue per response 5$20.If they have 10,000 people to send out their mailers to,
then they can expect to receive potential revenues of
10,000310%3$205$20,000, which would yield a net
return of $10,000. Typically, the mailers are sent out in
batches to spread costs over a period of time. Further
assume that these are sent out in batches of 1000. The
first question someone would ask is how to divide the list
of names into these batches. If the average expectation
of return is 10%, then would it not make a lot of sense to
just send one batch of mails to those prospects that
make up this 10% and be done with the campaign?
(Continued )
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00008-3
©2019 Elsevier Inc. All rights reserved.2638.1 CONFUSION MATRIX
Classification performance is best described by an aptly named tool called the
confusion matrix or truth table. Understanding the confusion matrix requires
becoming familiar with several definitions. But before introducing the defini-
tions, a basic confusion matrix for a binary or binomial classification must
first be looked at where there can be two classes (say, Y or N). The accuracy of
classification of a specific example can be viewed in one of four possible ways:
GThe predicted class is Y, and the actual class is also Y -this is a True
Positive or TP
GThe predicted class is Y, and the actual class is N -this is a False
Positive or FP
GThe predicted class is N, and the actual class is Y -this is a False
Negative or FN
GThe predicted class is N, and the actual class is also N -this is a True
Negative or TN
A basic confusion matrix is traditionally arranged as a 2 32 matrix as shown
inTable 8.1 . The predicted classes are arranged horizontally in rows and the
Table 8.1 Confusion Matrix
Actual Class (Observation)
YN
Predicted class (expectation) Y TPcorrect result FPunexpected result
N FNmissing result TNcorrect absence of result
TP, true positive; FP, false positive; FN, false negative; TN, true negative.(Continued )
Clearly this would save a lot of time and money and the
net return would jump to $19,000!
Can all of these 10 percenters be identified? While this is
clearly unrealistic, classification techniques can be used
to rank or score prospects by the likelihood that they
would respond to the mailers. Predictive analytics is after
all about converting future uncertainties into usable prob-
abilities ( Taylor, 2011 ). Then a predictive method can be
used to order these probabilities and send out the mailersto only those who score above a particular threshold (say
85% chance of response).
Finally, some techniques may be better suited to this prob-
lem than others. How can the different available methods
be compared based on their performance? Will logistic
regression capture these top 10 percenters better than
support vector machines? What are the different metrics
that can be used to select the best performing methods?
These are some of the things that will be discussed in this
chapter in more detail.264 CHAPTER 8: Model Evaluationactual classes are arranged vertically in columns, although sometimes this
order is reversed ( Kohavi & Provost, 1998 ). A quick way to examine this
matrix or a truth table as it is also called is to scan the diagonal from top left
to bottom right. An ideal classification performance would only have entries
along this main diagonal and the off-diagonal elements would be zero.
These four cases will now be used to introduce several commonly used terms
for understanding and explaining classification performance. As mentioned
earlier, a perfect classifier will have no entries for FP and FN (i.e., the number
of FP5number of FN 50).
1.Sensitivity is the ability of a classifier to select all the cases that need to
be selected. A perfect classifier will select all the actual Y ’s and will not
miss any actual Y ’s. In other words it will have no FNs. In reality, any
classifier will miss some true Y ’s, and thus, have some FNs. Sensitivity
is expressed as a ratio (or percentage) calculated as follows: TP/(TP 1
FN). However, sensitivity alone is not sufficient to evaluate a classifier.
In situations such as credit card fraud, where rates are typically around
0.1%, an ordinary classifier may be able to show sensitivity of 99.9%
by picking nearly all the cases as legitimate transactions or TP. The
ability to detect illegitimate or fraudulent transactions, the TNs, is also
needed. This is where the next measure, specificity, which ignores TPs,
comes in.
2.Specificity is the ability of a classifier to reject all the cases that need to
be rejected. A perfect classifier will reject all the actual N ’s and will not
deliver any unexpected results. In other words, it will have no FPs. In
reality, any classifier will select some cases that need to be rejected, and
thus, have some FPs. Specificity is expressed as a ratio (or percentage)
calculated as: TN/(TN 1FP).
3.Relevance is a term that is easy to understand in a document search
and retrieval scenario. Suppose a search is run for a specific term and
that search returns 100 documents. Of these, let us say only 70 were
useful because they were relevant to the search. Furthermore, the
search actually missed out on an additional 40 documents that could
actually have been useful. With this context, additional terms can be
defined.
4.Precision is defined as the proportion of cases found that were actually
relevant. From the example, this number was 70, and thus, the
precision is 70/100 or 70%. The 70 documents were TP, whereas the
remaining 30 were FP. Therefore, precision is TP/(TP 1FP).
5.Recall is defined as the proportion of the relevant cases that were
actually found among all the relevant cases. Again, with the example,
only 70 of the total 110 (70 found 140 missed) relevant cases were8.1 Confusion Matrix 265actually found, thus, giving a recall of 70/110 563.63%. It is evident
that recall is the same as sensitivity, because recall is also given by
TP/(TP 1FN).
6.Accuracy is defined as the ability of the classifier to select all cases that
need to be selected and reject all cases that need to be rejected. For a
classifier with 100% accuracy, this would imply that FN 5FP50. Note
that in the document search example, the TN has not been indicated,
as this could be really large. Accuracy is given by (TP 1TN)/
(TP1FP1TN1FN). Finally, error is simply the complement of
accuracy, measured by (1 2accuracy).
Table 8.2 summarizes all the major definitions. Fortunately, the analyst does
not need to memorize these equations because their calculations are always
automated in any tool of choice. However, it is important to have a good
fundamental understanding of these terms.
8.2 ROC AND AUC
Measures like accuracy or precision are essentially aggregates by nature, in
the sense that they provide the average performance of the classifier on
the dataset. A classifier can have a high accuracy on a dataset but have
poor class recall and precision. Clearly, a model to detect fraud is no
good if its ability to detect TP for the fraud 5yes class (and thereby its
class recall) is low. It is, therefore, quite useful to look at measures that
compare different metrics to see if there is a situation for a trade-off: for
example, can a little overall accura cy be sacrificed to gain a lot more
improvement in class reca ll? One can examine a model ’s rate of detecting
TPs and contrast it with its ability to detect FPs. The receiver operator
characteristic (ROC) curves meet thi s need and were originally developedTable 8.2 Evaluation Measures
Term Definition Calculation
Sensitivity Ability to select what needs to be selected TP/(TP 1FN)
Specificity Ability to reject what needs to be rejected TN/(TN 1FP)
Precision Proportion of cases found that were relevant TP/(TP 1FP)
Recall Proportion of all relevant cases that were
foundTP/(TP1FN)
Accuracy Aggregate measure of classifier performance (TP 1TN)/
(TP1TN1FP1FN)
TP, true positive; FP, false positive; FN, false negative; TN, true negative.266 CHAPTER 8: Model Evaluationin the field of signal detection ( Green, 1966 ). A ROC curve is created by
plotting the fraction of TPs (TP rate) versus the fraction of FPs (FP rate).
When a table of such values is generated, the FP rate can be plotted on
the horizontal axis and the TP rate (same as sensitivity or recall) on the
vertical axis. The FP can also be expressed as (1 2specificity) or TN rate.
Consider a classifier that could predict if a website visitor is likely to click on
a banner ad: the model would be most likely built using historic click-
through rates based on pages visited, time spent on certain pages, and other
characteristics of site visitors. In order to evaluate the performance of this
model on test data, a table such as the one shown in Table 8.3 can be
generated.
The first column “Actual Class ”consists of the actual class for a particular
example (in this case a website visitor, who has clicked on the banner ad).
The next column, “Predicted Class ”is the model prediction and the third col-
umn, “Confidence of response ”is the confidence of this prediction. In order
to create a ROC chart, the predicted data will need to be sorted in decreasing
order of confidence level, which has been done in this case. By comparing
columns Actual class and Predicted class, the type of prediction can be identi-
fied: for instance, spreadsheet rows 2 through 5 are all TPs and row 6 is the
first instance of a FP. As observed in columns “Number of TP ”and“Number
of FP, ”one can keep a running count of the TPs and FPs and also calculate
the fraction of TPs and FPs, which are shown in columns “Fraction of TP ”
and“Fraction of FP. ”
Observing the “Number of TP ”and “Number of FP ”columns, it is evident
that the model has discovered a total of 6 TPs and 4 FPs (the remaining 10
examples are all TNs). It can also be seen that the model has identified
nearly 67% of all the TPs before it fails and hits its first FP (row 6 above).
Finally, all TPs have been identified (when Fraction of TP 51) before the
next FP was run into. If Fraction of FP (FP rate) versus Fraction of TP (TP
rate) were now to be plotted, then a ROC chart similar to the one shown in
Fig. 8.1 would be seen. Clearly an ideal classifier would have an accuracy of
100% (and thus, would have identified 100% of all TPs). Thus, the ROC for
an ideal classifier would look like the dashed line shown in Fig. 8.1 . Finally,
an ordinary or random classifier (which has only a 50% accuracy) would
possibly be able to find one FP for every TP, and thus, look like the 45-
degree line shown.
As the number of test examples becomes larger, the ROC curve will become
smoother: the random classifier will simply look like a straight line drawn
between the points (0,0) and (1,1) —the stair steps become extremely small.
The area under this random classifier ’s ROC curve is basically the area of a8.2 ROC and AUC 267right triangle (with side 1 and height 1), which is 0.5. This quantity is
termed Area Under the Curve or AUC. AUC for the ideal classifier is 1.0.Thus, the performance of a classifier can also be quantified by its AUC:
obviously any AUC higher than 0.5 is better than random and the closer it
is to 1.0, the better the performance. A common rule of thumb is to selectthose classifiers that not only have a ROC curve that is closest to ideal, but
also an AUC higher than 0.8. Typical uses for AUC and ROC curves are to
compare the performance of different classification algorithms for the samedataset.
8.3 LIFT CURVES
Lift curves or lift charts were first deployed in direct marketing where theproblem was to identify if a particular prospect was worth calling or sending
an advertisement by mail. It was mentioned in the use case at the beginningTable 8.3 Classifier Performance Data Needed for Building a ROC Curve
Actual Class Predicted ClassConfidence of
“Response ” Type?Number
of TPNumber
of FPFraction
of FPFraction
of TP
Response Response 0.902 TP 1 0 0 0.167
Response Response 0.896 TP 2 0 0 0.333
Response Response 0.834 TP 3 0 0 0.500
Response Response 0.741 TP 4 0 0 0.667No response Response 0.686 FP 4 1 0.25 0.667Response Response 0.616 TP 5 1 0.25 0.833Response Response 0.609 TP 6 1 0.25 1No response Response 0.576 FP 6 2 0.5 1No response Response 0.542 FP 6 3 0.75 1No response Response 0.530 FP 6 4 1 1
No response No response 0.440 TN 6 4 1 1
No response No response 0.428 TN 6 4 1 1No response No response 0.393 TN 6 4 1 1No response No response 0.313 TN 6 4 1 1No response No response 0.298 TN 6 4 1 1No response No response 0.260 TN 6 4 1 1No response No response 0.248 TN 6 4 1 1No response No response 0.247 TN 6 4 1 1
No response No response 0.241 TN 6 4 1 1
No response No response 0.116 TN 6 4 1 1
ROC , receiver operator characteristic; TP, true positive; FP, false positive; TN, true negative.268 CHAPTER 8: Model Evaluationof this chapter that with a predictive model, one can score a list of prospects
by their propensity to respond to an ad campaign. When the prospects are
sorted by this score (by the decreasing order of their propensity to respond),
one now ends up with a mechanism to systematically select the most valu-
able prospects right at the beginning, and thus, maximize their return. Thus,
rather than mailing out the ads to a random group of prospects, the ads can
now be sent to the first batch of “most likely responders, ”followed by the
next batch and so on.
Without classification, the “most likely responders ”are distributed ran-
domly throughout the dataset. Suppose there is a dataset of 200 prospects
and it contains a total of 40 responders or TPs. If the dataset is broken up
into, say, 10 equal sized batches (calle d deciles), the likelihood of finding
TPs in each batch is also 20%, that is, four samples in each decile will be
TPs. However, when a predictive model is used to classify the prospects, a
good model will tend to pull these “most likely responders ”into the top
few deciles. Thus, in this simple example, it might be found that the
first two deciles will have all 40 TPs and the remaining eight deciles
have none.
1.2
0.8
0.6
0.4
0.2
0
0.2 0 0.4 0.6
% FP% TP
0.8 1 1.2ROC
Ideal ROC
Random ROC1
FIGURE 8.1
Comparing ROC curve for the example shown in Table 8.3 to random and ideal classifiers. ROC ,
receiver operator characteristic.8.3 Lift Curves 269Lift charts were developed to demonstrate this in a graphical way ( Rud,
2000 ). The focus is again on the TPs and, thus, it can be argued that they
indicate the sensitivity of the model unlike ROC curves, which can show the
relation between sensitivity and specificity.
The motivation for building lift charts was to depict how much better the
classifier performs compared to randomly selecting x% of the data (for prospects
to call) which would yield x% targets (to call or not) . Lift is the improvement
over this random selection that a predictive model can potentially yield
because of its scoring or ranking ability. For example, in the data from
Table 8.3 , there are a total of 6 TPs out of 20 test cases. If one were to take
the unscored data and randomly select 25% of the examples, it would be
expected that 25% of them were TPs (or 25% of 6 51.5). However, scoring
and reordering the dataset by confidence will improve this. As can be seen
inTable 8.4 , the first 25% or quartile of scored (reordered) data now con-
tains four TPs. This translates to a lift of 4/1.5 52.67. Similarly, the second
quartile of the unscored data can be expected to contain 50% (or three) of
the TPs. As seen in Table 8.4 , the scored 50% data contains all six TPs,
giving a lift of 6/3 52.00.
The steps to build lift charts are:
1.Generate scores for all the data points (prospects) in the test set using
the trained model.
2.Rank the prospects by decreasing score or confidence of response.
3.Count the TPs in the first 25% (quartile) of the dataset, and then the
first 50% (add the next quartile) and so on; see columns Cumulative
TP and Quartile in Table 8.4 .
4.Gain at a given quartile level is the ratio of the cumulative number of
TPs in that quartile to the total number of TPs in the entire dataset (six
in the example). The 1st quartile gain is, therefore, 4/6 or 67%, the
2nd quartile gain is 6/6 or 100%, and so on.
5.Liftis the ratio of gain to the random expectation at a given quartile
level. Remember that random expectation at the xth quartile is x%. In
the example, the random expectation is to find 25% of 6 51.5 TPs in
the 1st quartile, 50% or 3 TPs in the 2nd quartile, and so on. The
corresponding 1st quartile lift is, therefore, 4/1.5 52.667, the 2nd
quartile lift is 6/3 52.00, and so on.
The corresponding curves for the simple example are shown in Fig. 8.2 .
Typically lift charts are created on deciles not quartiles. Quartiles were chosen
here because they helped to illustrate the concept using the small 20-sample
test dataset. However, the logic remains the same for deciles or any other
groupings as well.270 CHAPTER 8: Model Evaluation8.4 HOW TO IMPLEMENT
A built-in dataset in RapidMiner will be used to demonstrate how all the
three classification performances (c onfusion matrix, ROC/AUC, and lift/
gain charts) are evaluated. The process shown in Fig. 8.3 uses the Generate
Direct Mailing Data operator to create a 10,000 record dataset. The objective
of the modeling (Naïve Bayes used here) is to predict whether a person is
likely to respond to a direct mailing campaign or not based on demo-
graphic attributes (age, lifestyle, ear nings, type of car, family status, and
sports affinity).
Step 1: Data Preparation
Create a dataset with 10,000 examples using the Generate Direct Mailing Data
operator by setting a local random seed (default 51992) to ensureTable 8.4 Scoring Predictions and Sorting by Confidences Is the Basis for Generating Lift Curves
Actual Class Predicted ClassConfidence of
“Response ” Type?Cumulative
TPCumulative
FP Quartile Gain Lift
Response Response 0.902 TP 1 0 1st 67% 2.666667
Response Response 0.896 TP 2 0 1st
Response Response 0.834 TP 3 0 1st
Response Response 0.741 TP 4 0 1st
No response Response 0.686 FP 4 1 1st
Response Response 0.616 TP 5 1 2nd 100% 2
Response Response 0.609 TP 6 1 2nd
No response Response 0.576 FP 6 2 2nd
No response Response 0.542 FP 6 3 2nd
No response Response 0.530 FP 6 4 2nd
No response No response 0.440 TN 6 4 3rd 100% 1.333333
No response No response 0.428 TN 6 4 3rd
No response No response 0.393 TN 6 4 3rd
No response No response 0.313 TN 6 4 3rd
No response No response 0.298 TN 6 4 3rd
No response No response 0.260 TN 6 4 4th 100% 1
No response No response 0.248 TN 6 4 4th
No response No response 0.247 TN 6 4 4th
No response No response 0.241 TN 6 4 4th
No response No response 0.116 TN 6 4 4th
TP, true positive; FP, false positive; TN, true negative.8.4 How to Implement 271120%
100%
80%
60%
40%
20%
0%
12
QuartileLift Gain
3400.511.5
LiftGain22.53
FIGURE 8.2
Lift and gain curves.
FIGURE 8.3
Process setup to demonstrate typical classification performance metrics.272 CHAPTER 8: Model Evaluationrepeatability. Convert the label attribute from polynomial (nominal) to
binominal using the appropriate operator as shown. This enables one to
select specific binominal classification performance measures.
Split data into two partitions: an 80% partition (8000 examples) for model
building and validation and a 20% partition for testing. An important point
to note is that data partitioning is not an exact science and this ratio can
change depending on the data.
Connect the 80% output (upper output port) from the Split Data operator to
theSplit Validation operator. Select a relative split with a ratio of 0.7 (70% for
training) and shuffled sampling.
Step 2: Modeling Operator and Parameters
Insert the naïve Bayes operator in the Tra ining panel of the Split Validation
operator and the usual Apply Model operator in the Testing panel. Add a
Performance (Binomial Classification) operator. Select the following options
in the performance operat or: accuracy, FP, FN, TP, TN, sensitivity, specific-
ity, and AUC.
Step 3: Evaluation
Add another Apply Model operator outside the Split Validation operator and
deliver the model to its mod input port while connecting the 2000
example data partition from Step 3 to the unl port. Add a Create Lift
Chart operator with these options selected: target class 5response, binning
type5frequency, and number of bins 510. Note the port connections as
shown in Fig. 8.3 .
Step 4: Execution and Interpretation
When the above process is run, the confusion matrix and ROC curve for the
validation sample should be generated (30% of the original 80% 52400
examples), whereas a lift curve should be generated for the test sample
(2000 examples). There is no reason why one cannot add another
Performance (Binomial Classification) operator for the test sample or create a
lift chart for the validation examples. (The reader should try this as an exer-
cise—how will the output from the Create Lift Chart operator be delivered
when it is inserted inside the Split Validation operator?)
The confusion matrix shown in Fig. 8.4 is used to calculate several common
metrics using the definitions from Table 8.1 . Compare them with the
RapidMiner outputs to verify understanding.8.4 How to Implement 273TP5629 ;TN51231 ;FP5394 ;FN5146
Term Definition Calculation
Sensitivity TP/(TP 1FN) 629/(629 1146)581.16%
Specificity TN/(TN 1FP) 1231/(1231 1394)575.75%
Precision TP/(TP 1FP) 629/(629 1394)561.5%
Recall TP/(TP 1FN) 629/(629 1146)581.16%
Accuracy (TP 1TN)/
(TP1TN1FP1FN)(62911231)/(629 1123113941146)5
77.5%
Note that RapidMiner makes a distinction between the two classes while cal-
culating precision and recall. For example, in order to calculate a class recall
for“no response, ”thepositive class becomes “no response ”and the corre-
sponding TP is 1231 and the corresponding FN is 394. Therefore, a class
recall for “no response ”is 1231/(1231 1394)575.75%, whereas the calcula-
tion above assumed that “response ”was the positive class. Class recall is an
important metric to keep in mind when dealing with highly unbalanced
data. Data are considered unbalanced if the proportion of the two classes is
skewed. When models are trained on unbalanced data, the resulting class
recalls also tend to be skewed. For example, in a dataset where there are only
2% responses, the resulting model can have a high recall for “no responses ”
but a very low class recall for “responses. ”This skew is not seen in the overall
model accuracy and using this model on unseen data may result in severe
misclassifications.
The solution to this problem is to either balance the training data so that
one ends up with a more or less equal proportion of classes or to insert pen-
alties or costs on misclassifications using the Metacost operator as discussed
in Chapter 5, Regression Methods. Data balancing is explained in more detail
in Chapter 13, Anomaly Detection.
The AUC is shown along with the ROC curve in Fig. 8.5 . As mentioned ear-
lier, AUC values close to 1 are indicative of a good model. The ROC captures
the sorted confidences of a prediction. As long as the prediction is correct for
the examples the curve takes one step up (increased TP). If the prediction is
wrong the curve takes one step to the right (increased FP). RapidMiner can
FIGURE 8.4
Confusion matrix for validation set of direct marketing dataset.274 CHAPTER 8: Model EvaluationFIGURE 8.5
ROC curve and AUC. ROC , receiver operator characteristic; AUC, area under the curve.show two additional AUCs called optimistic and pessimistic. The differences
between the optimistic and pessimistic curves occur when there are examples
with the same confidence, but the predictions are sometimes false and some-
times true. The optimistic curve shows the possibility that the correct predic-
tions are chosen first so the curve goes steeper upwards. The pessimistic
curve shows the possibility that the wrong predictions are chosen first so the
curve increases more gradually.
Finally, the lift chart outputs do not directly indicate the lift values as has
been demonstrated with the simple example earlier. In Step 5 of the process,
10 bins were selected for the chart and, thus, each bin will have 200 exam-
ples (a decile). Recall that to create a lift chart all the predictions will need to
be sorted by the confidence of the positive class (response), which is shown
inFig. 8.6 .
The first bar in the lift chart shown in Fig. 8.7 corresponds to the first bin of
200 examples after the sorting. The bar reveals that there are 181 TPs in this
bin (as can be seen from the table in Fig. 8.6 that the very second example,
Row No. 1973, is an FP). From the confusion matrix earlier, 629 TPs can be
seen in this example set. A random classifier would have identified 10% of
these or 62.9 TPs in the first 200 examples. Therefore, the lift for the first dec-
ile is 181/62.9 52.87. Similarly the lift for the first two deciles is
(1811167)/(2 362.9)52.76 and so on. Also, the first decile contains 181/
629528.8% of the TPs, the first two deciles contain (181 1167)/
629555.3% of the TPs, and so on. This is shown in the cumulative (per-
cent) gains curve on the right hand y-axis of the lift chart output.
As described earlier, a good classifier will accumulate all the TPs in the first
few deciles and will have extremely few FPs at the top of the heap. This will
result in a gain curve that quickly rises to the 100% level within the first few
deciles.
8.5 CONCLUSION
This chapter covered the basic performance evaluation tools that are typically
used in classification methods. Firstly the basic elements of a confusion
matrix were described and then the concepts that are important to under-
standing it, such as sensitivity, specificity, and accuracy were explored in
detail. The ROC curve was then described, which has its origins in signal
detection theory and has now been adopted for data science, along with the
equally useful aggregate metric of AUC. Finally, two useful tools were
described that have their origins in direct marketing applications: lift and
gain charts. How to build these curves in general and how they can be276 CHAPTER 8: Model EvaluationFIGURE 8.6
Table of scored responses used to build the lift chart.FIGURE 8.7
Lift chart generated.constructed using RapidMiner was discussed. In summary, these tools are
some of the most commonly used metrics for evaluating predictive models
and developing skill and confidence in using these is a prerequisite to devel-
oping data science expertise.
One key to developing good predictive models is to know when to use which
measures. As discussed earlier, relying on a single measure like accuracy can
be misleading. For highly unbalanced datasets, rely on several measures such
as class recall and precision in addition to accuracy. ROC curves are fre-
quently used to compare several algorithms side by side. Additionally, just as
there are an infinite number of triangular shapes that have the same area,
AUC should not be used alone to judge a model —AUC and ROCs should be
used in conjunction to rate a model ’s performance. Finally, lift and gain
charts are most commonly used for scoring applications where the examples
in a dataset need to be rank-ordered according to their propensity to belong
to a particular category.
References
Berry, M. A. (1999). Mastering data mining: The art and science of customer relationship management .
New York: John Wiley and Sons.
Black, K. (2008). Business statistics for contemporary decision making . New York: John Wiley and
Sons.
Green, D. S. (1966). Signal detection theory and psychophysics . New York: John Wiley and Sons.
Kohavi, R., & Provost, F. (1998). Glossary of terms. Machine Learning ,30, 271/C0274.
Rud, O. (2000). Data mining cookbook: Modeling data for marketing, risk and customer relationship
management . New York: John Wiley and Sons.
Taylor, J. (2011). Decision management systems: A practical guide to using business rules and predictive
analytics . Boston, Massachusetts: IBM Press.References 279CHAPTER 9
Text Mining
This chapter explores and formalizes how to extract patterns and discover
new knowledge by applying many of the techniques learned so far, not on
ordered data, but on unstructured natural language. This constitutes the vast
area of text and web mining. For all the techniques described up to this
point, a cleaned and organized table consisting of rows and columns of data
was fed as input to an algorithm. The output from the algorithm was a
model that could then be used to predict outcomes from a new data set or
to find patterns in data. But are the same techniques applicable to extract
patterns and predict outcomes when the input data looks like normal written
communication? This might seem baffling at first, but as shall be seen in this
chapter, there are ways of presenting text data to the same algorithms that
process normal data.
To start out a brief historical introduction to the field of text mining is given
to establish some context. In the following section, techniques that can con-
vert common text into a semi-structured format will be described that we,
and the algorithms introduced so far, can recognize. Finally, the text mining
concepts will be implemented using two case studies: one involving an unsu-
pervised (clustering) model and another involving a supervised support vec-
tor machine (SVM) model. The chapter will be closed with some key
considerations to keep in mind while implementing text mining.
Unstructured data (including text, audio, images, videos, etc.) is the new
frontier of data science. Eric Siegel in his book Predictive Analytics (Siegel,
2013 ) provides an interesting analogy: if all the data in the world was equiv-
alent to the water on earth, then textual data is like the ocean, making up a
majority of the volume. Text analytics is driven by the need to process natu-
ral human language, but unlike numeric or categorical data, natural language
does not exist in a structured format consisting of rows (of examples) and
columns (of attributes). Text mining is, therefore, the domain of unstruc-
tured data science.
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00009-5
©2019 Elsevier Inc. All rights reserved.281Some of the first applications of text mining came about when people were
trying to organize documents ( Cutting, 1992 ).Hearst (1999) recognized that
text analysis does not require artificial intelligence but “...a mixture of
computationally-driven and user-guided analysis, ”which is at the heart of
the supervised models used in predictive analytics that have been discussed
so far.
IT IS NLP, MY DEAR WATSON!
Perhaps the most famous application of text mining is
IBM’s Watson program, which performed spectacularly
when competing against humans on the nightly game
show Jeopardy! How does Watson use text mining?
Watson has instant access to hundreds of millions of
structured and unstructured documents, including the full
content of Wikipedia entries.
When a Jeopardy! question is transcribed to Watson, it
searches for and identifies candidate documents that
score a close match to the words of the question. The
search and comparison methods it uses are similar to
those used by search engines, and include many of the
techniques, such as n-grams and stemming, which will be
discussed in this chapter. Once it identifies candidate
documents, it again uses other text mining (also known as
natural language processing or NLP) methods to rank
them. For example, if the answer is, regarding this device,
Archimedes said, “give me a place to stand on, and I will
move the earth, ”a Watson search for this sentence in itsdatabases might reveal among its candidate documents
several with the term “lever. ”Watson might insert the word
“lever ”inside the answer text and rerun a new search to
see if there are other documents with the new combination
of terms. If the search result has many matches to the
terms in the sentence —as it most likely would in this
case —a high score is assigned to the inserted term.
If a broad and non-domain-focused program like Watson,
which relies heavily on text mining and NLP, can answer
open-ended quiz show questions with nearly 100% accu-
racy, one can imagine how successful specialized NLP
tools would be. In fact IBM has successfully deployed a
Watson-type program to help in decision making at health
care centers ( Upbin, 2013 ).
Text mining also finds applications in numerous business
activities such as email spam filtering, consumer senti-
ment analysis, and patent mining to name a few. A couple
of these will be explored in this chapter.
People in the data management domains can appreciate text mining in a
slightly different context. Here, the objective is not so much discovering new
trends or patterns, but cleaning data stored in business databases. For exam-
ple, when people make manual entries into a customer relationship manage-
ment software, there is a lot of scope for typographic errors: a salesperson ’s
name may be spelled “Osterman ”in several instances (which is perhaps the
correct spelling) and “Ostrerman ”in a few instances, which is a misspelling.
Text mining could be used in such situations to identify the right spelling
and suggest it to the entry operator to ensure that data consistency is main-
tained. Similar application logic could be used in identifying and streamlin-
ing call center service data ( McKnight, 2005 ).
Text mining, more than any other technique within data mining, fits the min-
ingmetaphor. Traditionally, mining refers to the process of separating dirt
from valuable metal and in the case of text mining, it is an attempt to sepa-
rate valuable keywords from a mass of other words (or relevant documents282 CHAPTER 9: Text Miningfrom a sea of documents) and use them to identify meaningful patterns or
make predictions.
9.1 HOW IT WORKS
The fundamental step in text mining involves converting text into semi-
structured data. Once the unstructured text is converted into semi-structured
data, there is nothing to stop one from applying any of the analytics techni-
ques to classify, cluster, and predict. The unstructured text needs to be con-
verted into a semi-structured data set so that patterns can be found and even
better, models could be trained to detect patterns in new and unseen text.
The chart in Fig. 9.1 identifies the main steps in this process at a high level.
Each of the main processes will now be examined in detail and some necessary
terminology and concepts will be introduced. But before these processes are
described, a few core ideas essential to text analytics will need to be defined.
9.1.1 Term Frequency /C0Inverse Document Frequency
Consider a web search problem where the user types in some keywords and
the search engine extracts all the documents (essentially, web pages) that
contain these keywords. How does the search engine know which web pages
to serve up? In addition to using network rank or page rank, the search
engine also runs some form of text mining to identify the most relevant web
pages. Suppose for example, that the user types in the following keywords:
“RapidMiner books that describe text mining. ”In this case, the search
engines run on the following basic logic:
1.Give a high weightage to those keywords that are relatively rare.
2.Give a high weightage to those web pages that contain a large number
of instances of the rare keywords.
FIGURE 9.1
A high-level process for text mining.9.1 How It Works 283In this context, what is a rare keyword? Clearly, English words like “that,”
“books, ”“describe, ”and “text”possibly appear in a large number of web
pages, whereas “RapidMiner ”and “mining ”may appear in a relatively smal-
ler number of web pages. (A quick web search returned 7.7 billion results for
the word “books, ”whereas only 584,000 results were returned for
“RapidMiner ”at the time of this writing.) Therefore, these rarer keywords
would receive a higher rating to begin with according to logic 1. Next, among
all those pages that contain the rare keywords, only those pages that contain
the largest number of instances of the rare keywords are likely to be the mostrelevant for the user and will receive high weightage according to logic 2.
Thus, the highest-weighted web pages are the ones for which the product of
these two weights is the highest. Therefore, only those pages that not onlycontain the rare keywords, but also have a high number of instances of the
rare keywords should appear at the top of the search results.
The technique of calculating this weighting is called term TF/C0IDF, which
stands for term frequency /C0inverse document frequency.
Calculating TF is extremely easy: it is simply the ratio of the number of times
a keyword appears in a given document, n
k(where kis the keyword), to the
total number of terms in the document, n:
TF5nk
nð9:1Þ
Considering the mentioned example, a common English word such as “that”
will have a fairly high TF score and a word such as “RapidMiner ”will have a
much lower TF score.
IDF is defined as follows:
IDF5log2N
Nk/C18/C19
ð9:2Þ
where Nis the number of documents under consideration (in a search
engine context, Nis the number of all the indexed web pages). For most text
mining problems, Nis the number of documents that one is trying to mine,
and Nkis the number of documents that contain the keyword, k. Again, a
word such as “that”would arguably appear in every document and, thus, the
ratio ( N/Nk) would be close to 1, and the IDF score would be close to zero
for. However, a word like “RapidMiner ”would possibly appear in a relatively
fewer number of documents and so the ratio ( N/Nk) would be much greater
than 1. Thus, the IDF score would be high for this less common keyword.
Finally, TF /C0IDF is expressed as the simple product as shown:
TF2IDF5nk
n3log2N
Nk/C18/C19
ð9:3Þ284 CHAPTER 9: Text MiningGoing back to the mentioned example, when the high TF for “that”is multi-
plied by its corresponding low IDF, a low (or zero) TF /C0IDF will be reached,
whereas when the low TF for “RapidMiner ”is multiplied by its correspond-
ing fairly high IDF, a relatively higher TF /C0IDF would be obtained.
Typically, TF /C0IDF scores for every word in the set of documents is calculated
in the preprocessing step of the three-step process described earlier.
Performing this calculation will help in applying any of the standard data sci-
ence techniques that have been discussed so far in this book. In the following
sections additional concepts that are commonly employed in text mining
will be described.
9.1.2 Terminology
Consider the following two sentences: “This is a book on data mining ”and
“This book describes data mining and text mining using RapidMiner. ”
Suppose the objective is to perform a comparison between them, or a simi-
larity mapping. For this purpose, each sentence is one unit of text that needs
to be analyzed.
These two sentences could be embedded in an email message, in two sepa-
rate web pages, in two different text files, or else they could be two sentences
in the same text file. In the text mining context, each sentence is considered a
distinct document . Furthermore, in the simplest case, words are separated by a
special character: a blank space. Each word is called a token , and the process
of discretizing words within a document is called tokenization . For the pur-
pose here, each sentence can be considered a separate document, although
what is considered an individual document may depend upon the context.
For now, a document here is simply a sequential collection of tokens.
Document 1 This is a book on data mining
Document 2 This book describes data mining and text mining using RapidMiner
Some form of structure can be imposed on this raw data by creating a matrix
where the columns consist of all the tokens found in the two documents and
the cells of the matrix are the counts of the number of times a token appears,
as shown in Table 9.1 .
Each token is now an attribute in standard data science parlance and each
document is an example. One, therefore, has a structured example set , to use
standard terminology. Basically, unstructured raw data is now transformed
into a format that is recognized, not only by the human users as a data table,
but more importantly by all the machine learning algorithms which require9.1 How It Works 285such tables for training. This table is called a document vector orterm document
matrix (TDM) and is the cornerstone of the preprocessing required for text
mining. Suppose a third statement is added, “RapidMiner is offered as an
open source software program. ”This new document will increase the num-
ber of rows of the matrix by one (Document 3); however, it increases the
number of columns by seven (seven new words or tokens were introduced).
This results in zeroes being recorded in nine other columns for row 3. As
more new statements are added that have little in common, one would end
up with a very sparse matrix.
Note that one could have also chosen to use the term frequencies for each
token instead of simply counting the number of occurrences and it would
still be a sparse matrix. TF can be obtained by dividing each row of Table 9.1
by number of words in the row (document). This is shown in Table 9.2 .1
Similarly, one could have also chosen to use the TF /C0IDF scores for each
term to create the document vector. This is also shown in Fig. 9.2 .
One thing to note in the two sample text documents was the occurrence of
common words such as “a,”“this,”“and,”and other similar terms. Clearly in
larger documents a higher number of such terms would be expected that do
not really convey specific meaning. Most grammatical necessities such as arti-
cles, conjunctions, prepositions, and pronouns may need to be filtered before
additional analysis is performed. Such terms are called stop words and usually
include most articles, conjunctions, pronouns, and prepositions. Stop word fil-
tering is usually the second step that follows immediately after tokenization .
Notice that the document vector has a significantly reduced size after apply-
ing standard English stop word filtering (see Fig. 9.3 ).
In addition to filtering standard stop words, some specific terms might also
need to be filtered out. For example, in analyzing text documents that pertain
to the automotive industry, one may want to filter away terms that are com-
mon to this industry such as “car,”“automobile, ”“vehicle, ”and so on. ThisTable 9.1 Building a Matrix of Terms From Unstructured Raw Text
This is a book on data mining describes text rapidminer and using
Document 1 1 1 1 1 1 1 1 0 0 0 0 0
Document 2 1 0 0 1 0 1 2 1 1 1 1 1
1RapidMiner does a double normalization while calculating TF scores. For example, in case of
Document 1, the TF score for the term “data”would be (0.1498)/ O(0.1498210.149821...10.14982)5
0.1498/ O7*(0.14982)50.3779. Similarly for all other terms, double normalization makes it easier to
apply algorithms such as SVM. This change in TF calculation is reflected in the TF-IDF scores.286 CHAPTER 9: Text MiningTable 9.2 Using Term Frequencies Instead of Term Counts in a TDM
This is a book on data mining describes text rapidminer and using
Document 1 1/7 50.1428 0.1428 0.1428 0.1428 0.1428 0.1428 0.1428 0 0 0 0 0
Document 2 1/10 50.1 0 0 0.1 0 0.1 0.2 0.1 0.1 0.1 0.1 0.1
TDM , Term document matrix.is generally achieved by creating a separate dictionary where these context-
specific terms can be defined and then term filtering can be applied to remove
them from the data. ( Lexical substitution is the process of finding an alterna-
tive for a word in the context of a clause and is used to align all the terms to
the same term based on the field or subject which is being analyzed —this is
especially important in areas with specific jargon, e.g., in clinical settings.)
Words such as “recognized, ”“recognizable, ”or“recognition ”may be
encountered in different usages, but contextually they may all imply the
same meaning. For example, “Einstein is a well-recognized name in physics ”
or“The physicist went by the easily recognizable name of Einstein ”or“Few
other physicists have the kind of name recognition that Einstein has. ”The so-
called root of all these highlighted words is “recognize. ”By reducing terms in
a document to their basic stems, the conversion of unstructured text to struc-
tured data can be simplified because now only the occurrence of the root
terms has to be taken into account. This process is called stemming . The most
common stemming technique for text mining in English is the Porter
method ( Porter, 1980 ). Porter stemming works on a bunch of rules where
the basic idea is to remove and/or replace the suffix of words. For example,
one rule would be: Replace all terms which end in ‘ies ’by ‘y,’such as replacing
the term “anomalies ”with “anomaly. ”Similarly, another rule would be to
stem all terms ending in “s”by removing the “s,”as in “algorithms ”to“algo-
rithm. ”While the Porter stemmer is extremely efficient, it can make mistakes
that could prove costly. For example, “arms ”and “army ”would both be
stemmed to “arm, ”which would result in somewhat different contextual
meanings. There are other stemmers available; which one is chosen is usually
guided by ones experience in various domains. Stemming is usually the next
process step following term filtering. (A word of caution: stemming is
FIGURE 9.2
Calculating TF /C0IDF scores for the sample TDM. TF/C0IDF, Term Frequency /C0Inverse Document Frequency; TDM , term document matrix.
FIGURE 9.3
Stop word filtering reduces the size of the TDM significantly.288 CHAPTER 9: Text Miningcompletely dependent on the human language being processed as well as the
period of the language being processed. Historical usage varies so widely that
comparing text across generations —Shakespeare to present-day literature for
instance —can raise concerns.)
There are families of words in the spoken and written language that typically
go together. For example, the word “Good ”is usually followed by either
“Morning, ”“Afternoon, ”“Evening, ”“Night, ”or in Australia, “Day. ”
Grouping such terms, called n-grams , and analyzing them statistically can
present new insights. Search engines use word n-gram models for a variety of
applications, such as automatic translation, identifying speech patterns,
checking misspelling, entity detection, information extraction, among many
other different uses. Google has processed more than a trillion words
(1,024,908,267,229 words back as far back as 2006) of running text and has
published the counts for all 1,176,470,663 five-word sequences that appear
at least 40 times ( Franz, 2006 ). While most text mining applications do not
require 5-grams, bigrams and trigrams are quite useful. The final preproces-
sing step typically involves forming these n-grams and storing them in the
document vector. Also, most algorithms providing n-grams become computa-
tionally expensive and the results become huge so in practice the amount of
“n”will vary based on the size of the documents and the corpus.
Fig. 9.4 shows a TF-based document vector for bigrams ( n52) from the
examples and as can be seen, terms like “data mining ”and “text mining ”
and “using RapidMiner ”can be quite meaningful in this context. Table 9.3
FIGURE 9.4
Meaningful n-grams show higher TF /C0IDF scores. TF/C0IDF, Term Frequency /C0Inverse Document Frequency.
Table 9.3 A Typical Sequence of PreProcessing Steps to Use in Text Mining
Step Action Result
1 Tokenize Convert each word or term in a document into a distinct
attribute
2 Stop word
removalRemove highly common grammatical tokens/words
3 Filtering Remove other very common tokens
4 Stemming Trim each token to its most essential minimum
5 n-grams Combine commonly occurring token pairs or tuples (more
than 2)9.1 How It Works 289summarizes a typical sequence of preprocessing steps that will convert
unstructured data into a semi-structured format.
Usually there is a preprocessing step before tokenization such as removing
special characters, changing the case (up-casing and down-casing), or some-
times even performing a simple spell check beforehand. Data quality in text
mining is just as important as in other areas.
9.2 HOW TO IMPLEMENT
A few essential concepts have been introduced that would be needed for a
basic text mining project. In the following sections, two case studies will be
examined that apply text mining. In the first example, several documents
(web pages) will be taken and keywords found in them will be grouped into
similar clusters . In the second example, a blog gender classification will be
attempted. To start with several blogs (documents) written by men and
women authors, will be used as training data. Using the article keywords as
features, several classification models will be trained, including a couple of
SVMs, to recognize stylistic characteristics of authors and to classify new
unseen blogs as belonging to one of the two author classes (male or female).
9.2.1 Implementation 1: Keyword Clustering
In this first example, some of the web mining features of RapidMiner will be
introduced and then a clustering model will be created with keywords data
mined from a website. The objective of this case study is to scan several pages
from a given website and identify the most frequent words within these pages
that also serve to characterize each page, and then to identify the most frequent
words using a clustering model. This simple example can be easily extended to
a more comprehensive document-clustering problem where the most common
words occurring in a document would be used as flags to group multiple docu-
ments. The predictive objective of this exercise is to then use the process to
identify any random webpage and determine if the page pertains to one of the
two categories which the model has been trained to identify.
The site ( http://www.detroitperforms.org ) that is being looked into is hosted
by a public television station and is meant to be used as a platform for reach-
ing out to members of the local community who are interested in arts and
culture. The site serves as a medium for the station to not only engage with
community members, but also to eventually aid in targeted marketing cam-
paigns meant to attract donors to public broadcasting. The site has pages for
several related categories: Music, Dance, Theater, Film, and so on. Each of
these pages contains articles and events related to that category. The goal
here is to characterize each page on the site and identify the top keywords290 CHAPTER 9: Text Miningthat appear on each page. To that end, each category page will be crawled,
the content extracted, and the information converted into a structured docu-
ment vector consisting of keywords. Finally, a k-medoids clustering process
will be run to sort the keywords and rank them. Medoid clustering is similar
to the k-means clustering described in Chapter 7, Clustering. A medoid is the
most centrally located object in a cluster ( Park & Jun, 2009 ).k-Medoids are
less susceptible to noise and outliers when compared to k-means. This is
because k-medoids tries to minimize dis-similarities rather than Euclidean
distances, which is what k-means does.
Before beginning with web page clustering in RapidMiner, make sure that the
web mining and text mining extensions are installed. (This is easily done by
going to Help -Updates and Extensions on the main menu bar.) RapidMiner pro-
vides three different ways to crawl and get content from websites. The Crawl
Web operator will allow setting up of simple crawling rules and based on these
rules will store the crawled pages in a directory for further processing. The Get
Page operator retrieves a single page and stores the content as an example set.
TheGet Pages operator works similarly but can access multiple pages identified
by their URLs contained in an input file. The Get Pages operator will be used
in this example. Both of the Get Page(s) operators allow the choosing of either
the GET or POST HTTP request methods for retrieving content.2
Step 1: Gather Unstructured Data
The first step in this process is to create an input text file containing a list of
URLs to be scanned by the Get Pages operator. This is specified in the Read
CSV (renamed in the process shown in Fig. 9.6 toRead URL List ) operator,
which initiates the whole process. The text file consists of three lines: a header
line that is needed for the link attribute parameter for Get Pages and two lines
containing the two URLs that are going to be crawled, as shown in Fig. 9.5 .3
FIGURE 9.5
Creating a URL read list.
2For more information on the differences between the two methods, and when to use which type of
request, refer to the tutorials on www.w3schools.com .
3Be aware that websites may frequently change their structure or content or be taken down altogether.
The results shown here for this example were obtained when the website listed was crawled at the time
of writing. Your results may differ depending upon when the process is executed.9.2 How to Implement 291The first URL is the Dance category page and the second one is the Film cat-
egory page on the website. Save the tex t file as pages.txt as shown in the
figure.
The output from the Get Pages operator consists of an example set that will
contain two main attributes: the URL and extracted HTML content.
Additionally, it also adds some metadata attributes that are not needed in
this example, such as content length (characters), date, and so on. These extra
attributes can be filtered out using the Select Attributes operator.
Step 2: Data Preparation
Next, connect the output from this to a Process Documents from Data operator.
This is a nested operator, which means this operator contains an inner sub-
process where all the preprocessing takes place. The first step in this prepro-
cessing is removing all the HTML tags and only preserving the actual content.
This is enabled by the Extract Content operator. Put this operator inside the
Process Documents from Data operator and connect the different operators as
shown in Figs. 9.6 and 9.7. Refer to Table 9.3 from earlier to see which
operators to use. The inset shows the operators inside the nested Process
Documents from Data operator. In this case, the word occurrences will need to
be used for the clustering. So, select Term Occurrences for the vector creation
parameter option when configuring the Process Documents from Data operator.
FIGURE 9.6
Overall process of creating keyword clustering from websites.292 CHAPTER 9: Text MiningStep 3: Apply Clustering
The output from the Process Documents from Data operator consists of (1) a
word list and (2) a document vector or TDM. The word list is not needed for
clustering; however, the document vector is. Recall that the difference
between the two is that in the document vector, each word is considered an
attribute and each row or example is a separate document (in this case the
web pages crawled). The values in the cells of the document vector can of
course be word occurrences, word frequencies, or TF /C0IDF scores, but as
noted in step 2, in this case the cells will have word occurrences. The output
from the Process Documents from Data operator is filtered further to remove
attributes that are less than 5 (that is all words that occur less than five times
inboth documents). Notice that RapidMiner will only remove those attri-
butes (words) which occur less than five times in both documents —for exam-
ple, the word “dance ”appears only two times in the Film category but is the
most common word in the Dance category; it is not and should not be
removed! Finally, this cleaned output is fed into a k-medoids clustering oper-
ator, which is configured as shown in Fig. 9.8 .
FIGURE 9.7
Configuring the nested preprocessing operator: process documents from data.9.2 How to Implement 293Upon running the process, RapidMiner will crawl the two URLs listed and exe-
cute the different operations to finally generate two clusters. To view these
clustering outputs, select either the Centroid Table or Centroid Plot views in
the Cluster Model (Clustering) results tab, which will clearly show the top key-
words from each of the two pages crawled. In Fig. 9.9 , see the top few key-
words that characterize each cluster. One can then use this model to identify if
the content of any random page would belong to either one of the categories.
9.2.2 Implementation 2: Predicting the Gender of Blog
Authors
The objective of this case study is to attempt to predict the gender of blog
authors based4on the content of the blog (should it be predicted? why? can
it be predicted?).
FIGURE 9.8
Configuring the k-medoids operator.
4A compressed version of this data can be downloaded from the Opinion Mining, Sentiment Analysis,
and Opinion Spam Detection website ( http://www.cs.uic.edu/ Bliub/FBS/sentiment-analysis.html ). The
data set is called Blog Author Gender Classification dataset associated with the paper ( Mukherjee,
2010 ). This site contains a lot of relevant information related to text mining and sentiment analysis, in
addition to several other useful data sets.294 CHAPTER 9: Text MiningStep 1: Gather Unstructured Data
The data set for this case study cons ists of more than 3000 individual blog
entries (articles) by men and women from around the world ( Mukherjee, 2010 ).
The data4is organized into a single spreadsheet consisting of 3227 rows and
two columns as shown in the sample in Table 9.4 . The first column is the
actual blog content and the second column is the author ’s gender, which has
been labeled.
For the purpose of this case study, the raw data will be split into two halves:
the first 50% of the data is treated as training data with known labels and
the remaining 50% is set aside to verify the performance of the training
algorithm.
While developing models involving large amounts of data, which is common
with unstructured text analysis, it is a good practice to divide the process into
several distinct processes and store the intermediate data, models, and results
from each process for recall at a later stage. RapidMiner facilitates this by pro-
viding special operators called Store and Retrieve . The Store operator stores an
input-ouput (IO) Object in the data repository and Retrieve reads an object
from the data repository. The use of these operators is introduced in the
coming sections.
Step 2: Data Preparation
Once the data is downloaded and uncompressed, it yields a single MS
Excel file, which can then be imported i nto the RapidMiner database using
theRead Excel operator. The raw data consists of 290 examples that do not
have a label and one example that h as no blog content but has a label!
This needs to be cleaned up. It is easier to delete this entry in the raw
data—simply delete the row (#1523) in the spreadsheet that contains this
missing entry and save the file before reading it into RapidMiner. Also
0VideoSenesPresentedMichiganBlackHistoryFilm FilmWorksTheaterCommunityMusicCenterArtsDance DanceKeyword Cluster
5 1 01 52 02 53 0
Keyword occurrences35 40 45 50 55 60 65
FIGURE 9.9
Results of the website keyword clustering process.9.2 How to Implement 295make sure that the Read Excel operator is configured to recognize the data
type in the first column as text and not polynominal (default) as shown in
Fig. 9.10 . Connect the output from Read Excel to a Filter Examples operator,
where the entries with missing labels will then be removed. (If inclined,
store these entries with missing labels for use as testing samples —this can
be accomplished with another Filter Examples ,b u tb yc h e c k i n g Invert Filter
box and then storing the output. In this case, however, examples with
missing labels will simply be discarded.) The cleaned data can now be sep-
arated with a 50/50 split using a Split Data operator. Save the latter 50%Table 9.4 Raw Data for the Blog Classification Study
Blog Gender
This game was a blast. You (as Drake) start the game waking up in a train that is dangling over the side of
a cliff. You have to climb up the train car, which is slowly teetering off the edge of the cliff, ready to
plummet miles down into a snowy abyss. From the snowy beginning there are flashbacks to what led
Drake to this predicament. The story unfolds in a very cinematic manner, and the scenes in between
levels, while a bit clichéd by Hollywood standards, are still just as good if not better than your average
brainless Mel Gibson or Bruce Willis action movie. In fact, the cheese is part of the fun and I would venture
to say it's intentionalM
My mother was a contrarian, she was. For instance, she always wore orange on St. Patrick's Day,
something that I of course did not understand at the time, nor, come to think of it do I understand today.
Protestants wear orange in Ireland, not here, but I'm pretty sure my mother had nothing against the
Catholics, so why did she do it? Maybe it had to do with the myth about Patrick driving the snakes, a.k.a.
pagans, out of Ireland. Or maybe it was something political. I have no idea and since my mother is long
gone from this earth, I guess I'll never knowF
LaLicious Sugar Soufflé body scrub has a devote d following and I now understand why. I received a
sample of this body scrub in Tahitian Flower and after one shower with this tub of sugary goodness, I
was hooked. The lush scent is deliciously intoxicating and it ended up inspiring compliments and
extended sniffing from both loved ones and stranger s alike. Furthermore, this scrub packs one heck of a
punch when it comes to pampering dry skin. In fact, L aLicious promises that this body scrub is so rich
that it will eliminate the need for applying your post-shower lotion. This claim is true —if you follow the
directionsF
Stopped by the post office this morning to pick up a package on my way to the lab. I thought it would be
as good a time as any to clean up my desk and at the very least make it appear that I am more organized
than I really am (seriously, it's a mess). It's pretty nice here on the weekends, it's quiet, there's less worry
of disturbing undergrad classes if I do any experiments in the daytimeM
Anyway, it turns out the T-shirt I ordered from Concrete Rocket arrived! Here's how the design looks
See here's the thing: Men have their neat little boxes through which they compartmentalize their lives.
Relationship over? Oh, I'll just close that box. It's not that easy for women. Our relationships are not just a
section of our lives —they run through the entire fabric, a hot pink thread which adds to the mosaic
composing who we are. Take out a relationship and you grab that thread and pull. Have you ever pulled a
thread on a knit sweater? That's what it's like. The whole garment gets scrunched and disfigured just
because that one piece was removed. And then you have to pull it back apart, smooth it out, fill in the
gaps. See here's the thing: men have their neat little boxes through which they compartmentalize their
lives. Relationship over? Oh, I'll just close that box. It's not that easy for womenF296 CHAPTER 9: Text Miningtesting data (1468 samples) to a new file with a Write Excel operator and
pass the remaining 50% training portion to a Process Documents from Data
operator.
This is a nested operator where all the preprocessing happens. Recall that this
is where the conversion of unstructured data into a structured format will
take place. Connect the different operators within as shown in Fig. 9.13 . The
only point to note here is that a Filter Stop word (Dictionary) operator will be
needed to remove any “nbsp ”(“&nbsp ”is used to represent a nonbreaking
space) terms that may have slipped into the content. Create a simple text file
with this keyword inside it and let RapidMiner know that this dictionary
exists by properly configuring the operator. To configure the Process
Documents from Data operator, use the options as shown in Fig. 9.11 .
The output from the process for step 2 consists of the document vector and a
word list. While the word list may not be of immediate use in the subse-
quent steps, it is a good idea to store this along with the very important doc-
ument vector. The final process is shown in Fig. 9.12 .
Step 3.1: Identify Key Features
The document vector that is the result of the process in step 2 is a structured
table consisting of 2055 rows —one for every blog entry in the training set —
and 2815 attributes or columns —each token within an article that meets the
filtering and stemming criteria defined by operators inside Process Documents
is converted into an attribute. Training learning algorithms using 2815 fea-
tures or variables is clearly an onerous task. The right approach is to further
filter these attributes by using feature selection methods.
FIGURE 9.10
Properly configuring the Read Excel operator to accept text (not polynomial).9.2 How to Implement 297Two feature selection methods will be employed using the Weight by
Information Gain and Weight by SVM operators that are available. Weight by
Information Gain (more details in Chapter 14: Feature Selection, on this oper-
ator) will rank a feature or attribute by its relevance to the label attribute (in
this case, gender) based on the information gain ratio and assign weights to
them accordingly. Weight by SVM will set the coefficients of the SVM hyper-
plane as attribute weights. Once they are ranked using these techniques, only
a handful of attributes can be selected (e.g., the top 20) to build the models.
Doing so will result in a reasonable reduction in modeling costs.
The results of this intermediate process will generate two weight tables, one
corresponding to each feature selection method. The process is started by
retrieving the document vector saved in step 2 and then the process is ended
by storing the weight tables for use in step 3.2 (see Fig. 9.14 ).
In the paper by Mukherjee and Liu ( Mukherjee, 2010 ) from which this data
set comes, they demonstrate the use of several other feature selection meth-
ods, including a novel one developed by the authors that is shown to yield a
much higher prediction accuracy than the stock algorithms (such as the ones
demonstrated here).
Step 3.2: Build Models
With the document vector and attribute weights now ready, one can experi-
ment using several different machine learning algorithms to understand
FIGURE 9.11
Configuring the preprocessing operator.298 CHAPTER 9: Text Miningwhich gives the best accuracy. The process illustrated in Figs. 9.15 and 9.16
will generate the models and store them (along with the corresponding per-
formance results) for later application. This is a key strength of RapidMiner:
once one has built up the necessary data for predictive modeling, switching
back and forth between various algorithms requires nothing more than drag-
ging and dropping the needed operators and making the connections.
As seen in Fig. 9.16 , there are four different algorithms nested inside the
X-Validation operator and can conveniently be switched back and forth as
needed. Table 9.5 shows that the LibSVM (linear) and W-Logistic operators
(Available through Weka extension for RapidMiner) seem to give the best
performance. Keep in mind that these accuracies are still not the highest and
are in line with the performances reported by Mukherjee and Liu in their
paper for generic algorithms.
FIGURE 9.12
Overall process for blog gender classification.9.2 How to Implement 299FIGURE 9.14
Using feature selection methods to filter attributes from the TDM. TDM , Term Document Matrix.
FIGURE 9.13
Preprocessing text data using the process documents from data operator.300 CHAPTER 9: Text MiningFIGURE 9.15
Training and testing predictive models for blog gender classification.
FIGURE 9.16
Switching between several algorithms.9.2 How to Implement 301To improve upon these, we may need to further optimize the best performers
so far by nesting the entire validation process within an optimization opera-
tor. This is described in the Chapter 15: Getting started with RapidMiner, in
the section on optimization.
Step 4.1: Prepare Test Data for Model Application
Going back to the original 50% of the unseen data that was saved for testing
purposes, the real-world performance of the best algorithm can actually be
evaluated in classifying blogs by author gender. However, keep in mind that
the raw data that was set aside as is, cannot be used (what would happen if
one did?). This raw test data would also need to be converted into a docu-
ment vector first. In other words, the step 2 process needs to be repeated
(without the filtering and split data operators) on the 50% of the data that
were set aside for testing. The Process Documents from Data operator can be
simply copied and pasted from the process in step 2. (Alternatively, the
entire data set could have been preprocessed before splitting!) The document
vector is stored for use in the next step. This process is illustrated in Fig. 9.17 .
Step 4.2: Applying the Trained Models to Testing Data
This is where the rubber hits the road! The last step will take any of the saved
models created in step 3.2 and the newly created document vector from stepTable 9.5 Comparing the Performance of Different Training Algorithms for Blog Gender
Classification
Algorithm Class Recall (M) Class Recall (F) Accuracy
LibSVM (linear) 87 53 72
W-Logistic 85 58 73
Naïve Bayes 86 55 72
SVM (polynomial) 82 42 63
FIGURE 9.17
Preparing the unseen data for model deployment.302 CHAPTER 9: Text Mining4.1 and apply the model on this test data. The process is shown in Figs. 9.18
and9.19. One useful operator to add is the Set Role operator, which will be
used to indicate to RapidMiner the label variable. Doing so will allow the
results to be sorted from the Apply Model by Correct Predictions and Wrong
Predictions using the View Filter in the Results perspective as shown here.
When this process is run, it can be observed that the LibSVM (linear) model
can correctly predict only 828 of the 1468 examples, which translates to a
poor 56% accuracy! The other models fare worse. Clearly the model and the
process are in need of optimization and further refinement. Using
RapidMiner ’s optimization operators, one can easily improve on this baseline
accuracy. A discussion about how to use the optimization operators in general
is provided in Chapter 15, Getting started with RapidMiner. The truly adven-
turous can implement the Mukherjee and Liu algorithm for feature selection in
RapidMiner based on the instructions given in their paper! Although this
implementation did not return a stellar predictive performance (and the moti-
vation of this classification is digital advertisement, as specified in the paper),
a word of caution is in order: algorithms have become increasingly more pow-
erful and reckless application of machine learning may lead to undesirable
consequences of discrimination (via gender, in this instance). Data scientists
are responsible for ensuring that their products are used in an ethical and
non-discriminatory manner.
FIGURE 9.18
Applying the models built in step 3 on the unseen data.9.2 How to Implement 303Bias in Machine Learning
Data Science is a powerful tool to extract value from data. Just like any other
tool, it can be put to good use, inappropriate use, malicious use or use it in
such a way that yield unintended consequences. Recently, a data science
model that was developed to filter and sort the resumes of job applicants
started to discriminate against women ( Gershgorn, 2018 ). The data science
modeling process would have started as a solution to a right business prob-
lem, which is to manage the influx of resumes and sorting the most relevant
ones to the top. In doing so, the text mining model favoured one applicant
class, leveraging some spurious pattern in the data. If the training data is
biased, the machine learning model will be biased. Specifically, if the train-
ing data is derived from a biased process, the machine learning automation
just amplifies the phenomenon. A loan approval model might not ask for
the race of the applicant (which would be unethical and illegal). However,
the location of an applicant might serve as a proxy for the race. It is impor-
tant to test and audit if the model provides fair prediction for all the classes
of users. These machine learning models have real world consequences. The
responsibility lies with the data scientist to build a transparent model adher-
ing to ethical principles. Creating robust tests to check for potential unfair-
ness in the models is imperative to gain trust in the discipline of data science
(Loukides et al., 2018 ).
9.3 CONCLUSION
Unstructured data, of which text data is a major portion, appears to be dou-
bling in volume every three years ( Mayer-Schonberger, 2013 ). The ability to
FIGURE 9.19
The results view.304 CHAPTER 9: Text Miningautomatically process and mine information from such digital data will
become an important skill in the future. These techniques can be used to
classify and predict just as the other techniques throughout the book, except
one is now working on text documents and even voice recordings that have
been transcribed to text.
This chapter described how unstructured data can be mined using any of the
available algorithms presented in this book. The key to being able to apply
these techniques is to convert the unstructured data into a semi-structured for-
mat. A high-level three-step process that will enable this was introduced. Some
key tools for transforming unstructured data, such as tokenization, stemming,
n-gramming, and stop word removal were discussed. How concepts such as
TF/C0IDF allow us to make the final transformation of a corpus of text to a
matrix of numbers, that can be worked on by the standard machine learning
algorithms was explained. Finally, a couple of implementation examples were
presented, which will allow one to explore the exciting world of text mining.
References
Cutting, D. K. (1992). Scatter/gather: A cluster-based approach to browsing large document col-
lections. In: Copenhagen proceedings of the 15th annual international ACM SIGIR conference on
research and development in information retrieval (pp. 318 /C0329). Copenhagen.
Franz, A. A. (2006, August 3). All our N-gram are belong to you . Research blog. Retrieved from ,http://
googleresearch.blogspot.com/2006/08/al l-our-n-gram-are-belong-to-you.html .Accessed
01.11.13.
Gershgorn, D. (2018). Companies are on the hook if their hiring algorithms are biased /C0
Quartz. Retrieved October 24, 2018, from ,https://qz.com/1427621/companies-are-on-the-
hook-if-their-hiring-algorithms-are-biased/ ..
Hearst, M. (1999, June 20 /C026). Untangling text data mining. In: Proceedings of Association for
Computational Linguistics, 37th annual meeting 1999 . University of Maryland.
L o u k i d e s ,M . ,M a s o n ,H . ,&P a t i l ,D .J .( 2 0 1 8 ) . Ethics and Data Science. Sebastopol. CA: O ’Reilly Media.
Mayer-Schonberger, V. A. (2013). Big data: A revolution that will transform how we live, work and
think . London: John Murray and Co.
McKnight, W. (2005, January 1). Text data mining in business intelligence . Information
Management. Retrieved from ,http://www.information-management.com/issues/20050101/
1016487-1.html#Login .Accessed 01.11.13.
Mukherjee, A. L. (2010). Improving gender classification of blog authors. In: Proceedings of confer-
ence on Empirical Methods in Natural Language Processing (EMNLP-10) . Cambridge, MA.
Park, H. S., & Jun, C. H. (2009). A simple and fast algorithm for K-medoids clustering. Expert
Systems with Applications ,36(2), 3336 /C03341.
Porter, M. F. (1980). An algorithm for suffix stripping. Program ,14(3), 130 /C0137.
Siegel, E. (2013). Predictive analytics: The power to predict who will click, buy, lie or die . Hoboken, NJ:
John Wiley and Sons.
Upbin, B. (2013, February 8). IBM ’s Watson gets its first piece of business in healthcare. Forbes .
Retrieved from ,https://www.forbes.com/sites/bruceupbin/2013/02/08/ibms-watson-gets-
its-first-piece-of-business-in-healthcare/#7010113b5402/ ..References 305CHAPTER 10
Deep Learning
To get a deep-learning system to recognize a hot dog, you might have to
feed it 40 million pictures of hot dogs. To get [a two year old] to recognize a
hot dog, you show her a hot dog.1
The newest and shiniest member of the data science algorithm toolbox
is deep learning. Today deep learning has captured the imagination of
scientists, business leaders, and lay people. A local bank teller may not
understand what deep learning is but talk to her about artificial intelligence
(AI) —which is the ubiquitous avatar of deep learning and it would be
surprising to learn how much she may have heard about it. The mentioned
quote humorously captures the essence of what makes today ’s AI methods.
Deep learning is a vast and rapidly emerging field of knowledge and requires
a book on its own merit considering the wide-ranging architectures and
implementation details it encompasses.
This chapter aims to provide an intuitive understanding of this complex
topic. The hope is that this would establish a solid framework for a much
more sophisticated understanding of the subject. Firstly, what constitutes
the core of deep learning will be discussed and in order to do that a little
computing history will be covered. The similarities between deep learning
and familiar algorithms like regression will then be discussed and how
deep learning is an extension of regression and artificial neural networks
encountered in Chapter 4, Classification will be demonstrated. The essential
differences between “traditional ”algorithms like multiple linear regression,
artificial neural networks and deep learning will, be pointed out by introduc-
ing the core concepts of deep learning that set it apart. One type of deep
learning technique will then be explored in sufficient detail and implementa-
tion information will be provided to make this knowledge applicable to real
life problems. Finally, a quick overview of some of the other newly emerging
1https://www.technologyreview.com/s/608911/is-ai-riding-a-one-trick-pony/ .
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00010-1
©2019 Elsevier Inc. All rights reserved.307BRINGING ARTIFICIAL INTELLIGENCE TO ENGINEERING
Computer-aided engineering (CAE) is a mainstay of analyt-
ical methods used by engineers ( Fig. 10.1 ). CAE works by
solving higher order partial differential equations to pre-
dict how engineered products respond to service loads
that are put on them. This helps engineers design shapes
and select materials for different components. For exam-
ple, how does an F-15 wing behave at supersonic speeds?
How much energy does the front bumper of a car absorb
in a crash event?.
So how can AI help in such sophisticated activities? To
understand this, one needs to dissect CAE its constituent
steps. CAE is not a monolithic endeavor, but involves
interaction between designers, engineers, and oftentimes
computer systems people. The core activities include:
creating geometric models of products, subdividing the
geometries into discrete “finite elements ”on which the
laws of physics are applied, converting these physical laws
into mathematical equations or formulations, solving these
formulations, visualizing the solutions on the original geo-
metric models (using intuitive heat maps, such as the one
shown in Fig. 10.1 .2). This entire process is not only time
consuming, but also requires deep domain knowledge in
graphics, mechanics, math, and high-performance
computing.
All this would suggest that today ’s state-of-the-art AI
could probably not help a whole lot. After all, from what isheard on the popular press AI is mostly chat-bots and
identifying cats on the internet! However, that would be a
superficial assessment. There are many smaller steps in
each of the mentioned processes which are prime candi-
dates for some of the techniques that will be learnt in this
chapter.
As an illustration, we can expand on one of these tasks:
subdivide the geometries in CAE. This is a crucial step and
poor handling of this step will compromise all of the
downstream activities. Unfortunately, it is also highly
dependent on the skill of the designer who actually works
on the discretization. Luckily CAE process engineers have
developed many “expert ”rules which even an entry level
designer can use to make sure that the end product of
their activities will meet rigorous standards. One expert
rule is something like this: “if a part looks like a rubber
gasket, use method A to discretize the geometry; if it
looks like an aluminum fastener, then use method B. ”
One can imagine in a modern-day car, for example, that
there are hundreds of such similar looking parts. It is a
time-consuming task —to go through an entire vehicle to
correctly identify components and apply the right discreti-
zation method. It would be real value added if an AI could
take over the determination of what “a part looks like ”
before letting an automated process apply the proper
method to discretize. In a way this is essentially an appli-
cation of the “cat identification ”AI in a new —and more
(Continued )
FIGURE 10.1
Computer-aided engineering.308 CHAPTER 10: Deep Learningtechniques will be provided, which are now considered as part of the deep
learning repertoire of techniques.
10.1 THE AI WINTER
The first ANN were the Perceptrons developed in the 1950s which were a
class of pattern recognition elements that weighed evidence and tested if it
exceeded a certain threshold in order to make a decision, that is, to classify
patterns. Fig. 10.2 shows the architecture of a single perceptron (later on
called neuron) which has retained its basic structure through the years.
Each input, xihas a weight wi, associated with it and a dot product Σwixiis
computed at the perceptron and passed to the activation function g.I f g
(Σwixi)evaluates above a threshold then the output is set to 1 (true) or(Continued )
serious domain. But that is not the only task which AI can
turbocharge. As will be covered in this chapter, neural
networks are at the core of deep learning. But what does
a neural network really do? ANN ’s establish a mapping
between the behavior of a complicated system and the
environment it functions in. For instance, they help create
a mapping between the behavior of a customer (churn vs
stay) and the customer ’s shopping and purchasing habits.
Or they create a mapping between the nature of a trans-
action (fraud vs legitimate) and the characteristics of the
transactional environment (location, amount, frequency,
and so on).
Another challenging task in CAE is to determine the per-
formance of a system given highly variable environmental
conditions. Will the fuselage require maintenance after
10,000 hours of flying or 1000 hours? Will an airbag deploy
as intended by design, in tropical climates? Will the reduc-
tion in gage thickness still meet the crash safety require-
ments? Today CAE helps to answer these questions by
utilizing physics based computational models to make
predictions —this is the mathematical formulation and
solution phase identified earlier. Such computations can
be costly (in CPU time) and cumbersome (model develop-
ment time) so that minor changes in design cannot be
quickly studied. Deep learning can help speed these up by
creating mappings between the product ’s design elements
and its final on-field performance provided sufficient data isgenerated (a one-time process) to train a model initially.
This idea is not new —classical CAE can be used to develop
what are called response surfaces to help with this. The
objective of a response surface is to effectively map the
design space and then attempt to find an optimum. But a
crucial problem with response surfaces was that highly
nonlinear or discontinuous behavior that physical systems
often exhibit would make it impossible to find optima using
conventional mathematical techniques and, thus, would
reduce response surfaces to mere toys. In an engineering
setting, the independent variable could represent a geo-
metric design feature such as the gage thickness of a
metal part whereas the dependent variable could represent
a performance metric such as energy absorption.3If the
data is linearly separable it can be handled by many of the
traditional classification algorithm we encountered earlier.
However, complex physical systems rarely exhibit such
behavior. Classifying (or mapping) such responses is not
possible without ANNs and more specifically without
“deep ”neural networks. As will be seen in this chapter, a
key strength of deep learning networks is in generating
nonlinear mappings.
2https://commons.wikimedia.org/wiki/File:
Static_Structural_Analysis_of_a_Gripper_Arm.jpg .
3For an actual example, see Fig. 3 here: https://pdfs.semanticscholar.
org/2f26/c851cab16ee20925c4e556eff5198d92ef3c.pdf .10.1 The AI Winter 309otherwise to 0 (false). The process of obtaining the weights, wi, is called
“learning ”or“training ”the perceptron. The perceptron learning rule was origi-
nally developed by Frank Rosenblatt (1957). Training data are presented to
the network ’s inputs and the output is computed. The weights wiare modi-
fied by an amount that is proportional to the product of the difference
between the actual output, y, and the desired output ,d, and the inputs, xi.
The perceptron learning rule is basically:
1Initialize the weights and threshold to small random numbers.
2Feed the inputs xito the perceptron and calculate the output.
3Update the weights according to: wi(t11)5wi(t)1η(d2y)x
Where:
dis the desired output,
tis the time step, and
ηis the learning rate, where 0.0 ,η,1.0
4Repeat steps 2 and 3 until:
a.the iteration error is less than a user-specified error threshold or
b.a predetermined number of iterations have been completed.
Note that learning only happens when the error is above a preset threshold,
otherwise the weights are not updated. Also, every time the weights need an
update, reading the input data (i.e., step 2) is required.
AI Winter: 1970 ’s
Perceptrons were able to solve a range of decision problems, in particular
they were able to represent logic gates such as “AND ”,“OR,”and “NOT. ”
The perceptron learning rule tended to converge to an optimal set of weights
for several classes of input patterns. However, this was not always guaran-
teed. Another limitation arose when the data were not linearly separable —
for example, the classic “XOR. ”A XOR gate resolves to “true”if the twoInputs, xiActivation 
function, g()
Output, y
Summation of 
inputs
FIGURE 10.2
Conceptual architecture of a perceptron.310 CHAPTER 10: Deep Learninginputs are different and resolves to “false ”if both inputs are the same
(Fig. 10.3 ). Minsky and Papert published these and other core limitations of
perceptrons in a 1969 book called Perceptrons , which arguably reduced fur-
ther interest in these types of ANN and the so-called AI Winter had set in.
Mid-Winter Thaw of the 1980s
ANN, however, had a brief resurgence in the 1980s with the development of
the multi-layer perceptron (MLP) which was heralded as the solution for
nonlinearly separable functions: for example, changing the activation
function in an MLP from a linear step function to a nonlinear type (such as
sigmoid) could overcome the decision boundary problem seen in the
XOR case.
Fig. 10.4 shows that with a linear activation function a two-layer MLP still
fails to achieve more than 50% accuracy on the XOR problem using
TensorFlow4playground. However, a simple switch of the activation func-
tion to the nonlinear “sigmoid ”helps achieve more than 80% accuracy with
the same architecture ( Fig. 10.5 ).
Another important innovation in the 1980s that was able to overcome some
of the limitations of the perceptron training rule was the use of “backpropaga-
tion”to calculate or update the weights (rather than reverting back to the
FIGURE 10.3
RapidMiner XOR example.
4playground.tensorflow.org.10.1 The AI Winter 311FIGURE 10.4
Exploring network architectures using TensorFlow playground.
FIGURE 10.5
Modifying network architecture to solve the XOR problem using Tensorflow playground.312 CHAPTER 10: Deep Learninginputs every time there was an error —step 2 of the perceptron learning rule).
The perceptron learning rule updated the weights by an amount that was
proportional to the error times of the inputs (the quantity η(d2y)xin step 2).
These weights wiare the heart of the network and training an multi-layer
perceptron (MLP) or ANN is really all about finding these weights. Finding a
robust and repeatable process for updating the weights becomes critical. To do
this an extra layer of neurons were added in between the input and the output
nodes. Now the error quantity ( d2y) becomes a summation and to avoid
sign bias, the error may be squared, that is ,Σ(dj2yj)2. The challenge now was
determining which direction to change the weights, wiso that this error quan-
tity is minimized. The algorithm now involved these steps:
1.Computing the output vector given the inputs and a random selection
of weights in a “forward ”computational flow.
2.Computing the error quantity.
3.Updating the weights to reduce this error at the output layer.
4.Repeat two and three for the hidden layer going backward.
This backpropagation method was introduced by Rumelhart, Hinton, and
Williams (1986) .5Their network was trainable to detect mirror symmetry; to
predict one word in a triplet when two of the words were given and other
such basic applications. More sophisticated ANNs were built using backpro-
pagation, that could be trained to read handwriting ( LeCun, 1989 ).6
However, successful business applications of ANN were still limited and it
failed to capture the public imagination the way it has currently.
Part of the reason was the state of computing hardware at the time when these
algorithms were introduced. But one can argue that a bigger hurdle preventing
a wider adoption back in the 1980s and 1990s was a lack of data. Many of
the machine learning algorithms were developed and successfully demon-
strated during this time: Hidden Markov Models and Convolutional Neural
Nets were described in 1984 and 1989 respectively. However, a successful
deployment of these algorithms on a practical business scale did not occur
until nearly a decade later. Data (or lack thereof) was the primary reason for
this. Data became more readily available and accessible only after the intro-
duction of the internet in 1993. Wissner-Gross (2016) cites several interesting
examples of breakthroughs in AI algorithms, effectively concluding that the
average time period for an AI innovation to become practical was 18 years
(after the introduction of the algorithm) but only 3 years after the first large
scale datasets (that could be used to train that algorithm) became available.7
5https://www.iro.umontreal.ca/ Bvincentp/ift3395/lectures/backprop_old.pdf .
6http://yann.lecun.com/exdb/publis/pdf/lecun-89.pdf .
7http://www.spacemachine.net/views/2016/3/datasets-over-algorithms .10.1 The AI Winter 313This brings us to the defining moment in the short but rapidly evolving
history of deep learning. In 2006, Hinton (the same Hinton who was part of
the team that introduced backpropagation) and Salakhutdinov, demon-
strated that by adding more layers of computation to make neural networks
“deep, ”larger datasets could be better utilized to solve problems such as
handwriting recognition ( Hinton and Salakhutdinov, 2006 ). These 3-hidden
layer deep learning networks had significantly lower errors than the tradi-
tional single hidden layer ANNs that were discussed in Chapter 5, Regression
Methods. With this innovation, the field of AI emerged from its long winter.
This emergence is best summarized in the authors ’own words8:
It has been obvious since the 1980s that backpropagation through deep
autoencoders would be very effective ...provided that computers were fast
enough, data sets were big enough, and the initial weights were close
enough to a good solution. All three conditions are now satisfied.
Using massive datasets, deep network architectures with new and powerful gra-
phics processing units (GPUs) originally developed for video games, real-world
AI applications such as facial recognition, speech processing and generation,
machines defeating humans at their own board games have become possible.
ANNs had moved decisively from the research lab to mainstream media hype.
The Spring and Summer of Artificial Intelligence:
2006—Today
In spite of all these exciting developments, today ’s AI is still far from being
what is considered artificial general intelligence (AGI). The quote at the
beginning of the chapter summarizes the main aspect of today ’s AI: the need
for massive amounts of data to train a machine to recognize concepts which
seem simple even to a 2-year-old human brain. There are two main ques-
tions one would have at this point: How far away from AGI is AI today?
What are the hurdles that stand in the way?
Defense Advanced Research Projects Agency (DARPA) has developed a nice
classification9of the evolution of AI into three “waves ”based on the main
dimensions which reflect the capabilities of the systems: ability to learn, abil-
ity to abstract, and ability to reason.10
8https://www.cs.toronto.edu/ Bhinton/science.pdf .
9https://www.darpa.mil/attachments/AIFull.pdf .
10DARPA also lists a 4th dimension: ability to perceive. However this dimension is very close to
“ability to learn ”. According to Webster, the difference between ‘learn ’and ‘perceive ’is tied to the
senses. Learning is defined as understanding by study or experience, perception is defined as
understanding or awareness through senses. For our purposes, we ignored these subtle differences. ”314 CHAPTER 10: Deep LearningThe first wave of AI evolution includes “handcrafted knowledge ”systems.
These are the expert systems and chess playing programs of the 1980s and
1990s. Humans encode into the machines an ability to make decisions based
on input data from a specific domain. In other words, these systems have a
limited ability to reason but no ability to learn let alone abstract.
Second wave systems include today ’s machine learning and deep learning
systems and are generally terms systems capable of “statistical learning. ”The
uniqueness of these systems is the ability to separate data into different sets
or patterns based on learning by relying on large volumes of data. While a
rule engine can be added to these statistical learners, these systems still lack
the ability to abstract knowledge. To clarify this: consider that while a facial
recognition system is successful at identifying faces, it cannot explicitly
explain why a particular face was categorized as such. On the other hand, a
human can explain that a particular face was classified as a man because of
the facial hair and body dimensions, for example.
In the yet-to-be developed third wave systems, AI cannot only apply encoded
rules and learn from data, but can also explain why a particular data point
was classified in a particular way. This is termed a “contextually adaptive ”
system. DARPA also calls these systems “Explainable AI ”or XAI11that
“produce more explainable models, while maintaining a high level of learning perfor-
mance (prediction accuracy). ”A first step toward developing XAI is the integra-
tion of the now conventional deep machine learning with reinforcement
learning (RL), which is introduced later in this chapter.
To conclude this section, before the technical brass-tacks of deep learning are
explained, it is helpful to acknowledge/recognize these facts:
GMajority of AI today is machine learning and deep learning.
GMajority of that learning is supervised.
GMajority of that supervised learning is classification.
10.2 HOW IT WORKS
In this section, the connection between conventional machine learning and
deep learning will be further discussed. Firstly, linear regression and how it
can be represented using an ANN will be examined more closely and then
logistic regression will be discussed to reinforce the similarities between con-
ventional machine learning techniques and deep learning. This will serve as
an entry point to introduce some of the fundamental concepts in ANN and
by extension deep learning. This in-depth understanding is essential to
11https://www.darpa.mil/program/explainable-artificial-intelligence .10.2 How It Works 315confidently navigate some of the more sophisticated and specialized deep
learning techniques that will come later.
To begin with, ANNs (and deep learning) should be regarded as a mathemati-
cal process. An ANN basically creates a mapping of data between outputs and
inputs that is established using calculus-based optimization techniques. In a
simplified mathematical format, an ANN can be essentially represented as:
Y5fðXÞ
Y5fðgðbÞÞwhere X5gðbÞð10:1Þ
where b,Xand Yare vectors or more generally, tensors. In this context,
vectors are one dimensional array of numbers, matricies are two dimensional
arrays and tensors are more general n-dimensional arrays. The process
of training ANN is mostly about finding values for the coefficients, b,t o
complete the mapping. The coefficients are calculated by performing a
constrained optimization of an error function (error is the difference between
a predicted output y0and the known output, y). A technique to iteratively
perform this optimization is called backpropagation which will be discussed
in this section. A basic difference between deep learning and “shallow ”
machine learning is in the count of the coefficients ,b. Deep learning deals
with weight or coefficient counts in the hundreds of thousands to millions
whereas conventional machine learning may deal with a few hundred at best.
This enhancement of the numerosity in the computation gives deep learning
their significant power to detect patterns within data.
10.2.1 Regression Models As Neural Networks
Eq. (10.1) is a vectorized form of Eq. (5.1) which was the general statement
of a multiple linear regression problem. Section 5.1.1 discussed how a
linear regression problem would be solved using methods of calculus, in
particular, gradient descent. The gradient descent technique is the corner-
stone of all deep learning algorithms and it is advisable to revisit
Section 5.1.1 at this time to become comfortable with it. The linear regres-
sion model of Section 5.1.1 can be rewritten as an ANN: Fig. 10.6 is the
simple linear regression model shown as a network. Note that when x051,
this captures Eq. (5.2). This network is only two layers deep: it has an input
layer and an output layer. Multiple regression models simply require addi-
tional nodes (one node for each variable/feature) in the input layer and no
additional/intermediate layers.
Similarly, a logistic regression model can also be represented by a simple
two-layer network model with one key difference. As was discussed in
Section 5.2.2 on Logistic Regression, the output of logistic regression is the
probability of an event, prather than a real number value as in linear316 CHAPTER 10: Deep Learningregression. So, what needed to be done was transform the output variable in
such a way that its domain now ranges from 0 to 1 instead of 2N/C0to1N.
It was observed that by replacing the right-hand side expression of 5.6 with
the log of the odds-ratio or the logit, this transformation was achieved.12
logp
12p/C18/C19
5b01b1x1 ð10:2Þ
.p51
11e2z/C18/C19
where z5b01b1x1after rearranging the terms ð10:3aÞ
more generally, the output is summarized as:
pyðÞ5σzðÞ ð 10:3bÞ
So, the output node in the above network could be rewritten as shown in
Fig. 10.7 .
Eqs. (10.3a) and (10.3b) represent the sigmoid function. The sigmoid ’s
domain is [0,1] for zE5[2N,N] so that any arbitrary values for band x
will always result in p(yn)5[0,1] .Note that p(yn)is the prediction from the
logistic regression model for sample n, which needs to be compared with the
actual class value, pnfor that sample, in order to evaluate the model. How
would one quantitatively compare these two across all data samples? Recall
that in linear regression squared error Σ(yn2yn0)2was used. The binary
nature of pnrequires that the error be maximum when the predicted p(yn)
and actual pnare opposite and vice versa.
10.2.2 Gradient Descent
In Section 5.2.2 on Logistic Regression, an error function or a cost function
was introduced, which can now be generalized by taking a log on the terms
so that a summation is obtained instead of a product when one needs toyx0
x1b1b0
FIGURE 10.6
Regression depicted as a network.
12Note that we are shortening probability of y, that is p(y) topfor ease of notation in this line.10.2 How It Works 317compute across all samples. Note that ynis the calculated value of probability
based on the model and can range between [0 /C01] and pnis the target which
is either 0 or 1. Nis the number of samples.
J52XN
n51pnlogðynÞ1ð12pnÞlogð12ynÞ/C2/C3
ð10:4Þ
Jis called the cross-entropy cost function. I nt h es p i r i to fc o n s i d e r i n gt h i sa sa
cost function, a negative sign is added in front and with the aim of
minimizing the value. Thus, b0s need to be found which minimizes this
function.13This has been done before using calculus in the case of linear
regression (Section 5.1.1). It is easy to use the chain rule of differentiation
to compute the derivative. But as will be seen this will turn out to be a con-
stant and, thus, bcannot be solved for by setting it equal to 0. Instead, once
an initial slope is obtained, gradient d escent will be used to iteratively find
the location where it is minimum.
Note that the cross-entropy cost function is easily expressed in terms of
weights b, by substituting:
y5σzðÞ51
11e2z ðÞwhere z5b0x01b1x1
The weights, b, can now be found by minimizing J, expressed in terms of b
by using the chain rule of differentiation and setting this derivative to 0:
dJ
db50
.dJ
dy3dy
dz3dz
db50 ð10:5Þσ(z)x0
x1b1b0
FIGURE 10.7
Adding an activation function to the regression “network. ”
13Why can the same cost function not be used that was used for linear regression? That is,1/23(pn2
yn)2? It turns out that this function is not a “convex ”function —in other words does not necessarily
have a single global optimum.318 CHAPTER 10: Deep LearningCalculate each derivative listed in 10.5 with these three steps:
Step I.dJ
dy5pn
yn/C16/C17
212pn
12yn/C16/C17
using Eq. (10.3a) and (10.3b)
Step II. y51/11e2z
.dy
dz52e2z
ð11e2zÞ
with proper substitution and rearrangement of terms, the right side of the
derivative can be reduced to:
.dy
dz5yn
ð12ynÞ
Finally, Step III. z5b0x01b1x15b01b1x1, noting that x0is usually set to 1
for the bias term, b0
Using subscript notation:
.dz
db5xiwhere i51;2;...;n:
Putting them all together now, the derivative can be written as:
dJ
db52XN
n51pn
yn/C18/C19
212pn
12yn/C18/C19 /C20/C21
3ynð12ynÞ/C2/C3
3x1½/C138
which simplifies to:
dJ
db52XN
n51ðpn2ynÞx1
Expanding it to a general matrix form, where B,P,X,a n d Yare vectors:
dJ
dB5Pn2Yn ðÞTUX ð10:6Þ
Note that Bis a ( d31) vector, where dis the number of independent
variables and the other three vectors are ( n31) where nis the number of
samples. Jand its derivative are scalars. The dot product between the two
vectors in 10.6, will account for the summation.
As mentioned before, rather than setting the above equation to 0, an iterative
approach is adopted to solve for the vector Busing gradient descent. Start
with an initial value for weight vector, Bj, where jis the iteration step and a
step size (called the learning rate) and use this slope Eq. (10.6) , to iteratively
reach the point where Jis minimized.
Bj115Bj2Learning Rate 3Pn2Yn ½/C138TUX ð10:7Þ10.2 How It Works 319In practice one would stop after a set number of iterations or when the incre-
mental difference between Bjand Bj11is very small. In general, one can
calculate the weights for any error function using a formula similar to 10.6.
The key is to compute the gradient of the cost function, dJ/dB , and plug it
into this form:
Bj115Bj1Learning Rate 3dJ
dBUX ð10:8Þ
Notice that the iterative update component of gradient computation for
logistic regression in (10.6) is remarkably similar to the one derived for lin-
ear regression, XT(^Y2Yi)—see Section 5.1.1. They both have the same form:
Predicted Vector 2Target Vector ðÞ UMatrix of Input Data ðÞ
if the slight inconsistency in the nomenclature of Yis disregarded. The
key difference is the way the Y0sa n dt h e Pare computed. In logistic
regression these are evaluated using the sigmoid transformation (y5σ(b0
1b1x)), whereas in linear regression a unit transformation (y5b01b1x)
is used that is, no scaling or transformation is applied to the computed
output.
This is essentially the concept of an activation function in ANN and deep
learning. Think of activation functions like a rule based weighted averaging
scheme. If the weighted average ( b01b1x)crosses a preset threshold, the
output evaluates to 1, if not it evaluates to 0 —which is what happens if the
activation function is the sigmoid . Clearly there are many candidate functions
which can be used (sigmoid is simply one of them). Shown below are exam-
ples of the most commonly used activation functions used in deep learning:
(1)sigmoid , (2) tanh, and (3) rectified linear unit (RELU) (Fig. 10.8 ).
Observe the similarity in shape between sigmoid and tanh —the only differ-
ence between the two is the scaling: tanh scales the output between [ 21, 1].
The RELU is also an interesting function —the output linearly increases if the
1.0 1.00 1.0
8
6
4
2
00.75
0.50
0.25
0.00
–0.25
–0.50
–0.75
–1.000.8
0.6
0.4
0.2
0.0
–10.0 –7.5 –5.0 –2.5 0.0 2.5 5.0 7.5 10.0 –10.0 –7.5 –5.0 –2.5 0.0 2.5 5.0 7.5 10.0 –10.0 –7.5 –5.0 –2.5 0.0 2.5 5.0 7.5 10.0
FIGURE 10.8
Commonly used activation functions: sigmoid (left), tanh (center), and RELU (right). RELU , Rectified linear unit .320 CHAPTER 10: Deep Learningweighted average exceeds a set threshold, otherwise it is set to 0. Note that
all three are nonlinear functions which is an important characteristic that
enables ANN and deep learning networks to classify linearly nonseparable
data discussed in Section 10.2 .
The table below summarizes the formulation of some common quantities
that arise in the application of gradient descent for linear and logistic regres-
sion. For the cost functions that arise in a general ANN, calculation of the
gradient in a closed form, like in Section 5.1.1, is not feasible, and it has to
be computed using numerical approximation. In practice, this is what
packages such as TensorFlow (TF) help implement. Such implementations
rely heavily on TF for the purposes of gradient calculation and associated ten-
sor or matrix method bookkeeping that is required.
Method Cost FunctionDerivative of
Cost FunctionClosed Form
Solution Gradient Form Solution
Linear
egressionJ51=NPN
i51ðyi2bTxiÞ2dJ=db52=NXN
i51
ðyi2bTxiÞð2xiÞB5ðXTXÞ21YTX bi115bi2ηdJ/dB,
where ηis the learning
rate and dJ/dB5
XT∙ð^Y2YiÞ
Logistic
regressionJ52X
ðpnlogyn
1ð12pnÞlogð12ynÞÞdJ
db5dJ
dydy
dzdz
db
where
y5σðzÞand
z5ΣbixiNone bi115bi2ηdJ/dB; where
dJ/dB5ðPn2YnÞTUX
10.2.3 Need for Backpropagation
Weighted averaging is one intuition that would allow a full ANN to be
conceptualized starting from the simple two-layer models of logistic and
linear regression. As discussed, in Section 10.2.2 , gradient descent is about
incrementally updating the weights based on the output generated from a
previous iteration. The fir st iteration is kicked off by randomly choosing the
weights. Can the process be made a bit more efficient and “well-rounded ”
by choosing a series of starting weights in parallel? That is, can one start by
building, say 3 logistic (or linear) regression units or models in parallel so
that instead of ( b1,b2) as the starting weights, there are 3 sets: ( b11,b21),
(b12,b22), and ( b13,b23)? Here the first subscript refers to the input node
o rf e a t u r ea n dt h es e c o n ds u b s c r i p tr e f e r st ot h en o d ei nt h ei n t e r m e d i a t eo r
so-called “hidden ”layer. This is illustrated in the Fig. 10.9 . It turns out
that by doing this, a small computational price might be paid at each
iteration, but the output may be arrived at quicker by reducing the number
of iterations. Finally, the output from the hidden layer is once again
weight-averaged by 3 more weights ( c1,c2,a n d c3) before the final output10.2 How It Works 321is computed. Also observe that the right-hand side figure is exactly
equivalent to the left-hand side, but r epresentationally simpler. With the
hidden layer in place, the output is first computed starting from left to
right: that is, σ(z1),σ(z2), and σ(z3) are computed as usual and weights
for c1:c3are assumed in order to compute the final sigmoid σ(v).
This would be the first iteration. The output can now be compared to the
c o r r e c tr e s p o n s ea n dt h em o d e lp e r f o r m a n c ee v a l u a t e du s i n gt h ec r o s s -
entropy cost function. The goal is now to reduce this error function for
which one would first need to incrementally update the weights c1:c3and
then work backwards to then update the weights b11:b23.T h i sp r o c e s si s
termed “backpropagation ”for obvious reasons. Backpropagation remains
relevant no matter how many hidden layers or output nodes are in question
and is fundamental to understanding of how all ANNs work. The actualmechanics of this computation was described with a simple example in
Section 4.6 and will not be repeated here. The main difference between the
example used in that case and this present one is the computation of theerror function.x1x1 b11
b11c1σ(z1)σ(z1)
σ(z2)σ(z2)
σ(z3)
σ(z3)σ(v)c2
c3b12b12
b13b13
b23b23b22b22b21
b21
x1x2
x2
x1x2
x2
FIGURE 10.9
Combining multiple logistic regression models into a neural network.322 CHAPTER 10: Deep Learning10.2.4 Classifying More Than 2 Classes: Softmax
Recall that by using logistic regression one is able to address a binary classifi-
cation problem. However, most real-world classifications require categoriza-
tion into one of more than two classes. For example, identifying faces,
numerals, or objects, and so on. One needs a handy tool to identify which of
the several classes a given sample belongs to. Also recall that in the network
models discussed so far in this chapter, there was only one output node and
the probability that a given sample belonged to a class was obtained. By
extension, one could add an output node for every class that needs to be cat-
egorized into and the probability that a sample belonged to that particular
class (see Fig. 10.10 ) could simply be computed. This is intuitively how the
“softmax ”function works.
Softmax does two calculations: it exponentiates the value received at each
node of the output layer and then normalizes this value by the sum of the
exponentiated values received at all the output nodes. For instance, when the
output layer has two nodes (one each for class one and class two), the proba-
bility of each class can be expressed as:
pY5class 1 ðÞ :ez1
ðez11ez2Þ
pY5class 2 ðÞ :ez2
ðez11ez2Þ
b11
b12
b13
b23b22b21c11
c12
c21
c22
c31
c32σ(v1)
σ(z2)σ(z1)
σ(z3)x2x1
σ(v2)
FIGURE 10.10
Softmax output layer in a network.10.2 How It Works 323If one divides the numerator and the denominator of the first class by ez1,
one would get:
pY5class 1 ðÞ :1
11ez22z1 ðÞ
which is the same expression as the sigmoid, Eq. (10.3a) , if one had only
one output node for a binary classification problem (i.e. z250). In general,
for a k-class problem, softmax is computed as:
pY5k ðÞ 5ezk
Σk
i51½ezi/C138ð10:9Þ
Note that what the ziare has not been stated. In practice they have the same
form as in Eq. (10.3a) , that is, zi5Σbixi.Another thing to keep in mind is
that in logistic regression based on the sigmoid, the thresholding is used or
the cut-off value of the output probability to assign the final class —if the
value is .0.5 then a “hard maximum ”is applied to assign it to one class
over the other.
The triple concepts of activation functions, backpropagation, and (calculus
based) gradient descent form what may be loosely termed as the “ABCs ”of
deep learning and remain at the mathematical core of these algorithms.
Hopefully this section provided an intuitive understanding of these impor-
tant concepts using simple networks such as linear and logistic regression
models. In combination with the material presented on ANN in Chapter 4,
on Classification one should now be in a strong enough position to grasp
the extension of these techniques to larger scale networks such as convolu-
tional neural networks.
There is no strict definition of what constitutes a “deep ”learning network. A
common understanding is that any network with three or more hidden layers
between the input and output layers is considered “deep. ”Based on the
math described in the preceding section it should be easy to understand how
adding more hidden layers increases the number of weight parameters, bij.I n
practice it is not uncommon to have networks where the number of weight
parameters (termed trainable parameters ) run into millions. In the next few
sections some typical use cases or applications of deep learning methods will
be explored and the practical implementations discussed.
10.2.5 Convolutional Neural Networks
A convolution is a simple mathematical operation between two matrices.
Consider a 6 36 matrix A, and a 3 33 matrix B.324 CHAPTER 10: Deep LearningA51 099877
9 88766
9 88766
1 11000
1 11000
0 001002
66666666643
7777777775
B5111
000
2121212
643
75
Convolution between Aand B, mathematically denoted as A(*)B, results in
a new matrix, Cwhose elements are obtained by a sum-product between the
individual elements of Aand B. For the given example,
c1151031193119311930183018301932 11832 11832 153
c125931183118311830183017301832 11832 11732 153
and so on.
It is easier to understand this operation by visualizing as shown in
Fig. 10.11 . Matrix Bis the lighter shaded one which essentially slides over
the larger matrix A(darker shade) from left (and top) to right (and bottom).
At each overlapping position, the corresponding elements of Aand Bare
multiplied and all the products are added as indicated in the figure to obtain
the corresponding element for C. The resulting output matrix Cwill be smal-
ler in dimension than Abut larger than B. So, what is the utility of this
FIGURE 10.11
Computing a convolution.10.2 How It Works 325convolution operation? Matrix Ais typically a raw image where each cell in
the matrix is a pixel value and matrix Bis called a filter (or kernel) which
when convolved with the raw image results in a new image that highlights
only certain features of the raw image.
As shown in the Fig. 10.12 ,Aand Bare basically pixel maps which
when convolved together yield another pixel map, C. One can see that C
accentuates or highlights the horizontal edge in Awhere the pixels jump
from a high value to a low value. In this case, the accent appears to be
thick —but in the case of a real image where the matrix sizes are in the order
of 1000s (e.g., a 1-megapixel image is approximately a 1000 31000 matrix
of pixels) —the accent line will be finer and clearly demarcate or detect any
horizontal edge in the picture. Another thing to note about the filter is that
when it is flipped by 90 degrees or transposed, that is, use BTinstead of B,a
convolution performed on a raw image would be able to detect vertical edges.
By judiciously choosing B, it will be possible to identify edges in any
orientation.
Thus, convolutions are useful in order to identify and detect basic features in images
such as edges. The challenge is of course to determine the right filter for a
given image. In the example this was done intuitively —however, machine
learning can be used to optimally determine the filter values. Observe that
determining the filter was a matter of finding the 3 3359 values for the
matrix Bin the example. A couple of things to note: the matrix Aisnwidth3
nheight in terms of pixels for a grayscale image. Standard color images have
three channels: red, green, and blue. Color images are easily handled by con-
sidering a 3-dimensional matrix nwidth3nheight3nchannels which are con-
volved with as many different filters are there are color channels.
0–0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
012 31
2
3
4
5
01× =
2345–0.5
0.0
0.5
1.0
1.5
2.0
2.5
–0.5 0.0 0.5 1.0 1.5 2.0 2.5
(A) (B) (C)
FIGURE 10.12
Effect of performing a convolution on a raw image.326 CHAPTER 10: Deep LearningInFig. 10.12 , one can observe that Cis smaller than the raw image A. If the
dimensions of Aand Bare known, the dimensions of Ccan be determined.
For simplicity, assume that nwidth5nheight5nwhich is quite common in real
applications. If the filter is also a square matrix of size f, then the output Cis
square of dimension n2f11. In this case n56,f53 and, therefore, Cwas
434.
As the process of convolution reduces the raw image size, it is sometimes
useful to enlarge the raw image with dummy pixels so that the original size
is retained. This process is called “padding ”as illustrated in Fig. 10.13 .I fpi s
the number of padded pixels, then the output dimension is given by
n12p2f11. Thus, in the example, the output of Cwill be 6 36 if one unit
of padding is added.
Another important consideration in computing convolutions is the “stride. ”
This is the number of pixels the filter is advanced each time to compute the
next element of the output. In the examples discussed so far, a stride of 1 has
been assumed. Fig. 10.14 illustrates this point. With stride, s, the output
dimension can be computed as ( n12p2f)/s11.
So far, it has been taken on intuition that a convolution helps to identify
basic image features such as edges. Thus, a filter strides over an image and
at each location of the output a sum-product of the corresponding
elements can be computed between the image and filter. However, instead of
performing a sum-product if one simply used the highest pixel value in the
overlapping cells at each compute location ( Fig. 10.15 ), a process called max
pooling would be obtained. Max pooling can be thought of as an operation
that highlights the most dominant feature in an image (e.g., an eye within a
face). Max pooling is another feature detection tactic that is widely used for
6×6 + 1 unit padding 3 ×3× =
6×6
FIGURE 10.13
Padding a convolution.10.2 How It Works 327image processing. Similar to max pooling, an average pooling can also be
done to somewhat “airbrush ”the raw image, where the values are averaged
in the overlapping cells instead of doing a sum-product. The calculation for
output dimension after pooling still remains (n12p-f)/s 11.
A convolution is a linear function similar to Eq. 10.1 which is a general
form of any neural networkwhere the weights, Bare now the pixels of the fil-
ter matrix and the inputs, Aare the image pixel values. The sum-product
FIGURE 10.15
Max pooling visualized.
FIGURE 10.14
Visualizing the stride during a convolution.328 CHAPTER 10: Deep Learningoutput Cis analogous to zin 10.3a . Similar to applying a nonlinear opera-
tor to zin 10.3b, the elements of Care typically passed through a RELU non-
linearity. This entire process, as will be summarized, forms one
convolutional layer. The output of this convolutional layer is sent to the next
layer which can be another convolutional layer (with a different filter) or
“flattened ”and sent to a regular layer of nodes, called a "Fully Connected"
layer which will be described later on.
In the Fig. 10.16 ,A[0]is the raw image and Cis the result of its convolution
with the filter B.A[1]is the result of adding a bias term to each element of C
and passing them through a RELU activation function. Bis analogous to a
weight matrix bof10.1, while Cis analogous to σ(z)in10.2b . The point of
making these comparisons is to highlight how convolutions can be used as a
part of a deep learning network. In a neural network, backpropagation can
be used to compute the elements of the weight matrix, b, and a similar pro-
cess can be applied to determine the elements of the filter matrix B.
One additional note: TF and such tools allow one to apply multiple filters in
the same layer. So for example, one can let B1be a horizontal edge detector,
B2be a vertical edge detector, and apply both filters on A[0]. The output C
can be tensorially represented as a volume, in this example, Cwill have the
dimension of 4 3432, which is essentially a stack of two 4 34m a t r i c e s ,e a c h
one is the result of convolution between Aand Bi(i51,2). Fig. 10.17 shows
how adding multiple filters at a layer will result in a volumetric network.
In order to determine the filter elements, remember that backpropagation
will be used. So, a cost-function will need to be computed, such as the one
in10.3, and minimize/maximize it using gradient descent. The cost function
is now dependent on 3 3332518 parameters in this example. In order to
A[0] B RELU ( C)RELU
=== ×
× = A[1]
FIGURE 10.16
Combining a convolution with an activation function.10.2 How It Works 329completely define the cost-function one can “flatten ”the output 4 3432
matrix into 32( 543432) nodes and connect each node to a logistic regres-
sion output node or softmax output nodes.
Fig. 10.18 shows a classic CNN architecture introduced by LeCun (1989) .I t
consists of several convolutional layers interspersed with max pool layers and
finally followed by what are known as fully connected layers where the last
convolution output matrix is flattened into its constituent elements and passed
through a few hidden layers before terminating at a softmax output layer. This
was one of the first CNNs and was used to recognize handwritten digits.
CNNs are highly capable deep learning networks which function highly effi-
ciently because of a couple of reasons. The feature detection layers (such as
6×63 ×3× 2= ×
4×4×2
FIGURE 10.17
Multiple filters of convolution.
FIGURE 10.18
Architecture of a typical CNN model.330 CHAPTER 10: Deep LearningConv1, Conv2, etc.,) are computationally quite quick because there are few
parameters to train (e.g., each Conv1 filter is 5 35 which yields 25 11 for
bias526 times 6 filters 5156 parameters) and not every parameter in the
output matrix is connected to every other parameter of the next layer as in a
typical neural network (as happens in the fully connected layers down the
network). Thus the fully connected layers FC1 and FC2 have 576 times
120569,120 parameters to train. Because of their flexibility and computa-
tional efficiency, CNNs are now one of the most common deep learning
techniques in practical applications.
Layers are the high-level building blocks in deep learning. As observed in
Fig. 10.18 , there are several convolutional layers and several fully connected
layers (also called “Dense ”layers) Each layer receives the inputs from the pre-
vious layer, applies the weights and aggregates with an activation function. A
couple of other key concepts that are in usage in deep learning are summa-
rized here.
10.2.6 Dense Layer
A dense or fully connected layer is one where all the nodes in the prior layer
are connected to all the nodes in the next layer. Several layers of dense layers
form different levels of representation in the data. ( Fig. 10.19 ).
10.2.7 Dropout Layer
A dropout layer helps to prevent model overfitting by dropping the nodes
randomly in the layer. The probability of dropping a node is controlled by a
factor which ranges from 0 to 1. A dropout factor closer to one drops more
nodes from the layer. This is a form of regularization that reduces the com-
plexity of the model. This concept is depicted in Fig. 10.20
FIGURE 10.19
Illustrating a dense or fully connected layer.10.2 How It Works 33110.2.8 Recurrent Neural Networks
The field of deep learning has exploded in the last decade due to a variety of
reasons outlined in the earlier sections. This chapter provided an intuition
into one of the most common deep learning methodologies: CNN. These are
but one representation of a deep neural network architecture. The other
prominent deep learning method in widespread use is called recurrent neural
network (RNN). RNNs find application in any situation where the data have
a temporal component. Prominent examples are time series coming from
financial data or sensor data, language related data such as analyzing the sen-
timent from a series of words which make up a sentence, entity recognition
within a sentence, translating a series of words from one language to another
and so on. In each of these instances, the core of the network still consists of
a processing node coupled with an activation function as depicted in
Fig. 10.1 .
Suppose the time series problem is a text entity recognition. So, here is a set
of training examples consisting of sentences from which one can identify cer-
tain named entities in each sentence such as proper nouns, places, dates, and
so on. The training set, thus, looks like:
Sample x,1.x,2.x,3.x,4.
1 This is an egg
2 I love scrambled eggs
3 Do you like omelettes?
4 Green eggs and Ham
5 My name is Sam
... ... ... ... ...
FIGURE 10.20
Illustrating a dense or fully connected layer.332 CHAPTER 10: Deep LearningThe objective here is to predict that y,j.is a named entity such as a proper
noun. So, y,4.would be 1, whereas y,1,2,3.are 0 ’s. The idea behind an
RNN is to train a network by passing the training data through it in a
sequence (where each example is an ordered sequence). In the schematic in
Fig. 10.21 ,x,t.are the inputs where ,t.indicates the position in the
sequence. Note that there are as many sequences as there are samples. y,t.
are the predictions which are made for each position based on the training
data. What does the training accomplish? It will determine the set of weights
of this (vertically depicted) network, bxwhich in a linear combination with
xi,t.and passed through a nonlinear activation (typically tanh), produces
an activation matrix a,t.. So:
a,t.5gb xx,t./C0/C1
However, RNNs also use the value of the activation from the previous time
step (or previous word in the sequence) because typically in most
sequences —such as sentences —the prediction of a next word is usually
dependent on the previous word or set of words. For example, the previous
words “My”,“name ”,a n d “is”would almost certainly make the next word a
proper noun (so y51). This information is helpful in strengthening the
prediction. Therefore, the value of the activation matrix can be modified by
adding the previous steps ’activation multiplied by another coefficient, ba:
a,t.5gb aa,t21.1bxx,t./C0/C1
FIGURE 10.21
A basic representation of RNN. RNN , Recurrent neural network.10.2 How It Works 333Finally, the prediction itself for the position ,t.is given by:
y,t.5gb ya,t./C0/C1
where byis another set of coefficients. All the coefficients are obtained during
the learning process by using the standard process of backpropagation.
Because of the temporal nature of data, RNNs typically do not have struc-
tures that are as deep as in CNNs. It is not common to see more than 4 /C05
layers which are all temporally connected.
10.2.9 Autoencoders
So far, deep learning has been discussed in the context of supervised learning
where an explicit labeled output dataset was used to train a model. Deep
learning can also be used in an unsupervised context and this is where
Autoencoders are useful.
An autoencoder is deep learnings answer to dimensionality reduction (which
is introduced in chapter 14 on Feature Selection). The idea is pretty simple:
transform the input through a series of hidden layers but ensure that the
final output layer is the same dimension as the input layer. However, the
intervening hidden layers have progressively smaller number of nodes (and
hence, reduce the dimension of the input matrix). If the output matches or
encodes the input closely, then the nodes of the smallest hidden layer can be
taken as a valid dimension reduced dataset.
This concept is illustrated in Fig. 10.22 .
FIGURE 10.22
Concept of dropout.334 CHAPTER 10: Deep Learning10.2.10 Related AI Models
Two other algorithms will briefly be mentioned which fall more into the
domain of AI rather than the straightforward function approximation objec-tive used so far. However, many researchers and experts tend to consider
these as newer applications of deep learning because deep networks may be
used as part of the algorithms.
Reinforcement Learning (RL) is an agent-based goal seeking technique where
an (AI) agent tries to determine the best action to take in a given environ-
ment depending on a reward. The agent has access to data which correspondto the various states in an environment and a label for each action. A deep
learning network may be used to take in an observation or state-array and
output probabilities for each action (or label). The most popular implemen-tation of RL is Google ’s alpha-go AI which defeated a top-ranked human Go
player. Practical applications of RL include route optimization strategies for a
self-driving vehicle, for example. Most such applications are experimental asof this publication.
Generative adversarial network (GAN) are at the cutting edge of deep
learning implementations —they were first introduced in 2014 and are yet
to find mainstream applications. GANs are proposed to be used to generate
new samples similar to the data they were originally trained on. Forexample, creating new “photographs of faces ”after being trained on a large
set of facial recognition data. GANs consist of two parts: A Generator and a
Discriminator. Both of these are deep neural networks, the generator“mashes ”up new samples while the discrim inator evaluates if the new
samples are “valid ”or not. One can think of the analogy of counterfeiting
currency. A counterfeiter tries to pass off poorly forged notes at first tobe rejected by say, a vendor. The counterfeiter learns from this experience
and gets increasingly sophisticated in his forgery until such time that the
vendor can no longer discriminate between a forged note and a real oneand, thereby, accepts the new note as a valid sample of the known
distribution.
10.3 HOW TO IMPLEMENT
Deep-learning architecture in RapidMiner can be implemented by a couple
of different paths. The simple artificial neural networks with multiple hiddenlayers can be implemented by the Neural Net operator introduced in
Chapter 4, Classification, Artificial Neural Network. The operator parameter
can be configured to include multiple hidden layers and nodes within each10.3 How to Implement 335layer. By default, the layer configuration is dense . This operator lacks the vari-
ety of different layer designs that distinguishes deep-learning architecture
from simple Neural Networks.
The Keras extension for RapidMiner offers a set of operators specific
for deep learning. It utilizes the Keras neural network library for Python.
Keras is designed to run on top of popular deep learning frameworks like
TensorFlow and Microsoft Cognitive Toolkit. The Keras extension in
RapidMiner enables a top-level, visu al, deep-learning process along with
data science preprocessing and postprocessing. The Keras extension requires
Python and related libraries to be installed.14The modeling and execution
of the deep-learning process in production application requires machines
running GPUs as computation using normal machines can be time consum-
ing. The following implementation uses Keras extension operators and can
b er u no ng e n e r a l - p u r p o s em a c h i n e s15.
Handwritten Image Recognition
Optical character recognition is an i mage recognition technique where
handwritten or machine-written characters are recognized by computers. A
deep learning-based (con volutional neural network) numeric character rec-
ognition model is developed in this section. As with any deep-learning
model, the learner needs plenty of tra ining data. In this case, a large num-
ber of labeled handwritten images a re needed to develop a deep learning
model. The MNIST database16(Modified National Institute of Standards
and Technology database) is a large database of labeled handwritten digits
used commonly for training image proc essing systems. The dataset consists
of 70,000 images of handwritten digits (0,1,2,...,9). Fig 10.23 shows sample
training images for the digits 2, 8, and 4.
The complete RapidMiner process for implementing handwritten image rec-
ognition on the MNIST dataset is shown in Fig. 10.24 .17The process has a
Execute Python operator to convert the 28 x 28 image pixels to a data frame.
The Keras Model contains a deep-learning model with several convolutional
and dense layers. The model was applied to the test dataset and performance
was evaluated. The dataset and the process are available at www.
IntroDataScience.com
14https://community.rapidminer.com/t5/RapidMiner-Studio-Knowledge-Base/Keras-Deep-Learning-
extension/ta-p/40839 .
15Note that on CPU the runtime is likely to be 100x to 1000x slower than on GPU. For this example,
the run time was 37 hours on a 2.5 GHz core i7.
16MNIST Database - http://yann.lecun.com/exdb/mnist/ .
17This implementation example was generously provided by Dr. Jacob Cybulski of Deakin University,
Australia.336 CHAPTER 10: Deep LearningStep 1: Dataset Preparation
The key challenge in implementing this well-known model using
RapidMiner is in preparing the datase t. RapidMiner expects the data in the
form of a standard data frame (a table of rows as samples and columns asattributes) organized into rows and c olumns and in its current version (as
of this publication) cannot use the raw image data. The raw data consist of
70,000 images which are 28 x 28 pixels. This needs to be transformed intoa Pandas data frame which is 70,000 rows (or samples) by 784 columns
(pixel values) and then split up into 60,000 sample training and 10,000
sample test sets. The 28 x 28 (pixels ) yields 784 pixel values. The first
FIGURE 10.24
RapidMiner process for Deep learning
FIGURE 10.23
Three different handwritten samples of the digits 2,8, and 4 from the MNIST dataset.10.3 How to Implement 337operator in this process is a Python script executor which takes the raw
data and transforms it. In the Execu te Python operator (Note: this is
very similar to the Execute R operator discussed in Chapter 15: Getting
started with RapidMiner, and works an alogously), the data is read from the
MNIST dataset, its shape saved, the pixel data transformed into floats from
integers, and finally both training and test “x”and “y”vectors are
“unfolded ”and merged into Pandas data frames with the “y”column
defined as a “label ”(using rm_metadata attribute). Fig. 10.25 shows
graphically what the Execute Python b lock accomplishes. The shape infor-
mation is alsoreturned as a data frame, so that it can later be used to set
image sizes and shapes in the convolu tional nets. The results from the
Execute Python block consists of three data frames: (i) a training data frame
60,000 rows x 785 columns; (ii) test data frame 10,000 x 785; and (iii) and
a shape information data frame which stores information about the
data (each image is 28 x 28 x 1 tensor, the 1 refers to the channel). After
flattening the 28 x 28 image we have 784 columns and we add one more
column to contain the “label ”information (i.e., which digit is encoded by
the 784 columns).
Step 2: Modeling using the Keras Model
The rest of process is very straightforward after the image data is transformed.
It passes the training data through the Keras Model and applies the
model on the test dataset (Apply Keras Model) and then checks the model
performance. The Extract Macro and Generate Macro operators pull the shape
information into variables called img_shape, img_row, img_cols, and
img_channels which are the used to reshape the data frame into a tensor that
the Keras Model operator can then use. The main modeling operator for
FIGURE 10.25
Reshaping the MNIST image tensor into a data frame.338 CHAPTER 10: Deep Learningdeep learning is the Keras Model operator under Keras > Modelling folder.
The Keras operator is a meta operator which has a subprocess of deep-learn-
ing layers within it. The main parameters for the Keras modeling operator
chosen are: Input shape: [img_size (=img_rows*img_cols*img_channels)],
Loss: categorical_crossentropy, Optimizer: Adadelta, Learning rate, Epochs,
batch_size are initially set at 1.0, 12 and 128 respectively. These can be
adjusted later for optimal performance.
The architecture of the Keras deep learning network is shown in
Fig. 10.26 . The main thing to note is that we need to include an extra
initial step (Add reshape) to fold the data back into its original form (28 x
2 8x1 )b a s e do nt h es i z ea n ds h a p ei n f o r m a t i o np a s s e di n t oR a p i d M i n e r .
There are two Conv2D layers: the first uses 32 filters (n 532) and the
second uses 64 filters. All filters are 3 x 3 (i.e., f 53) and stride (s 51).
MaxPooling (with f 52a n ds 52) and a Dropout layer which randomly
eliminates 25% of the nodes (see Fig. 10.20 ), complete the CNN portion of
the network before a flattened laye r converts the 2D images into a column
of 9216 nodes. This is connected to a 128 -node layer, and another Dropout
layer (50%) is applied before terminating in a 10-unit softmax output
layer. As an exercise, the reader is encouraged to write this process as a
schematic similar to Fig. 10.18 to make sure the internal workings of the
network are clearer. As you move from one layer to the next, Keras
FIGURE 10.26
Deep learning subprocess for keras operators.10.3 How to Implement 339automatically calculates the output size (i.e., number of output nodes
for that layer). The user must make sure that the output of the previous
layer is compatible with the input of the next layer by specifying the correct
input_size. The reader should also try to calculate the number of weights or
parameters that need to be learned at each layer. Fig. 10.27 shows the
model summary. As seen, this model has more than one million weights
(called “Trainable params ”) that need to be trained or learned making it a
truly deep-learning model. Observe that the largest number of weights
occur in the first dense layer.
Step 3: Applying the Keras Model
The model developed by the Keras modeling operator can be applied on the
test dataset using Apply Keras model under Keras > Scoring folder. This
FIGURE 10.27
A summary of the deep-learning model and its constituent layers.340 CHAPTER 10: Deep Learningoperator is similar to other Apply Model operators. The input for the
operator is the test dataset with 10,000 labeled image data frame and the
model. The output is the scored dataset. The scored output is then connected
to the Performance (Classification) operator to compare the deep learning-
based digits recognition with the actual value of the label.
Step 4: Results
Fig. 10.28 shows the results of the prediction performed by the CNN-based
deep-learning model. As seen from the confusion matrix, the performance
measured via recall, precision, or overall accuracy is around 99% across all the
digits, indicating that the model performs exceptionally well. This implemen-
tation shows both the power and the complexity of the deep learning process.
10.4 CONCLUSION
Deep learning is a fast-growing field of research and its applications run the
gamut of structured and unstructured data (text, voice, images, video, and
others). Each day brings new variants of the deep architectures and with it,
new applications. It is difficult to do justice to this field within the scope of a
broad data science focused book. This chapter provided a brief high-level
overview of the collection of techniques known as deep learning. An over-
view on how the majority of today ’s AI applications are supported solely by
deep learning was provided. The fundamental similarity between deep learn-
ing and function fitting methods such as multiple linear regression and logis-
tic regression were demonstrated. The essential differences between deep
learning and traditional machine learning was also discussed. Some time was
then spent detailing one of the most common deep learning techniques —
convolutional neural networks and how it can be implemented using
FIGURE 10.28
Comparing the deep learning predicted values to actual values.10.4 Conclusion 341Rapidminer was discussed. Finally, some of the other deep learning techni-
ques were highlighted which are rapidly gaining ground in research and
industry.
References
Explainable Artificial Intelligence. (2016). Broad agency announcement . Defense Advanced
Research Projects Agency. ,https://www.darpa.mil/program/explainable-artificial-
intelligence ..
Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural
networks. Science ,313, 504 /C0507.
Le Cun, Y. (1989). Generalization and network design Strategies. University of Toronto technical
report CRG-TR-89-4 .
Marczyk, J., Hoffman, R., Krishnaswamy, P. (1999). Uncertainty management in automotive crash:
From analysis to simulation .,https://pdfs.semanticscholar.org/2f26/c851cab16ee20925-
c4e556eff5198d92ef3c.pdf ..
Minsky, M., and Papert, S (1969). Perceptrons: an introduction to computational geometry .
Cambridge, Massachusetts: MIT Press.
Rosenblatt, F. (1957) The perceptron: A Probabilistic model for Visual Perception, Procs. of the
15th International Congress of Psychology, North Holland, 290-297.
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-
propagating errors. Nature ,323(9), 533 /C0536.
Somers, J. (2017). Is AI riding a one-trick pony? MIT Technology Review, September 29, 2017.
,https://www.technologyreview.com/s/608911/is-ai-riding-a-one-trick-pony/ ..
Wissner-Gross, A. (2016). Datasets over algorithms. Edge ,https://www.edge.org/response-
detail/26587 ..342 CHAPTER 10: Deep LearningCHAPTER 11
Recommendation Engines
Now a days, everyone experiences the increasing impact of products and
applications that utilize machine learning. Out of all the applications of
machine learning, recommendation engine techniques underwrite some of
the most prolific utilities in everyday experience. Recommendation engines
are a class of machine learning techniques that predict a user preference for
anitem. A user is a customer or a prospective customer of an item or a prod-
uct. In essence, the purpose of an enterprise is to connect a customer to a
product that the customer wants. Recommendation engines influence this
important connection between the customer and the product. Hence, recom-
mendation engines play a cardinal role in how a relevant product is intro-
duced to a customer. Whether it is a snippet of news feed on a social media
homepage, what to watch next in a streaming service, suggested article in an
online news portal, a new friend recommendation, or a suggested product to
buy from an e-commerce site, the influence of automated recommendation
engines are inescapable in today ’s digital economy.
WHY DO WE NEED RECOMMENDATION ENGINES?
A brick and mortar store has a limitation of space which dictates the number
of unique products it can hold. An enterprise with a few dozen products, by
design, has a limited number of choices for its customers. In either case, it is
relatively easy to predict what available products the customers want. It is
common to see retailers recommending their top selling products to their
customers. To capitalize on what readers might commonly buy, bookstores
often display the top selling books, new releases, or store recommendations
at the front of the store. Newspapers run top stories in politics, business,
sports, and entertainment on the front page, anticipating they would be the
most likely read articles. These tactics became the recommendations or prod-
uct predictions for customer consumption as a whole. In this approach, there
is one common recommendation list for allthe customers. It served well for
the customers because most of them purchased top selling products (and,
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00011-3
©2019 Elsevier Inc. All rights reserved.343hence, they are in best sellers!) and served the enterprise well because they
made it easy for customers to purchase the top sellers.
The legacy recommendation tactics that served the brick and mortar settings
do not take into account an individual customer preference. After all, not all
the customers are the same. Some may prefer business books, but not mys-
tery novels, some may like to read sports news and may not really care aboutcurrent affairs or politics. At a deeper level, none of the users are the same
and everyone has their own unique interests and preferences of the products
they want to consume. It would be ideal to have a unique recommendation
list for every user, taking into consideration their unique interests and wants.
The customers and the enterprises alike want a personalized storefront tomaximize the positive experience of customer-product interactions. Both the
editor-curated and best-selling lists are good global approximations of what
customers would purchase. However, both the approaches do not fit the pre-ferences of an individual user, unless the user is the prototypical average
user.
The advent of online businesses greatly expanded the access of the product
selection an enterprise can offer. In the span of a few years, customers could
have instant access to half a million movies on a streaming service, millions
of products on an e-commerce platform, millions of news articles and usergenerated content. Shelf space is not a constraint anymore and there is an
abundance of unique products at the customer ’s disposal. However, not all
products are consumed at the same rate. There are blockbuster movies andthere is a long tail of obscure movies that are consumed less frequently. The
long tail of products tend to serve customers with unique interests. A cus-
tomer who is interested in thrillers might have watched the movie The Girl
with the Dragon Tattoo , a Swedish-American psychological thriller from 2011.
Given that the customer watched The Girl with the Dragon Tattoo , the cus-
tomer might be extremely interested in an obscure subgenre of Scandinavian
Crime Fiction and discovers titles like Headhunters ,The Killing ,The Bridge , etc.
This pattern of consumption is made possible when there is an abundant
selection of products that can be consumed instantly. It is also a scenariowhere the one size fits all approach of the best seller lists and common edito-
rial staff picks are insufficient. The more customer-product connections an
enterprise makes, the more it can keep the customer engaged. Moreover, the
long tail products tend to be less expensive than licensing the best seller pro-
ducts. In those cases, it will be more profitable for the business to keep thecustomer engaged in the long tail products.
Search engines offer a good solution to discover items in the long tail only if
the user knows what to look for. The information retrieval system works ifthe customer types in a specific movie title, like “Headhunters ”or in some344 CHAPTER 11: Recommendation Enginescases, if the information retrieval system is capable, users can search for
“Scandinavian crime fiction ”and get a list of movies. But, what if there is no
such thing as a Scandinavian crime fiction subgenre or if the user is unaware
of such a genre? There is nothing recommended when the credits roll.
Most customers are best served by their personalized storefronts where they
can discover the most relevant products for them, out of the millions of
available products. If an enterprise can predict what the user is going to con-
sume next, it will be serving both the customers and itself. Recommendation
engines make the personalized experience happen.
APPLICATIONS OF RECOMMENDATION ENGINES
Recommendation engines have become an indispensable part of online, and
in some cases offline, experiences. It is difficult to escape the reach of the
applications that utilize recommendation engines every day. These models
are behind some of the familiar phrases in digital life: friends you may
know, who to follow, recommended connections, news feeds, tweet feeds,
customers who bought this item also bought, featured products, suggested
dates, you may also like, related stories, recommended movies, what to
watch next, etc. Recommendation engines power these online experiences
(Underwood, 2017 ). Some of the applications of the recommenders are:
Product recommendations: An e-commerce website or any organization
that has thousands of products, use product recommendation engines. It
is one of the earliest applications of recommenders in the digital realm
(Schafer, Konstan, & Riedl, 2001 ). The recommended listings, which are
predicted by the company as the most relevant products for a specific
customer, are shown in the homepage, product pages, shopping cart,
checkout, and even in the order confirmation page. The recommendation
list is based on past purchases or past clickstream data of the customer.
E-commerce companies often complement the list with the best seller
lists, new products, and curated featured products. Once the customer is
in a product page, assuming the customer is interested in the product, a
new modified recommendation list is shown as “related products ”or as
“other customer also bought this, ”lists. Similar features can be extended
to offline enterprises, to an extent. Customizable marketing offers,
customer relevant coupon codes, and dynamic call center scripts are
some of the ways recommendation engines are used in offline
experiences.
Content recommendation: A personalized news feed is one way to increase
the stickiness of content sites. Whether it is a desktop, mobile, or any
other device, users have limited time to consume the content. ContentApplications of Recommendation Engines 345companies are motivated to keep users on the site by feeding them the
most relevant news stories, a friend ’s social update, or a tweet. All these
recommendations are based on the past behavior of the user and the
behavior of other users on the site ( Thorson, 2008 ).
Entertainment recommendation: Binge watching as a phenomenon was
unknown just a few years ago. Digital consumption of entertainment
products has become the predominant way the media is consumed in
many segments. Music streaming providers have a vast collection of
tracks and cater to a user ’s unique taste in music. They can recommend a
new artist ’s track to a listener because it shares the same traits as the track
For What it is Worth by Buffalo Springfield that has already been liked by
the listener.
In 2006, when Netflix Inc. announced N etflix Prize, a public competition
to award the best recommendation engine model that could beat the com-
pany ’s own recommender —CineMatch, research into recommendation
engines came into the spotlight. For it s part Netflix released about half a
million anonymized user ratings of 17,000 movies. Movies are rated with
a1/C05-star scale by the users, presumab ly after watching the movie. Even
though, recommendation engines existed before this, new research began
into this problem of predicting user-product interactions ( Koren, Bell, &
Volinsky, 2009 ).
11.1 RECOMMENDATION ENGINE CONCEPTS
Whatever the application may be, a recommendation engine should predict
a customer ’s interest in a particular product and provide highly personalized
recommendations to the customer. To do this, the recommendation engine
needs some information about the customer, the product and the customer-
product preference. The input information for the recommendation engine
takes the form of the past inclinations of the customer ’s preference of the
product, for example, the movie ratings by a customer of the content stream-
ing service. The customer discloses this rating information with an implicit
understanding that the company will keep offering them highly relevant titles
and with the satisfaction of their voice being heard. The company uses this
data point and the data points collected from other users to feed into the rec-
ommendation engine. They invest in recommendation engines in an effort to
keep offering customers highly personalized content, keep customers
engaged, and maintain the trust of the customers for giving them informa-
tion about the products they like.
The general model of a recommendation engine is shown in Fig. 11.1 .
Throughout this chapter, the customer will be referred to as the userand the346 CHAPTER 11: Recommendation Enginesproduct as an item. An item can be a product, movie, track, album, artist,
topic, news article, etc. The user-item preference is quantified in the form of
ratings . The ratings can be of any ordinal scale: 1 /C05-star ratings, scores, like
or dislike flags, etc.
The most common input format for a recommendation engine is the Utility
matrix orRatings matrix . It is a simple two-dimensional crosstab with items in
the columns, users in the rows and ratings in the cells. A few cells in the rat-
ings matrix have rating values which indicate the user ’s preference for an
item. Table 11.1 shows the ratings matrix with the user Josephine rating the
movie The Godfather as 5 stars, on a 1 /C05-star scale. Alternatively, the cells in
the ratings matrix may indicate a Boolean flag, whether or not a user has pur-
chased, viewed, or searched for an item. In the ratings matrix with rating
data, the blank cells may represent a few possibilities. Most likely the reason
is that the user has not used or experienced the item yet. Other reasons can
be that the user has not bothered to give a rating after using the item. In
either case, the lack of rating cannot be assumed to indicate that the user dis-
liked the item.
FIGURE 11.1
The general model of a recommendation engine.
Table 11.1 Known Ratings Matrix
The
Godfather2001: A Space
OdysseyThe Hunt for Red
October FargoThe Imitation
Game ...
Josephine 5 4 1 1
Olivia 2 2 4
Amelia 5 1 4 4 1
Zoe 2 2 5 1 1
Alanna 5 5 1 1
Kim 4 1 2 5
...11.1 Recommendation Engine Concepts 347It is lucky to see so many ratings for the items by the users in the ratings
matrix shown in Table 11.1 . In reality, however, most of the cells would be
empty in a ratings matrix. Say there is an “Etsy”style e-commerce company
with 500,000 items and a couple of million users. Each user can give a 1 /C05-
star rating for a product they are familiar with. If all the users rate all the
items, that would be 1012possible ratings! How many distinct products
would a typical user buy? Perhaps 100. Out of which, how many items
would a typical user rate? Perhaps 10. The remaining of 499,990 items for
the user will have a blank cell in the ratings matrix. A sparse matrix provides
both a challenge and an opportunity for recommendation engines. The
model would have to sift through the user ’s potential preference in the other
499,990 items and provide a list of a dozen or so items the user would be
most interested in. The objective of the recommendation engine is to fill the blank
cells of the ratings matrix with predicted ratings for each user and the item, which
the user presumably hasn ’t used or experienced yet .
The association analysis technique discussed in Chapter 6, Association
Analysis is similar in approach to a recommendation engine. After all, both
the techniques recommend items to a user and both employ machine learn-
ing algorithms. However, the results of the association analysis technique are
not personalized to a specific user. If the association analysis model recom-
mends beer the moment diapers are placed in the user ’s shopping basket,
this same recommendation would be served to allusers, whether or not a
user liked beer or ever even bought beer in the past. It is a global rule-based
recommendation for all users. Moreover, in the association analysis model,
the session window of consideration is restricted to the current visit to the
supermarket. The purchase history of the individual user in previous visits is
not taken into account. In the case of recommendation engines, past ratings
or purchase history of a specific user is central to building future user-item
recommendations. The past history data is not readily accessible when a user
enters a supermarket or is visiting a website as an anonymous user. In these
cases, the global recommendation by association analysis provides a good
approximation of user-item preferences.
Building up the Ratings Matrix
The ratings matrix shown in Table 11.1 can be represented with a ratings
function f.
Ratings functionf :U3I-R ð11 :1Þ
where Uis the set of users, Iis the item set and Ris the ratings matrix. The ini-
tialknown ratings in matrix Ris sparse because the user-item interaction is
miniscule compared to all the possible item-user combinations. The objective348 CHAPTER 11: Recommendation Enginesof the recommender is to fill the blank cells with predicted unknown ratings.
The process of building a completed ratings matrix consists of three steps.
1.Assemble known ratings
2.Predict unknown ratings
3.Prediction evaluation
Step 1: Assemble Known Ratings
Collecting and assembling known ratings in the form of a ratings matrix is
the most time-consuming step, riddled with data capture challenges. The rat-
ings can be explicit, where the user directly provides ratings about an item.
The user can be offered a list of previously consumed items or be provided
with choices to select their favorite items out of a pre-selected list. The choice
list is helpful to seed some ratings when a user is new to the service. The rat-
ings could also be derived from the wish-lists where the user has expressed
interest in an item. The challenge in the explicit rating is that it is not scal-
able. A typical user is not good at voluntarily providing a rating. How often
does one leave a rating for a product that has been purchased or seen? The
typical response rate is very low, in the range of 1% /C010% ( Weise, 2017 ).
This limits the amount of data that can be used to build a reliable recom-
mendation engine.
The implicit collection of ratings provides an alternative to, or complements,
explicit data collection. Implicit data collection is inferred preference of an
item from the past user actions, for example, searching for an item, previous
purchases (not ratings), watching a movie, product views, etc. While one can
get an abundance of data about user-item interactions using implicit collec-
tion, the challenge is that it is very difficult to interpret low or high ratings
from implicit action. The user might have seen the movie and completely
detested the movie. It is incorrect to assume that a user liked the movie just
because the user watched it. A hybrid combination of both explicit and
implicit rating is used in large scale production deployments to overcome
the limitations of individual methods ( Hu, Koren, & Volinsky, 2008 ).
Regardless of either explicit or implicit data collection, there is no available
known rating for a new user or a new item. Hence, providing relevant recom-
mendations is difficult when a new user or new item is introduced into the sys-
tem. This is called the cold start problem. The recommenders cannot conclude
any inferences from users or items when the system has not obtained sufficient
data about them. Cold start is a major issue to recommenders because new
items and users are continuously added to the system. Some of the algorithms
used to build a recommendation engine discussed in this chapter are highly
susceptible to dealing with the cold start problem, while others are robust.11.1 Recommendation Engine Concepts 349Step 2: Rating Prediction
Predicting ratings for prospective user-item interaction is the key objective for
a recommendation engine. In some cases, the objective is to show 1 /C05-star
ratings for all the movies in the catalog indicating how much a user will pre-
fer a movie. In most cases, however, it is not essential to predict ratings for
allthe user-item interactions. The objective is just to show only the top
recommended items from the massive catalog of movies. Hence, it might be
adequate to show items with top predicted ratings instead of all the items
with the predicted ratings.
Step 3: Evaluation
Any machine learning-based prediction model needs to go through an evalua-
tion step to check if the prediction is generalized and the model is not overfit-
ting the training dataset. Some of the known rating data can be reserved as a
test dataset to evaluate the training prediction. The performance of the recom-
mendation engine is often measured as RMSE (root mean square error) or
MAE (mean absolute error) of ratings, which measures the delta between the
actual ratings and the predicted ratings. RMSE metric is penalized for larger
errors, while MAE has the advantage of easy interpretation. The accuracy of the
deployed recommendation engine has to be continuously measured whenever
a new known rating is captured in the system. The new actual rating is com-
pared against the previous prediction of the recommender.
The Balance
A good recommendation engine has reasonable accuracy of predicted ratings,
balanced with a few other secondary objectives. Sometimes a user might not
have discovered a particular genre and could benefit from serendipitous dis-
covery of an item appearing on the recommendation list. A new popular
movie might not have a user ’s favorite cast or director but might be worth
recommending because the movie is popular. The recommendation engine
accentuates a phenomenon called the Filter bubble , where users are cocooned
with a singular viewpoint ( Bozdag, 2013 ). It is not uncommon to find
recommended articles leaning towards one political affiliation or all the
recommended movies from one genre, because the user happened to sample
a particular news article or watched a movie from a specific genre. The same
user might like a different genre, say classic drama, had the user been pro-
vided with an opportunity to watch it. Providing a mix of diverse selection
and enabling serendipitous discovery might complement the recommenda-
tion engine ’s predictions on user-item preferences. Companies deal with the350 CHAPTER 11: Recommendation Enginesfilter bubble problem by creating hybrid lists like new, noteworthy, or popu-
lar items along with the listing from recommendation engines.
11.1.1 Types of Recommendation Engines
The machine learning methods for building a recommendation engine can
be classified into a taxonomy shown in Fig. 11.2 . There is a wide selection of
algorithms for building a recommendation engine, each with its own
strengths and limitations ( Bobadilla, Ortega, Hernando, & Gutiérrez, 2013 ).
Association analysis is included as a type of recommendation engine method
in this taxonomy, but it is distinct because of the absence of user personaliza-
tion and the window of data it considers for a recommendation ( Cakir &
Aras, 2012 ). Recommendation engine methods are broadly classified into
Collaborative Filtering and Content-based Filtering .
Collaborative filtering :Collaborative filtering is a class of recommenders that
leverage only the past user-item interactions in the form of a ratings matrix.
It operates under the assumption that similar users will have similar likes. It
uses rating information from all other users to provide predictions for a user-
item interaction and, thereby, whittles down the item choices for the users,
from the complete item set. Hence, the name collaborative filtering. The col-
laborative filtering techniques can be further classified into:
1.Neighborhood methods :Neighborhood methods predict the user-item
preferences by first finding a cohort of users or items which are similar
to the user or the item whose ratings are to be predicted. In the case of
FIGURE 11.2
Taxonomy of recommendation engines.11.1 Recommendation Engine Concepts 351theuser-based neighborhood method, the algorithm is designed to find
a cohort of users who have a similar rating pattern to the user in
question and who have rated the item in question. The predicted ratingis deduced from those cohort of similar users. The case is the same
with the item-based neighborhood method where a cohort of similar
items is found which have been given similar ratings when rated by thesame user. The rating of similar items approximates the rating of the
item in question for a user. Both these techniques leverage the
similarity scores discussed in the k-Nearest Neighbor section inChapter 4, Classification.
2.Latent factor models :Actively managed mutual fund managers look for
suitable companies to invest in their pursuit to outperform the marketaverages. Out of hundreds of thousands of publicly traded companies,
the fund managers select dozens of companies using certain criteria or
factors. The factors can be management stability, market positioning,strength of the balance sheet, cash flow metrics, or market share of the
products. They even explain the preference of the dozens of companies
where they have taken positions using the factors. Factors are the samegeneralized dimensions that the company is ranked against (Company
Acme Inc . has products with high market share) and the fund
manager ’s preference for the factor (The fund Cash cow looks for
companies with high market share). The latent factor models tend to
explain the ratings by representing both the users and the items on a
set of common factors. This set of factors is inferred from the knownuser-item ratings matrix. The factors can take the form of
interpretable dimensions like "science-fiction movies with aliens" or
uninterpretable dimensions. It maps both the users and items againstthe same set of factors. For example, if a science-fiction movie has a
plot involving aliens and if a user has a preference for movies with
aliens, a recommendation is made. Both the movie and the user aremapped with a degree of association with the factor —aliens.
Content-based filtering :In addition to the ratings matrix used by collaborative
filtering methods, content-based filtering leverages attribute data about items.
For example, it may use all the available information about the contents of a
movie (item): the cast, the producer, the director, released year, genre,description, etc., in addition to the user-item ratings matrix. Content-based
filtering uses the information about an item in the format of an item profile.
After the item profile is sourced, the content-based recommenders predict theuser-item preference with two different approaches:
1.User profile method :Once an item profile is developed, a user profile
can be developed to show the affinity of a user to the attribute used in
the item profile. For example, if a user has given high ratings for the352 CHAPTER 11: Recommendation Enginesmovies Fargo ,The Big Lebowski , and No Country for Old Men , the user
profile can be built to show the preference for the Coen brothers ’
(filmmakers, Joel Coen and Ethan Coen) work, who directed all theaforementioned movies. This technique is leveraged by user profile
methods. The predicted rating for a user-item is computed by the
similarity of the user and the item profiles. Since the movie True Grit is
associated with the Coen brothers in the item profile and the user has
shown an association with the Coen brothers in the user profile, a user-
item recommendation can be made.
2.Supervised learning (Classification or Regression) models :Consider the case
of a user who has shown explicit preference for a few movies by
providing ratings. The item profile of the movies has a set of attributes,for example, the cast, the director, genre, key words in the plot
description, etc., for all the items. If these two datasets were joined viz.,
the ratings matrix for one user and the item profile using the common
link—the item , then movie ratings (response or label) and item attributes
(input or predictor) are obtained for the user. The joined dataset is
similar to the training data used to create classification or regressionmodels. With the item profile and ratings matrix, it is possible to develop
a classification or a regression model, say a decision tree, for each user
and predict the rating for an unseen item using its attributes. By thesupervised learning method, a personalized classification or regression
model is built for every single user in the system. The model scores the
items based on the user ’s affinity towards specific item attributes.
11.2 COLLABORATIVE FILTERING
Collaborative Filtering is based on a simple idea —a user will prefer an item
if it is recommended by their like-minded friends. Suppose there is a new
restaurant in town. If a friend, who happens to have the same interests as the
user, raves about it —the user might like it as well. The idea makes intuitive
sense. Collaborative filtering leverages this insight into an algorithm that pre-
dicts a user ’s preference of an item by finding a cohort of users who happen
to have the same preference as the user. The preference or predicted rating
for an item can be deduced by the rating given by a cohort of similar users.
The distinguishing feature of the collaborative filtering method is that the
algorithm considers only the ratings matrix that is, the past user-item interac-
tions. Hence, the collaborative filtering method is item domain independent.
The same algorithm can be applied to predicting ratings for movies, music,books, and gardening tools. In fact, the algorithm does not have any knowl-
edge about the items except the ratings given by the users.11.2 Collaborative Filtering 353Some of the real-world applications of collaborative filtering include Google
News recommendations ( Das, Datar, Garg, & Rajaram, 2007 ), music recom-
mendations by Last.fm, and recommended Twitter follows ( Gupta et al.,
2013 ). The general framework for collaborative filtering is shown in
Fig. 11.3 . There are two distinct approaches in processing the ratings matrix
and extracting the rating prediction. The neighborhood method finds a
cohort of similar users or items. The latent factor method explains the ratings
matrix through a set of dimensions called latent factors. Matrix factorization is
the common realization of the latent factor method, where both users and
items are mapped to a common set of self-extracted factors.
11.2.1 Neighborhood-Based Methods
Neighborhood methods calculate how similar the users (or the items) are in
the known ratings matrix. The similarity scores or the measure of proximity,
discussed in the k-Nearest Neighbor section in Chapter 4, Classification tech-
niques, are used in neighborhood-based methods to identify similar users
and items. Commonly used similarity measures are: Jaccard similarity,
Cosine similarity, and Pearson correlation coefficient.
The general approach for neighborhood-based methods consists of two steps
to find the predicted rating for a user-item:
1.Find the cohort of other similar users (or the items) who have rated
the item (or the user) in question.
2.Deduce the rating from the ratings of similar users (or the items).
The two methods for the neighborhood-based systems, that is, user-based
and item-based, are extremely similar. The former starts to identify similar
users and the latter starts by identifying similarly rated items by the same
FIGURE 11.3
Collaborative filtering recommender.354 CHAPTER 11: Recommendation Enginesusers. For the ratings matrix shown in Table 11.1 , both the methods share
the same technique, where one starts with finding similar rows and the other
finds similar columns.
User-Based Collaborative Filtering
The user-based collaborative filtering method operates on the assumption
that similar users have similar likes. The two-step process of identifying new
unseen user-item preferences consists of filtering similar users and deducing
the ratings from similar users. The approach for user-based collaborative fil-
tering is quite similar to the k-NN classification algorithm.
1.For every user x, find a set of N other users who are similar to the user
xand who have rated the item i.
2.Approximate the rating of the user xfor the item i, by aggregating
(averaging) the rating of N similar users.
Consider the known ratings matrix shown in Table 11.2 . Say the task is to
find whether the user Olivia will like the movie 2001: A Space Odyssey . The
first step will be to find N users similar to the user Olivia who have already
rated the movie 2001: A Space Odyssey . The second step will be to deduce a
rating for the user Olivia from the cohort of similar users.
Step 1: Identifying Similar Users
The users are similar if their rating vectors are close according to a distance
measure. Consider the rating matrix shown in Table 11.2 as a set of rating
vectors. The rating for the user Amelia is represented as ramelia5{5,1,4,4,1}.
The similarity between the two users is the similarity between the rating vec-
tors. A quantifying metric is needed in order to measure the similarity
between the user ’s vectors. Jaccard similarity, Cosine similarity, and Pearson
correlation coefficient are some of the commonly used distance and
Table 11.2 Known Ratings Matrix and a User-item Rating Prediction
The
Godfather2001: A Space
OdysseyThe Hunt for
Red October FargoThe Imitation
Game
Josephine 5 4 1 1
Olivia ? 22 4
Amelia 5 1 4 4 1
Zoe 2 2 5 1 1
Alanna 5 5 1 1
Kim 4 1 2 511.2 Collaborative Filtering 355similarity metrics. The cosine similarity measure between two nonzero user
vectors for the user Olivia and the user Amelia is given by the Eq. (11.2)
Cosine similarity jx:yj ðÞ5xy
jjxjj jjyjj
Cosine similarity rolivia ;ramelia ðÞ 50351031123412341431ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
22122142p
3ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ
52112142142112p 50:53
ð11 :2Þ
Note that the cosine similarity measure equates the lack of ratings as zero
value ratings, which can also be considered as a low rating. This assumption
works fine for the applications for where the user has purchased the item or
not. In the movie recommendation case, this assumption can yield the wrong
results because the lack of rating does not mean that the user dislikes the
movie. Hence, the similarity measure needs to be enhanced to take into con-
sideration the lack of rating being different from a low rating for an item.
Moreover, biases in the ratings should also be dealt with. Some users are
more generous in giving ratings than others who are more critical. The user's
bias in giving ratings skews the similarity score between users.
Centered cosine similarity measure add resses the problem by normalizing the
ratings across all the users. To achieve th is, all the ratings for a user is subtracted
from the average rating of the user. Th us, a negative number means below aver-
age rating and a positive number means above average ratings given by the same
user. The normalized version of the ratings matrix is shown in Table 11.3 .E a c h
value of the ratings matrix is normalized with the average rating of the user.
Centered cosine similarity or Pearson correlation between the ratings for the
users Olivia and Amelia is calculated by:
Centered cosine similarity ðrolivia ;ramelia Þ
5032:0103/C02:01/C00:731:010:731:011:33/C02:0ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð/C00:7Þ21ð0:7Þ21ð1:3Þ2p
/C3ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2:0Þ21ð/C02:0Þ21ð1:0Þ21ð1:0Þ21ð/C02:0Þ2p 52 0:65
Table 11.3 Normalized Ratings Matrix
The
Godfather2001: A Space
OdysseyThe Hunt for
Red October FargoThe Imitation
Game
Josephine 2.3 1.3 21.8 21.8
Olivia 20.7 20.7 1.3
Amelia 2.0 22.0 1.0 1.0 22.0
Zoe 20.2 20.2 2.8 21.2 21.2
Alanna 2.0 2.0 22.0 22.0
Kim 1.0 22.0 21.0 2.0356 CHAPTER 11: Recommendation EnginesThe similarity score can be pre-computed between all the possible pairs of
users and the results can be kept ready in a user-to-user matrix shown in
sample Table 11.4 for ease of calculation in the further steps. At this point
the neighborhood or cohort size, k, has to be declared, similar to kin the k-
NN algorithm. Assume kis 3 for this example. The goal of this step is to find
three users similar to the user Olivia who have also rated the movie 2001: A
Space Odyssey . From the table, the top three users can be found similar to the
user Olivia, who are Kim (0.90), Alanna ( 20.20), and Josephine ( 20.20).
Step 2: Deducing Rating From Neighborhood Users
Rating R5X
neighborhoodðsimilarity score 3rating Þð 11 :3Þ
Once the cohort of users are found, deducing the predicted rating is straight
forward. The predicted rating for Olivia for the movie 2001: A Space Odyssey
is the average of the ratings given by Kim, Alanna, and Josephine, for the
same movie. One can use weighted average based on the similarity score for
more accuracy and the general formula for the predicted ratings is given by
Eq. (11.4) .
rxi5X
uANsxurui
X
uANsxuð11 :4Þ
where Nis a set of k users similar to user xand have rated the item i,ris the
predicted rating for user xand item i.sxuis the similarity score of user xand
user u. Using Eq. (11.4) and the similarity matrix, the predicted rating for
Movie 2: 2001: A Space Odyssey can be computed for Olivia as:
rxi50:931 ðÞ 1/C00:232 ðÞ 1/C00:231:25 ðÞ
0:9/C00:2/C00:2 ðÞ50:05Table 11.4 User-to-User Similarity Matrix
Josephine Olivia Amelia Zoe Alanna Kim
Josephine 1.00 20.20 0.28 20.30 0.74 0.11
Olivia 1.00 20.65 20.50 20.20 0.90
Amelia 1.00 0.33 0.13 20.74
Zoe 1.00 0.30 20.67
Alanna 1.00 0.00
Kim 1.0011.2 Collaborative Filtering 357The predicted rating is 0.05 above the average score for Olivia, which is 2.7.
The final predicted rating for the movie is therefore 2.75 stars. If k is 1, the
rating would have been 3.7 stars.
The user-based neighborhood technique provides an intuitive way to fill the
ratings matrix. The steps for finding similar users and rating deductions is
repeated for every blank ratings cell in the ratings matrix. However, the collab-
orative filtering process can be time consuming. One way to speed-up the pro-
cess is to pre-compute the user-to-user similarity matrix shown in Table 11.4
once for all the users and reuse the results for the first step in identifying simi-
lar users. The user-to-user similarity matrix should be updated for any new
user. However, a new user similarity with other users can be computed only
when their preference information is known —cold start problem !
The cold start problem is the most significant limitation of the collaborative fil-
tering technique ( Isinkaye, Folajimi, & Ojokoh, 2015 ). A new user to the system
would not have item preference ratings. The system could ask the new user to
declare the preferences by providing information on their most preferred items.
Regardless, the user will have a minimal number of item ratings to start with.
For the new users, the cohort neighborhoods will be small and, thus, matching
the user with other cohort users will be limited, which in turn leads to less
recommendations. Similarly, a new item will not be part of any previous user-
item interactions and will lead to less recommendations for the new item to
the users. In some cases, because of the fewer recommendations, the new items
stay less popular. For existing users or items, the neighborhood method needs
more data for the algorithm to be more effective, which can be a limiting factor
for the applications that rely exclusively on the explicit data collection method.
The cold start problem can be mitigated by a couple of strategies. First, the
system can ask all the new users to select or enter their preferred items once
they have signed up. This can be selecting a few movies from a curated list of
movies or tagging them from the movie catalog. The new user onboarding
process incorporates this step, so the recommendation engines have some
seed information about the new users. Second, the system can rely heavily
on implicit data collection through search or clickstream activity until a solid
item preference profile can be build.
Popular items are preferred by a large number of users. Collaborative filter-
ing tends to recommend popular items to users because the cohort selection
might be skewed towards higher ratings for popular items. Recommending
the popular items is not necessarily a bad decision. However, one of the
objectives of recommendation engine is to discover the personalized idiosyn-
cratic and unique items of the user. Moreover, the nonpersonalized popular
items are usually shown by best seller ortrending now lists, which have the
same content for all the users.358 CHAPTER 11: Recommendation EnginesItem-Based Collaborative Filtering
The item-based neighborhood method operates on the assumption that users
prefer items that are similar to previously preferred items. In this context,
items that are similar tend to be rated similarly by the same users. If a user
liked the dark comedy crime movie Fargo , then the user might like the crime
thriller movie No Country for Old Men , provided that both movies are rated
similarly by the same users. Both the movies are created by the same writers-
directors and it is presumed that if a user has given high ratings for Fargo , the
same user might have given high ratings for No Country for Old Men as well.
If the rating patterns of two items are similar, then the items are in the neigh-
borhood and their item vectors are close to each other. Hence, the item ’s
neighbor is a set of other items that tend to get similar ratings when rated by
the same user. The two-step process of identifying new user-item preference
using item-based collaborative filtering include identifying similar items and
deducing a rating from similar items.
1.For every item i, find a set of Nother items which have similar ratings
when rated by the same user.
2.Approximate the rating for the item iby aggregating (averaging) the
rating of Nsimilar items rated by the user.
The ratings matrix shown in Table 11.2 can be used to compute the predicted
rating for the same unseen cell —for the user Olivia for the movie 2001: A
Space Odyssey-using the item-based method . To realize the item-based neighbor-
hood method, the rating matrix shown in Table 11.2 has to be transposed
(swapping rows and columns) and continued with the same steps as the
user-based (or row based) neighborhood method. Table 11.5 shows the
transposed version of the original ratings matrix.
The centered cosine or Pearson correlation coefficient metric is used to calcu-
late the similarity between movies based on the ratings pattern. Since the
objective is to find the rating for 2001: A Space Odyssey , the similarity score
would need to be found for all the movies with 2001: A Space Odyssey.
Table 11.5 Transposed Ratings Matrix
Josephine Olivia Amelia Zoe Alanna Kim
The Godfather 5 5 2 5
2001: A Space Odyssey 4 ? 1254
The Hunt for Red October 1 2 4 5 1
Fargo 2 4 1 1 2
The Imitation Game 1 4 1 1 1 511.2 Collaborative Filtering 359Table 11.6 shows the centered rating values along with the similarity score of
all the movies with 2001: A Space Odyssey . The similarity score is calculated
using Eq. (11.2) on the centered rating values. Since the centered ratings can
be negative, the similarity score can be positive or negative. Depending on
the number of neighbors specified, the top k neighbors to the movie 2001: A
Space Odyssey can now be narrowed down using the magnitude of the simi-
larity score. Assume kis 2.
From Table 11.6 the nearest two movies to 2001: A space Odyssey can be con-
cluded, rated by Olivia, are Fargo and The Hunt for Red October . The predicted
centered rating for the 2001: A space Odyssey for Olivia, using Eq. (11.4) is:
rxi50:243/C00:7 ðÞ 12 0:3632 0:7 ðÞ
0:24/C00:36 ðÞ5/C00:67
The normalized rating for Olivia and 2001: A space Odyssey is20.67 and the
real ratings for 2001: A space Odyssey by Olivia is 2. Note that the predicted
ratings for the same user-item pair is different when user-based collaborative
filtering is used.
User-Based or Item-Based Collaborative Filtering?
The neighborhood technique for predicting a rating for a user-item combina-
tion, either with user-based or item-based, is very similar. After all, if the rat-
ings matrix is transposed at the beginning, the item-based approach is
exactly the same as the user-based approach. However, the predicted rating is
different when these two approaches are used on the same ratings matrix.
Conceptually, finding similar items is relatively easier than finding similar
users. Items tend to get aligned with specific genres or types of the items. A
movie can belong to either Classics or Science fiction genres or, less likely, inTable 11.6 Normalized Ratings and Similarity With a Movie
Josephine Olivia Amelia Zoe Alanna KimSimilarity with the
Movie 2001: A
Space Odyssey
The Godfather 2.3 2.0 20.2 2.0 20.10
2001: A Space Odyssey 1.3 22.0 20.2 2.0 1.0 1.00
The Hunt for Red October 21.8 20.7 1.0 2.8 22.0 20.36
Fargo 20.7 1.0 21.2 22.0 21.0 0.24
The Imitation Game 21.8 1.3 22.0 21.2 22.0 2.0 20.43360 CHAPTER 11: Recommendation Enginesboth the genres. However, a user may like both Classics and Science fiction
genres. It is common for the users to have interests in multiple genres and
develop unique taste profiles. It is easier to find similar courtroom drama
movies. It is difficult to find similar users when a user has preference to
courtroom drama, science fiction, movies directed by Francis Ford Coppola
and Pixar movies. If all the users prefer one genre, it is easy to isolate the pre-
ferences and, thereby, have no confounding of ratings. In practice, that
doesn ’t work, and it is difficult to extract the user preference from the ratings
matrix when users have multiple overlapping preference profiles. Hence,
finding similar items is more effective than finding similar users and yields
better performance in most applications.
In some business applications, the objective of the recommendation engine is
to provide a list of top recommended items for each user. To accomplish this
in the user-based approach, for every user, one can pre-compute the step of
finding similar users and then aggregate the ratings for items. This vastly nar-
rows the search space. However, in the item-based approach, all the item-to-
item similarity combinations need to be pre-computed before getting into user
level recommendation. The user-based approach has a leg up when it comes
to computational time because most applications are concerned with provid-
ing recommendations at the user level.
Neighborhood based Collaborative Filtering - How to Implement
The neighbor-based method involves rel atively simple calculations, albeit
time consuming. The entire step-by-ste p process of finding similar users (or
similarly rated items) and deducing a rating can be implemented in a pro-
gramming tool or by using RapidMiner. If using the latter, ratings data
should be prepared in the form of a ratings matrix to find similar users or
items. Similar users can be found using the operator Data to Similarity Data
operator with Cosine Similarity as the parameter. Alternatively, the prebuilt
operators for Recommendation Engines using the Recommenders extension
(Mihel ˇci´c, Antulov-Fantulin, Bo ˇsnjak, & ˇSmuc, 2012 ) can be leveraged.
The operators that come with the extension expand the available library
and abstract the low level tasks to high-level tasks geared toward
Recommendation Engine application.
Dataset
The dataset used in this chapter is sourced from MovieLens database. It is a
sample of real-world anonymized data from GroupLens Inc. The ratings data
can be downloaded from the GroupLens website1which provides multiple
data size options. The following implementation uses 100,000 ratings given
1http://grouplens.org/datasets/movielens11.2 Collaborative Filtering 361by 1000 users for 1700 titles. There are two datasets in each of the down-
loaded packages. The first dataset (ratings dataset) contains ratings, user IDs,
and movie ID attributes. The second dataset (items dataset) contains limited
metadata about each movie —movie ID, movie title, and concatenated genre
attributes. The attributes of the movies can be further expanded from other
third-party databases like IMDB2using the movie name. Table 11.7 shows
the MovieLens input data.
Implementation Steps
The outline of the full RapidMiner process is shown in Fig. 11.4 . The high-
level process for a recommendation engine is no different from the other pre-
dictive analytics process. It contains data preparation, modeling, applying the
model to a test set, and performance evaluation steps to predict the user-
item ratings.Table 11.7 MovieLens Datasets —Ratings and Movie Attributes
User ID Movie ID Rating Timestamp
1 31 2.5 1260759144
1 1029 3 1260759179
1 1061 3 1260759182
1 1129 2 1260759185
1 1172 4 1260759205
1 1263 2 1260759151
1 1287 2 1260759187
1 1293 2 1260759148
1 1339 3.5 1260759125
1 1343 2 1260759131
Movie ID Title Genres
1 Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy
2 Jumanji (1995) Adventure|Children|Fantasy
3 Grumpier Old Men (1995) Comedy|Romance
4 Waiting to Exhale (1995) Comedy|Drama|Romance
5 Father of the Bride Part II (1995) Comedy
6 Heat (1995) Action|Crime|Thriller
7 Sabrina (1995) Comedy|Romance
8 Tom and Huck (1995) Adventure|Children
9 Sudden Death (1995) Action
10 GoldenEye (1995) Action|Adventure|Thriller
2http://imdb.com362 CHAPTER 11: Recommendation Engines1.Data Preparation: The central operator in the neighborhood-based
recommender process is the modeling operator called User—KNN or
Item—KNN . The former implements a user-based and the latter
implements an item-based neighborhood recommender. As acollaborative filtering method, the only input needed for the modeling
operator is the ratings matrix. The ratings matrix is in the form of User
identification, Item identification, and the label (rating). Any otherattribute from the dataset is not used in the core modeling operator.
Whatever the format of the input data, it needs to be transformed to a
dataset with user identification, item identification, and numericalrating label.
All the information needed for the modeling is available in the
MovieLens rating dataset. However, more context can be added to themovies by joining the ratings dataset with the item dataset. Attributes
not needed for the process are discarded using the Select attribute
operator. The Set role operator is used to declare which attribute is
“user identification, ”“item identification, ”and the label. The “user
identification ”and“item identification ”are custom roles used by
Recommender operators. Mapping the attribute user ID to “user
identification ”role and item ID to “item identification ”role is
FIGURE 11.4
Item k-NN recommender process.11.2 Collaborative Filtering 363specified in the parameter settings for the Set role operator. The ratings
data is randomly split between training (95%) and test dataset (5%)
using the Split data operator
2.Modeling: The modeling operator used in this recommender process
item K-NN can be found under Recommenders .Item rating
prediction .Collaborative Filtering Rating Prediction. There are a few
parameters to configure based on the application.
a.k: Nearest neighbor cohort size. This is the same “k”as in k-NN
classification. The cohort size is set as 10 for this process.
b.Min and Max ratings: The ratings range in the training dataset. The
movie ratings dataset has ratings from 0 to 5.
c.Similarity measure: Pearson or Cosine similarity measure is the
commonly used similarity measure for recommenders. Cosine
similarity measure is used for this recommender process.
3.Apply Model to test set: The Apply Model (Rating Prediction) operator
under Recommenders .Recommender Performance .Model
Application is used to apply the tra ining model to the test dataset.
A portion of the original dataset th at is reserved for testing is used
as an input for the Apply Model operator which calculates the
predicted ratings for each user-it em combination in the test dataset.
The Apply Model is also used in t he final deployment where one
can supply user-item combinations for which the rating is to be
predicted.
4.Performance evaluation: T h eo u t p u to ft h ep r e d i c t e dt e s td a t a s e t
from the Apply Model operator is connected to the Performance (Rating
Prediction) operator under the Recommender extension folder to
evaluate if the predicted rating is close to the actual rating provided
by the users. As in the Apply Model operator, the ratings range is
declared in the parameters for Performance (Rating Prediction)
operator.
The whole process shown in Fig. 11.4 can be saved and executed. The Results
window shows the predicted values for the test dataset. In addition to the
attributes in the original test dataset, a new prediction column is appended
to the dataset. A sample of the first ten records are shown in Fig. 11.5 , where
recommender model prediction and the actual rating given by the users can
be observed.
The performance vector tab in the Results window shows the output of the
Performance evaluation operator. RMSE of the rating prediction is 0.873 stars
for the neighborhood-based recommender using the MovieLens dataset. The
MAE is 0.665 stars. On an average, the predicted star rating will be off from
the actual prediction by 0.665 stars. Not bad for using a simple recom-
mender model!364 CHAPTER 11: Recommendation EnginesConclusion
Neighborhood-based recommendation systems use just the ratings matrix as
in input for training the model. It is content agnostic or domain independent
and does not have to learn about the details of the items a priori. Moreover,
the same model can be used to recommend different types of items. An
ecommerce platform can recommend books, movies, and other products
with the same recommendation engine. The model using the neighborhood
method does not care about what the items mean ,it just focuses on the user-item
interactions using past ratings . As with all the collaborative filtering techniques,
the neighborhood method suffers from the cold start problem and popular
item bias.
One limitation of neighborhood techniques is the processing complexity and
scalability. It is computationally intensive to find similar users or similar items
for every user or an item, when there are millions of users and items. Since it
is practically impossible to carry out these calculations at runtime, the neigh-
borhood calculation for finding neighborhoods for each user or item can be
pre-computed. If R is the ratings matrix, the processing complexity of the
neighborhood-based approach is given by O(|R|). The pre-computation pro-
cessing complexity increases to O(n.|R|) where, n is the number of items ( Lee,
Sun, & Lebanon, 2012 ). As the number of items or users increase in the sys-
tem, so to do the computations required for collaborative filtering approaches.
In the current form, collaborative filtering approaches do not scale well with
the increase in users and items in a system.
The items like a pack of Oatmeal and a pack of Instant Oatmeal may look
quite similar, but they are different items. Synonymy is the tendency of particu-
larly similar items with different distinctions. Collaborative filtering techniques
will have a hard time equating these two items because they do not have any
FIGURE 11.5
Predicted ratings for Item k-NN recommender.11.2 Collaborative Filtering 365information on the content of the items. Ideally, two users having preferences
on two similar items should be treated the same. Collaborative filtering techni-
ques have difficulty managing the synonymy of the items.
11.2.2 Matrix Factorization
The ratings matrix discussed so far in this chapter has information on the
user, item, and the strength of user-item interaction. For every user, the rat-
ings information is at the level of individual item, that is, every individual
movie, track, product. The number of items usually ranges from thousands
to millions of unique values. If one were to ask someone what movies or
books they would prefer to watch or read, the answer is usually in the form
of some generalized dimension. For example, expressing the preference for
science fiction movies, movies directed by Christopher Nolan, movies with
strong female leads, crime novels or 1920s literature. The subsequent follow-
up might be providing specific item examples which belong to those group-
ings, for example titles like, Interstellar ,Iron Lady ,The Great Gatsby , etc. The
generalized categories can be a predefined genre, works by a creator, or some-
times it might be a yet-to-be named or vague categorization. Perhaps the
user just likes a set of items with no specific generalized category.
TheLatent Factor model, like the neighborhood methods, uses just the ratings
matrix as the only input. It tries to generalize and explain the ratings matrix
with a set of latent factors or generalized dimensions. The factors, which usu-
ally range from a dozen to a few hundred, are automatically inferred from
the ratings matrix, as long as the number of factors is specified. There is no
separate input to the model that provides pre-classified genres or factors. In
fact, there are no interpretable names for the latent factors. The inferredGLOBAL BASELINE
The recommendation engine methods discussed in this
chapter so far have underpinnings in machine learning.
Alternatively, there is a simple straightforward (naive)
method to calculate the predicted rating for a user-item
interaction - Global baseline, as shown in Eq. (11.5) .
rui5μ1bu1bi ð11 :5Þ
where μis the global average of all ratings, buis the user
bias. On average, the rating given by a user uisμ1bu,bi
is the item bias. On average, the rating for the item iby all
the users is μ1bi.buand biis the delta from the global
average of all the ratings, given by the user uand for the
item i. For example, consider the average star rating forall the movies is 2.5. The user Amelia tends to be gener-
ous in her rating and on average she has given 3.5 stars
for all the movies rated by her. The user bias is 11.0
stars. On an average the movie Interstellar received 2.0
stars from all the users. The item bias is 20.5. The pre-
dicted star rating, using Global baseline, by the user
Amelia for the movie Interstellar is 3.0
rui52:511:020:553:0
The global average serves as a baseline to measure the
performance of more complex machine learning-based
recommenders discussed in this chapter.366 CHAPTER 11: Recommendation Enginesfactors might resemble genre like classification (science fiction or family
movies) and in some cases they are just uninterpretable groupings of items.
It will be interesting to research and generalize why a group of items are
rated highly against a factor. Once the factors are discovered, the model
associates an item ’s membership towards a factor and a user ’s inclination
towards the same factor. If the user and item are near to each other when
plotted against a factor or a set of factors, then there is a strong user-item
preference.
Fig. 11.6 shows the users (circles) and items (squares) mapped against two
illustrative factors: production scale and comedic content. The items or movies
are plotted on the chart based on how movies express themselves against the
latent factors. The users are plotted based on their preferences against the same
latent factors. From the chart, it can be concluded that the user Amelia prefers
the movie No country for Old men because both the user and the movie are
close to one another when expressed against the latent factors. Similarly,
Alanna prefers the movie Headhunters instead of Titanic . The user-item prefer-
ence is calculated by the dot product of the user vector and the item vector
expressed latent factors. The similarity between the user and the item vectors
dictates the preference of the user to the item ( Koren et al., 2009 ).
FIGURE 11.6
Items and users in latent factor space.11.2 Collaborative Filtering 367Matrix Factorization is a technique to discover the latent factors from the rat-
ings matrix and to map the items and the users against those factors.
Consider a ratings matrix Rwith ratings by nusers for mitems. The ratings
matrix Rwill have n3mrows and columns. The matrix Rcan be decom-
posed into two thin matrices Pand Q.Pwill have n3fdimensions and Q
will have m3fdimensions where fis the number of latent factors. In the
example used in Fig. 11.6 , there are two latent factors. The matrix Rcan be
decomposed in such a way that the dot product of the matrix Pand trans-
posed Qwill yield a matrix with n3mdimensions that closely approximates
the original ratings matrix R(Fig. 11.7 ).
R/C25P∙QTð11 :6Þ
The decomposition of the ratings matrix into the user matrix and the item
matrix makes intuitive sense. The rating of a user-item combination, say
Olivia to Toy Story , can be explained by Olivia ’s preference to comedy movies
and whether the movie Toy Story is acclaimed highly in a comedic content
scale or not. This generalization approach greatly reduces the size of the
matrices. The massive m3ndimensional ratings matrix with 2 million users
and a half a million items can be decomposed to two thin matrices: Pwith 2
million users against 100 factors and Qwith half a million items against 100
factors. The dot product of Pand Qwill closely yield the original ratings
matrix, with 2 million users and a half a million items, but with more infor-
mation! Allthe cells in the rating matrix will be filled with the predicted rat-
ings from the dot product, including the sparse known ratings and the vast
unknown ratings, using the matrix dot product of Pand Q.
FIGURE 11.7
Decomposition of ratings matrix into latent factor matrices.368 CHAPTER 11: Recommendation EnginesThe approach is similar to expressing a number, 61, as a product of two
numbers. As a prime number, it is not possible to decompose 61 as a prod-
uct of two numbers, but the original number can be closely approximated
with 6310. The error in this decomposition is 1. Similarly, each rating rcan
be expressed by the user ufor the item ias:
brui5pu:qT
i ð11 :7Þ
where puis the user vector with fdimensions and qiis the item vector with f
dimensions. For each item i, the vector qimeasures the extent to which the
item ipossesses the latent factors. For each user u, the vector pumeasures the
preference of the user for the items that have a high possession of those fac-
tors. The dot product of the vectors puand qT
igives the approximation of the
user’s preference for the item.
The objective of the matrix factorization method is to learn the vectors Pand
Qfrom the ratings matrix R, where Pexpresses an item ’s rating in terms of
the factors fand Q expresses the interest of users to the factors. Pand Q
should be learned in such a way that one can minimize the delta between
known ratings and predicted ratings. Assume Kis the set of known, non-
blank cells in the ratings matrix R. The objective is to minimize the predic-
tion error or the loss function, in Eq. (11.8)
minX
ðu;iÞAKðrui2bruiÞ2
minX
ðu;iÞAKðrui2puqT
iÞ2ð11 :8Þ
where Kis the set of ( u,i) pairs of known ratings.
Similar to all machine learning algorithms for predictive tasks, overfitting is a
problem for recommenders using the matrix factorization method. The key
objective is to predict the ratings of items which are not rated, more than
accurately predicting the ratings of the known items. To reduce the impact of
overfitting, one can introduce regularization models (as introduced in
Chapter 5: Regression Methods). Regularization is a technique that penalizes
learning more complex and flexible models to avoid the risk of overfitting.
Regularization avoids overfitting by minimizing the regularized square error
shown in Eq. (11.9) . It regularizes or forces the coefficient estimates toward
zero. Hence, the magnitude of the learned parameters (coefficients) is penal-
ized through regularization. The extent of regularization is controlled
through the tuning parameter λ, which ranges from 0 (no effect) to N(max-
imum effect)
minX
ðu;iÞAKrui2puqT
i/C0/C121λjjpujj21jjqijj2/C0/C1
ð11 :9Þ11.2 Collaborative Filtering 369Much of the observed variance in the rating matrix can be explained by the
user-item interactions. However, in real-world applications, there are signifi-
cant biases that affect the user-item preference. After all, some users are more
critical in providing ratings than others. Some blockbuster movies gather gen-
erous ratings because they are ...blockbusters. To factor in the effects of bias
to overall ratings the Global Baseline model can be used given by Eq. (11.5) ,
as a proxy for overall bias in the system.
bui5μ1bu1bi ð11 :10Þ
The effects of the bias can be used in the predicted ratings, in addition to the
user-item interactions as calculated by the matrix factorization method.
Eq. (11.11) shows the predicted rating with bias taken into consideration.
brui5μ1bu1bi1pu:qT
i ð11 :11Þ
The overall objective function for the matrix factorization methods is shown
inEq. (11.12) . The parameters for the learning algorithm are: regularization
λ, user bias bu, item bias bi;and the global average rating μ
minX
ðu;iÞAKðrui2ðμ1bu1bi1pu:qT
iÞ21λðjjpujj21jjqijj21bu21bi2Þð 11 :12Þ
The algorithm commonly used to learn the factor vectors puand qiby mini-
mizing the loss function is Stochastic Gradient Descent (SGD). SGD is an iter-
ative optimization method based on gradient descent touched upon in
c h a p t e r5a n da l s oi nc h a p t e r1 0 ,t om i n imize the objective function like
the one in Eq. (11.9) . For the given factor dimension f, the SGD algorithm
initializes the vectors Pand Qand calculates the error rate, which is the
delta between the real and predicted r atings. The algorithm slowly changes
the value of Pand Qto minimize the error and halts when there is no
meaningful change in the error rate ( Gemulla, Nijkamp, Haas, & Sismanis,
2011 ).
Matrix Factorization - How to Implement
Implementing a matrix factorization-based recommender from scratch is an
effort intensive process. The Recommenders extension ( Mihel ˇci´c et al., 2012 )
offers prebuilt operators to implement Biased Matrix Factorization recom-
mendation engines. As with the neighborhood-based recommender imple-
mentation, the MovieLens dataset from GroupLens3with 100,000 ratings
given by 1000 users for 1700 titles is used for building the recommender
using matrix factorization.
3http://grouplens.org/datasets/movielens370 CHAPTER 11: Recommendation EnginesImplementation Steps
The outline of the RapidMiner process for the matrix factorization-based rec-
ommender is shown in Fig. 11.8 . The process is similar to the process used
in neighborhood-based methods, with the only change to the modeling
operator —Biased Matrix Factorization (BMF)
1.Data Preparation: The central operator in this is process is the
modeling operator called BMF. As a collaborative filtering method, the
only input needed for the modeling operator is ratings matrix. The Set
roleoperator is used to declare the attributes for “user identification ”,
“item identification, ”and the label. The data is randomly split between
training (95%) and test dataset (5%).
2.Modeling: The modeling operator is BMF —found under
Recommenders .Item rating prediction .Collaborative Filtering
Rating Prediction. These parameters can be configured in the modeling
operator to suit the application:
a.Min and Max ratings: The ratings range in the training dataset. The
movie ratings dataset has ratings from 0 to 5.
FIGURE 11.8
Matrix factorization recommender process.11.2 Collaborative Filtering 371b.Num Factors ( f): The number of latent factors inferred from the
ratings dataset. Specifying the number of latent factors is critical in
matrix factorization modeling. However, similar to k-NN and
clustering techniques, specifying an optimal number of factors is
tricky. Hence, the parameter optimization technique discussed in
Chapter 15 Getting started with RapidMiner may need to be
employed to find the best number of latent factors for the dataset to
obtain optimal performance. In this process, the number of latent
factors is set as 12.
c.Bias: Bias regularization parameter. The default value is set as 0.0001.
d.Learn Rate ( α):Learning rate in the Stochastic Gradient Descent
algorithm. SGD is used to optimize the parameters to minimize the
function in Eq. (11.12) . The value is set as 0.005 for this process.
e.Regularization ðλÞ:Regularization tuning parameter. The default
value is set as 0.015.
3.Apply Model to test set: TheApply Model (Rating Prediction) operator is
used to apply the training model to the test dataset. The model and the
test dataset serve as input to the apply model operator and the output
is the predicted rating for all the test records.
4.Performance evaluation: The output of the predicted test dataset from
theApply Model operator is connected to the Performance (Rating
Prediction) operator to evaluate if the predicted rating is close to the
actual rating provided by the users.
The RapidMiner process shown in Fig. 11.8 can be saved and executed. The
result window shows the predicted dataset, the recommender model and the
performance vector. In addition to the attributes in the test dataset, a new
FIGURE 11.9
Predicted ratings using matrix factorization recommender.372 CHAPTER 11: Recommendation Enginesrating prediction column is appended to the dataset. A sample of ten test
records are shown in Fig. 11.9 with the original unseen ratings and the pre-
dicted ratings.
The results window also contains the performance vector which shows the
RMSE and MAE for the ratings prediction for the test dataset. The RMSE for
the BMF model is 0.953 stars and MAE is 0.730 stars.
11.3 CONTENT-BASED FILTERING
The collaborative filtering method uses the past user-item interaction data as
the only input for building the recommenders. Content or attribute-based
recommenders use the explicit properties of an item (attributes) in addition
to the past user-item interaction data as inputs for the recommenders. They
operate under the assumption that the items with similar properties have
similar ratings at the user level. This assumption makes intuitive sense. If a
user liked the movies Frago ,No Country for Old Men , and Burn After Reading ,
then the user would most likely prefer The Big Lebowski , which are all directed
by the same people —the Coen brothers. Content-based recommendation
engines keep suggesting an item to a user similar to the items rated highly by
the same user. The user will most likely get recommendations about movies
with the same cast or director as the user preferred in the past.
The distinguishing feature of the content-based recommendation engine is
that it sources attributes of an item, also known as building the item profile.
The attribute data about the movies are readily available in public databases
like IMDB,4where the cast, directors, genre, description, and the year of the
title can be sourced. The item attributes can be derived from structured cata-
logs, tags, or unstructured data from the item description and images.
The general framework of a content-based recommendation engine is shown
inFig. 11.10 . The model consumes ratings matrix and item profiles. The out-
put of the model either fills the entire ratings matrix or provides just the top
item recommendation for each user.
Predicting ratings using a content-based recommendation method involves
two steps. The first step is to build a good item profile. Each item in the cata-
log can be represented as a vector of its profile attributes. The second step is
to extract the recommendations from the item profile and ratings matrix.
There are two distinct methods used for extracting recommendations: a user
profile-based approach and a supervised learning-based approach.
4https://www.imdb.com/11.3 Content-Based Filtering 373The user profile approach computes the preference of users to the item attri-
butes from the ratings matrix. The proximity of the users and the items
against the item attribute space indicates the preference of the user to the
items. Whereas, the supervised learning approach treats the user preference
of attributes as a user level classification or regression problem with the rat-
ings matrix serving as the label (target) and item attributes as the predictors
(feature). If one uses a decision tree as the supervised learning technique,
then each user will have a personalized decision tree. The nodes in the deci-
sion tree will be checking an item attribute to predict whether the user will
prefer the item or not.
Building an Item Profile
An item profile is a set features or discrete characteristics about an item in
the form of a matrix. Features, also called attributes, provide a description of
an item. Each item can be considered as a vector against the set of attributes.
In case of the books, the attributes may be the publisher, author, genre, sub-
genre, etc. In the case of movies, the attributes may be individual cast mem-
bers, year, genre, director, producer, etc. A matrix can be built with columns
as the universe of attributes for all the items where each row is a distinct item.
The cells can be Boolean flags indicating if the item is associated with the
attribute or not. Similar to the document vectors discussed in Chapter 9, Text
Mining, the number of columns or attributes will be large and the matrix
will be sparse. Table 11.8 shows a sample item profile or item feature matrix
for the movies with the attributes as cast, directors, genre, etc.
The item profile can be sourced from the providers of the item (e.g., product
sellers in an e-commerce platform) or from third-party metadata providers
(IMDB has metadata on a vast selection of movies). The item description pro-
vides a wealth of features about the products. Chapter 9, Text Mining discusses
relevant tools like term frequency-inverse document frequency (TF-IDF) to
FIGURE 11.10
Model for content-based recommendation engine.374 CHAPTER 11: Recommendation Enginesextract features from documents such as item description. If the items are news
articles in an online news portal, text mining is used to extract features from
news articles. In this case, the item profile contains important words in the col-umns and the cells of the matrix indicate whether the words appear in the
document of the individual items. At the end item profile creation process,
there will be a set of attributes that best describe the characteristics of eachitem. The matrix contains information on whether each item possess those
attributes or not. Once the item profile is assembled, the recommendations
can be extracted using either a user profile computation approach or a super-vised learning approach.
11.3.1 User Profile Computation
The user profile approach computes the user-item preference by building a
user feature matrix in addition to the item feature matrix. The user feature
matrix or the user profile maps the user preference to the same features used
in the item feature matrix, thereby, mea suring the strength of preference of
the user to the features. Just like item profile vector, a user profile can be
represented in a vector fo rm in the feature space. Th e proximity of the user
and the item vector indicates the strength of preference of the items to the
users. Proximity measures like cente red cosine metric discussed in the
neighborhood based methods is used to measure the preference between auser and an item. The proximity measure between user and item is used to
provide recommendations of the item to the user. This approach is similar
to matrix factorization as both the methods express the ratings in terms ofau s e r ’s preference to a set of features and items associated to the same fea-
tures. Content-based recommenders using user profile computation differs
from matrix factorization with regards to how the features are derived.Content-based recommenders start w ith a known feature set for the items
and matrix factorization infers a set of specified features from the ratings
matrix.Table 11.8 Item Profile
MovieTom
HanksHelen
Miren ...Joel
CoenKathryn
Bigelow ... Romantic Action
Fargo 1
Forrest Gump 1
Queen 1
Sleepless in
Seattle11
Eye in the Sky 1 111.3 Content-Based Filtering 375The user profile is built from the combination of the item profile and the
known ratings matrix. Suppose R is the ratings matrix with m users and n
items. I is the item profile matrix with n items and f features or attributes.
The extracted user profile will be the matrix U with m users and exactly the
same f features from the item profile. Fig. 11.11 shows the visual representa-
tion of the matrix operation.
Consider the two matrices shown in Tables 11.9 and 11.10 . The matrix Ris
the ratings matrix with six users and five movies. This is a Boolean matrix
with 1 indicating that the user likes the movie and the blank indicating no
explicit preference for the movie. The matrix Iis the item profile with fcol-
umns, starting with the cast, ..., director, movie genre. In practice, fwill span
thousands of columns, which is a superset of all the cast members, directors,
and genres of all the movies in the catalog.
The user profile Ucan be derived in such a way than the value-shown user
profile is percent of the time that the feature appears in the movies liked by
the user. For example, Olivia likes the movies Fargo ,Queen , and Eye in the
Sky. Two-thirds of all the movies liked by Olivia have Helen Mirren in the
cast ( Queen and Eye in the Sky ). All the movies ( Forrest Gump and Sleepless in
Seattle ) liked by Josephine have Tom Hanks in the cast. The generic formula
for the cells in the user profile U is the number of times the feature appears
FIGURE 11.11
User profile from utility matrix and item profile.376 CHAPTER 11: Recommendation Enginesin the movies liked by the user divided by the number of movies liked by
the user. Table 11.11 shows the user profile U.
The user feature vector for Amelia is U5{1,0, ...,0,0, ...,0,0} and the item
feature vector for Fargo isI5{0,0, ...,1,0, ...,0,0} and Forrest Gump isI5
{1,0, ...,0,0, ...,0,0}. Out of these two item vectors, Forrest Gump is closer (in
fact, perfect match in this example) to Amelia ’s user vector and, hence, it gets
recommended.
As more becomes known about the user ’s preferences (when the user “likes”
a title), the user profile is updated. Consequently, the weights of the user-
feature matrix are updated and the latest user profile is used to compare
against all the items in the item profile. Note that in this method, unlike the
collaborative filtering approach, no information from other users is needed to
make recommendations. This feature makes the content-based recommenda-
tion system a good fit to address the cold start problem, especially when a
new item is added to the system. When a new movie title is added to the sys-
tem, the item attributes are already known a priori. Therefore, the new items
can be instantly recommended to the relevant users. However, the content-
based method needs more consistent information about each item to makeTable 11.9 Ratings Matrix R
FargoForrest
Gump QueenSleepless
in SeattleEye in the
Sky
Josephine 1 1
Olivia 1 1 1
Amelia 1
Zoe 1
Alanna
Kim 1
Table 11.10 Item Profile I
Movie Tom
HanksHelen
Mirren... Joel
CoenKathryn
Bigelow... Romantic Action
Fargo 1
Forrest
Gump1
Queen 1
Sleepless
in Seattle11
Eye in the
Sky1111.3 Content-Based Filtering 377meaningful recommendations to the users. The content-based recommenders
do not entirely address the cold start problem for new users. Some informa-
tion is still needed on the new user ’s item preferences to make a recommen-
dation for new users.
Content-Based Filtering - How to Implement
In addition to the standard ratings matrix, a content-based recommender
needs the item profile. Sourcing item attribute dataset is one more additional
data pre-processing step to be built in the data science tool for creating a
content-based recommendation engine. In RapidMiner, implementing the
content-based recommenders can be accomplished using the Recommender
extension operators.
Dataset
The same MovieLens5ratings matrix dataset used earlier in collaborative fil-
tering is used to implement content-based recommenders. There are two
datasets provided by MovieLens. The first datafile contains a ratings matrix,
with user ID, movie ID, and ratings. The ratings matrix has 100,000 ratings
given by 1000 users for 1700 titles. The movie datafile contains limited
metadata about the movie ID: title and concatenated genres. This second
dataset will serve as the item profile to build the content-based filtering.
Implementation steps
The outline of the RapidMiner process for the recommender is shown in
Fig. 11.12 . The high-level process for building a content-based recommenda-
tion engine consists of preparing the ratings matrix, preparing the item pro-
file, recommender model building, applying the model to known test
ratings, and the performance evaluation.
1.Data preparation: The modeling operator for the content-based
recommender process is item attribute k-NN which can be found underTable 11.11 User Profile U
Tom
HanksHelen
Mirren ...Joel
CoenKathryn
Bigelow ... Romantic Action
Josephine 1 1/2
Olivia 2/3 1/3
Amelia 1
Zoe 1
Alanna
Kim 1
5GroupLens.org378 CHAPTER 11: Recommendation EnginesRecommenders .Item rating prediction .Attribute-based Rating
Prediction. The inputs for the modeling operator are (1) the ratings
matrix and (2) the item profile. The ratings matrix is in the form of
User identification (user ID), Item identification (movie ID), and the
label (Rating), similar to the process used in collaborative filtering. Set
roleoperator is used to declare which attribute is the “user
identification ”,“item identification, ”or the label.
2.Item profile preparation: Content-based recommenders will need
the item profile for building the recommender model. The second
dataset from MovieLens is in the form of Movie ID, Title, and
Genres, as shown in Fig. 11.14 A .T h ec o l u m nG e n r e si sa
concatenated list of genres that a title will belong to with each
genre separated by | character. For example, the movie Toy Story
belongs to Adventure, Animation, Children, Comedy, and
Fantasy genres. However, the format of the item profile that is
needed by the Item attribute k-NN modeling operator is shown in
Fig. 11.14 C where each record has one distinct movie-genre
combination. The movie Toy Story (Movie ID 51 )i sp i v o t e di n t of i v e
rows, one for each genre (Genre 54,5,6,7,11). The item profile
dataset represented in Fig. 11.14 A would need to be transformed
into the item profile shown in Fig. 11.15 C. The following section
FIGURE 11.12
Recommender process using content-based filtering.11.3 Content-Based Filtering 379highlights the fact that the most time-consuming part of a data
science process is gathering and pro cessing data. The subprocess of
transforming the raw item profile into the one needed for the
modeling operator is shown in Fig. 11.13 A. The key steps in this
data transformation task are:
a.Text mining : The Genre attributes contain values like Adventure|
Animation|Children|Comedy|Fantasy. The text to attributes using text
mining operator transforms the concatenated values into
independent attributes shown in Fig. 11.14B . This process is
discussed in the Chapter 9, Text Mining. The key operator in the
text mining subprocess is Tokenize —to convert the genre words to
FIGURE 11.13
(A) Subprocess to create an Item profile. (B) Text mining subprocess to convert text to attributes.380 CHAPTER 11: Recommendation Engines(A)
(B)
(C)
FIGURE 11.14
(A) Item profile with concatenated genre. (B) Item profile with attributes. (C) Item profile with attribute information in rows.11.3 Content-Based Filtering 381attributes. The parameter to split the words is the character (|)
which is specified in the tokenize operator.
b.Attribute naming for De-Pivot: Before the columns can be de-pivoted
to rows, the columns should be renamed so the de-Pivot operator
can work this dataset. The columns are renamed to generic attr1,
attr2, ..., etc.
c.De-Pivot :Thede-pivot operator converts the columns information to
rows. The Genre in the column for each movie is now a distinct
row. This operator expands the row count of the table from count
(movie ID) to count [movie ID 3distinct (genre)].
d.Filtering and set role :The output of de-pivot has both the negative
and positive examples. Only the positive examples are needed, and
the negative examples can be filtered out. The Set role operator is
used to declare Movie ID as “item identification ”and Genre as
“attribute identification ”. The dataset is now suitable to be used for
the modeling operator. The attribute columns for the items are
folded into one column (Genre) which indicates all the attributes
the item possesses.
3.Recommender Modeling : The modeling operator item attribute K-NN
receives both the ratings matrix and the item profile as the inputs.
There are a few parameters to configure based on the application and
the input data
a.k:Nearest neighbor cohort size. The predicted rating is based on the
distance between the user vector and the item vector. kis set as 10
in this process.
b.Min and Max ratings: Ratings range in the dataset. It is set as 0 and
5 for minimum and maximum ratings.
4.Apply Model to test set: TheApply Model (Rating Prediction) operator
is used to apply the training model to the test dataset. A portion of
the original dataset that was reserved for testing is used as an input
for the Apply Model operator which calculates the predicted ratings
for each user-item combination in the test dataset.
5.Performance evaluation: The output of the predicted test dataset from
theApply Model operator is connected to the Performance (Rating
Prediction) operator to evaluate if the predicted rating is close to the
actual rating provided by the users.
The RapidMiner process shown in Fig. 11.12 can be saved and executed. The
result window shows the original dataset with a new prediction column
appended to the dataset. A sample of ten records are shown in Fig. 11.15 .
The result tab also has the performance vector.
The performance vector shown as the result of the performance evaluation
operator reports the RMSE and MAE of rating prediction. RMSE of the rating382 CHAPTER 11: Recommendation Enginesprediction is 0.915 stars for the neighborhood-based recommender using the
MovieLens dataset. The MAE is 0.703 stars.
11.3.2 Supervised Learning Models
A supervised learning model-based recommender approaches the problem of
user-item preference prediction at the individual user level. If a user has
expressed interest in a few items and if those items have features, then the
interest of the users to those features can be inferred. Consider the user-item
ratings matrix shown in Table 11.8 and the item profile matrix shown in
Table 11.10 . The item profile matrix can be customized just for one user ,s a y
Olivia, by introducing a new column in the item profile to indicate whether
Olivia likes the movie. This yields the item profile matrix for one user
(Olivia) shown in Table 11.12 . This matrix is strikingly similar to the training
data used in the supervised models discussed in Chapter 4, Classification and
Chapter 5, Regression Methods. With the exception of the first column (Movie
FIGURE 11.15
Predicted rating using content-based filtering.
Table 11.12 Item Profile With Class Label for One User
MovieTom
HanksHelen
Mirren ...Joel
CoenKathryn
Bigelow ... Romantic ActionClass label
for Olivia
Fargo 1 1
Forrest Gump 1 0
Queen 1 1
Sleepless in
Seattle11 0
Eye in the Sky 1 1 111.3 Content-Based Filtering 383title) which can be ignored as ID, all the attributes in the item profile table are
features or independent variables in the training set. The last column, newly
introduced indicator for one user, is the class label or the dependent variable. A
classification model can be built to generalize the relationship between the
attributes to the user preference and the resultant model can be used to predict
the preference for any unseen new items for a specific user.
The classification model, say a decision tree, can be built by learning the
attribute preferences for Olivia and the model can be applied to the catalog
for all the movies not seen by Oliva. Classification models predict user pref-
erence of the item attributes. The supervised learning model-based approach
treats recommendation tasks as a user-specific classification or regression
problem and learns a classifier for the user ’s likes and dislikes based on the
product features.
In the supervised learning model /C0based approach, each user has a personalized
decision tree. Suppose one has a straightforward preference for movies: they
only like it if the movie has Helen Mirren in the cast or is directed by the Coen
brothers. Their personalized decision tree would be like the one in Fig. 11.16
The decision tree shown in Fig. 11.16 is a classification tree for the user Oliva
using the item profile shown in Table 11.12 . For another user, the tree would
be different. For the ratings prediction problem, one could use regression mod-
els to predict the numerical ratings. Given that a user provides explicit ratings
only for a few items, the challenge is inf erring a pattern from a few positive rat-
ings for items and from thousands of attributes of those items. Complementing
the explicit ratings with the implicit outcomes (watching a movie, product
views, items in the cart) will be helpful to boost the recommendations.
FIGURE 11.16
Personalized decision tree for one user in the system.384 CHAPTER 11: Recommendation EnginesSupervised Learning Models - How to Implement
The supervised learning (classification) model approach for a content-based
recommendation engine builds a classification model for each user. Hence,
the model building is shown for one user and the process can be repeated in
the loop for each user in the ratings matrix. The implementation step closely
resembles the modeling process discussed in Chapter 4, Classification. The
rule induction modeling technique is used in this recommender implementa-
tion. It can be replaced with various other classification or regression model-
ing techniques to suit the application and the data.
Dataset
The datasets used for the process is from MovieLens database with two data-
sets. The first dataset (ratings dataset) contains ratings, user ID and movie ID
attributes. The second dataset (item dataset) contains limited metadata about
each movie —movie ID, movie title, and concatenated genre attributes. To
create the supervised learning model, these two datasets have to be merged
to one dataset that has labels and attributes for one user.
Implementation steps
The outline of the complete RapidMiner process is shown in Fig. 11.17 .
The high-level process for a model-based approach to content-based recom-
mendation engine contains: data preparation for classification modeling,
model building, apply the model to test set and performance evaluation.
1.Data Preparation: The movie dataset structure is shown in Fig. 11.18 A.
Each movie has a title and the concatenated genres separated by |
symbol. The first step is to convert this data structure to the structure
FIGURE 11.17
Classification process for one user in the system.11.3 Content-Based Filtering 385shown in Fig. 11.18 B. The structure shown in Fig. 11.18 B is conducive
for classification modeling where each genre is shown as a separate
attribute, with the value as 0 or 1 based on whether the movie is of
that genre. For example, the movie Toy story (movie id 51)is listed in
(A)
(B)
(C)
FIGURE 11.18
(A) Item profile with concatenated genre. (B) Item profile with genre as attributes. (C) Item profile with class label for the user 549.386 CHAPTER 11: Recommendation Enginesfollowing categories: Adventure, Comedy, Animation, Children, and
Fantasy.
Text mining data processing is used to convert the data structure from
Fig. 11.18 A to B. In the text mining subprocess, the genre attribute is
converted to text. Each word is tokenized from the concatenated genre
text using the delimiter |. Each token is then case converted and
stemmed to provide word vector for each movie. The text mining
subprocess is shown in Fig. 11.19 .
The movie-genre vector shown in Fig. 11.18 B, can now be merged with
the preference information of a user. The ratings matrix is filtered for
one user, user 549, using the Filter example operator. To simplify the
rating scale, the 0 /C05 scale is converted to boolean true (3 or more) or
false (below 3) using the formula in Generate Attributes . A numeric
rating label is preferable in production models because it contains
more information than the boolean true/false. The five-point rating is
converted from numeric to categorical just to illustrate a simple
classification model for this implementation. The rating information
for the user can be merged to the movie profile using the inner Join
operator. In this process, the result set will just contain the titles rated
by the user. The resultant dataset is shown in Fig. 11.18 C, where
rating_b is the preference class label and the rest of the columns are
predictors.
FIGURE 11.19
Subprocess to convert the item profile to classification training set.11.3 Content-Based Filtering 3872.Modeling: A rule induction is used for classification modeling. The
rule induction model and parameters are discussed in Chapter 4,
Classification. The parameters specified in this process are:
criterion 5information gain and sample ration 50.9. A decision tree or
logistic regression can be used as an alternative to the rule induction.
The whole process shown in Fig. 11.17 can be saved and executed. The result
window shows the model from rule induction ( Fig. 11.20 ). Ideally, this
model should have been tested with a test dataset and performance evalu-
ated. Based on the rule induction model, user 549 has a preference for the
Crime genre. This model can be run against all other movies in the catalog
which are not rated and the predicted rating can be used to recommend titles
to the user 549.
The model built using the RapidMiner process is customized for one user,
user 549. This entire process can be looped for each user and the model can
be stored in a separate dataset. The looping process is discussed in
Chapter 15 Getting started with RapidMiner. Each user in the system will
have a personalized model. The effectiveness of the model depends on each
user’s past rating and is independent of ratings from other users.
Content-based recommendation engines are better at explaining why the sys-
tem is making the recommendation because it has generalized the features the
user is interested in. For example, the system might recommend Apollo 13 ,
Saving private Ryan ,a n d Captain Phillips because the user is interested in movies
that have Tom Hanks in the cast. Unlike the collaborative filtering method,
data from other users is not needed for the content-based systems, however,
additional data from the items are essential. This feature is significant because
when a new user is introduced into the system, the recommendation engine
does not suffer from the cold start problem. When a new item is introduced,
say a new movie, the attributes of the movie are already known. Hence, the
addition of a new item or a user is quite seamless for the recommenders.
FIGURE 11.20
Rule induction model for one user in the system.388 CHAPTER 11: Recommendation EnginesThe key datasets involved in the recommendation, that is, the item profile,
user profile or classification models, and the recommendation list, can be
pre-computed. Since the main objective in many cases is finding the toprecommended items instead of filling the complete ratings matrix, decision
trees can focus only on the attributes relevant to the user.
Content-based recommenders tend to address unique preferences for the
user. For example: users interested in Scandinavian crime thrillers. There may
not be enough users who like this subgenre for collaborative filtering to be
effective because a sizable cohort of users is needed to prefer these unique
genres and other items in the catalog.
Even though rating information from other users are not necessary, an
exhaustive item profile is essential for obtaining the relevant recommenda-
tions from content-based systems. The features of the item are hard to get to
begin with and some attributes like genre taxonomy are difficult to master.Blockbuster items, are by definition, watched by a wider audience, beyond
the fanatics of a particular genre. For example, one doesn ’t have to be a fan
of science fiction to watch Avatar . Just because a user watched Avatar it
doesn ’t mean that the recommender can be inundated with other science fic-
tion movies. Special handling is required so the system doesn ’t conflate the
success of blockbusters with the user preference for specific attributes in theblockbusters.
Content-based recommenders are content specific. For example, it makes logi-
cal sense to recommend Moroccan cooking utensils if the user has shown an
interest in books focused on North African cuisine, if the ecommerce platform
offers both the categories. Content-based systems will find this task to be diffi-cult because the knowledge gained in a books category is hard to translate to a
kitchen utensils category. Collaborative filtering is content agnostic.
11.4 HYBRID RECOMMENDERS
Each recommendation engine discussed in this chapter has a central idea andsolves the problem of predicting the unseen user-item rating through a unique
approach. Each recommendation model has its own strengths and limitations,that is, each works better in specific data setting than others. Some recommen-
ders are robust at handling the cold start problem, have model biases, and tend
to overfit the training dataset. As in the case of the ensemble classifiers, hybrid
recommenders combine the model output of multiple base recommenders into
one hybrid recommender. As long as the base models are independent, this
approach limits the generalization error, improves the performance of the rec-ommender, and overcomes the limitation of a single recommendation tech-
nique. The diversity of the base models can be imparted by selecting different11.4 Hybrid Recommenders 389modeling techniques like neighborhood BMF, content-based and supervised
model-based recommenders.
Fig. 11.21 shows the RapidMiner process of Hybrid recommender with base
recommenders such as BMF, User K-NN and Item K-NN. The Model Combiner
operator is the ensemble operator which combines all three models into one
meta operator. It weighs the base models equally. All the base recommenders
operate on the same training dataset. The performance of this hybrid recom-
mender process for the MovieLens dataset is RMSE of 0.781 and MAE of
0.589. In this case, the performance of the Hybrid recommender is better than
the results achieved with neighborhood or matrix factorization alone.
11.5 CONCLUSION
How does one know if the recommendations are relevant for the users? One
would hope to see users taking up the offer of using the recommended items
FIGURE 11.21
Hybrid recommenders using model combiner.390 CHAPTER 11: Recommendation Engines(watching, purchasing, reading, ...) more than users acting on a random
selection of items. An A/B experimental testing of “with”and “without ”rec-
ommendation engines can help determine the overall impact of the recom-
mendation engine in the system.
The data used by recommendation engines is considered sensitive under
any standards because it is at the user level and indicates a user ’s prefer-
ences. A great deal of information about a person can be deduced when
one knows the books they read, movies they watch, or the products they
buy, let alone the combination of all three! ( Siegel, 2016 ) Users implicitly
entrust enterprises to safeguard their private information by implicitly or
explicitly agreeing to the terms and conditions. Users prefer personalized
experiences and they are willing to give some information about them-
selves by explicit rating information a nd implicit views or search informa-
tion. How is the need for personalization balanced with the privacy of
users? Transparency can be the enabler that strikes a balance between pri-
vacy and the benefits of personaliz ation. Users have the right to know
what inputs about them are used for recommenders: list of previously
liked titles, searches, or any other clickstream data used as an input for
recommenders. Users can inspect or re move their preference for the titles.
The explainability of recommendations increases the trust that users have
in the system. For example: one might like a list of titles because they
have watched Fargo . In some jurisdictions for certain applications, users
have the legal right to know why a recommendation or disposition is
made about a user.
Personalization can lead to filter bub ble, which is a state of intellectual
isolation ( Bozdag, 2013 ). Hyper personalized recommendation engines
keep feeding items that a user is interested in and attenuate the usage of
more diverse set of items. Personalization in news feeds causes reinforce-
ment of one ’s political leaning and results in an echo chamber of view-
points. Personalization, in a way, acts as a censor to filter out viewpoints
that the user doesn ’t explicitly prefer. The algorithms that control person-
alization, especially in news sites and social media, play a significant role
in shaping the political opinions of pe ople. Enterprises have responded to
the bias of filter bubbles by introducing the equivalent of best sellers:
trending now, top news, etc., —where a selection of un-personalized item
selections are provided to the users, in addition to the personalized
recommendations.
Summary of the Types of Recommendation Engines
See Table 11.13 for a summary of different types of recommendation
engines.11.5 Conclusion 391Table 11.13 Comparison of Recommendation Engines
Type Inputs Assumption Approach Advantages Limitations Use Cases
Collaborative
filteringRatings
matrix with
user-itempreferencesSimilar users or
items have
similar likesDerives ratings from like-
minded users (or items)
and the correspondingknown user-item
interactionsThe only input
needed is the
ratings matrix.Domain agnostic.
More accurate
than content-based filtering inmost applicationsCold start problem
for new users and
items.Sparse entries in
the rating matrix
leads to poorcoverage andrecommendations.
Computation
grows linearly withthe number ofitems and users.eCommerce,
music, new
connectionrecommendations
from Amazon,
Last.fm, Spotify,LinkedIn, andTwitter
User-based CF
(neighborhood)Ratings
matrixSimilar users rate
items similarlyFinds a cohort of users
who have providedsimilar ratings. Derives
the outcome rating from
the cohort usersUser-to-user
similarity can bepre-computed
Item-based CF
(neighborhood)Ratings
matrixUsers prefer
items that aresimilar to
previously
preferred itemsFinds a cohort of items
which have been givensimilar ratings by the
same users. Derives
rating from cohort itemsMore accurate
than user-basedCF
Latent matrix
factorizationRatings
matrixUser ’s
preference of an
item can be
better explainedby theirpreference of an
item ’s
characteristicsDecomposes the User-
Item matrix into two
matrices ( PandQ) with
latent factors. Fills theblank values in theratings matrix by dot
product of PandQWorks in sparse
matrix.
More accurate
thanneighborhood-based
collaborative
filtering.Cannot explain
why the prediction
is madeContent-based
filteringUser-item
ratingmatrix and
item profileRecommends
items similar tothose the user
liked in the pastAbstracts the features of
the item and builds anitem profile. Uses the
item profile to evaluate
the user preference forthe attributes in the itemprofileAddresses cold
start problem fornew items and
new users.
Can provideexplanations onwhy the
recommendation
is made.Requires item
profile dataset inaddition to the
ratings matrix.
Recommendersare domainspecific.
Popular items
skew the resultsMusic
recommendationfrom Pandora and
CiteSeer ’s citation
indexing
User profile
basedUser-item
rating
matrix and
item profileUser ’s
preference for an
item can be
expressed bytheir preference
for an item
attributeBuilds a user profile with
the same attributes as
the item profile.
Computes the ratingbased on similarity of
the user profile and the
item profileProvides
descriptive
recommendations.
Supervised
learning modelbasedUser-item
ratingmatrix and
Item profileEvery time a
user prefers anitem, it is a vote
of preference for
the item ’s
attributesA personalized
classification orregression model for
every single user in the
system. Learns aclassifier based on user
likes or dislikes of an
item and its relationshipwith the item attributesEvery user has a
separate modeland could be
independently
customized.References
Bobadilla, J., Ortega, F., Hernando, A., & Gutiérrez, A. (2013). Recommender systems survey.
Knowledge-Based Systems ,46, 109 /C0132. Available from https://doi.org/10.1016/J.
KNOSYS.2013.03.012 .
Bozdag, E. (2013). Bias in algorithmic filtering and personalization. Ethics and Information
Technology ,15(3), 209 /C0227. Available from https://doi.org/10.1007/s10676-013-9321-6 .
Cakir, O., & Aras, M. E. (2012). A Recommendation engine by using association rules.
Procedia /C0Social and Behavioral Sciences ,62, 452 /C0456. Available from https://doi.org/10.1016/
j.sbspro.2012.09.074 .
Das, A.S., Datar, M., Garg, A., & Rajaram, S. (2007). Google news personalization. In: Proceedings
of the 16th international conference on World Wide Web /C0WWW ’07(p. 271). New York: ACM
Press.,https://doi.org/10.1145/1242572.1242610 ..
Gemulla, R., Nijkamp, E., Haas, P.J., & Sismanis, Y. (2011). Large-scale matrix factorization with
distributed stochastic gradient descent. In: Proceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data mining /C0KDD ’11(p. 69). New York: ACM Press.
,https://doi.org/10.1145/2020408.2020426 ..
Gupta, P., Goel, A., Lin, J., Sharma, A., Wang, D., & Zadeh, R. (2013). WTF: The who to follow ser-
vice at twitter. In: Proceedings of the 22nd international conference on World Wide Web - WWW
’13(pp. 505 /C0514). New York: ACM Press. ,https://doi.org/10.1145/2488388.2488433 ..
Hu, Y., Koren, Y., & Volinsky, C. (2008). Collaborative filtering for implicit feedback datasets. In
2008 eighth IEEE international conference on data mining (pp. 263 /C0272). IEEE. ,https://doi.
org/10.1109/ICDM.2008.22 ..
Isinkaye, F. O., Folajimi, Y. O., & Ojokoh, B. A. (2015). Recommendation systems: Principles,
methods and evaluation. Egyptian Informatics Journal ,16(3), 261 /C0273. Available from
https://doi.org/10.1016/J.EIJ.2015.06.005 .
Koren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques for recommender sys-
tems. Computer ,42(8), 30 /C037. Available from https://doi.org/10.1109/MC.2009.263 .
Lee, J., Sun, M., & Lebanon, G. (2012). A comparative study of collaborative filtering algorithms , pp.
1/C027. Retrieved from ,http://arxiv.org/abs/1205.3193 ..
Mihel ˇci´c, M., Antulov-Fantulin, N., Bo ˇsnjak, M., & ˇSmuc, T. (2012). Extending RapidMiner with
recommender systems algorithms. In: RapidMiner community meeting and conference .
Schafer, J. Ben, Konstan, J. A., & Riedl, J. (2001). E-commerce recommendation applications.
Data Mining and Knowledge Discovery ,5(1/2), 115 /C0153. Available from https://doi.org/
10.1023/A:1009804230409 .
Siegel, Eric (2016). Predictive analytics: The power to predict who will click, buy, lie, or die . John Wiley
& Sons Incorporated.
Thorson, E. (2008). Changing patterns of news consumption and participation. Information,
Communication & Society ,11(4), 473 /C0489. Available from https://doi.org/10.1080/
13691180801999027 .
Underwood, C. (2017). Use cases of recommendation systems in business /C0current applications and
methods . Retrieved on June 17, 2018, From ,https://www.techemergence.com/use-cases-rec-
ommendation-systems/ ..
Weise, E. (2017). That review you wrote on Amazon? Priceless. Retrieved on October 29, 2018,
From ,https://www.usatoday.com/story/tech/news/2017/03/20/review-you-wrote-amazon-
priceless/99332602/ ..394 CHAPTER 11: Recommendation EnginesCHAPTER 12
Time Series Forecasting
Time series is a series of observations listed in the order of time. The data
points in a time series are usually recorded at constant successive time inter-
vals. Time series analysis is the process of extracting meaningful non-trivial
information and patterns from time series. Time series forecasting is the pro-
cess of predicting the future value of time series data based on past observa-
tions and other inputs. Time series forecasting is one of the oldest known
predictive analytics techniques. It is widely used in every organizational set-
ting and has deep statistical foundations. An example of a time series is
shown in Fig. 12.1 . It shows the time series value and period —monthly reve-
nue of a product for a few years.
Up to this point in this book, supervised model building has been about
collecting data from several different attributes of a system and using these
attributes to fit a function to predict a desired quant ity or target variable.
For example, if the system was a housing market, the attributes may have
been the price of a house, location, square footage, number of rooms,
number of floors, age of the house, and so on. A multiple linear regres-
sion model or a neural network model could be built to predict the house
price (target) variable given the ind ependent (predictor) variables.
Similarly, purchasing managers may use data from several different raw
material commodity prices to model the cost of a product. The common
thread among these predictive models is that predictors or independent
variables that potentially influenc e a target (house price or product cost)
are used to predict that target variable. The objective in time series fore-
casting is slightly different: to use hi storical information about a particular
quantity to make forecasts about the value of the same quantity in the
future.
In general, there are two important differences between time series analysis
and other supervised predictive mo dels. First, time is an important
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00012-5
©2019 Elsevier Inc. All rights reserved.395predictor in many applications. In time series analysis one is concerned
with forecasting a specific variable, given that it is known how this
variable has changed over time in the past. In all other predictive
models discussed so far, the time component of the data was either
ignored or was not available. Such data are known as cross-sectional data.
Consider the problem of predicting the house price based on location,
square footage, number of rooms, number of floors, and age of the house.
The predictors are observed in a point in time (like a cross-section of a
block of wood in Fig. 12.2 ) and the price is predicted in the same point
in time. However, it is important to take the time variable (length of
wood) to predict something like house prices. It fluctuates up and down
based on economic conditions, supply and demand, etc., which are all
influenced by time.
1Total monthly scripts for pharmaceutical products falling under ATC code A10, as recorded by the
Australian Health Insurance Commission. July 1991 /C0June 2008 ( Hyndman & Athanasopoulos, 2018 ).
FIGURE 12.1
Time series of monthly antidiabetic drug sales.1396 CHAPTER 12: Time Series ForecastingSecond, one may not be interested in or might not even have data for other
attributes that could potentially influence the target variable. In other words,
independent or predictor variables are not strictly necessary for univariate
time series forecasting but are strongly recommended for multivariate
time series.
TAXONOMY OF TIME SERIES FORECASTING
The investigation of time series can also be broadly divided into descriptive
modeling, called time series analysis, and predictive modeling, called time
series forecasting. Time Series foreca sting can be further classified into
four broad categories of techniques: Forecasting based on time series
decomposition, smoothing based techniques, regression based techniques,
and machine learning-based techniques. Fig. 12.3 shows the classification
of time series forecasting techniques.
Time series decomposition is the process of deconstructing a time series
into the number of constituent components with each representing an
underlying phenomenon. Decomposition splits the time series into a
trend component, a seasonal component, and a noise component. The
trend and seasonality components are predictable (and are called system-
atic components), whereas, the noise, by definition, is random (and is
called the non-systematic component). Before forecasting the time series,
it is important to understand and describe the components that make the
time series. These individual components can be better forecasted using
regression or similar techniques and combined together as an aggregated
FIGURE 12.2
Cross-sectional data is a subset of time series data.Taxonomy of Time Series Forecasting 397forecasted time series. This technique is called forecasting with
decomposition.
Time series can be thought as past observations informing future predictions.
To forecast future data, one can smooth past observations and project it to
the future. Such time series forecasting methods are called smoothing basedforecasting methods. In smoothing methods, the future value of the time
series is the weighted average of past observations.
Regression based forecasting techniques are similar to conventional super-
vised predictive models, which have independent and dependent variables,
but with a twist: the independent variable is now time. The simplest of such
methods is of course a linear regression model of the form:
yt5a3t1b ð12:1Þ
where ytis the value of the target variable at time t. Given a training set, the
values of coefficients aand bcan be estimated to forecast future yvalues.
Regression based techniques can get pretty complicated with the type of
function used to model the relationship between future value and time.Commonly used functions are exponential, polynomial, and power law functions.
FIGURE 12.3
Taxonomy of time series forecasting techniques.398 CHAPTER 12: Time Series ForecastingMost people are familiar with the trend line function in spreadsheet pro-
grams, which offer several different function choices. The regression based
time series forecast differs from a regular function-fitting predictive model in
the choice of the independent variable. A more sophisticated technique is
based on the concept of autocorrelation. Autocorrelation refers to the fact
that data from adjacent time periods are correlated in a time series. The most
well-known among these techniques is ARIMA, which stands for Auto
Regressive Integrated Moving Average.
Any supervised classification or regression predictive models can be used
to forecast the time series too, if the time series data are transformed to a
particular format with a target label and input variables. This class of
techniques are based on supervised machine learning models where the
input variables are derived from the time series using a windowing tech-
nique. The windowing technique transforms a time series to a cross-
sectional like dataset where the input variables are lagged data points for
an observation. The artificial neural net work-based time series forecasting
has particular relevance because of its resemblance with the ARIMA
technique.
FORECASTING DEMAND OF A PRODUCT
A common application of a time series is to forecast the
demand for a product. A manufacturing company
makes anti-corrosion wax tapes for use in gas and oil
pipelines. The company ma kes more than a dozen vari-
eties of wax tape products using a handful of assembly
lines. The demand for these products varies depending
on several factors. For example, routine pipeline main-
tenance is typically done during warm weather seasons.
So, there could be a seasonal spike in the demand.
Also, over the last severa ly e a r s ,g r o w t hi ne m e r g i n g
economies has meant that the demand for their pro-
ducts has been growing. Fina lly, any upcoming changes
in pricing, which the company may announce ahead of
time, may also trigger stockpiling by their customers,
resulting in sudden jumps in demand. So, there can be
both trend and seasonality factors, as shown in a sam-
ple series Fig. 12.4 .The product manager of the product needs to be able to
predict the demand of their products on a monthly, quar-
terly, and annual basis so that they can plan the produc-
tion using their limited resources and their department ’s
budget. They make use of the time series forecasting
models to predict the potential demand for each of their
product lines. By studying the seasonal patterns and
growth trends, they can better prepare their production
lines. For example, studying seasonality in the sales for
the #2 wax tape, which is heavily used in cold climates,
reveals that March and April are the months with the
highest number of orders placed as customers buy them
ahead of the maintenance seasons starting in the summer
months. So, the plant manager can dedicate most of their
production lines to manufacturing the #2 tape during
these months. This insight would not be known unless a
time series analysis and forecasting was performed.
(Continued )Taxonomy of Time Series Forecasting 39912.1 TIME SERIES DECOMPOSITION
Time series data, are univariate, an amalgamation of multiple underlying
phenomenon. Consider the example shown in Fig. 12.1 .I ti sat i m e
series of monthly antidiabetic drug s ales. A few observations can be made
about this time series. Firstly, the ove rall drug sales is trending upward and
the upward trend accelerates in the 2000s . Secondly, there is clear seasonal-
ity in the time series of drug sales. In particular, it is a yearly seasonality.
T h e r ei sas p i k ei nd r u gs a l e sa tt h es t a r to ft h ey e a ra n dad i pi ne v e r y
February. This seasonal variation is consistent every year. However, even
when accounting for the trend and the seasonal variations there is
one more phenomenon that could not be explained. For example, the
pattern in 2007 is odd when compared with prior years or 2008.(Continued)
FIGURE 12.4
A time series analysis can reveal trends and seasonal patterns.400 CHAPTER 12: Time Series ForecastingThis unattributable phenomenon can be assumed as noise in the time
series. Now that an intuitive underst anding of the underlying phenomena
of the time series has been developed, the formal definitions of different
components that make up a time series can be discussed.
Trend: Trend is the long-term tendency of the data. It represents change
from one period to next. If a trend line has been inserted in over a chart
in a spreadsheet, it is the regression equation line that represents the
trend component of a time series. The trends can be further split into a
zero-basis trend and the level in the data. The level in the time series does
not change with respect to time while the zero-basis trend does change with
the time.
Seasonality: Seasonality is the repetitive behavior during a cycle of time.
These are repeated patterns appearing over and over again in the time series.
Seasonality can be further split into hourly, daily, weekly, monthly, quarterly,
and yearly seasonality. Consider the revenue generated by an online finance
portal, like Yahoo Finance or Morningstar. The visits would clearly indicate a
daily seasonality with the difference in the number of people accessing the
portal or the app during the day time, especially during the market open
hours, and the nighttime. Weekdays will have higher traffic (and revenue)
than during the weekends. Moreover, the online advertising spend shows
quarterly seasonality as the advertisers adjust marketing spend during the
end of the quarter. The revenue will also show yearly seasonality where end
of the year, after Christmas, shows weakness in revenue because of holidays.
Cycle: Cyclic component represents longer-than-a-year patterns where there is
no specific time frames between the cycles. An example here is the economic
cycle of booms and crashes. While the booms and crashes exhibit a repeated
pattern, the length of a boom period, the length of a recession, the time
between subsequent booms and crashes (and even two consecutive crashes —
double dip) is uncertain and random, unlike the seasonality components.
Fig. 12.1 , on the antidiabetic drug sales, shows an accelerated upward trend
after the year 2000 which may represent a cyclic component or a non-linear
trend. It is hard to conclude anything in this time series because of the lim-
ited time frame in the dataset.
Noise: In a time series, anything that is not represented by level, trend, sea-
sonality, or cyclic component is the noise in the time series. The noise com-
ponent is unpredictable but follows normal distribution in ideal cases. All
the time series datasets will have noise. Fig. 12.5 shows the decomposition
of quarterly production data, into trend, seasonality, and noise components.
T h et r e n da n ds e a s o n a l i t ya r et h es y s t e m a t i cc o m p o n e n t so fat i m es e r i e s .
Systematic components can be forecasted. It is impossible to forecast12.1 Time Series Decomposition 401noise —the non-systematic component of a time series. In the time series
decomposition can be classified into additive decomposition and multiplicative
decomposition , based on the nature of the different components and how
they are composed. In an additive decomposition, the components are
decomposed in such a way that when they are added together, the original
time series can be obtained.
Time series 5Trend1Seasonality 1Noise
In the case of multiplicative decomposition the components are decomposed
in the such a way that when they are multiplied together, the original time
series can be derived back.
Time series 5Trend3Seasonality 3Noise
Both additive and multiplicative time series decompostions can be repre-
sented by these equations:
yt5Tt1St1Et ð12:2Þ
yt5Tt3St3Et ð12:3Þ
where Tt,St, and Etare trend, seasonal, and error components respectively.
The original time series ytis just an additive or multiplicative combination of
components. If the magnitude of the seasonal fluctuation or the variation in
trend changes with the level of the time series, then multiplicative time series
decomposition is the better model. The antidiabetic drug sales time series
from Fig. 12.1 shows increasing seasonal fluctuation and exponential rise in
the long-term trend. Hence, a multiplicative model is more appropriate.
Fig. 12.6 shows both the additive and multiplicative decomposition of the
FIGURE 12.5
Decomposition of time series.402 CHAPTER 12: Time Series Forecastingantidiabetic drug sales time series. Note the scale of the seasonal component
is different for both decompositions. The noise in the additive decomposi-
tion is higher post 2005, as it ineffectively struggles to represent increases in
the seasonal variation and trend changes. The noise is much smaller in the
multiplicative decomposition.
To decompose the time series data to its individual components, there are a
few different techniques available. In this section, some common approaches
to describe time series data will be reviewed and how the knowledge can be
used for forecasting time series to future time periods.
12.1.1 Classical Decomposition
The classical decomposition technique is simple, intuitive, and serves as a
baseline of all other advanced decomposition methods. Suppose the time
series has yearly seasonality with monthly data as shown in Fig. 12.1 .m
represents seasonal period, which is 12 for monthly data with yearly season-
ality. The classical decomposition technique first estimates the trend compo-
nent by calculating the long term (say 12 month) moving average. The trend
component is removed from the time series to get remaining seasonal and
noise components. The seasonal component can be estimated by average
Jan, Feb, Mar, ..., variance in the remaining series. Once the trend and sea-
sonal components are removed, what is left is noise. The algorithm for classic
additive decomposition is:
1.Estimate the trend Tt:I fmis even, calculate 2 3mmoving average
(m-MA and then a 2-MA); if mis odd, calculate m-moving average. A
moving average is the average of last m data points.
FIGURE 12.6
Additive and multiplicative decomposition of time series.12.1 Time Series Decomposition 4032.Calculate detrended series: Calculate yt2Ttfor each data point in the
series.
3.Estimate the seasonal component St:a v e r a g e( yt2Tt)f o re a c h m
period. For example, calculate the average of all January values of
(yt2Tt) and repeat for all the months. Normalize seasonal value in
such a way that mean is zero.
4.Calculate the noise component Et:Et5(yt2Tt2St) for each data
point in the series.
Multiplicative decomposition is similar to additive decomposition: Replace
subtraction with division in the algorithm described. An alternative approach
is to convert multiplicative decomposition into additive by applying the log-
arithmic function on both the sides of Eq. (12.3) .
12.1.2 How to Implement
The decomposition algorithm is quite simple, and can be implemented in a
spreadsheet. A practical example of a time series additive decomposition
using RapidMiner will be briefly described. The process shown in Fig. 12.7
has simple data pre-processing operators to estimate the trend, seasonal, and
error components. The data used in this process is the quarterly Australian
beer production dataset2which can be found on the companion website
www.IntroDataScience.com . Steps to convert the time series into the compo-
nents are:
GTrend : Since the data show four quarter seasonality, a four period
moving average operator and a two-period moving average operator is
used to estimate the trend component.
GSeasonality : The Generate attribute operator is used to calculate the
detrended series by finding the difference between the time series and
the 234 month moving average values. The same operator also
extracts quarter value (Q1, Q2, Q3, Q4) from the Year (e.g., 1996 Q1)
attribute. The detrended series has seasonality and noise. To calculate
the seasonality, the quarter values have to be averaged using the
Aggregate operator. This gives seasonality values for one year. The Join
operator is used to repeat the seasonal component for every Q1, Q2,
Q3 and Q4 records.
GNoise : Noise is calculated by the difference of the time series with the
combination of trend and seasonality.
The result of the process is shown in Fig. 12.8 . The initial time series is
decomposed into its components. Note that the noise is randomly
2Total quarterly beer production in Australia from 1956 to 2010. Australian Bureau of Statistics. Cat.
8301.0.55.001 ( Hyndman & Athanasopoulos, 2018 ).404 CHAPTER 12: Time Series Forecastingdistributed with the mean as zero. Even though the classical decomposition
is straight forward, it has some serious limitations. Classical decomposition
assumes the occurrence of the same seasonal pattern throughout the entire
time series. That is too constrictive of an assumption for practical use cases.As the 2 3mmoving average is used, the first m/2 and the last m/2 data
points are not used in the modeling. Moreover, classical decomposition is
inept at handling anomalies in the data.
There are quite a range of advanced time series decomposition techniques.
STL (Seasonal and Trend decomposition using Loess), SEATS (Seasonal
Extraction in ARIMA Time Series), and X11 (from US Census Bureau) aresome of the advanced decomposition techniques. All these methods have
additional steps to deal with the limitations of the classical decomposition
technique, particularly to deal with the change in the seasonality, decompos-ing quarterly, monthly, weekly, and daily seasonality, and handling changes
in the trend.
FIGURE 12.7
Process for time series decomposition.12.1 Time Series Decomposition 405Forecasting Using Decomposed Data
While decomposing the time series is used to describe and increase the
understanding of time series data, it is useful for forecasting as well. The idea
is to breakdown the time series into its parts, forecast the parts, and put itback together for forecasted future time series values. Why? Forecasting the
time series through their components is a much easier task.
^yt5^St1^Tt ð12:4Þ
It is assumed that the seasonal component of the time series does
not change. Hence, the forecast of the seasonal component is the same
a st h ev a l u e se x t r a c t e df r o mt h et i m es e r i e s .T h et i m es e r i e sd a t aw i t h o u t
FIGURE 12.8
Time series and decomposed data.406 CHAPTER 12: Time Series Forecastingthe seasonal component is called seasonally adjusted time series. It
contains just the trend component and noise in the data. The seasonally
adjusted time series can be forecasted by relatively easier methods: linear or
polynomial regression, Holt ’s method, or ARIMA. For example, the trend
components in Figs. 12.6 and 12.8 can be extended using linear regression.
Noise is normally distributed wit h the mean as zero. Hence, it is not
forecasted. The time series foreca st for the future value is the sum of
the seasonal forecast and the seasonally adjusted forecast of the trend
component.
12.2 SMOOTHING BASED METHODS
In the smoothing based approaches, an observation is a function of past few
observations. It is helpful to start out with a basic notation system for time
series in order to understand the different smoothing methodologies.
GTime periods: t 51, 2, 3, ...,n. Time periods can be seconds, days,
weeks, months, or years depending on the problem.
GData series: Observation corresponding to each time period above: y1,
y2,y3,...yn.
GForecasts :Fn1his the forecast for the hthtime period following n.
Usually h51, the next time period following the last data point.
However hcan be greater than 1. his called the horizon .
GForecast errors :et5yt2Ftfor any given time, t.
In order to explain the different methods, a simple time series data function
will be used, Y(t).Yis the observed value of the time series at any time t.
Furthermore, the observations are made at a constant time interval. If in the
case of intermittent data, one can assume that an interpolation scheme is
applied to obtain equally spaced (in time) data points.
12.2.1 Simple Forecasting Methods
Naïve Method
Probably the simplest forecasting “model. ”Here one simply assumes that
Fn11, the forecast for the next period in the series, is given by the last data
point of the series, yn
Fn115yn ð12:5Þ
Seasonal Naive Method
If the time series is seasonal, one can forecast better than the naive point esti-
mate. The forecast can assume the value as the previous value of the same12.2 Smoothing Based Methods 407season. For example, the next January revenue can be assumed as the last
known January revenue.
Fn115yn2s ð12:6Þ
where sis the seasonal period. In the case of monthly data with yearly sea-
sonality, seasonal period is 12.
Average Method
Moving up a level, one could compute the next data point as an average ofall the data points in the series. In other words, this model calculates theforecasted value, F
n11,a s :
Fn115Average yn;yn21;yn22;...;y1 ðÞ ð 12:7Þ
Suppose one has monthly data from January 2010 to December 2010 and
they want to predict the next January 2011 value, they would simply average
the values from January 2010 to December 2010.
Moving Average Smoothing
The obvious problem with a simple average is figuring out how many pointsto use in the average calculation. As the observational data grows (as nincreases), should one still use all the n time periods to compute the next
forecast? To overcome this problem, one can select a window of the last “k”
periods to calculate the average, and as the actual data grows over time, onecan always take the last k samples to average, that is, n,n21,...,n2k11.
In other words, the window for averaging keeps moving forward and, thus,
returns a moving average. Suppose in the simple example of the windowk53; then to predict the January 2021 data, a three-month average would be
taken using the last three months. When the actual data from January comes
in, the February 2021 value is forecasted using January 2021 ( n), December
2020 ( n21) and November 2020 ( n2311o r n22). This model will
result in problems when there is seasonality in the data (e.g., in December for
retail or in January for healthcare insurance), which can skew the average.Moving average smoothens the seasonal information in the time series.
Fn115Average yn;yn21;...;yn2k ðÞ ð 12:8Þ
Weighted Moving Average Smoothing
For some cases, the most recent value could have more influence than some
of the earlier values. Most exponential growth occurs due to this simpleeffect. The forecast for the next period is given by the model:
Fn115a3yn1b3yn211c3yn2k ðÞ =a1b1c ðÞ ð 12:9Þ408 CHAPTER 12: Time Series Forecastingwhere typically a.b.c.Fig. 12.9 compares the forecast results for the sim-
ple time series introduced earlier. Note that all of the mentioned methods
are able to make only one-step-ahead forecasts due to the nature of their for-
mulation. The coefficients a, b, and c may be arbitrary, but are usually based
on some previous knowledge of the time series.
12.2.2 Exponential Smoothing
Exponential smoothing is the weighted average of the past data, with the
recent data points given more weight than earlier data points. The weights
decay exponentially towards the earlier data points, hence, the name. The
exponential smoothing is given by the equation 12.10 .
Fn115αyn1α12α ðÞ yn211α12α ðÞ2yn221?: ð12:10Þ
αis generally between 0 and 1. Note that α51 returns the naïve forecast of
Eq. (12.5) . As seen in the charts in Fig. 12.10 , using a higher αresults in put-
ting more weight on actual values and the resulting curve is closer to the
actual curve, but using a lower αresults in putting more emphasis on
FIGURE 12.9
Comparing one-step-ahead forecasts for basic smoothing methods.12.2 Smoothing Based Methods 409previously forecasted values and results in a smoother but less accurate fit.
Typical values for αrange from 0.2 to 0.4 in practice.
To forecast the future values using exponential smoothing, Eq. (12.10) can
be rewritten as:
Fn115α3yn112α ðÞ 3Fn ð12:11Þ
Eq. (12.11) is more useful because it involves both actual value ynand fore-
casted value Fn. A higher αvalue gives an exact fit and a lower value gives a
smoother fit. Going back to the monthly example, if one wanted to make
the February 2011 forecast using not only the actual January 2011 value but
also the previously forecasted January 2011 value, the new forecast would
have “learnt ”the data better. This is the concept behind basic exponential
smoothing ( Brown, 1956 ).
FIGURE 12.10
Fitted time series —exponential smoothing with different αlevels.410 CHAPTER 12: Time Series ForecastingThis simple exponential smoothing is the basis for a number of common
smoothing based forecasting methods. However, the model is suited only for
time series without clear trend or seasonality. The smoothing model has only
one parameter, α, and can help smooth the data in a time series so that it is
easy to extrapolate and make forecasts. Like many methods discussed earlier,
the forecast will be flat that is, there is no trend or seasonality factored in.
Only the level is forecasted. Also, if Eq. (12.10) were examined, one would
see that forecasts cannot be made more than one-step ahead, because to
make a forecast for step ( n11), the data for the previous step, n, is needed.
It is not possible to make forecasts several steps ahead, that is, ( n1h), using
the methods described (where it was simply assumed that Fn1h5Fn11),
where his the horizon. This obviously has limited utility. For making longer
horizon forecasts, that is, where hc1, the trend and seasonality information
also needs to be considered. Once trend and seasonality are captured, one
can forecast the value for any time in the future, not just the values for one
step ahead.
Holt ’s Two-Parameter Exponential Smoothing
Anyone who has used a spreadsheet for creating trend lines on scatter plots
intuitively knows what a trend means. A trend is an averaged long-term ten-
dency of a time series. The simplified exponential smoothing model
described earlier is not particularly effective at capturing trends. An extension
of this technique, called Holt ’s two-parameter exponential smoothing, is
needed to accomplish this.
Recall that exponential smoothing [ Eq. (12.10) ] simply calculates the average
value of the time series at n11. If the series also has a trend, then an aver-
age slope of the series needs to be estimated as well. This is what Holt ’s two-
parameter smoothing does by means of another parameter, β. A smoothing
equation similar to Eq. (12.10) is constructed for the average trend at n11.
With two parameters, αandβ, any time series with a trend can be modeled
and, therefore, forecasted. The forecast can be expressed as a sum of these
two components, average value or “level ”of the series, Ln, and trend, Tn,
recursively as:
Fn115Ln1Tn ð12:12Þ
where,
Ln5α3yn11/C0α ðÞ 3ðLn/C011Tn/C01Þand Tn5β3ðLn/C0Ln/C01Þ11/C0β ðÞ 3Tn/C01
ð12:13Þ
To make future a forecast over an horizon, one can modify Equation 12.12 to:
Fn1h5Ln1h3Tn ð12:14Þ12.2 Smoothing Based Methods 411The values of the parameter can be estimated based on the best fit with the
training (past) data.
Holt-Winters ’Three-Parameter Exponential Smoothing
When a time series contains seasonality in addition to a trend, yet another
parameter, γ, will be needed to estimate the seasonal component of the time
series ( Winters, 1960 ). The estimates for value (or level) are now adjusted by
a seasonal index, which is computed with a third equation that includes γ.
(Shmueli, 2011; Hyndman, 2014; Box, 2008 ).
Ft1h5Lt1hTt ðÞ St1h/C0p ð12:15Þ
Lt5αyt=St2p112α ðÞ Lt211Tt21 ðÞ ð 12:16Þ
Tt5βLt2Lt21 ðÞ 112β ðÞ Tt21 ð12:17Þ
St5γyt=Lt/C0/C1
112γ ðÞ St2p ð12:18Þ
where pis the seasonality period. One can estimate the value of the para-
meters α,β, and γfrom the fitting of the smoothing equation with the train-
ing data.
12.2.3 How to Implement
Holt-Winters ’three parameter smoothing provides a good framework to
forecast time series data with level, trend, and seasonality, as long as
the seasonal period is well defined . One can implement the time series
forecasting model in RapidM iner using R extension. R3is a powerful,
widely used statistical tool and this use case warrants the integration of
RapidMiner with R functionality. Th e key operator in this process is
Execute R which accepts the data as the input, processes the data, and out-
puts the data frame from R.
The data used in the process is the Australian Beer Production
time series dataset used in prior time series process. After a bit of data
pre-processing, like renaming the attr ibutes and selecting the production
data, R operator is invoked for Holt and Holt Winters ’forecasting function
(Fig. 12.11 ).
The Execute R operator executes the R script entered in the operator. The R
script for the Holt-Winters ’forecasting is shown below. It uses the forecast
package library and invokes the forecasting function holt(inp, h510) for
Holts and hw(inp, h510) for Holt-Winters ’forecasting. The data frame
(dataset in R) is returned to RapidMiner process.
3https://www.r-project.org/ .412 CHAPTER 12: Time Series ForecastingR Script for Holt-Winters ’Forecasting
rm_main 5function(data)
{
library(forecast)
inp,- ts(data, freq 54)
y,- holt(inp, h 510) # or hw(inp, h 510) for Holt Winters ’smoothing
df,- as.data.frame(y)
return(list(df))
}
The output from the Execute R operator has point forecasts and the interval
forecasts. For this implementation, the point forecast is selected using Select
Attributes operator. The forecasted dataset is appended with the original data-
set for better visualization using Generate Attribute and Append operators.
Fig. 12.12 shows the forecasted result of both Holt ’s and Holt-Winters ’expo-
nential smoothing based forecast. Note that in Holt ’s forecast, only trend
and level are forecasted and in Holt Winters ’forecasting both trend and sea-
sonality are forecasted.
12.3 REGRESSION BASED METHODS
In the regression based methods, the variable time is the predictor or
independent variable and the time series value is the dependent
FIGURE 12.11
Holts and Holt-Winters ’forecasting using R.12.3 Regression Based Methods 413variable. Regression based methods are generally preferable when the
time series appears to have a globa l pattern. The idea is that the
model parameters will be able to captu re these patterns and, thus, enable
one to make predictions for any step ahead in the future under the assump-
tion that this pattern is going to be repeated. For a time series with local
patterns instead of a global pattern , using a regression based approach
requires one to specify how and when the patterns change, which is diffi-
cult. For such a series, smoothing approaches work best because these
methods usually rely on extrapolating the most recent local pattern as seen
earlier.
Fig. 12.13 shows two time series: Fig. 12.13 A shows antidiabetic drug reve-
nue and Fig. 12.13 B shows the economy class passenger volume in Sydney-
Melbourne4route. A regression based forecasting method would work well
for the antidiabetic drug revenue series because it has a global pattern.
However the passenger volume series shows no clear start or end for any pat-
terns. It is preferable to use smoothing based methods to attempt to forecast
this second series.
FIGURE 12.12
Holt’s and Holt-Winters ’smoothing.
4Ansett Airlines (defunct). The dip shows the industrial dispute in 1989. FPP2 package in R.414 CHAPTER 12: Time Series Forecasting12.3.1 Regression
The simplest of the regression based approaches for analyzing a time series is
using linear regression. As mentioned in the introduction to the chapter, one
assumes the time period is the independent variable and attempts to predict
the time series value using this. For the Australian beer dataset used so far,
the chart in Fig. 12.14 shows a linear regression fit. As can be seen, the linear
regression model is able to capture the long-term tendency of the series, but
it does a poor job of fitting the data.
Sophisticated polynomial functions can be used to improve the fit.
Polynomial regression is similar to linear regression except that higher-degree
functions of the independent variable are used (squares and cubes on the
time variable). Since the global trend here is straight decline, it is difficult to
argue that the cubic polynomial does a significantly better job. However in
either of these cases, one is not limited to a one-step-ahead forecast of the
simple smoothing methods.
12.3.2 Regression With Seasonality
The linear regression trend-fitting model can be significantly improved by
simply accounting for seasonality. This is done by introducing seasonal
dummy variables for each period (quarter), of the series, which triggers either
1 or 0 in the attribute values as seen in Fig. 12.15 . In this example, four new
attributes are added to the original dataset to indicate which quarter the
record belongs to. The attribute value of (Quarter 5Q1) is turned 1 if the
quarter is Q1 and so on.
FIGURE 12.13
Global and local patterns. (A) Antidiabetic drug revenue - global pattern and (B) Airline economy class passenger volume between
Sydney and Melbourne - local pattern.12.3 Regression Based Methods 415Just this trivial addition to the predictors of the linear regression model can
yield a surprisingly good fit in most datasets with clear seasonality. Although
the model equation may appear a bit complicated, in reality it is just a linearregression model with four variables: the time period and four dummy vari-
ables for each quarter of a year. The independent variable time captures the
level and the long-term trend. The four dummy seasonal variables capturethe seasonality. This regression equation can be used for predicting any
future value beyond n11, and thus, has significantly more utility than the
simpler counterparts in the smoothing side.
Forecast 5442 :18921:1323time227:2683Quarter 5Q2 ðÞ
216:3363Quarter 5Q3 ðÞ 192:8823Quarter 5Q4 ðÞ
There is of course no reason to use linear regression alone to capture both
trend and seasonality. More sophisticated models can easily be built using
polynomial equations along with the sine and cosine function to model
seasonality.
FIGURE 12.14
Linear regression model.416 CHAPTER 12: Time Series ForecastingHow to implement
The implementation process for seasonal linear regression is similar to the
linear regression model discussed in Chapter 5, Regression Methods. The
additional step is to set up data pre-processing to add seasonal dummy vari-
ables. Fig. 12.16 shows the complete RapidMiner process for seasonal linear
regression for the Australian beer production dataset. The steps in building
the forecasting model are:
1.Add time attribute : After the dataset is read, a new attribute “time”is
generated to indicate the consecutive time period of each example
record. First record “1992 Q1 ”is tagged as 1 and the next as 2, and so
on. This is essential in all regression based forecasting because time is the
independent variable and will be part of the regression equation. It is
important to sort the dataset before adding the sequential time attribute.
2.Extract seasonal attribute : The next step is to extract “Q1”from the
“1992 Q1 ”attribute. It can be achieved with Generate attribute and cut
() function to extract Q1 from the text. If the time series has weekly or
monthly seasonality, corresponding identifiers like weekday or month
id have to be extracted.
3.Generation of independent seasonal attributes :The quarter attribute has to
be pivoted to “Is Q1 ”,“Is Q2 ”,...attributes. The Nominal to Numerical
conversion operator is used to generate these attributes. The value of
the attribute is either 1 or 0, depending on the record ’s seasonal
period.
FIGURE 12.15
Seasonal attributes.12.3 Regression Based Methods 4174.Modeling : A linear regression model is used to fit the equation with the
training data. A polynomial regression learner can be used for a better fit.
5.Forecasting : The dataset also contains the placeholders for future
periods. The Apply model operator is used to visualize the fit of the
model for the training period and to forecast the future horizon.
The process can be saved and executed. The result window shows the time
series of both actual and forecasted data. As shown in Fig. 12.17 , the sea-
sonal linear regression forecasting has done a good job of fitting the model
for the past data and projected the seasonality for the future horizon.
Contrast this result with Fig. 12.14 where the linear regression model cap-
tures only the level and long-term trend.
12.3.3 Autoregressive Integrated Moving Average
ARIMA stands for Auto regressive Integrated Moving Average model and is
one of the most popular models for time series forecasting. The ARIMA
methodology originally developed by Box and Jenkins in the 1970s
FIGURE 12.16
Process for seasonal linear regression.418 CHAPTER 12: Time Series Forecasting(Box, 1970 ). Even though the steps of training an ARIMA model are more
involved, the implementation is relatively straightforward using statistical
packages that support ARIMA functions. In this section, the concepts of auto-
correlation, autoregression, stationary data, differentiation, and moving aver-
age of error are introduced, which are used to increase the understanding on
time series and serve as the building blocks of ARIMA models.
Autocorrelation
Correlation measures how two variables are dependent on each other or if
they have a linear relationship with each other. Consider the time series
shown in Fig. 12.18 . The second column “prod ”shows the data for the sim-
ple time series. In the third column, data are lagged by one step. 1992 Q1
data is shown in 1992 Q2. This new series of values is termed a “1-lag ”
series. There are an additional 2-lag, 3-lag, ...,n-lag series in the dataset.
Notice that there is a strong correlation between the original time series
“prod ”and 4-lag “prod-4. ”They tend to move together. This phenomenon is
FIGURE 12.17
Forecasting using seasonal linear regression.12.3 Regression Based Methods 419called autocorrelation, where the time series is correlated with its own data
points, with a lag.
As in a multivariate correlation matrix (Chapter 3: Data Exploration), one
can measure the strength of correlation between the original time series and
all the lag series. The plot of the resultant correlation matrix is called an
Autocorrelation Function (ACF) chart. The ACF chart is used to study all the
available seasonality in the time series. From Fig. 12.19 it can be concluded
that the time series is correlated with the 4th, 8th, and 12th lagged quarter
due to the yearly seasonality. It is also evident than Q1 is negatively corre-
lated with Q2 and Q3.
Autoregressive Models
Autoregressive models are regression models applied on lag series generated
using the original time series. Recall in multiple linear regression, the output
is a linear combination of multiple input variables. In the case of autoregres-
sion models, the output is the future data point and it can be expressed as a
linear combination for past pdata points. pis the lag window. The autore-
gressive model can be denoted as the equation:
yt5l1α1yt211α2yt221?1αpyt2p1e ð12:19Þ
FIGURE 12.18
Lag series and autocorrelation.420 CHAPTER 12: Time Series Forecastingwhere, lis the level in the dataset and eis the noise. αare the coefficients
that need to be learned from the data. This can be referred to as an autore-
gressive model with p lags or an AR(p) model. In an AR( p) model, lag series
is a new predictor used to fit the dependent variable, which is still the origi-
nal series value, Yt.
Stationary Data
In a time series with trends or seasonality, the value is affected by time
(hence, one would be interested in this subject). A time series is called sta-
tionary when the value of time series is not dependent on time. For instance,
random white noise is a stationary time series. Daily temperature at a loca-
tion is not stationary as there will be a seasonal trend and it is affected by
time. Meanwhile, the noise component of a time series is stationary.
Stationary time series do not have any means of being forecasted as they are
completely random. Fig. 12.20 A is an example of nonstationary data because
FIGURE 12.19
ACF chart. ACF, Autocorrelation Function.12.3 Regression Based Methods 421of the presence of both trend and seasonality and Fig. 12.20B is an example
of stationary data because there is no clear trend or seasonality.
Differencing
A non-stationary time series can be converted to a stationary time series
through a technique called differencing. Differencing series is the change
between consecutive data points in the series.
y0
t5yt2yt21 ð12:20Þ
This is called first order differencing. Fig. 12.20 shows a time series and a first
order differenced time series. In some cases, just differencing once will still
yield a nonstationary time series. In that case a second order differencing is
required. Second order differencing is the change between two consecutive
data points in a first order differenced time series. To generalize, differencing
of order dis used to convert nonstationary time series to stationary time
series.
Seasonal differencing is the change between the same period in two different
seasons. Assume a season has period, m.
y0
t5yt2yt2m ð12:21Þ
This is similar to the Year-over-Year metric used commonly in business finan-
cial reports. It is also called as m-lag first order differencing. Fig. 12.21 shows
the seasonal differencing of the Australian Beer production dataset and the
seasonal first order differencing of the same series with the seasonal lag as
4—to factor in the number of quarters in a year.
FIGURE 12.20
(A) Non-stationary Time series and (B) Stationary time series.422 CHAPTER 12: Time Series ForecastingMoving Average of Error
In addition to creating a regression of actual past “p”values as shown in
Eq. (12.19) , one can also create a regression equation involving forecast
errors of past data and use it as a predictor. Consider this equation with:
yt5I1et1θ1et211θ2et221?1θqet2q ð12:22Þ
where eiis the forecast error of data point i. This makes sense for the past
data points but not for data point tbecause it is still being forecasted. Hence,
etis assumed as white noise. The regression equation for ytcan be under-
stood as the weighted ( θ) moving average of past qforecast errors. This is
called Moving Average with qlags model or MA(q) .
Autoregressive Integrated Moving Average
The Autoregressive Integrated Moving Average (ARIMA) model is a combina-
tion of the differenced autoregressive model with the moving average model.
It is expressed as:
y0
t5I1α1y0
t211α2y0
t221?1αpy0
t2p1et1θ1et211θ2et221?1θqet2q ð12:23Þ
The AR part of ARIMA shows that the time series is regressed on its own past
data. The MA part of ARIMA indicates that the forecast error is a linear com-
bination of past respective errors. The Ipart of ARIMA shows that the data
values have been replaced with differenced values of dorder to obtain sta-
tionary data, which is the requirement of the ARIMA model approach. Why
would one need to get to this level of complexity? The ARIMA model is effec-
tive in fitting past data with this combination approach and help forecast
future points in a time series.
Eq. (12.23) shows the predictors are the lagged pdata points for the autore-
gressive part and the lagged qerrors are for the moving average part, which
FIGURE 12.21
(A) Time series (B) Seasonal differencing of the time series.12.3 Regression Based Methods 423are all differenced. The prediction is the differenced ytin the dthorder. This is
called the ARIMA( p,d,q)model. Estimating the coefficients αand θfor a
given p,d,qis what ARIMA does when it learns from the training data in a
time series. Specifying p,d,qcan be tricky (and a key limitation) but one can
tryout different combinations and evaluate the performance of the model.
Once the ARIMA model is specified with the value of p,d,q, the coefficients of
Eq. (12.23) need to be estimated. The most common way to estimate is
through the Maximum Likelihood Estimation. It is similar to the Least
Square Estimation for the regression equation, except MLE finds the coeffi-
cients of the model in such a way that it maximizes the chances of finding
the actual data.
ARIMA is a generalized model. Some of the models discussed in this chapter
are special cases of an ARIMA model. For example,
GARIMA (0,1,0) is expressed as yt5yt211e. It is the naive model with
error, which is called the Random walk model.
GARIMA (0,1,0) is expressed as yt5yt211e1c. It is a random walk
model with a constant trend. It is called random walk with drift.
GARIMA (0,0,0) is yt5eor white noise
GARIMA (p,0,0) is the autoregressive model
How to Implement
The dataset used to build an ARIMA trainer is the Australian beer production
dataset. The task is to forecast the production for ten more quarters.
RapidMiner provides an extension (Time Series Extension) for time series
operators to describe and predict time series. ARIMA modeling and the fore-
casting are implemented by ARIMA trainer to build the model and Apply fore-
castto apply the model and forecast for ten more quarters. The process is
shown in Fig. 12.22
Step 1: The Australian beer production dataset has two attributes: Year and
beer production value in megaliters. The role of Year should be set as ID and
the only attribute in the dataset should be the time series value. A sequential
time period is generated and added to the dataset as an anchor for the
charts.
Step 2: The ARIMA trainer operator can be found under Extension .Time
Series.Forecast .ARIMA. The operator has these parameters:
Gp: order of autoregression part. It is set as 1
Gd: degree of differencing. Set as 0
Gq: order of the moving average of the error part. Set as 1
Gcriterion: criterion to minimize for coefficient selection. Set as aic.
As a start, the model parameters are specified as ARIMA(1,0,1). One can fur-
ther optimize the value of p,d,qusing the Optimize Parameters operator where424 CHAPTER 12: Time Series Forecastingall the combination of p,d,qare tried out to select the most optimal
combination.
Step 3: The Apply the forecast operator takes the input of the forecasted
model and applies the model for future horizon. Forecast horizon is set as
10 to predict 10 more data points at the end of the time series. The fore-
casted data is then joined with the original dataset using the Join operator
so the forecasted data can be visualized along with the original dataset
(Fig. 12.23 ).
The process shown in Fig. 12.22 can be saved and run. The result window
has the output of the Joinoperator. The time series chart provides the plot of
time series, both the original and forecasted value. The model has done a
decent job of capturing the level and trend in the data, but the forecasted
value does not have any seasonality, unlike the actual data. This is because
seasonality was not expressed in the model. The ARIMA model discussed
thus far does not take into account of the seasonality component. One
would have a few additional steps to complete in order to incorporate the
seasonality in the ARIMA modeling.
FIGURE 12.22
Process to implement ARIMA model. ARIMA , Autoregressive Integrated Moving Average.12.3 Regression Based Methods 42512.3.4 Seasonal ARIMA
The ARIMA model can be further enhanced to take into account of the sea-
sonality in the time series. Seasonal ARIMA is expressed by the notion
ARIMA( p,d,q)(P,D,Q)mwhere
pis the order of nonseasonal autoregression
dis the degree of differencing
qis the order of nonseasonal moving average of the error
Pis the order of the seasonal autoregression
Dis the degree of seasonal differencing
Qis the order of seasonal moving average of the error
mis the number of observations in the year (for yearly seasonality)
In terms of the equation, the seasonal part of ARIMA is similar to the terms
used in the nonseasonal part, except that it is backshifted m times , where mis
FIGURE 12.23
Forecast using ARIMA. ARIMA , Autoregressive Integrated Moving Average.426 CHAPTER 12: Time Series Forecastingthe seasonal period. For example, the differencing is performed not with the
consecutive data points but with the data points with mlags. If one has quar-
terly seasonality in monthly grain, each data point is differenced with the
same month last quarter.
How to Implement
To implement seasonal ARIMA, Execute R operator from the Rextension for
RapidMiner is used. The RapidMiner process shown in Fig. 12.24 looks simi-
lar to the process built for the Holt-Winters ’smoothing model. The differ-
ence is in the R code inside the Execute R operator. The process has a data
preparation step before feeding data to Rand a post process to combine the
forecast and original data for visualization. The Australian beer production
dataset is used for this process.
The R code inside the Execute R operator is shown here. It uses the Forecast
package in R to activate the arima() function. In this processes the auto.ari-
ma() function was used, which automatically selects the best parameters for
(p,d,q)(P,D,Q)m. The optimal parameters for the Beer production dataset is
ARIMA(1,0,0)(1,1,0) 4. The seasonal ARIMA model is used to forecast the
future 12 data points using the forecast() function.
FIGURE 12.24
Seasonal ARIMA process using R. ARIMA , Autoregressive Integrated Moving Average.12.3 Regression Based Methods 427rm_main 5function(data)
{
library(forecast)
inp,- ts(data, freq 54)
y,- auto.arima(inp)
df,- as.data.frame(forecast(y, h 512))
return(list(df))
}
The process can be saved and executed. The forecast output is shown in
Fig. 12.25 . The model seems to have accounted for both the trend and sea-
sonality in the time series. ARIMA is one of the most used forecasting techni-
ques in businesses today. It has strong underpinning in statistics, has been
field tested for decades, and is effective in modeling seasonal and
FIGURE 12.25
Forecast using seasonal ARIMA. ARIMA , Autoregressive Integrated Moving Average.428 CHAPTER 12: Time Series Forecastingnonseasonal time series. Specifying the ARIMA modeling parameters before-
hand might seem arbitrary. However, one can test the fit of the model for
many combination or use meta functions like auto.arima() for the Optimize
parameters operators. There is one more class of time series forecasting that
has gained contemporary popularity —Machine learning /C0based time series
forecasting.
12.4 MACHINE LEARNING METHODS
A time series is a unique dataset where the information used to predict future
data points can be extracted from past data points. A subset of the past
known data points can be used as inputs to an inferred model to compute
the future data point as an output. Fig. 12.26 shows the general framework
of the approach, which is similar to supervised learning techniques. Standard
FIGURE 12.26
Lagged inputs and target.12.4 Machine Learning Methods 429machine learning techniques are used to build a model based on the inferred
relationship between input (past data) and target (future data).
In order to use the supervised learners on time series data, the series is trans-
formed into cross-sectional data using a technique called windowing . This
technique defines a set of consecutive time series data as a window, where
the latest record forms the target while other series data points, which are
lagged compared to the target, form the input variables ( Fig. 12.27 ). As con-
secutive windows are defined, the same data point in the series may function
as the target for one cross-sectional window and the input variable for other
cross-sectional windows. Once a sufficient number of windows are extracted
from the dataset, a supervised model can be learned based on the inferred
relationship between the lagged input variables and the target variable. This
is similar to an autoregressive model where past p data points are used to
predict the next data point. Any of the supervised learners discussed in the
Classification or Regression chapter can be applied to learn and predict the
target variable —the next time step in the series. The inferred model can be
used to predict a future time series data point based on the last window of
the time series. This gives visibility into one future data point. The new pre-
dicted data point can be used to define a new window and predict one more
data point into the future. This subroutine can be repeated until all future
predictions are made.
12.4.1 Windowing
The purpose of windowing is to transform the time series data into a generic
machine learning input dataset. Fig. 12.28 shows a sample windowing and
cross-sectional data extraction from the time series dataset.
The characteristics of the windows and the cross-sectional data extractions
are specified by the parameters of the windowing process. The following
parameters of windowing allow for changing the size of the windows, the
FIGURE 12.27
Machine learning model for time series.430 CHAPTER 12: Time Series Forecastingoverlap between consecutive windows, and the prediction horizon which is
used for forecasting.
1.Window Size : Number of lag points in one window excluding the target
data point.
2.Step: Number of data points between the first value of the two
consecutive windows. If the step is 1, maximum number of windows
can be extracted from the time series dataset.
3.Horizon width : The prediction horizon controls how many records in
the time series end up as the target variable. The common value for the
horizon width is 1.
4.Skip: Offset between the window and horizon. If the skip is zero, the
consecutive data point(s) from the window is used for horizon.
InFig. 12.28 , the window size is 6, step is 1, horizon width is 1, and skip is 0.
Thus, the series data are now converted into a generic cross-sectional dataset
that can be predicted with learning algorithms like regression, neural net-
works, or support vector machines. Once the windowing process is done,
then the real power of machine learning algorithms can be brought to bear
on a time series dataset.
FIGURE 12.28
Windowing process. (A) original time series and (B) cross-sectional data set with consecutive windows.12.4 Machine Learning Methods 431Model Training
Consider the time series dataset shown in Fig. 12.28 A. The dataset refers to
historical monthly profits from a product, from January 2009 to June 2012.
Suppose the objective in this exercise is to develop profitability forecasts for
the next 12 months. A linear regression model can be used to fit the cross-
sectional dataset shown in Fig. 12.28 B using the technique described in
Chapter 5, Regression Methods. The model will be:
input Yt11labelðÞ50:4933input Yt2510:2583input Yt2410:1073input Yt23
20:0983input Yt2220:0733input Yt2110:3293input Yt2010:135
Training the model is quite straightforward. The inferred relationship
between a data point in the time series with the previous six data points is
established. In other words, if one knows six consecutive data points in a
time series, they can use the model to predict the seventh unseen data point.
Since a new data point has been forecasted, it can be used along with the
five preceding data points to predict one more data point and so on. That ’s
time series forecasting one data point at a time!
How to Implement
Implementing a time series forecasting process using supervised learning is
similar to a classification or regression process. The distinguishing step in
time series forecasting is the conversion of a time series dataset to a cross-
sectional dataset and stacking the forecast one data point at a time. The
RapidMiner process is shown in Fig. 12.29 . It uses operators from the Time
Series extension. Although the process looks complicated, it consists of three
functional blocks: (1) conversion to cross-sectional data, (2) training an
machine learning model, and (3) forecasting one data point at a time in a
loop. The dataset used in the process is the Product profit5dataset (the data-
set can be downloaded from www.IntroDataScience.com ) shown in
Fig. 12.28 . The time series has two attributes: Date and Input Yt.
Step 1: Set Up Windowing
The process window in Fig. 12.29 shows the necessary operators for window-
ing. The time series dataset has a date column, and this must be treated with
special care. The operator must be informed that one of the columns in the
dataset is a date and should be considered as an “id.”This is accomplished
with the Set Role operator. If the input data has multiple time series, Select
Attributes operator can be used to select the one to be forecasted. In this case,
only a one value series is used and strictly speaking this operator is not
needed. However, to make the process generic it has been included and the
5Available for download from www.IntroDataScience.com .432 CHAPTER 12: Time Series Forecastingcolumn labeled “inputYt ”has been selected. Optionally, one may want to
use the Filter Examples operator to remove any data points that may have
missing values. The central operator for this step is the Windowing operator
in the Time series extension. The main parameters for the Windowing opera-
tor are:
1.Window size: Determines how many “attributes ”are created for the
cross-sectional data. Each row of the original time series within the
window size will become a new attribute. In this example, w56 was
chosen.
2.Step size: Determines how to advance the window. s51 was used.
3.Horizon width: Determines how far out to make the forecast. If the
window size is 6 and the horizon is 1, then the seventh row of the
original time series becomes the first sample for the “label ”variable.
h51 was used.
Fig. 12.28 shows the original data and the transformed output from the win-
dowing process. The window operator adds six new attributes named
input Yt25through input Yt20.
FIGURE 12.29
Process for time series forecasting using machine learning.12.4 Machine Learning Methods 433Step 2: Train the Model
When training any supervised model using this data, the attributes labeled
input Yt25through input Yt20form the independent variables. In this case, lin-
ear regression is used to fit the dependent variable called label, using the inde-
pendent variables input Yt25through input Yt20. The Vector Linear Regression
operator is being used to infer the relationship between six dependent vari-
ables and the dependent variable. The model output for the dataset is:
label50:4933input Yt2510:2583input Yt2410:1073input Yt2320:098
3input Yt2220:0733input Yt2110:3293input Yt2010:135
Step 3: Generate the Forecast in a Loop
Once the model fitting is done, the next step is to start the forecasting pro-
cess. Note that given this configuration of the window size and horizon, one
can now only make the forecast for the next step. In the example, the last
row of the transformed dataset corresponds to December 2011. The indepen-
dent variables are values from June /C0November 2011 and the target or label
variable is December 2011. The regression equation is be used to predict
December 2011 value. The same regression equation is also used for predict-
ing January 2012 value. All one needs to do is insert the values from
July /C0December into the regression equation to generate the January 2012
forecast. Next, a new row of data needs to be generated that would run from
August /C0January to predict February 2012 using the regression equation. All
the (actual) data from August /C0December is available as well as the predicted
value for January. Once the predicted February value is obtained, there is
nothing preventing the actual data from September /C0December plus the pre-
dicted January and February values from being used to forecast March 2012.
To implement this in RapidMiner process, one would need to break this up
into two separate parts. First, take the last forecasted row (in this case,
December 2011), drop the current value of input Yt25(current value is
1.201), rename input Yt24to input Yt25, rename input Yt23to input Yt24,
rename input Yt22to input Yt23, rename input Yt21to input Yt22, rename
input Yt20to input Yt21, and finally rename predicted label (current value is
1.934) to input Yt20. With this new row of data, the regression model can be
applied to predict the next date in the series: January 2012. Fig. 12.30 shows
the sample steps. Next, this entire process need to be put inside a Loop opera-
tor that will allow these steps to be repeatedly run for as many future periods
as needed.
The Loop operator will contain all the mechanisms for accomplishing the
renaming and, of course, to perform the forecasting ( Fig. 12.31 ). Set the itera-
tions in the Loop operator to the number of future months to forecast434 CHAPTER 12: Time Series ForecastingFIGURE 12.30
Forecasting one step ahead.
FIGURE 12.31
Looping subroutine to forecast one data point at a time.(horizon). In this case, this is defined by a process variable called
futureMonths whose value can be changed by the user before process execu-
tion. It is also possible to capture the Loop counts in a macro if the set itera-
tion macro box is checked. A macro in RapidMiner is nothing but a process
variable that can be called by other operators in the process. When set itera-
tion macro is checked and a name is provided in the macro name box, a vari-
able will be created with that name whose value will be updated each time,
one loop is completed. An initial value for this macro is set by the macro start
value option. Loops may be terminated by specifying a timeout , which is
enabled by checking the limit time box. A macro variable can be used by any
other operator by using the format %{macro name} in place of a numeric
value.
Before the looping is started, the last forecasted row needs to be stored in a
separate data structure. This is accomplished by a new Windowing operator
and the macro titled Extract Example Set .T h e Filter Example operator simply
deletes all rows of the transformed dataset except the last forecasted row.
Finally the Remember operator stores this in memory and allows one to
“recall ”the stored value once inside the loop.
The loop parameter iterations will determine the number of times the inner
process is repeated. Fig. 12.31 shows that during each iteration, the model is
applied on the last forecasted row, and bookkeeping operations are per-
formed to prepare application of the model to forecast the next month. This
includes incrementing the month (date) by one, changing the role of the pre-
dicted label to that of a regular attribute, and finally renaming all the attri-
butes. The newly renamed data are stored and then recalled before the next
iteration begins.
The output of this process is shown in Fig. 12.32 as an overlay on top of the
actual data. As seen, the simple linear regression model seems to adequately
capture both the trend and seasonality of the underlying data. The Linear
Regression operator of Step 2: Train the model can be quickly swapped to a
Support Vector Machine operator and its performance tested without having to
do any other programming or process modifications.
12.4.2 Neural Network Autoregressive
The windowing process allows time series dataset to be transformed to a
cross-sectional horizontal dataset th at is conducive to learners to create a
forecasting model. That includes artif icial neural networks to forecast the
time series. Consider a simple ANN arc hitecture with six inputs, one hidden
layer with five nodes and an output, as shown in Fig. 12.33 .I ti sm u l t i -
layer feed forward network tha t maps nonlinear functions.436 CHAPTER 12: Time Series ForecastingTime series data, which is transformed into a cross-sectional dataset, can be
fed into a neural network as inputs to predict the output. The weights for
the links can be estimated with the training dataset, with lagged inputs and
label, to build the neural network mo del. A feed forward neural network
with one hidden layer, in the context of Time series is denoted as Neural
Network Autoregressive —NNAR( p,P,k)mwhere pis the number of lagged
inputs (order of the autoregressive model), kis the number of nodes in the
hidden layer, Pis the auto regressive part of the seasonal component and m
is the seasonal period. The neural network NNAR( p,P,k)mhas a particular
relevance when it comes to time s eries forecasting. A NNAR( p,P,k)m
functions similarly to a seasonal ARIMA ( p,0,0) ( P,0,0) mmodel ( Hyndman
& Athanasopoulos, 2018 ).
FIGURE 12.32
Forecasted time series.12.4 Machine Learning Methods 437How to Implement
The implementation of NNAR is the same as the previous Windowing
process shown in Fig. 12.29 with the Neural Net operator replacing the
Vector Linear Regression modeling operator. The neural network operator
has a parameter called Hidden layer where one can specify the number of
hidden layers and nodes within each layer. For the NNAR model, the
number of hidden layers is 1 and the number of nodes is one less than
the number of the input nodes ( Hyndman & Athanasopoulos, 2018 ).
Fig. 12.34 shows the Rapidminer process for NNAR. This process has six
lagged inputs and the number of nodes in the hidden layer is 5.
The forecasting is done one step ahead. The Loop operator takes the latest
output and applies it to the input for the next iteration. The process can be
saved and executed. The time series forecast is shown in Fig. 12.35 . It can be
observed that the NNAR forecast is different from the forecast achieved with
the linear regression model.
An important point about any time series forecasting is that one should
not place too much emphasis on the point forecasts. A complex quantity
like sales demand for a product is influenced by too many factors and to
claim that any forecasting will predict the exact value of demand three
months in advance is unrealistic. However, what is far more valuable is the
fact that recent undulations in the de mand can be effectively captured and
predicted.
FIGURE 12.33
Neural network autoregressive model.438 CHAPTER 12: Time Series Forecasting12.5 PERFORMANCE EVALUATION
How does one know if the time series forecasting model provides accurate
forecasts? forecast accuracy metrics have to be created to compare the perfor-mance of one model with another and to compare the models across differ-
ent use cases. For example, the performance of a revenue forecasting model
can be contrasted with other models built by different techniques or differentparameters; and even to compare with models to forecast the customer com-
plaints for a product.
12.5.1 Validation Dataset
One way to measure accuracy is to measure the actual value post-fact and
compare against the forecast and calculate the error. However, one would
FIGURE 12.34
Process for neural network autoregressive model.12.5 Performance Evaluation 439have to wait for the time to pass and the actual data to be measured.
Residues are calculated when a forecast model is fitted on training data. By
all means, the model might have overfitted the training data and perform
poorly with unseen future forecasts. For this reason, training residuals are
not a good way to measure how the forecast models are going to perform in
the real world. Recall that in the case of the supervised learners, a set of
known data is reserved for model validation. Similarly, some data points can
be reserved in the time series as a validation dataset just to test the accuracy of
the model. The training process uses data by restricting it to a point and the
rest of the later time series is used for validation as shown in Fig. 12.36 .A s
the model has not seen the validation dataset, deviation of actuals from the
forecast is the forecast error of the model. The forecast error is the difference
FIGURE 12.35
Forecast using neural network autoregression.440 CHAPTER 12: Time Series Forecastingbetween the actual value yiand the forecasted value y^i. The error or the resi-
due for the ithdata point is given by Eq. (12.24)
ei5yi/C0^yi ð12:24Þ
The forecast error shown in Eq. (12.24) is scale dependent. The error mea-
sured for each of the data points and can be aggregated to one metric to indi-
cate the error of the forecasting model. Some of the commonly used forecast
accuracy aggregate metrics are:
Mean Absolute Error
The error of the individual data point may be positive or negative and may can-
cel each other out. To derive the overall forecast for the model, calculate the
absolute error to aggregate all the residuals and average it.
Mean absolute error 5mean jeijðÞ ð 12:25Þ
FIGURE 12.36
Validation dataset.12.5 Performance Evaluation 441MAE is a simple metric and it is scale dependent. It is convenient to commu-
nicate the error of the revenue forecasting model as, for example,
6$900,000 per day.
Root Mean Squared Error
In some cases it is advantageous to penalize the individual point error with
higher residues. Even though two models have the same MAE, one might
have consistent error and the other might have low errors for some points
and high error for other points. RMSE penalizes the latter.
Root mean squared error 5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ
mean ðe2Þp
ð12:26Þ
RMSE is scale dependent and is used in situations where penalizing high rel-
ative residue is necessary. On the other hand, it is slightly difficult to under-
stand the RMSE for a stand-alone model.
Mean Absolute Percentage Error
Percentage error of a data point is pi5100ei
yi. It is a scale independent error
that can be aggregated to form mean absolute percentage error.
Mean absolute percentage error 5mean pi/C12/C12/C12/C12/C0/C1
ð12:27Þ
MAPE is useful to compare against multiple models across the different fore-
casting applications. For example, the quarterly revenue forecast, measured in
USD, for a car brand might be 65% and the forecast for world-wide car
demand, measured in quantity, might be 63%. The firm's ability to forecast
the car demand is higher than the revenue forecast for one brand. Even
though MAPE is easy to understand and scale independent, MAPE has a sig-
nificant limitation when it applies to intermittent data where zero values are
possible in actual time series. For example, profit or defects in a product. Zero
value in the time series yields an infinite error rate (if the forecast is non-
zero) and skews the result. MAPE is also meaningless when the zero point is
not defined or arbitrarily defined, as in non-kelvin temperature scales.
Mean Absolute Scaled Error
MASE is scale independent and overcomes the key limitations of MAPE by
comparing the forecast values against a naive forecast. Naive forecast is a sim-
ple forecast where the next data point has the same value as the previous
data point ( Hyndman & Koehler, 2006 ). Scaled error is defined as:
MASE5PT
i51jej
T
T21PT
i52jyi2yi21jð12:28Þ
Tis the total number of data points. Scaled error is less than one if the fore-
cast is better than naive forecast and greater than one if it is worse than naive442 CHAPTER 12: Time Series Forecastingforecast. One would want a scaled error much less than one for a good fore-
casting model.
12.5.2 Sliding Window Validation
Sliding window validation is a process of backtesting time series models built
through machine learning based methods. The whole cross-sectional datasetis divided into different training windows by specifying the window width. A
model is trained using a training window and applied on the testing window
to compute the performance for the first run. For the next run, the trainingwindow is slid to new set of training records and the process is repeated until
all the training windows are used. By this technique, an average performance
metric can be calculated across the entire dataset. The performance metricderived through sliding window validation is generally more robust than
split validation technique.
12.6 CONCLUSION
Time series forecasting remains one of the cornerstones of data science tech-
niques. It is one of the most widely used analytical applications in businessand organizations. All organizations are forward looking and want to plan
for the future. Time series forecasting, thus, forms a lynchpin to look into the
most probable future and plan accordingly. Time series forecasting, like any
other data science technique, has a diverse set of techniques and methods.
This chapter covered the most important techniques that have practical rele-
vance in a business setting.
Univariate time series forecasting treats prediction, essentially, as a single-
variable problem, whereas, multivariate time series may use many time-
concurred value series for prediction. If one has a series of points spaced overtime, conventional forecasting uses smoothing and averaging to predict where
the next few points will likely be. However, for complex systems such as the
economy or demand of a product, point forecasts are unreliable because
these systems are functions of hundreds if not thousands of variables. What
is more valuable or useful is the ability to predict trends, rather than pointforecasts. Trends can be predicted with greater confidence and reliability (i.e.,
Are the quantities going to trend up or down?), rather than the values or
levels of these quantities. For this reason, using an ensemble of differentmodeling schemes such as artificial neural networks or support vector
machines or polynomial regression can sometimes give highly accurate trend
forecasts. If the time series is not highly volatile (and, therefore, more pre-dictable), time series forecasting can help understand the underlying struc-
ture of the variability better. In such cases, trends or seasonal components
have a stronger signature than random components do.12.6 Conclusion 44312.6.1 Forecasting Best Practices
While the science part of time series forecasting is covered in this chapter,
there is a bit of artin getting robust forecasts from the time series models.
Here is a list of suggested practices to build a robust forecasting model.
1.Understand the metric : Investigate how the time series metric is derived.
Is the metric influenced by other metrics or phenomenon that can be
better candidates for the forecasting? For example, instead of forecasting
profit, both revenue and cost can be forecasted, and profit can be
calculated. This is particularly suitable when profit margins are low and
can go back and forth between positive and negative values (loss).
2.Plot the time series : A simple time series line chart reveals a wealth of
information about the metric being investigated. Does the time series
have a seasonal pattern? Long-term trends? Are the seasonality and
trend linear or exponential? Is the series stationary? If the trend is
exponential, can one derive log() series? Aggregate daily data to weeks
and months to see the normalized trends.
3.Is it forecastable : Check if the time series is forecastable using stationary
checks.
4.Decompose : Identify trends and seasonality using decomposition
methods. These techniques show how the time series can be split into
multiple meaningful components.
5.Try them all : Try several different methods mentioned in the forecasting
taxonomy in Fig. 12.3 , after splitting, training, and validation samples.
For each method:
a.Perform residual checks using MAE or MAPE metric.
b.Evaluate forecasts using the validation period.
c.Select the best performing method and parameters using
optimization functions.
d.Update the model using the full dataset (training 1validation) for
future forecasts.
6.Maintain the models : Review models on a regular basis. Time series
forecast models have a limited shelf life. Apart from feeding the latest
data to the model, the model should be refreshed to make it relevant
for the latest data. Building a model daily is not uncommon.
References
Box, G. A. (1970). Time series analysis: Forecasting and control . San Francisco, CA: Holding Day.
Box, G. J. (2008). Time series analysis: Forecasting and control . Wiley Series in Probability and
Statistics.
Brown, R. G. (1956). Exponential smoothing for predicting demand . Cambridge, MA: Arthur D.
Little.444 CHAPTER 12: Time Series ForecastingHyndman R.A. (2014). Forecasting: Principles and practice .,Otexts.org ..
Hyndman, R. J., & Athanasopoulos, G. (2018). Forecasting: Principles and practice . 2nd edition.
,Otexts.org ..
Hyndman, Rob J., & Koehler, Anne B. (2006). Another look at measures of forecast accuracy.
International Journal of Forecasting ,22, 679 /C0688.
Shmueli G. (2011). Practical time series forecasting: A hands on guide .,statistics.com ..
Winters, P. (1960). Forecasting sales by exponentially weighted moving averages. Management
Science ,6(3), 324 /C0342.References 445CHAPTER 13
Anomaly Detection
Anomaly detection is the process of finding outliers in a given dataset.
Outliers are the data objects that stand out amongst other data objects and
do not conform to the expected behavior in a dataset. Anomaly detection
algorithms have broad applications in business, scientific, and security
domains where isolating and acting on the results of outlier detection is criti-
cal. For identification of anomalies, algorithms discussed in previous chap-
ters such as classification, regression, and clustering can be used. If the
training dataset has objects with known anomalous outcomes, then any of
the supervised data science algorithms can be used for anomaly detection. In
addition to supervised algorithms, there are specialized (unsupervised) algo-
rithms whose whole purpose is to detect outliers without the use of a labeled
training dataset. In the context of unsupervised anomaly detection, algo-
rithms can either measure distance from other data points or density around
the neighborhood of the data point. Even clustering techniques can be lever-
aged for anomaly detection. The outlier usually forms a separate cluster from
other clusters because they are far away from other data points. Some of the
techniques discussed in previous chapters will be revisited in the context of
outlier detection. Before discussing the algorithms, the term outlier or anom-
aly has to be defined and the reason such data points occur in a dataset will
need to be understood.
13.1 CONCEPTS
An outlier is a data object that is markedly different from the other objects in
a dataset. Hence, an outlier is always defined in the context of other objects
in the dataset. A high-income individual may be an outlier in a middle-class
neighborhood dataset, but not in the membership of a luxury vehicle owner-
ship dataset. By nature of the occurrence, outliers are also rare and, hence,
they stand out amongst other data points. For example, the majority of com-
puter network traffic is legitimate, and the one malicious network attack
would be the outlier.
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00013-7
©2019 Elsevier Inc. All rights reserved.44713.1.1 Causes of Outliers
Outliers in the dataset can originate from either error in the data or from
valid inherent variability in the data. It is important to understand the prove-nance of the outliers because it will guide what action, if any, should be per-
formed on the identified outliers. However, pinpointing exactly what caused
an outlier is a tedious task and it may be impossible to find the causes ofoutliers in the dataset. Here are some of the most common reasons why an
outlier occurs in the dataset:
Data errors: Outliers may be part of the dataset because of measurement
errors, human errors, or data collection errors. For example, in a dataset
of human heights, a reading such as 1.70 cm is obviously an error and
most likely was wrongly entered into the system. These data points areoften ignored because they affect the conclusion of the data science task.
Outlier detection here is used as a preprocessing step in algorithms such
as regression and neural networks. Data errors due to human mistakecould be either intentional introduction of error or unintentional error
due to data entry error or significant bias.
Normal variance in the data: In a normal distribution, 99.7% of data
points lie within three standard deviations from the mean. In other
words, 0.26% or 1 in 370 data points lie outside of three standarddeviations from the mean. By definition, they don ’t occur frequently and
yet are a part of legitimate data. An individual earning a billion dollars in
a year or someone who is more than 7 ft tall falls under the category ofoutlier in an income dataset or a human height dataset respectively.
These outliers skew some of the descriptive statistics like the mean of the
dataset. Regardless, they are legitimate data points in the dataset.Data from other distribution classes: The number of daily page views for a
customer-facing website from a user IP address usually range from one to
several dozen. However, it is not unusual to find a few IP addressesreaching hundreds of thousands of page views in a day. This outlier
could be an automated program from a computer (also called a bot)
making the calls to scrape the content of the site or access one of theutilities of the site, either legitimately or maliciously. Even though they
are an outlier, it is quite “normal ”for bots to register thousands of page
views to a website. All bot traffic falls under the distribution of adifferent class “traffic from programs ”other than traffic from regular
browsers that fall under the human user class.
Distributional assumptions: Outlier data points can originate from
incorrect assumptions made on the data or distribution. For example, if
the data measured is usage of a library in a school, then during term
exams there will be an outlier because of a surge in the usage of thelibrary. Similarly, there will be a surge in retail sales during the day after448 CHAPTER 13: Anomaly DetectionThanksgiving in the United States. An outlier in this case is expected and
does not represent the data point of a typical measure.
Understanding why outliers occur w ill help determine what action to
perform after outlier detection. In a fe w applications, the objective is to iso-
late and act on the outlier as can be se en in credit card transaction fraud
monitoring. In this case, credit card transactions exhibiting different behav-
ior from most normal transactions (such as high frequency, high amounts,
or very large geographic separation b etween points of consecutive transac-
tions) has to be isolated, alerted, and the owner of the credit card has to be
contacted immediately to verify the aut henticity of the transaction. In other
cases, the outliers have to be filtered out because they may skew the final
outcome. Here outlier detection is used as a preprocessing technique for
other data science or analytical tasks. For example, ultra-high-income earn-
ers might need to be eliminated in order to generalize a country ’si n c o m e
patterns. Here outliers are le gitimate data points but are intentionally disre-
garded in order to generalize conclusions.
DETECTING CLICK FRAUD IN ONLINE ADVERTISING
The rise in online advertising has underwritten success-
ful Internet business models and enterprises. Online
advertisements make free Internet services, like web
searches, news content, social networks, mobile applica-
tion, and other services, viable. One of the key challenges
in online advertisement is mitigating click frauds .C l i c k
fraud is a process where an automated program or a
person imitates the action of a normal user clicking on
an online advertisement, with the malicious intent of
defrauding the advertiser, publisher, or advertisement
network. Click fraud could be performed by contracting
parties or third parties, like competitors trying to deplete
advertisement budgets or to tarnish the reputation of the
sites. Click fraud distorts the economics of advertising
and poses a major challenge for all parties involved in
online advertising ( Haddadi, 2010 ). Detecting, eliminating,
or discounting click fraud makes the entire marketplace
trustworthy and even provides competitive advantage for
all the parties.
Detecting click fraud takes advantage of the fact that
fraudulent traffic exhibits an atypical web browsing pat-
tern when compared with typical clickstream data.Fraudulent traffic often does not follow a logical sequence
of actions and contains repetitive actions that would differ-
entiate from other regular traffic ( Sadagopan & Li, 2008 ).
For example, most of the fraudulent traffic exhibits either
one or many of these characteristics: they have very high
click depth (number of web pages accessed deep in the
website); the time between each click would be extremely
short; a single session would have a high number of clicks
on advertisements as compared with normal users; the
originating IP address would be different from the target
market of the advertisement; there would be very little
time spent on the advertiser ’s target website; etc. It is not
one trait that differentiates fraudulent traffic from regular
traffic, but a combination of the traits. Detecting click fraud
is an ongoing and evolving process. Increasingly, the click
fraud perpetrators are getting more sophisticated in imi-
tating the characteristics of a normal web browsing user.
Hence, click fraud cannot be fully eliminated, however, it
can be contained by constantly developing new algorithms
to identify fraudulent traffic.
To detect click fraud outliers, first clickstream data would
need to be prepared in such a way that detection using
(Continued )13.1 Concepts 44913.1.2 Anomaly Detection Techniques
Humans are innately equipped to focus on outliers. The news cycle experi-
enced every day is mainly hinged on o utlier events. The interest around
knowing who is the fastest, who earns the most, and who wins the most
medals or scores the most goals is in part due to increased attention to
outliers. If the data is in one dimen sion like taxable income for indivi-
duals, outliers can be identifie d with a simple sorting function.
Visualizing data by scatter, histogram , and box-whisker charts can help to
identify outliers in the case of sin gle attribute datasets as well. More
advanced techniques would fit the da ta to a distribution model and use
data science techniques to detect outliers.
Outlier Detection Using Statistical Methods
Outliers in the data can be identified by creating a statistical distribution
model of the data and identifying the data points that do not fit into the
model or data points that occupy the ends of the distribution tails. The
underlying distribution of many prac tical datasets fall into the Gaussian
(normal) distribution. The parameters for building a normal distribution
(i.e., mean and standard deviation) can be estimated from the dataset
and a normal distribution curve can be created like the one shown
inFig. 13.1 .
Outliers can be detected based on where the data points fall in the standard
normal distribution curve. A threshold for classifying an outlier can be speci-
fied, say, three standard deviations from the mean. Any data point that is
more than three standard deviations is identified as an outlier. Identifying
outliers using this method considers only one attribute or dimension at a
time. More advanced statistical techniques take multiple dimensions into
account and calculate the Mahalanobis distance instead of the standard(Continued )
data science is easier. A relational column-row dataset
can be prepared with each visit occupying each row and
the columns being traits like click depth, time between
each click, advertisement clicks, total time spent in target
website, etc. This multidimensional dataset can be used
for outlier detection using data science. Clickstream traits
or attributes have to be carefully considered, evaluated,
transformed, and added into the dataset. In multidimen-
sional data space, the fraudulent traffic (data point) isdistant from other visit records because of their attributes,
such as the number of ad clicks in a session. A regular
visit usually has one or two ad clicks in a session, while a
fraudulent visit would have dozens of ad clicks. Similarly,
other attributes can help identify the outlier more pre-
cisely. Outlier-detection algorithms reviewed in this chap-
ter assign an outlier score (fraud score) for all the
clickstream data points and the records with a higher
score are predicted to be outliers.450 CHAPTER 13: Anomaly Detectiondeviations from the mean in a univariate distribution. Mahalanobis distance
is the multivariate generalization of finding how many standard deviations
away a point is from the mean of the multivariate distribution. Outlier detec-
tion using statistics provides a simple framework for building a distribution
model and for detection based on the variance of the data point from the
mean. One limitation of using the distribution model to find outliers is that
the distribution of the dataset is not previously known. Even if the distribu-
tion is known, the actual data don ’t always fit the model.
Outlier Detection Using Data Science
Outliers exhibit a certain set of characteristics that can be exploited to find
them. Following are classes of techniques that were developed to identify
outliers by using their unique characteristics ( Tan, Steinbach, & Kumar,
2005 ). Each of these techniques has multiple parameters and, hence, a data
point labeled as an outlier in one algorithm may not be an outlier to
another. Hence, it is prudent to rely on multiple algorithms before labeling
the outliers.
Distance-based :By nature, outliers are different from other data objects in
the dataset. In multidimensional Cartesian space they are distant from
other data points, as shown in Fig. 13.2 . If the average distance of the
nearest N neighbors is measured, the outliers will have a higher value
than other normal data points. Distance-based algorithms utilize this
property to identify outliers in the data.
FIGURE 13.1
Standard normal distribution and outliers.13.1 Concepts 451Density-based :The density of a data point in a neighborhood is inversely
related to the distance to its neighbors. Outliers occupy low-density areas
while the regular data points congregate in high-density areas. This isderived from the fact that the relative occurrence of an outlier is low
compared with the frequency of normal data points.
Distribution-based :Outliers are the data points that have a low
probability of occurrence and they occupy the tail ends of the
distribution curve. So, if one tries to fit the dataset in a statistical
distribution, these anomalous data points will stand out and, hence, canbe identified. A simple normal distribution can be used to model the
dataset by calculating the mean and standard deviation.
Clustering :Outliers by definition are not similar to normal data points in
a dataset. They are rare data points far away from regular data points and
generally do not form a tight cluster. Since most of the clustering
algorithms have a minimum threshold of data points to form a cluster,the outliers are the lone data points that are not clustered. Even if the
outliers form a cluster, they are far away from other clusters.
Classification techniques :Nearly all classification techniques can be used
to identify outliers, if previously known classified data are available. In
classification techniques for detecting outliers, a known test dataset is
needed where one of the class labels should be called “Outlier. ”The
outlier-detection classification model that is built based on the test
dataset can predict whether the unknown data is an outlier or not. The
challenge in using a classification model is the availability of previouslylabeled data. Outlier data may be difficult to source because they are
rare. This can be partially solved by stratified sampling where the outlier
records are oversampled against normal records.
FIGURE 13.2
Distance-based outlier.452 CHAPTER 13: Anomaly DetectionSupervised classification methods have been discussed in past chapters and
unsupervised outlier-detection methods will be discussed in the coming sec-
tions. The focus will mainly be placed on distance and density-based detec-
tion techniques in the coming sections.
13.2 DISTANCE-BASED OUTLIER DETECTION
Distance or proximity-based outlier detection is one of the most fundamen-
tal algorithms for anomaly detection and it relies on the fact that outliers are
distant from other data points. The proximity measures can be simple
Euclidean distance for real values and cosine or Jaccard similarity measures
for binary and categorical values. For the purpose of this discussion, consider
a dataset with numeric attributes and Euclidean distance as the proximity
measure. Fig. 13.3 shows a two-dimensional scatterplot of a sample dataset.
Outlier False True
2.75
2.50
2.25
2.00
–2.75
–3.0 –2.5 –1.5 –0.5 0.5 1.0 1.5 0.0 –2.0 2.0 2.5 3.0 3.5
pc_1–1.0pc_2
–2.50–2.25–2.001.75
1.50
1.25
–1.75–1.50–1.251.00
–1.000.75
–0.750.50
–0.500.25
–0.250.00
FIGURE 13.3
Dataset with outliers.13.2 Distance-Based Outlier Detection 453Outliers are the data points marked as gray and can be visually identified
away from the groups of data. However, when working with multidimen-
sional data with more attributes, visual techniques show limitations quickly.
13.2.1 How It Works
The fundamental concept of distance-based outlier detection is assigning a
distance score for all the data points in the dataset. The distance score should
reflect how far a data point is separated from other data points. A similar
concept was reviewed in the k-nearest neighbor ( k-NN) classification tech-
nique in Chapter 4, Classification. A distance score can be assigned for each
data object that is the distance to the kth-nearest data object. For example, a
distance score can be assigned for every data object that is the distance to the
third-nearest data object. If the data object is an outlier, then it is far away
from other data objects; hence, the distance score for the outlier will be high-
er than for a normal data object. If the data objects are sorted by distance
score, then the objects with the highest scores are potentially outlier(s). As
with k-NN classification or any algorithm that uses distance measures, it is
important to normalize the numeric attributes, so an attribute with a higher
absolute scale, such as income, does not dominate attributes with a lower
scale, such as credit score.
In distance-based outlier detection, there is a significant effect based on the
value of k, as in the k-NN classification technique. If the value of k51, then
two outliers next to each other but far away from other data points are not
identified as outliers. On the other hand, if the value of kis large, then a
group of normal data points which form a cohesive cluster will be misla-
beled as outliers, if the number of data points is less than kand the cluster is
far away from other data points. With a defined value of k, once the distance
scores have been calculated, a distance threshold can be specified to identify
outliers or pick the top nobjects with maximum distances, depending on the
application and the nature of the dataset. Fig. 13.4 shows the results of two
different outlier-detection algorithms based on distance for the Iris dataset.
Fig. 13.4 A shows the outlier detection with k51 and Fig. 13.4 B shows the
detection of the same dataset with k55.
13.2.2 How to Implement
Commercial data science tools offer specific outlier-detection algorithms and
solutions as part of the package either in the modeling or data cleansing sec-
tions. In RapidMiner, unsupervised outlier-detection operator can be found
in Data Transformation .Data Cleansing .Outlier Detection .Detect
Outlier Distance. The example set used in this process is the Iris dataset with
four numerical attributes and 150 examples.454 CHAPTER 13: Anomaly Detection2.75
2.50
2.25
2.00
–2.75
(B)(A)pc_2 pc_2–2.50–2.25–2.001.75
1.50
1.25
–1.75–1.50–1.251.00
–1.000.75
–0.750.50
–0.500.25
–0.250.00
2.75
2.50
2.25
2.00
–2.75–2.50–2.25–2.001.75
1.50
1.25
–1.75–1.50–1.251.00
–1.000.75
–0.750.50
–0.500.25
–0.250.00–3.0 –2.5 –1.5 –0.5 0.5 1.0 1.5 0.0 –2.0 2.0 2.5 3.0 3.5 pc_1–1.0
–3.0 –2.5 –1.5 –0.5 0.5 1.0 1.5 0.0 –2.0 2.0 2.5 3.0 3.5
pc_1–1.0Outlier False True
Outlier False True
FIGURE 13.4
Top five outliers of Iris dataset when (A) k51 and (B) k55.13.2 Distance-Based Outlier Detection 455Step 1: Data Preparation
Even though all four attributes of the Iris dataset measure the same quantity
(length) and are measured on the same scale (centimeters), a normalization
step is included as a matter of best practice for techniques that involve dis-
tance calculation. The Normalize operator can be found in Data
Transformation .Value modification .Numerical. The attributes are con-
verted to a uniform scale of mean 0 and standard deviation 1 using Z-
transformation.
For the purposes of this demonstration, a two-dimensional scatterplot with
two attributes will be helpful to visualize outliers. However, the Iris dataset
has four attributes. To aid in this visualization objective, the four numerical
attributes will be reduced to two attributes (principal components) using the
principal component analysis (PCA) operator. Please note that the use of the
PCA operator is optional and not required for outlier detection. The results
of the outlier detection with or without PCA in most cases will be
unchanged. But visualization of the results will be easy with two-
dimensional scatterplots. PCA will be discussed in detail in Chapter 14,
Feature Selection. In this process a variance threshold has been specified for
thePCA operator of 0.95. Any principal component that has a variance
threshold more than 0.95 is removed from the result set. The outcome of the
PCA operator has two principal components.
Step 2: Detect Outlier Operator
TheDetect Outlier (Distances) operator has a data input port and outputs data
with an appended attribute called outlier . The value of the output outlier
attribute is either true or false. The Detect Outlier (Distances) operator has
three parameters that can be configured by the user.
Number of neighbors : This is the value of kin the algorithm. The default
value is 10. If the value is made lower, the process finds smaller outlier
clusters with less data points.
Number of outliers : The individual outlier score is not visible to the users.
Instead the algorithm finds the data points with the highest outlier
scores. The number of data points to be found can be configured using
this parameter.
Distance function : As in the k-NN algorithm, the distance measurement
function needs to be specified. Commonly used functions are Euclidean
and cosine (for document vectors).
In this example, k51, number of outliers 510, and the distance function is
set to Euclidian. The output of this operator is the example set with an
appended outlier attribute. Fig. 13.5 provides the RapidMiner process with456 CHAPTER 13: Anomaly Detectiondata extraction, PCA dimensional reduction, and outlier-detection operators.
The process can now be saved and executed.
Step 3: Execution and Interpretation
The result dataset can be sorted by outlier attribute, which has either a true
or false value. Since 10 outliers have been specified in the parameter of the
Detect outlier operator, that number of outliers can be found in the result set.
An efficient way of exploring the outliers is to look at the scatterplot in the
Chart view of results set. The X- and Y-axes can be specified as the principal
components and the color as the outlier attribute. The output scatterplot
shows the outlier data points along with all the normal data points as shown
inFig. 13.6 .
Distance-based outlier detection is a simple algorithm that is easy to imple-
ment and widely used when the problem involves many numeric variables.
The execution becomes expensive when the dataset involves a high number
of attributes and records, because the algorithm has to calculate distances
with other data points in high-dimensional space.
13.3 DENSITY-BASED OUTLIER DETECTION
Outliers, by definition, occur less frequently compared to normal data
points. This means that in the data space outliers occupy low-density areas
and normal data points occupy high-density areas. Density is a count of data
points in a normalized unit of space and is inversely proportional to the
FIGURE 13.5
Process to detect outliers based on distance.13.3 Density-Based Outlier Detection 457distances between data points. The objective of a density-based outlier algo-
rithm is to identify those data points from low-density areas. There are a few
different implementations to assign an outlier score for the data points. The
inverse of the average distance of all kneighbors can be found. The distance
between data points and density are inversely proportional. Neighborhood
density can also be calculated by calculating the number of data points from
a normalized unit distance. The approach for density-based outliers is similar
to the approach discussed for density-based clustering and for the k-NN clas-
sification algorithm.
13.3.1 How It Works
Since distance is the inverse of density, the approach of a density-based
outlier can be explained with two parameters, distance ( d) and proportion of
data points ( p). A point Xis considered an outlier if at least pfraction
of points lie more than ddistance from the point ( Knorr & Ng, 1998 ).
Fig. 13.7 provides a visual illustration of outlier detection. By the given defi-
nition, the point Xoccupies a low-density area. The parameter pis specified
FIGURE 13.6
Outlier detection output.458 CHAPTER 13: Anomaly Detectionas a high value, above 95%. One of the key issues in this implementation is
specifying distance. It is important to normalize the attributes so that the dis-
tance makes sense, particularly when attributes involve different measuresand units. If the distance is specified too low, then more outliers will be
detected, which means normal points have the risk of being labeled as out-
liers and vice versa.
13.3.2 How to Implement
The RapidMiner process for outlier detection based on density is similar tooutlier detection by distance, which was reviewed in the previous section.The process developed for previous distance-based outliers can be used, but
theDetect Outlier (Distances) operator would be replaced with the Detect
Outlier (Densities) operator.
Step 1: Data Preparation
Data preparation will condition the data so the Detect Outlier (Densities)
operator returns meaningful results. As with the outlier detection by distance
technique, the Iris dataset will be used with normalization and the PCA oper-
ator so that the number of attributes is reduced to two for easy visualization.
Step 2: Detect Outlier Operator
The Detect Outlier (Densities) operator can be found in Data Transformation
.Data Cleansing .Outlier Detection, and has three parameters:
Distance (d) : Threshold distance used to find outliers. For this example,
the distance is specified as 1.Proportion (p) : Proportion of data points outside of radius dof a point,
beyond which the point is considered an outlier. For this example, the
value specified is 95%.Distance measurement : A measurement parameter like Euclidean, cosine,
or squared distance. The default value is Euclidean.
FIGURE 13.7
Outlier detection based on distance and propensity.13.3 Density-Based Outlier Detection 459Any data point that has more than 95% of other data points beyond distance
dis considered an outlier. Fig. 13.8 shows the RapidMiner process with the
Normalization ,PCA, and Detect Outlier operators. The process can be saved
and executed.
Step 3: Execution and Interpretation
The process adds an outlier attribute to the example set, which can be used
for visualization using a scatterplot as shown in Fig. 13.9 . The outlier attri-
bute is Boolean and indicates whether the data point is predicted to be an
outlier or not. In the scatterplot, a few data points marked as outliers can be
found. The parameters dand pof the Detect Outlier operator can be tuned to
find the desired level of outlier detection.
Density-based outlier detection is closely related to distance-based outlier
approaches and, hence, the same pros and cons apply. As with distance-
based outlier detection, the main drawback is that this approach does not
work with varying densities. The next approach, local outlier factor (LOF) is
designed for such datasets. Specifying the parameter distance ( d) and propor-
tion ( p) is going to be challenging, particularly when the characteristics of
the data are not previously known.
13.4 LOCAL OUTLIER FACTOR
The LOF technique is a variation of density-based outlier detection, and
addresses one of its key limitations, detecting the outliers in varying density.
Varying density is a problem in simple density-based methods, including
FIGURE 13.8
Process to detect outliers based on density.460 CHAPTER 13: Anomaly DetectionDBSCAN clustering (see Chapter 7: Clustering). The LOF technique was pro-
posed in the paper LOF: Identifying Density-Based Local Outliers (Breunig,
Kriegel, Ng, & Sander, 2000 ).
13.4.1 How it Works
LOF takes into account the density of the data point and the density of the
neighborhood of the data point as well. A key feature of the LOF technique is
that the outlier score takes into account the relative density of the data point.
Once the outlier scores for data points are calculated, the data points can be
sorted to find the outliers in the dataset. The core of the LOF lies in the cal-
culation of the relative density. The relative density of a data point Xwith k
neighbors is given by the following equation:
Relative density of X5Density of X
Average density of all data points in the neighborhoodð13 :1Þ
Outlier False True
2.75
2.252.50
2.00
–2.75
–3.0 –2.5 –1.5 –0.5 0.5 1.0 1.5 0.0 –2.0 2.0 2.5 3.0 3.5
pc_1–1.0pc_2
–2.50–2.25–2.001.75
1.50
1.25
–1.75–1.50–1.251.00
–1.000.75
–0.750.50
–0.500.25
–0.250.00
FIGURE 13.9
Output of density-based outlier detection.13.4 Local Outlier Factor 461where the density of Xis the inverse of the average distance for the nearest k
data points. The same parameter kalso forms the locality of the neighbor-
hood. By comparing the density of the data point and density of all the datapoints in the neighborhood, whether the density of the data point is lower
than the density of the neighborhood can be determined. This scenario indi-
cates the presence of an outlier.
13.4.2 How to Implement
A LOF-based data science process is similar to the other outlier processes
explained in RapidMiner. The Detect Outlier (LOF) operator is available in
Data Transformation .Data Cleansing .Outlier Detection. The output of
theLOF operator contains the example set along with a numeric outlier
score. The LOF algorithm does not explicitly label a data point as an outlier;
instead the score is exposed to the user. This score can be used to visualize acomparison to a threshold, above which the data point is considered an out-
lier. Having the raw score means that the data science practitioner can “tune ”
the detection criteria, without having to rerun the scoring process, by chang-ing the threshold for comparison.
Step 1: Data Preparation
Similar to the distance- and density-based outlier-detection processes, thedataset has to be normalized using Normalize operator. The PCA operator is
FIGURE 13.10
RapidMiner process for LOF outlier detection. LOF, Local outlier factor.462 CHAPTER 13: Anomaly Detectionused to reduce the four-dimensional Iris dataset to two dimensions, so that
the output can be visualized easily.
Step 2: Detect Outlier Operator
The LOF operator has minimal points ( MinPts ) lower bound and upper
bound as parameters. The MinPts lower bound is the value of k, the neigh-
borhood number. The LOF algorithm also takes into account a MinPts upper
bound to provide more stable results ( Breunig et al., 2000 ).Fig. 13.10 shows
the RapidMiner process.
Step 3: Results Interpretation
After using the Detect Outlier operator, the outlier score is appended to the
result dataset. Fig. 13.11 shows the result set with outlier score represented as
the color of the data point. In the results window, the outlier score can be
used to color the data points. The scatterplot indicates that points closer to
the blue spectrum (left side of the outlier scale in the chart legend) are
predicted to be regular data points and points closer to the red spectrum
2.750.908 4.245
2.252.50
2.00
–2.75
–3.0 –2.5 –1.5 –0.5 0.5 1.0 1.5 0.0 –2.0 2.0 2.5 3.0 3.5
pc_1–1.0pc_2
–2.50–2.25–2.001.75
1.50
1.25
–1.75–1.50–1.251.00
–1.000.75
–0.750.50
–0.500.25
–0.250.00Outlier
FIGURE 13.11
Output of LOF outlier detection. LOF, Local outlier factor.13.4 Local Outlier Factor 463(right side of the outlier scale in the chart legend) are predicted to be out-
liers. If an additional Boolean flag indicating whether a data point is an out-
lier or not is needed, a Numeric to Binominal operator can be added to the
result dataset. The Numeric to Binominal operator converts the numeric outlier
score to a binominal true or false based on the threshold specification in the
parameter of the operator and to the score output from the LOF operator.
In addition to the three data science techniques discussed for outlier detec-
tion, the RapidMiner Anomaly Detection extension (RapidMiner Extension:
Anomaly Detection, 2014 ) offers more algorithms to identify outliers.
RapidMiner extensions can be installed by accessing Help .Updates and
Extensions.
13.5 CONCLUSION
In theory, any classification algorithm can be used for outlier detection, if a
previously classified dataset is available. A generalized classification model tries
to predict outliers the same way it predicts the class label of the data point.
However, there is one key issue in using classification models. Since the proba-
bility of occurrence of an outlier is really low, say, less than 0.1%, the model
can just “predict ”the class as “regular ”for all the data points and still be
99.9% accurate! This method clearly does not work for outlier detection, since
therecall measure (see Chapter 8: Model Evaluation for details about recall) is
0%. In practical applications, like detecting network intrusion or fraud preven-
tion in high-volume transaction networks, the cost of not detecting an outlier
is very high. The model can even have an acceptable level of false alarms, that
is, labeling a regular data point as an outlier. Therefore, special care and prepa-
ration is required to improve the detection of the outliers.
Stratified sampling methods can be used to increase the frequency of occur-
rence of outlier records in the training set and reduce the relative occurrence
of regular data points. In a similar approach, the occurrence of outliers and
regular records can be sampled with replacements so that there are an equal
number of records in both classes. Stratified sampling boosts the number of
outlier records in the test dataset with respect to regular records in an attempt
to increase both the accuracy and recall of outlier detection. In any case, it is
important to know the biases in any algorithm that might be used to detect
outliers and to specially prepare the training dataset in order to make the
resulting model effective. In practical applications, outlier-detection models
have to be updated frequently as the characteristics of an outlier changes
over time, and hence, the relationship between outliers and normal records
changes as well. In constant real time data streams, outlier detection creates
additional challenges because of the dynamic distribution of the data and464 CHAPTER 13: Anomaly Detectiondynamic relationships within the data ( Sadik & Gruenwald, 2013 ). Outlier
detection remains one of the most profound applications of data science as
it impacts the majority of the population through financial transaction moni-
toring, fraud prevention, and early identification of anomalous activity in the
context of security.
References
Breunig, M. M., Kriegel, H., Ng, R. T., & Sander, J. (2000). LOF: Identifying density-based local
outliers. In Proceedings of the ACM SIGMOD 2000 international conference on management of
data (pp. 1 /C012).
Haddadi, H. (2010). Fighting online click-fraud using bluff ads. ACM SIGCOMM Computer
Communication Review ,40(2), 21 /C025.
Knorr, E. M., & Ng, R. T. (1998) Algorithms for mining distance-based outliers in large datasets.
InProceedings of the 24th VLDB conference (pp. 392 /C0403). New York, USA.
RapidMiner Extension: Anomaly Detection. (2014). German research center for artificial intelligence .
DFKI GmbH. Retrieved from ,http://madm.dfki.de/rapidminer/anomalydetection ..
Sadagopan, N., & Li, J. (2008) Characterizing typical and atypical user sessions in clickstreams.
InProceeding of the 17th international conference on World Wide Web —WWW ‘08 885 .
,https://doi.org/10.1145/1367497.1367617 ..
Sadik, S., & Gruenwald, L. (2013). Research issues in outlier detection for data streams. ACM
SIGKDD Explorations Newsletter ,15(1), 33 /C040.
Tan, P.-N., Steinbach, M., & Kumar, V. (2005). Anomaly detection .Introduction to data mining
(pp. 651 /C0676). Boston, MA: Addison Wesley.References 465CHAPTER 14
Feature Selection
In this chapter the focus will be on an important component of dataset prep-
aration for data science: feature selection. An overused rubric in data science
circles is that 80% of the analysis effort is spent on data cleaning and prepa-
ration and only 20% is typically spent on modeling. In light of this it may
seem strange that this book has devoted more than a dozen chapters to
modeling techniques and only a couple to data preparation! However, data
cleansing and preparation are things that are better learned through experi-
ence and not so much from a book. That said, it is essential to be conversant
with the many techniques that are available for these important early process
steps. In this chapter the focus will not be on data cleaning, as it was partially
covered in Chapter 2, Data Science Process, but rather on reducing a dataset
to its essential characteristics or features. This process is known by various
terms: feature selection, dimension reduction, variable screening, key parame-
ter identification, attribute weighting or regularization. Regularization was
briefly covered in Chapter 5 as applied to multiple linear regression. There it
was introduced as a process that helps to reduce overfitting, which is essen-
tially what feature selection techniques implicitly achieve. [Technically, there
is a subtle difference between dimension reduction and feature selection.
Dimension reduction methods —such as principal component analysis
(PCA), discussed in Section 14.2 —combine or merge actual attributes in
order to reduce the number of attributes of a raw dataset. Feature selection
methods work more like filters that eliminate some attributes.]
First a brief introduction to feature selection along with the need for this pre-
processing step is given. There are fundamentally two types of feature selec-
tion processes: filter type and wrapper type . Filter approaches work by selecting
only those attributes that rank among the top in meeting certain stated crite-
ria (Blum & Langley, 1997; Yu & Liu, 2003 ). Wrapper approaches work by
iteratively selecting, via a feedback loop, only those attributes that improve
the performance of an algorithm ( Kohavi & John, 1997 ). Among the filter-
type methods, one can further classify based on the data types: numeric
versus nominal. The most common wrapper-type methods are the ones
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00014-9
©2019 Elsevier Inc. All rights reserved.467associated with multiple regression: stepwise regression, forward selection,
and backward elimination. A few numeric filter-type methods will be
explored: PCA, which is strictly speaking a dimension reduction method;
information gain /C0based filtering; and one categorical filter-type method: chi-
square-based filtering.
14.1 CLASSIFYING FEATURE SELECTION METHODS
IDENTIFYING WHAT MATTERS
Feature selection in data science refers to the process of
identifying the few most important variables or attributes
that are essential to a model for an accurate prediction. In
today ’s world of big data and high-speed computing, one
might be forgiven for asking, why bother? What is the rea-
son to filter any attributes when the computing horse-
power exists? For example, some argue that it is
redundant trying to fit a model to data; rather one should
simply use a fast brute-force approach to sift through
data to identify meaningful correlations and make deci-
sions based on this ( Bollier, 2010 ).
However, models are still useful for many reasons.
Models can improve decision making and help advanceknowledge. Blindly relying on correlations to predict future
states also has flaws. The now popular “My TiVo thinks
I’m gay ”example ( Zaslow, 2002 ), illustrated how the TiVo
recommendation engine, which works on large data and
correlations, resulted in a humorous mismatch for a cus-
tomer. As long as the use of models is needed, feature
selection will be an important step in the process. Feature
selection serves a couple of purposes: it optimizes the
performance of the data science algorithm and it makes it
easier for the analyst to interpret the outcome of the
modeling. It does this by reducing the number of attri-
butes or features that one must contend with.
There are two powerful technical motivations for incorporating feature selec-
tion in the data science process. First, a dataset may contain highly correlated
attributes, such as the number of items sold, and the revenue earned by the
sales of the item. Typically, there is no new information gained by including
both of these attributes. Additionally, in the case of multiple regression /C0type
models, if two or more of the independent variables (or predictors) are corre-
lated, then the estimates of coefficients in a regression model tend to be
unstable or counter intuitive. This is the multicollinearity discussed in
Section 5.1. In the case of algorithms like naïve Bayesian classifiers, the attri-
butes need to be independent of each other. Further, the speed of algorithms
is typically a function of the number of attributes. So, by using only one
among the correlated attributes the performance is improved.
Second, a dataset may also contain redundant information that does not
directly impact the predictions: as an extreme example, a customer ID num-
ber has no bearing on the amount of revenue earned from the customer.
Such attributes may be filtered out by the analyst before the modeling pro-
cess begins. However, not all attribute relationships are that clearly known in468 CHAPTER 14: Feature Selectionadvance. In such cases, one must resort to computational methods to detect
and eliminate attributes that add no new information. The key here is to
include attributes that have a strong correlation with the predicted or depen-
dent variable.
So, to summarize, feature selection is needed to remove independent variables
that may be strongly correlated to one another, and to keep independent vari-
ables that may be strongly correlated to the predicted or dependent variable.
Feature selection methods can be applied before the modeling process starts
and, thus, unimportant attributes will get filtered out, or feature selection
methods can be applied iteratively within the flow of the data science pro-
cess. Depending on the logic, there are two feature selection schemes: filter
schemes or wrapper schemes. The filter scheme does not require any learning
algorithm, whereas the wrapper type is optimized for a particular learning
algorithm. In other words, the filter scheme can be considered unsupervised
and the wrapper scheme can be considered a supervised feature selection
method. The filter model is commonly used:
1.When the number of features or attributes is really large
2.When computational expense is a criterion
The chart in Fig. 14.1 summarizes a high-level taxonomy of feature selection
methods, some of which will be explored in the following sections, as indi-
cated. This is not meant to be a comprehensive taxonomy, but simply a
FIGURE 14.1
Taxonomy of common feature selection methods and the sections in this chapter that discuss them.14.1 Classifying Feature Selection Methods 469useful depiction of the techniques commonly employed in data science and
described in this chapter.
14.2 PRINCIPAL COMPONENT ANALYSIS
To start with a conceptual introduction to PCA will be provided before the
mathematical basis behind the computation is shown. Then a demonstration
of how to apply PCA to a sample dataset using RapidMiner will be given.
Assume that there is a dataset with mattributes. These could be for example,
commodity prices, weekly sales figures, number of hours spent by assembly
line workers, etc.; in short, any business parameter that can have an impact
on a performance that is captured by a label or target variable. The question
that PCA helps to answer is fundamentally this: Which of these mattributes
explain a significant amount of variation contained within the dataset? PCA
essentially helps to apply an 80/20 rule: Can a small subset of attributes
explain 80% or more of the variation in the data? This sort of variable
screening or feature selection will make it easy to apply other data science
techniques and also make the job of interpreting the results easier.
PCA captures the attributes that contain the greatest amount of variability in
the dataset. It does this by transforming the existing variables into a set of
principal components or new variables that have the following properties
(van der Maaten, Postma, & van den Herik, 2009 ):
1.They are uncorrelated with each other.
2.They cumulatively contain/explain a large amount of variance within
the data.
3.They can be related back to the original variables via weighting factors.
The original variables with very low weighting factors in their principal com-
ponents are effectively removed from the dataset. The conceptual schematic
inFig. 14.2 illustrates how PCA can help in reducing data dimensions with a
hypothetical dataset of mvariables.
14.2.1 How It Works
The key task is computing the principal components, zm, which have the
properties that were described. Consider the case of just two variables: v1and
v2. When the variables are visualized using a scatterplot, something like the
one shown in Fig. 14.3 would be observed.
As can be seen, v1and v2are correlated. But v1and v2could be transformed
into two new variables z1and z2, which meet the guidelines for principal
components, by a simple linear transformation. As seen in the chart, this470 CHAPTER 14: Feature Selectionamounts to plotting the points along two new axes: z1and z2. Axis z1con-
tains the maximum variability, and one can rightly conclude that z1explains
a significant majority of the variation present in the data and is the first prin-cipal component. z
2, by virtue of being orthogonal to z1, contains the next
highest amount of variability. Between z1and z2, 100% of the total variabil-
ity in the data can be accounted for (in this case of two variables).
FIGURE 14.2
A conceptual framework illustrating the effectiveness of using PCA for feature selection. The final dataset includes only PC 1 and PC 2.
PCA, Principal Component Analysis.
FIGURE 14.3
Transforming variables to a new basis is at the core of PCA. PCA, Principal Component Analysis.14.2 Principal Component Analysis 471Furthermore, z1and z2are uncorrelated. As the number of variables, vm,
increases it ’s possible that only the first few principal components are suffi-
cient to express all the data variances. The principal components, zm, are
expressed as a linear combination of the underlying variables, vm:
zm5X
wi3xi ð14 :1Þ
When this logic is extended to more than two variables, the challenge is to
find the transformed set of principal components using the original variables.
This is easily accomplished by performing an eigenvalue analysis of the covari-
ance matrix of the original attributes.1The eigenvector associated with the
largest eigenvalue is the first principal component; the eigenvector associated
with the second largest eigenvalue is the second principal component and so
on. The covariance explains how two variables vary with respect to their cor-
responding mean values —if both variables tend to stay on the same side of
their respective means, the covariance would be positive, if not it would be
negative. (In statistics, covariance is also used in the calculation of correlation
coefficient.)
Cov ij5EV iVj/C2/C3
2EV i½/C138EV j/C2/C3
ð14 :2Þ
where expected value E[v]5vkP(v5vk). For the eigenvalue analysis, a matrix
of such covariances between all pairs of variables vmis created. For more
details behind the eigenvalue analysis refer to standard textbooks on matrix
methods or linear algebra ( Yu & Liu, 2003 ).
14.2.2 How to Implement
In this section, with the use of a publicly available dataset,2RapidMiner will
be used to perform the PCA. Furthermore, for illustrative reasons, non-
standardized or non-normalized data will be used. In the next part the data
will be standardized and why it may be important to sometimes do so will
be explained.
The dataset includes information on ratings and nutritional information on
77 breakfast cereals. There are a total of 16 variables, including 13 numerical
parameters ( Table 14.1 ). The objective is to reduce this set of 13 numerical
predictors to a much smaller list using PCA.
1LetAbe an nXn matrix and xbe an nX1 vector. Then the solution to the vector equation
[A][x]5λ[x], where λis a scalar number, involves finding those values of λfor which this equation is
satisfied. The values of λare called eigenvalues and the corresponding solutions for x(x6¼0) are
called eigenvectors.
2https://www.kaggle.com/jeandsantos/breakfast-cereals-data-analysis-and-clustering/data .472 CHAPTER 14: Feature SelectionTable 14.1 Breakfast Cereals Dataset for Dimension Reduction Using PCA
Name mfr Type Calories Protein Fat Sodium Fiber Carbo Sugars Potass Vitamins Shelf Weight Cups Rating
100%_Bran N C 70 4 1 130 10 5 6 280 25 3 1 0.33 68.402973
100%_Natural_Bran Q C 120 3 5 15 2 8 8 135 0 3 1 1 33.983679
All-Bran K C 70 4 1 260 9 7 5 320 25 3 1 0.33 59.425505
All-Bran_with_Extra_Fiber K C 50 4 0 140 14 8 0 330 25 3 1 0.5 93.704912
Almond_Delight R C 110 2 2 200 1 14 8 -1 25 3 1 0.75 34.384843
Apple_Cinnamon_Cheerios G C 110 2 2 180 1.5 10.5 10 70 25 1 1 0.75 29.509541
Apple_Jacks K C 110 2 0 125 1 11 14 30 25 2 1 1 33.174094
Basic_4 G C 130 3 2 210 2 18 8 100 25 3 1.33 0.75 37.038562Step 1: Data Preparation
Remove the non-numeric parameters such as Cereal name, Manufacturer,
and Type (hot or cold), as PCA can only work with numeric attributes. These
are the first three columns in Table 14.1 . (In RapidMiner, these can be con-
verted into ID attributes if needed for reference later. This can be done during
the import of the dataset into RapidMiner during the next step if needed; in
this case these variables will simply be removed. The Select Attributes operator
may also be used following the Read Excel operator to remove these
variables.) Read the Excel file into RapidMiner: this can be done using the
standard Read Excel operator as described in earlier sections.
Step 2: PCA Operator
Type in the keyword PCA in the operator search field and drag and drop the
PCA operator into the main process window. Connect the output of Read
Excel into the ports of the PCA operator.
The three available parameter settings for dimensionality reduction are none,
keep variance , and fixed number . Here use keep variance and leave the variance
threshold at the default value of 0.95% or 95% (see Fig. 14.4 ). The variance
threshold selects only those attributes that collectively account for or explain
95% (or any other value set by user) of the total variance in the data.
Connect all output ports from the PCA operator to the results ports.
Step 3: Execution and Interpretation
By running the analysis as configured ab ove, RapidMiner will output several tabs
in the results panel ( Fig. 14.5 ). By clicking on the PCA tab, three PCA related
tabs will be seen —Eigenvalues, Eigenvectors, a nd Cumulative Variance Plot.
Using Eigenvalues, information can be obtained about the contribution to
the data variance coming from each principal component individually and
cumulatively.
FIGURE 14.4
Configuring the PCA operator. PCA, Principal Component Analysis.474 CHAPTER 14: Feature SelectionIf, for example, our variance threshold is 95%, then PC 1, PC 2, and PC 3
are the only principal components that need to be considered because they
are sufficient to explain nearly 97% of the variance. PC 1 contributes to a
majority of this variance, about 54%.
One can then deep dive into these three components and identify how they
are linearly related to the actual or real parameters from the dataset. At this
point only those real parameters can be considered that have significant
weight contribution to the each of the first three PCs. These will ultimately
form the subset of reduced parameters for further predictive modeling.
The key question is how does one select the real variables based on this infor-
mation? RapidMiner allows the eigenvectors (weighting factors) to be sorted
for each PC and one can decide to choose the two to three highest (absolute)
valued weighting factors for PCs 1 /C03. As seen from Fig. 14.6 , the highlighted
real attributes have been chosen —calories, sodium, potassium, vitamins, and
rating —to form the reduced dataset. This selection was done by simply identi-
fying the top three attributes from each principal component.3
For this example, PCA reduces the number of attributes from 13 to 5, a more
than 50% reduction in the number of attributes that any model would need
FIGURE 14.5
Output from PCA. PCA, Principal Component Analysis.
3More commonly, only the top three principal components are directly selected for building
subsequent models. This route was taken here to explain how PCA, which is a dimension reduction
method, can be applied for feature selection.14.2 Principal Component Analysis 475to realistically consider. One can imagine the improvement in performance
as one deals with the larger datasets that PCA enables. In practice, PCA is an
effective and widely used tool for dimension reduction, particularly when allthe attributes are numeric. It works for a variety of real-world applications,
but it should not be blindly applied for variable screening. For most practical
situations, domain knowledge should be used in addition to PCA analysisbefore eliminating any of the variables. Here are some observations that
explain some of the risks to consider while using PCA.
1.The results of a PCA must be evaluated in the context of the data . If the data
is extremely noisy, then PCA may end up suggesting that the noisiest
variables are the most significant because they account for most of thevariation! An analogy would be the total sound energy in a rock
concert. If the crowd noise drowns out some of the high-frequency
vocals or notes, PCA might suggest that the most significantcontribution to the total energy comes from the crowd —and it will be
right! But this does not add any clear value if one is attempting to
distinguish which musical instruments are influencing the harmonics,for example.
2.Adding uncorrelated data does not always help. Neither does adding data thatmay be correlated, but irrelevant . When more parameters are added to the
FIGURE 14.6
Selecting the reduced set of attributes using the Eigenvectors tab from the PCA operator. PCA, Principal Component Analysis.476 CHAPTER 14: Feature Selectiondataset, and if these parameters happen to be random noise, effectively
the same situation as the first point applies. On the other hand,
caution also has to be exercised and spurious correlations have to
looked out for. As an extreme example, it may so happen that there is
a correlation between the number of hours worked in a garment
factory and pork prices (an unrelated commodity) within a certain
period of time. Clearly this correlation is probably pure coincidence.
Such correlations again can muddy the results of a PCA. Care must be
taken to winnow the dataset to include variables that make business
sense and are not subjected to many random fluctuations before
applying a technique like PCA.
3.PCA is very sensitive to scaling effects in the data . If the data in the
example is closely examined, it will be observed that the top attributes
that PCA helped identify as the most important ones also have the
widest range (and standard deviation) in their values. For example,
potassium ranges from 21 to 330 and sodium ranges from 1 to 320.
Comparatively, most of the other factors range in the single or low
double digits. As expected, these factors dominate PCA results because
they contribute to the maximum variance in the data. What if there
was another factor such as sales volume, which would potentially
range in the millions (of dollars or boxes), that were to be considered
for a modeling exercise? Clearly it would mask the effects of any other
attribute.
To minimize scaling effects, one can range normalize the data (using for
example, the Normalize operator). When this data transformation is applied,
all the attributes are reduced to a range between 0 and 1 and scale effects
will not matter anymore. But what happens to the PCA results?
AsFig. 14.7 shows, eight PCs are now needed to account for the same 95%
total variance. As an exercise, use the eigenvectors to filter out the attributes
that are included in these eight PCs and it would be observed that (applying
the top three rule for each PC as before), none of the attributes would be
eliminated!
This leads to the next section on feature selection methods that are not scale
sensitive and also work with non-numerical datasets, which were two of the
limitations with PCA.
14.3 INFORMATION THEORY-BASED FILTERING
In Chapter 4, Classification, the concepts of information gain and gain ratio
were encountered. Recall that both of these methods involve comparing the
information exchanged between a given attribute and the target or label14.3 Information Theory-Based Filtering 477attribute ( Peng, Long, & Ding, 2005 ). As discussed in Section 14.1 , the key
to feature selection is to include attributes that have a strong correlation with
the predicted or dependent variable. With these techniques, one can rank
attributes based on the amount of information gain and then select only
those that meet or exceed some (arbitrarily) chosen threshold or simply
select the top k (again, arbitrarily chosen) features.
Recall the golf example discussed first in Chapter 4, Classification. The data
is presented here again for convenience in Fig. 14.8 A. When the information
gain calculation methodology that was discussed in Chapter 4, Classification,
is applied to compute information gain for all attributes (see Table 4.2), the
feature ranking in Fig. 14.8 B will be reached, in terms of their respective
influence on the target variable Play. This can be easily done using the
Weight by Information Gain operator in RapidMiner. The output looks almost
identical to the one shown in Table 4.2, except for the slight differences in
the information gain values for Temperature and Humidity. The reason is
that for that dataset, the temperature and humidity had to be converted into
nominal values before computing the gains. In this case, the numeric attri-
butes are used as they are. So, it is important to pay attention to the discreti-
zation of the attributes before filtering. Use of information gain feature
selection is also restricted to cases where the label is nominal. For fully
numeric datasets, where the label variable is also numeric, PCA or
correlation-based filtering methods are commonly used.
FIGURE 14.7
Interpreting RapidMiner output for principal component analysis.478 CHAPTER 14: Feature SelectionFig. 14.9 describes a process that uses the sample Golf dataset available in
RapidMiner. The various steps in the process convert numeric attributes,
Temperature and Humidity, into nominal ones. In the final step, the Weight
by Information Gain operator is applied to both the original data and the con-
verted dataset in order to show the difference between the gain computed
using different data types. The main point to observe is that the gain compu-
tation depends not only on the data types, but also on how the nominal
FIGURE 14.8
(A) Revisiting the golf example for feature selection. (B) Results of information gain /C0based feature selection.
FIGURE 14.9
Process to discretize the numeric Golf dataset before running information gain /C0based feature selection.14.3 Information Theory-Based Filtering 479data is discretized. For example, one gets slightly different gain values (see
Table 14.2 ) if Humidity is divided into three bands (high, medium, and
low) as opposed to only two bands (high and low). These variants can be
tested easily using the process described. In conclusion, the top-ranked attri-
butes are selected. In this case, they would be Outlook and Temperature if
one chose the non-discretized version, and Outlook and Humidity in the dis-
cretized version.
14.4 CHI-SQUARE-BASED FILTERING
In many cases the datasets may consist of only categorical (or nominal) attri-
butes. In this case, what is a good way to distinguish between high influence
attributes and low or no influence attributes?
A classic example of this scenario is the gender selection bias. Suppose one
has data about the purchase of a big-ticket item like a car or a house. Can
the influence of gender on purchase decisions be verified? Are men or
women the primary decision makers when it comes to purchasing big-ticket
items? For example, is gender a factor in color preference of a car? Here attri-
bute 1 would be gender and attribute 2 would be the color. A chi-square test
would reveal if there is indeed a relationship between these two attributes. If
there are several attributes and one wishes to rank the relative influence of
each of these on the target attribute, the chi-square statistic can still be used.
Back to the golf example in Fig. 14.10 —this time all numeric attributes have
been converted into nominal ones. Chi-square analysis involves counting
occurrences (number of sunny days or windy days) and comparing these vari-
ables to the target variable based on the frequencies of occurrences. The chi-
square test checks if the frequencies of occurrences across any pair of attri-
butes, such as Outlook 5overcast and Play 5yes, are correlated. In other
words, for the given Outlook type, overcast, what is the probability that
Play5yes (existence of a strong correlation)? The multiplication law of prob-
abilities states that if event A happening is independent of event B, then the
probabilities of A and B happening together is simply pA3pB. The next stepTable 14.2 Results of Information Gain Feature Selection
Attribute Info Gain Weight (Not Discretized) Info Gain Weight (Discretized)
Outlook 0.247 0.247
Temperature 0.113 0.029
Humidity 0.102 0.104
Wind 0.048 0.048480 CHAPTER 14: Feature Selectionis to convert this joint probability into an expected frequency, which is given
bypA3pB3N, where Nis the sum of all occurrences in the dataset.
Foreach attribute, a table of observed frequencies, such as the one shown in
Table 14.3 , is built. This is called a contingency table . The last column and row
(the margins) with heading ‘Totals ’are simply the sums in the corresponding
rows or columns as can be verified. Using the contingency table, a corre-
sponding expected frequency table can be built using the expected frequency
definition ( pA3pB3N) from which the chi-square statistic is then computed
by comparing the difference between the observed frequency and expected
frequency for each attribute. The expected frequency table for Outlook is
shown in Table 14.4 .
FIGURE 14.10
Converting the golf example set into nominal values for chi-square feature selection.
Table 14.3 Contingency Table of Observed Frequencies for Outlook and the
Label Attribute, Play
Outlook Sunny Overcast Rain Total
Play5no 3 0 2 5
Play5yes 2 4 3 9
Total 54 5 1 414.4 Chi-Square-Based Filtering 481The expected frequency for the event [Play 5no and Outlook 5sunny] is cal-
culated using the expected frequency formula: (5/14 35/14314)51.785
and is entered in the first cell as shown. Similarly, the other expected fre-
quencies are calculated. The formula for the chi-square statistic is the summa-
tion of the square of the differences between observed and expected
frequencies, as given in Eq. (14.3) :
χ25XX fo2fe ðÞ2
feð14 :3Þ
where fois the observed frequency and feis the expected frequency. The test
of independence between any two parameters is done by checking if the
observed chi-square is less than a critical value which depends on the confi-
dence level chosen by the user ( Black, 2007 ). In this case of feature weight-
ing, all the observed chi-square values are simply gathered and used to rank
the attributes. The ranking of attributes for the golf example is generated
using the process described in Fig. 14.11 and is shown in the table ofTable 14.4 Expected Frequency Table
Outlook Sunny Overcast Rain Total
Play5no 1.785714 1.428571 1.785714 5
Play5yes 3.214286 2.571429 3.214286 9
Total 5451 4
FIGURE 14.11
Process to rank attributes of the Golf dataset by the chi-square statistic.482 CHAPTER 14: Feature Selectionobserved chi-square values in Fig. 14.14 . Just like in information gain feature
selection, most of the operators shown in the process are simply transform-
ing the data into nominal values to generate it in the form shown in
Fig. 14.10 .
Compare the output of the chi-square ranking to the information
gain /C0based ranking (for the nominalized or discretized attributes) and it
will be evident that the ranking is identical (see Fig. 14.12 ).
Note that the Normalize weights option is sometimes also used, which is a
range normalization onto the interval 0 to 1.
14.5 WRAPPER-TYPE FEATURE SELECTION
In this section of the chapter, wrapper scheme feature reduction methods
will briefly be introduced using a linear regression example. As explained ear-
lier, the wrapper approach iteratively chooses features to add or to remove
from the current attribute pool based on whether the newly added or
removed attribute improves the accuracy.
Wrapper-type methods originated from the need to reduce the number of
attributes that are needed to build a high-quality regression model. A thor-
ough way to build regression models is something called the “all possible
regressions ”search procedure. For example, with three attributes, v1,v2, and
v3, one could build the different regression models in Table 14.5 .
In general, if a dataset contains kdifferent attributes, then conducting all pos-
sible regression searches implies that one builds 2k21 separate regression
FIGURE 14.12
Results of the attribute weighting by the chi-square method.14.5 Wrapper-Type Feature Selection 483models and picks the one that has the best performance. Clearly this is
impractical.
A better way, from a computational resource consumption point of view, to
do this search would be to start with one variable, say v1, and build a base-
line model. Then add a second variable, say v2, and build a new model to
compare with the baseline. If the performance of the new model, for exam-
ple, the R2(see Chapter 5: Regression Methods), is better than that of the
baseline, this model can be made to be the new baseline, add a third vari-
able, v3, and proceed in a similar fashion. If, however, the addition of the
second attribute, v2, did not improve the model significantly (over some
arbitrarily prescribed level of improvement in performance), then a new attri-
bute v3 can be chosen, and a new model built that includes v1 and v3. If this
model is better than the model that included v1 and v2, proceed to the next
step, where the next attribute v4 can be considered, and a model built that
includes v1,v3, and v4. In this way, one steps forward selecting attributes
one by one until the desired level of model performance is achieved. This
process is called forward selection.4
A reverse of this process is where the baseline model with all the attributes is
started, v1,v2, ...,vkand for the first iteration, one of the variables, vj,i s
removed and a new model is constructed. However, how does one select
which vjto remove? Here, it is typical to start with a variable that has the
lowest t-stat value, seen in the case study described below.5If the new model
is better than the baseline, it becomes the new baseline and the search con-
tinues to remove variables with the lowest t-stat values until some stoppingTable 14.5 All Possible Regression Models With Three Attributes
Model Independent Variables Used
1 v1 alone
2 v2 alone
3 v3 alone
4 v1 and v2 only
5 v1 and v3 only
6 v2 and v3 only
7 v1,v2, and v3 all together
4Forward selection is considered a “greedy ”approach, and does not necessarily yield the globally
optimum solution.
5RapidMiner typically tries removing attributes one after the other. Vice versa for forward selection:
first it tries out all models having just one attribute. It selects the best, then adds another variable,
again trying out every option.484 CHAPTER 14: Feature Selectioncriterion is met (usually if the model performance is not significantly
improved over the previous iteration). This process is called backward
elimination .
As observed, the variable selection process wraps around the modeling proce-
dure, hence, the name for these classes of feature selection. A case study will
now be examined, using data from the Boston Housing dataset first intro-
duced in Chapter 5, Regression Methods, to demonstrate how to implement
the backward elimination method using RapidMiner. Recall that the data
consists of 13 predictors and 1 response variable. The predictors include
physical characteristics of the house (such as the number of rooms, age, tax,
and location) and neighborhood features (schools, industries, zoning),
among others. The response variable is the median value (MEDV) of the
house in thousands of dollars. These 13 independent attributes are consid-
ered to be predictors for the target or label attribute. The snapshot of the
data table is shown again in Table 14.6 for continuity.
14.5.1 Backward Elimination
The goal here is to build a high-quality multiple regression model that
includes as few attributes as possible, without compromising the predictive
ability of the model.
The logic used by RapidMiner for applying these techniques is not linear, but
a nested logic. The graphic in Fig. 14.13 explains how this nesting was used
in setting up the training and testing of the Linear Regression operator for the
analysis done in Chapter 5, Regression Methods, on the Boston Housing
data. The arrow indicates that the training and testing process was nested
within the Split Validation operator.
In order to apply a wrapper-style feature selection method such as backward
elimination, the training and testing process will need to be tucked inside
another subprocess, a learning process. The learning process is now nested
inside the Backward Elimination operator. Therefore, a double nesting as sche-
matically shown in Fig. 14.13 is now obtained. Next, the Backward
Elimination operator can be configured in RapidMiner. Double clicking on
theBackward Elimination operator opens up the learning process, which can
now accept the Split Validation operator that has been used many times.
The Backward Elimination operator can now be filled in with the Split
Validation operator and all the other operators and connections required to
build a regression model. The process of setting these up is exactly the same
as discussed in Chapter 5, Regression Methods, and, hence, is not repeated
here. Now the configuration of the Backward Elimination operator will be
examined. Here one can specify several parameters to enable feature14.5 Wrapper-Type Feature Selection 485Table 14.6 Sample View of the Boston Housing Dataset
CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV
0.00632 18 2.31 0 0.538 6.575 65.2 4.09 1 296 15.3 396.9 4.98 24
0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.9 9.14 21.6
0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 34.70.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 33.40.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.9 5.33 36.20.02985 0 2.18 0 0.458 6.43 58.7 6.0622 3 222 18.7 394.12 5.21 28.70.08829 14.5 7.87 0 0.524 6.012 66.6 5.5605 5 311 15.2 395.6 14.43 22.90.14455 14.5 7.87 0 0.524 6.172 96.1 5.9505 5 311 15.2 396.9 19.15 27.1selection. The most important one is the stopping behavior . The choices are
“with decrease, ”“with decrease of more than, ”and “with significant
decrease. ”The first choice is highly parsimonious —a decrease from one itera-
tion to the next will stop the process. But if the second choice is chosen, one
now has to now indicate a “maximal relative decrease. ”In this example, a
10% decrease has been indicated. Finally, the third choice is very stringent
and requires achieving some desired statistical significance by allowing one
to specify an alpha level. But it has not been said by how much the perfor-
mance parameter should decrease yet! This is specified deep inside the nest-
ing: all the way at the Performance operator that was selected in the Testing
window of the Split Validation operator. In this example, the performance cri-
terion was squared correlation. For a complete description of all the other
Backward Elimination parameters, the RapidMiner help can be consulted.
There is one more step that may be helpful to complete before running this
model. Simply connecting the Backward Elimination operator ports to the out-
put will not show the final regression model equation. To be able to see
that, one needs to connect the example port of the Backward Elimination oper-
ator to another Linear Regression operator in the main process. The output of
this operator will contain the model, which can be examined in the Results
perspective. The top level of the final process is shown in Fig. 14.14 .
Comparing the two regression equations ( Fig. 14.15 and in Chapter 5:
Regression Methods, see Fig. 5.6A) it is evident that nine attributes have
been eliminated. Perhaps the 10% decrease was too aggressive. As it happens,
theR2for the final model with only three attributes was only 0.678. If the
stopping criterion was changed to a 5% decrease, one would end up with an
FIGURE 14.13
Wrapper function logic used by RapidMiner.14.5 Wrapper-Type Feature Selection 487R2of 0.812 and now have 8 of the 13 original attributes ( Fig. 14.16 ). It is
also evident that the regression coefficients for the two models are different
as well. The final judgment on what is the right criterion and its level can
only be made with experience with the dataset and of course, good domain
knowledge.
FIGURE 14.14
Final setup of the backward elimination wrapper process.
FIGURE 14.15
Aggressive feature selection. Maximal relative decrease 510%.488 CHAPTER 14: Feature SelectionEach iteration using a regression model either removes or introduces a vari-
able, which improves model performance. The iterations stop when a preset
stopping criterion or no change in performance criterion (such as adjusted r2
or RMS error) is reached. The inherent advantage of wrapper-type methods
are that multicollinearity issues are automatically handled. However, no prior
knowledge about the actual relationship between the variables will be
gained. Applying forward selection is similar and is recommended as an
exercise.
14.6 CONCLUSION
This chapter covered the basics of an important part of the overall data sci-
ence paradigm: feature selection or dimension reduction. A central hypothe-
sis among all the feature selection methods is that good feature selection
results in attributes or features that are highly correlated with the class, yet
uncorrelated with each other ( Hall, 1999 ). A high-level classification of fea-
ture selection techniques were presented and each of them were explored in
some detail. As stated at the beginning of this chapter, dimension reduction
is best understood with real practice. To this end, it is recommended that
one applies all the techniques described in this chapter on all the datasets
provided. The same technique can yield quite different results based on the
selection of analysis parameters. This is where data visualization can play an
FIGURE 14.16
A more permissive feature selection with backward elimination. Maximal relative decrease 55%.14.6 Conclusion 489important role. Sometimes, examining a correlation plot between the various
attributes, like in a scatterplot matrix, can provide valuable clues about which
attributes are likely redundant and which ones can be strong predictors of
the label variable. While there is usually no substitute for domain knowl-
edge, sometimes data are simply too large or mechanisms are unknown. This
is where feature selection can actually help.
References
Black, K. (2007). Business statistics . New York: John Wiley and Sons.
Blum, A. L., & Langley, P. (1997). Selection of relevant features and examples in machine learn-
ing. Artificial Intelligence ,97(1/C02), 245 /C0271.
Bollier, D. (2010). The promise and perils of big data . Washington, D.C.: The Aspen Institute.
Hall, M.A. (1999). Correlation based feature selection for machine learning (Ph.D. thesis). University
of Waikato: New Zealand.
Kohavi, R., & John, G. H. (1997). Wrappers for feature subset selection. Artificial Intelligence ,97
(1/C02), 273 /C0324.
Peng, H., Long, F., & Ding, C. (2005). Feature selection based on mutual information: Criteria of
max-dependency, max-relevance and min-redundancy. IEEE Transactions on Pattern Analysis
and Machine Intelligence ,27(8), 1226 /C01238.
van der Maaten, L. J. P., Postma, E. O., & van den Herik, H. J. (2009). Dimensionality reduction:
A comparative review. In: Tilburg University technical report .TiCC-TR .
Yu, L., Liu, H. (2003). Feature selection for high dimensional data: A fast correlation based filter
solution. In: Proceedings of the twentieth international conference on machine learning (ICML-
2003) . Washington, DC.
Zaslow, J. (December 4, 2002). Oh No! My TiVo thinks I ’m gay. Wall Street Journal .490 CHAPTER 14: Feature SelectionCHAPTER 15
Getting Started with RapidMiner
For someone who has never attempted any analysis using RapidMiner, this
chapter would be the best place to start. In this chapter the attention will be
turned away from the data science concepts and processes toward an actual
tool set needed for data science. The goal for this chapter is to get rid of any
trepidation that one may have about using the tool if this entire field of ana-
lytics is totally new. If perhaps someone has done some data science with
RapidMiner but gotten frustrated or stuck somewhere during the process of
self-learning using this powerful set of tools, then this chapter should hope-
fully help.
RapidMiner is an open source data science platform developed and main-
tained by RapidMiner Inc. The software was previously known as YALE (Yet
Another Learning Environment) and was developed at the University of
Dortmund in Germany ( Mierswa, 2006 ).
RapidMiner Studio is a graphical user interface or GUI-based software where
data science workflows can be built and deployed. Some of the advanced fea-
tures are offered at a premium. In this chapter, some of the common func-
tionalities and terminologies of the RapidMiner Studio platform will be
reviewed. Even though one specific data science tool is being emphasized,
the approach, process, and terms are all similar to other commercial and
open source data science tools.
Firstly a brief introduction to the RapidMiner Studio GUI will be given to set
the stage. The first step in any data analytics exercise is of course to bring the
data to the tool, and this is what will be covered next. Once the data is
imported, one may want to actually visualize the data and if necessary select
subsets or transform the data. Basic visualization is covered, followed by
selecting data by subsets. An overview of the fundamental data scaling and
transformation tools will be provided and data sampling and missing value
handling tools will be explained. Then some advanced capabilities of
RapidMiner will be presented such as process design and optimization.
Data Science. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00015-0
©2019 Elsevier Inc. All rights reserved.49115.1 USER INTERFACE AND TERMINOLOGY
It is assumed at this point that the software is already downloaded and
installed on a computer.1Once RapidMiner is launched, the screen in
Fig. 15.1 will be seen.
We start by assuming that a new process needs to be created. With that in
mind, a brand new process is started with by clicking on the “Blank ”option
seen at the top of Fig. 15.1 . Once this is done, the view changes to that
shown in Fig. 15.2 . Only two of the main sections: Design and Results panels
will be introduced, as the others (Turbo Prep and Auto Model) are not avail-
able in the free edition.
Views : The RapidMiner GUI offers two main views . The Design view is where
one can create and design all the data science processes and can be thought
of as the canvas where all the data science programs and logic will be created.
This can also be thought of as a workbench. The Results view is where all the
recently executed analysis results are available. Switching back and forth
between the Design and Results views several times during a session is very
FIGURE 15.1
Launch view of RapidMiner.
1Download the appropriate version from http://rapidminer.com .492 CHAPTER 15: Getting Started with RapidMinermuch an expected behavior for all users. When a new process is created one
starts with a blank canvas or uses a wizard-style functionality that allows
starting from predefined processes for applications such as direct marketing,
predictive maintenance, customer churn modeling, and sentiment analysis.
Panel : When a given view is entered, there will be several display elements
available. For example, in the Design view, there is a panel for all the avail-
able operators, stored processes, help for the operators, and so on. These
panels can be rearranged, resized, removed, or added to a given view. The
controls for doing any of these are shown right on the top of each panel tab.
First-time users sometimes accidentally delete some of the panels. The easi-
est way to bring back a panel is to use the main menu item View -Show
View and then select the view tha t was lost or reset using View -Restore
Default View, see Fig. 15.2 .
Terminology
There are a handful of terms that one must be comfortable with in order to
develop proficiency in using RapidMiner.
FIGURE 15.2
Activating different views inside RapidMiner.15.1 User Interface and Terminology 493Repository: Arepository is a folder-like structure inside RapidMiner where users
can organize their data, processes, and models. The repository is, thus, a
central place for all the data and analysis processes. When RapidMiner is
launched for the first time, an option to set up the New Local Repository
will be given ( Fig. 15.3 ). If for some reason this was not done correctly, it
can always be fixed by clicking on the New Repository icon (the one with
a“1Add Data ”button) in the Repositories panel. When that icon is
clicked on, a dialog box like the one shown in Fig. 15.3 will be given
where one can specify the name of a repository under Alias and its loca-
tion under Root Directory . By default, a standard location automatically
selected by the software is checked, which can be unchecked to specify a
different location.
Within this repository, folders and subfolders can be organized to store data,
processes, results, and models. The advantage of storing datasets to be ana-
lyzed in the repository is that metadata describing those datasets are stored
alongside. This metadata is propagated through the process as it is built.
Metadata is basically data about the data and contains information such as
the number of rows and columns, types of data within each column, missing
values if any, and statistical information (mean, standard deviation, and
so on).
FIGURE 15.3
Setting up a repository on one ’s local machine.494 CHAPTER 15: Getting Started with RapidMinerAttributes and examples: Adata set or data table is a collection of columns
and rows of data. Each column represents a type of measurement. For exam-
ple, in the classic Golf data set ( Fig. 15.4 ) that is used to explain many of the
algorithms within this book, there are columns of data containing
Temperature levels and Humidity levels. These are numeric data types. There
is also a column that identifies if a day was windy or not or if a day was
sunny, overcast, or rainy. These columns are categorical or nominal data
types. In all cases, these columns represent attributes of a given day that
would influence whether golf is played or not. In RapidMiner terminology,
columns of data such as these are called attributes . Other commonly used
names for attributes are variables, factors, or features. One set of values for
such attributes that form a row is called an example in RapidMiner terminol-
ogy. Other commonly used names for examples are records, samples, or
instances. An entire dataset (rows of examples) is called an example set in
RapidMiner.
Operator: Anoperator is an atomic piece of functionality (which in fact is a
chunk of encapsulated code) performing a certain task. This data science task
FIGURE 15.4
RapidMiner terminology: attributes and examples.15.1 User Interface and Terminology 495can be: importing a data set into the RapidMiner repository, cleaning it by
getting rid of spurious examples, reducing the number of attributes by using
feature selection techniques, building predictive models, or scoring new data-
sets using models built earlier. Each task is handled by a chunk of code,
which is packaged into an operator (see Fig. 15.5 ).
Thus, there is an operator for importing an Excel spreadsheet, an operator for
replacing missing values, an operator for calculating information gain /C0based
feature weighting, an operator for building a decision tree, and an operator
for applying a model to new unseen data. Most of the time an operator
requires some sort of input and delivers some sort of output (although there
are some operators that do not require an input). Adding an operator to a
process adds a piece of functionality to the workflow. Essentially, this
amounts to inserting a chunk of code into a data science program and, thus,
operators are nothing but convenient visual mechanisms that will allow
RapidMiner to be a GUI-driven application rather than an application which
uses programming language like R or Python.
Process: A single operator by itself cannot perform data science. All data sci-
ence problem solving requires a series of calculations and logical operations.
There is typically a certain flow to these problems: import data, clean and
prepare data, train a model to learn the data, validate the model and rank its
performance, then finally apply the model to score new and unseen data. All
of these steps can be accomplished by connecting a number of different
operators, each uniquely customized for a specific task as seen earlier. When
such a series of operators are connected together to accomplish the desired
data science, a process has been built which can be applied in other contexts.
A process that is created visually in RapidMiner is stored by RapidMiner as a
platform-independent XML code that can be exchanged between RapidMiner
users ( Fig. 15.6 ). This allows different users in different locations and on dif-
ferent platforms to run your RapidMiner process on their data with minimal
FIGURE 15.5
An operator for building a decision tree.496 CHAPTER 15: Getting Started with RapidMinerreconfiguration. All that needs to be done is to send the XML code of the
process to a colleague across the aisle (or across the globe). They can simply
copy and paste the XML code in the XML tab in the Design view and switch
back to the Process tab (or view) to see the process in its visual representa-tion and then run it to execute the defined functionality.
15.2 DATA IMPORTING AND EXPORTING TOOLS
RapidMiner offers dozens of different operators or ways to connect to data.
The data can be stored in a flat file such as a comma-separated values (CSV)
FIGURE 15.6
Every process is automatically translated to an XML document.15.2 Data Importing and Exporting Tools 497file or spreadsheet, in a database such as a Microsoft SQLServer table, or it
can be stored in other proprietary formats such as SAS or Stata or SPSS, etc.
If the data is in a database, then at least a basic understanding of databases,
database connections and queries is essential in order to use the operator
properly. One could choose to simply connect to their data (which is stored
in a specific location on disk) or to import the data set into the local
RapidMiner repository itself so that it becomes available for any process
within the repository and every time RapidMiner is opened, this data set is
available for retrieval. Either way, RapidMiner offers easy-to-follow wizards
that will guide through the steps. As shown in Fig. 15.7 , to simply connect to
data in a CSV file on disk using a Read CSV operator, drag and drop the
operator to the main process window. Then the Read CSV operator would
need to be configured by clicking on the Import Configuration Wizard,
which will provide a sequence of steps to follow to read the data in2.T h e
search box at the top of the operator window is also useful —if one knows
even part of the operator name then it ’s easy to find out if RapidMiner pro-
vides such an operator. For example, to see if there is an operator to handle
CSV files, type “CSV ”in the search field and both Read and Write will show
up. Clear the search by hitting the red X. Using search is a quick way to navi-
gate to the operators if part of their name is known. Similarly try “principal ”
and the operator for principal component analysis can be seen, if there is
uncertainty about the correct and complete operator name or where to look
initially. Also, this search shows the hierarchy of where the operators exist,
which helps one learn where they are.
FIGURE 15.7
Steps to read in a CSV file. CSV, Comma-Separated Values.
2Data example shown here is available from IMF, (2012, October). World economic outlook
database. International Monetary Fund. Retrieved from ,http://www.imf.org/external/pubs/ft/weo/
2012/02/weodata/index.aspx .Accessed 15.03.13.498 CHAPTER 15: Getting Started with RapidMinerOn the other hand, if the data is to be imported into a local RapidMiner
repository, click on the down arrow “Import Data ”button in the
Repositories tab (as shown in Fig. 15.7 ) and select Import CSV File. The
same five-step data import wizard will immediately be presented. In either
case, the data import wizard consists of the following steps:
1.Select the file on the disk that should be read or imported.
2.Specify how the file should be parsed and how the columns are
delimited. If the data have a comma “,”as the column separator in the
configuration parameters, be sure to select it.
3.Annotate the attributes by indicating if the first row of the dataset
contains attribute names (which is usually the case). If the dataset has
first row names, then RapidMiner will automatically indicate this as
Name. If the first few rows of the dataset has text or information, that
would have to be indicated for each of the example rows. The available
annotation choices are “Name, ”“Comment, ”and “Unit. ”See the
example in Fig. 15.8 .
4.In this step the data type of any of the imported attributes can be changed
and whether each column or attribute is a regular attribute or a special
FIGURE 15.8
Properly annotating the data.15.2 Data Importing and Exporting Tools 499type of attribute can be identified. By default, RapidMiner autodetects the
data types in each column. However, sometimes one may need to
override this and indicate if a particul ar column is of a different data type.
The special attributes are columns th at are used for identification (e.g.,
patient ID, employee ID, or transaction ID) only or attributes that are to
be predicted. These are called “label ”attributes in RapidMiner
terminology.
5.In this last step, if connecting to the data on disk using Read CSV,
simply hit Finish ( Fig. 15.9 ). If the data are being imported into a
RapidMiner repository (using Import CSV File), then the location in
the repository for this will need to be specified.
FIGURE 15.9
Finishing the data import.500 CHAPTER 15: Getting Started with RapidMinerWhen this process is finished, there should be either a properly connected
data source on disk (for Read CSV) or a properly imported example set in
their repository that can be used for any data science process. Exporting data
from RapidMiner is possible in a similar way using the Write CSV operator.
15.3 DATA VISUALIZATION TOOLS
Once a dataset is read into RapidMiner, the next step is to explore the dataset
visually using a variety of tools. Before visualization is discussed, however, it
is a good idea to check the metadata of the imported data to verify if all the
correct information is there. When the simple process described in
Section 15.2 is run (be sure to connect the output of the read operator to the
“result ”connector of the process), an output is posted to the Results view of
RapidMiner. The data table can be used to verify that indeed the data has
been correctly imported under the Data tab on the left (see Fig. 15.10 ).
By clicking on the Statistics tab (see Fig. 15.11 ), one can examine the type,
missing values, and basic statistics for all the imported dataset attributes. The
data type of each attribute (integer, real, or binomial), and some basic statis-
tics can also be identified. This high-level overview is a good way to ensure
that a dataset has been loaded correctly and exploring the data in more detail
using the visualization tools described later is possible.
FIGURE 15.10
Results view that is shown when the data import process is successful.15.3 Data Visualization Tools 501There are a variety of visualization tools available for univariate (one attri-
bute), bivariate (two attributes), and multivariate analysis. Select the Charts
tab in the Results view to access any of the visualization tools or plotter.
General details about visualization are available in Chapter 3, Data
Exploration.
Univariate Plots
1.Histogram: A density estimation for numeric plots and a counter for
categorical ones.
2.Quartile (Box and Whisker): Shows the mean value, median, standard
deviation, some percentiles, and any outliers for each attribute.
3.Series (or Line): Usually best used for time series data.
Bivariate Plots
All 2D and 3D charts show dependencies between tuples (pairs or triads) of
variables.3
FIGURE 15.11
Metadata is visible under the Statistics tab.
3A 2D plot can also depict three dimensions, for example using color. Bubble plots can even depict
four dimensions! This categorization is done somewhat loosely.502 CHAPTER 15: Getting Started with RapidMiner1.Scatter: The simplest of all 2D charts, which shows how one variable
changes with respect to another. RapidMiner allows the use of color;
the points can be colored to add a third dimension to thevisualization.
2.Scatter Multiple: Allows one axis to be fixed to one variable while
cycling through the other attributes.
3.Scatter Matrix: Allows all possible pairings between attributes to be
examined. Color as usual adds a third dimension. Be careful with this
plotter because as the number of attributes increases, rendering all thecharts can slow down processing.
4.Density: Similar to a 2D scatter chart, except the background may be
filled in with a color gradient corresponding to one of the attributes.
5.SOM: Stands for a self-organizing map. It reduces the number of
dimensions to two by applying tra nsformations. Points that are
similar along many attributes will be placed close together. It isbasically a clustering visualization method. There are more
details in Chapter 8, Model Evalua tion, on clustering. Note that
SOM (and many of the parameterized reports) do not runautomatically, so switch to that report, there will be a blank screen
until the inputs are set and then in the case of SOM the calculate
button is pushed.
Multivariate Plots
1.Parallel: Uses one vertical axis for each attribute, thus, there are as
many vertical axes as there are attributes. Each row is displayed as a
line in the chart. Local normalization is useful to understand the
variance in each variable. However, a deviation plot works better forthis.
2.Deviation: Same as parallel, but displays mean values and standard
deviations.
3.Scatter 3D: Quite similar to the 2D scatter chart but allows a three-
dimensional visualization of three attributes (four, the color of the
points is included)
4.Surface: A surface plot is a 3D version of an area plot where the
background is filled in.
These are not the only available plotters. Some additional ones are notdescribed here such as pie, bar, ring, block charts, etc. Generating any of the
plots using the GUI is pretty much self-explanatory. The only words of
caution are that when a large dataset is encountered, generating some of thegraphics intensive multivariate plots can be quite time consuming depending
on the available RAM and processor speed.15.3 Data Visualization Tools 50315.4 DATA TRANSFORMATION TOOLS
Many times the raw data is in a form that is not ideal for applying standard
machine learning algorithms. For exampl e, suppose there are categorical attri-
butes such as gender, and the requirement is to predict purchase amounts basedon (among several other attributes) the ge nder. In this case, the categorical (or
nominal) attributes need to be converted into numeric ones by a process called
dichotomization . In this example, two new variables are introduced called
Gender 5Male and Gender 5Female, which can take (numeric) values of 0 or 1.
In other cases, numeric data may be given but the algorithm can only
handle categorical or nominal attributes. A good example, is where the label
variable being numeric (such as the market price of a home in the Boston
Housing example set discussed in Chapter 5 on regression) and one wants touse logistic regression to predict if the price will be higher or lower than a
certain threshold. Here the need is to convert a numeric attribute into a bino-
mial one.
In either of these cases, the underlying data types may need to be trans-
formed into some other types. This activity is a common data preparation
step. The four most common data type conversion operators are:
1.Numerical to Binominal: TheNumerical to Binominal operator changes
the type of numeric attributes to a binary type. Binominal attributes canhave only two possible values: true or false. If the value of an attribute is
between a specified minimal and maximal value, it becomes false;
otherwise it is true. In the case of the market price example, thethreshold market price is $30,000. Then all prices from $0 to $30,000
will be mapped to false and any price above $30,000 is mapped to true.
2.Nominal to Binominal: Here if a nominal attribute with the name
“Outlook ”and possible nominal values “sunny, ”“overcast, ”and “rain”
is transformed, the result is a set of three binominal attributes,
“Outlook 5sunny, ”“Outlook 5overcast, ”and “Outlook 5rain”whose
possible values can be true or false. Examples (or rows) of the original
dataset where the Outlook attribute had values equal to sunny, will, in
the transformed example set, have the value of the attributeOutlook 5sunny set to true, while the value of the Outlook 5overcast
and Outlook 5rain attributes will be false.
3.Nominal to Numerical: This works exactly like the Nominal to Binominal
operator if one uses the Dummy coding option, except that instead of
true/false values, one would see 0/1 (binary values). If unique integers
option was used, each of the nominal values will get assigned a uniqueinteger from 0 upwards. For example, if Outlook was sunny, then
“sunny ”gets replaced by the value 1, “rain”may get replaced by 2, and
“overcast ”may get replaced by 0.504 CHAPTER 15: Getting Started with RapidMiner4.Numerical to Polynominal: Finally, this operator simply changes the type
(and internal representation) of selected attributes, that is, every new
numeric value is considered to be another possible value for the
polynominal attribute. In the golf example, the Temperature attribute
has 12 unique values ranging from 64 to 85. Each value is considered a
unique nominal value. As numeric attributes can have a huge number of
different values even in a small range, converting such a numeric
attribute to polynominal form will generate a huge number of possible
values for the new attribute. A more sophisticated transformation
method uses the discretization operator, which is discussed next.
5.Discretization: When converting numeric attributes to polynominal, it
is best to specify how to set up the discretization to avoid the
previously mentioned problem of generating a huge number of
possible values —each numeric value should not appear as a unique
nominal one but would rather be binned into some intervals. The
Temperature in the golf example can be discretized by several methods:
one can discretize using equal-sized bins with the Discretize by Binning
operator. If two bins (default) are selected there will be two equal
ranges: [below 74.5] and [above 74.5], where 74.5 is the average value
of 64 and 85. Based on the actual Temperature value, the example will
be assigned into one of the two bins. The number of rows falling into
each bin ( Discretize by Size operator) can instead be specified rather
than equal bin ranges. One could also discretize by bins of equal
number of occurrences by choosing to Discretize by Frequency , for
example. Probably the most useful option is to Discretize by User
Specification . Here explicit ranges can be provided for breaking down a
continuous numeric attribute into several distinct categories or
nominal values using the table shown in Fig. 15.12 A. The output of
the operator performing that discretization is shown in Fig. 15.12 B.
Sometimes the structure of an example set may need to be transformed or
rotated about one of the attributes, a process commonly known as pivoting
or creating pivot tables. Here is a simple example of why this operation
would be needed. The table consists of three attributes: a customer ID, a
product ID, and a numeric measure called Consumer Price Index (CPI) (see
Fig. 15.13 A). It can be seen that this simple example has 10 unique custo-
mers and 2 unique product IDs. What should preferably be done is to rear-
range the data set so that one gets two columns corresponding to the two
product IDs and aggregate4or group the CPI data by customer IDs. This is
4CAUTION: The Pivot operator does not aggregate! If the source dataset contains combinations of
product ID and customer ID occurring multiple times, one would have to aggregate before applying
the Pivot operator in order to produce a dataset containing only unique combinations first.15.4 Data Transformation Tools 505because the data needs to be analyzed on the customer level, which means
that each row has to represent one customer and all customer features have
to be encoded as attribute values. Note that there are two missing values
(Fig. 15.13 A, in the ‘customer id ’column indicated by the thin rectangles.
These are missing entries for customer ids 6 and 8. However the result of the
pivot operation will have 10 x 2 520 entries because there are 10 customers
(c1:c10) and 2 products (v1:v2).
(A)
(B)
FIGURE 15.12
(A) Discretize operator. (B) The output of the operation.506 CHAPTER 15: Getting Started with RapidMinerThis is accomplished simply with the Pivot operator. Select “customer id ”as
the group attribute and “product id ”as the index attribute as shown in
Fig. 15.13 B. If familiar with Microsoft Excel ’s pivot tables, the group attribute
parameter is similar to “row label ”and the index attribute is akin to “column(A)
(B)
FIGURE 15.13
(A) A simple dataset to explain the pivot operation using RapidMiner. (B) Configuring the Pivot operator. (C) Results of the pivot operation.15.4 Data Transformation Tools 507label. ”The result of the pivot operation is shown in Fig. 15.13 C. Observe
that the column labels are prefixed by the name of the column label attribute
e.g. CPI_v1. Missing entries from the original table now become missing
values indicated by “?”.
A converse of the Pivot operator is the De-pivot operator, which reverses the
process described and may sometimes also be required during the data
preparation steps. In general a De-pivot operator converts a pivot table into a
relational structure.
In addition to these operators, one may also need to use the Append operator
to add examples to an existing dataset. Appending an example set with new
rows (examples) works just as the name sounds —the new rows end up get-
ting attached to the end of the example set. One has to make sure that the
examples match the attributes exactly with the main dataset. Also useful is
the classic Join operator, which combines two example sets with the same
observations units but different attributes. The Joinoperator offers the tradi-
tional inner, outer, and left and right join options. An explanation for joins
is available in any of the books that deal with SQL programming as well as
the RapidMiner help, which also provides example processes. They will not
be repeated here.(C)
FIGURE 15.13
(Continued).508 CHAPTER 15: Getting Started with RapidMinerSome of other common operators used in the various chapters of the book
(and are explained there in context) are: Rename attributes, Select attributes,
Filter examples, Add attributes, Attribute weighting, etc.
15.5 SAMPLING AND MISSING VALUE TOOLS
Data sampling might seem out of place in today ’s big data /C0charged environ-
ments. Why bother to sample when one can collect and analyze all the data
they can? Sampling is a perhaps a vestige of the statistical era when data was
costly to acquire and computational effort was costlier still. However, there
are many situations today where “targeted ”sampling is of use. A typical sce-
nario is when building models on data where some class representations are
extremely low. Consider the case of fraud prediction. Depending on the
industry, fraudulent examples range from less than 1% of all the data col-
lected to about 2% /C03%. When classification models are built using such
data, the models tend to be biased and would not be able to detect fraud in
the majority of the cases with new unseen data, literally because they have
not been trained on enough fraudulent samples!
Such situations call for balancing datasets where the training data needs to
be sampled and the proportion of the minority class needs to be increased
so that the models can be trained better. The plot in Fig. 15.14 shows an
example of imbalanced data: the negative class indicated by a circle is dispro-
portionately higher than the positive class indicated by a cross.
This can be explored using a simple example. The dataset shown in the pro-
cess in Fig. 15.15 is available in RapidMiner ’s Samples repository and is
called “Weighting. ”This is a balanced dataset consisting of about 500 exam-
ples with the label variable consisting of roughly 50% positive and 50% neg-
ative classes. Thus, it is a balanced dataset. When one trains a decision tree to
classify this data, they would get an overall accuracy of 84%. The main thing
to note here is that the decision tree recall on both the classes is roughly the
same:B86% as seen in Fig. 15.15 .
A sub-process called Unbalance is now introduced, which will resample the
original data to introduce a skew: the resulting dataset has more positive
class examples than negative class examples. Specifically, the dataset is now
with 92% belonging to the positive class (as a result the model inflates the
predicted class recall to 99.2%) and 8% belonging to negative class (as a
result the model underestimates the predicted class recall to 21.7%). The pro-
cess and the results are shown in Fig. 15.16 . So how do we address this data
imbalance?15.5 Sampling and Missing Value Tools 509FIGURE 15.15
Performance of decision trees on well-balanced data.
negative
positive
0123456
xy
012345678910
7 8 91 0
FIGURE 15.14
Snapshot of an imbalanced dataset.510 CHAPTER 15: Getting Started with RapidMinerThere are several ways to fix this situation. The most commonly used method
is to resample the data to restore the balance. This involves undersampling
the more frequent class —in this case, the positive class —and oversampling
the less frequent negative class. The rebalance sub-process achieves this in the
final RapidMiner process. As seen in Fig. 15.17 , the overall accuracy is now
back to the level of the original balanced data. The decision tree also looks a
little bit similar to the original (not visible in the figures shown, but the
reader can verify with the completed processes loaded into RapidMiner),
whereas, for the unbalanced dataset it was reduced to a stub. An additional
check to ensure that accuracy is not compromised by unbalanced data is to
replace the accuracy by what is called balanced accuracy . It is defined as the
arithmetic mean of the class recall accuracies, which represent the accuracy
obtained on positive and negative examples, respectively. If the decision tree
performs equally well on either class, this term reduces to the standard accu-
racy (i.e., the number of correct predictions divided by the total number of
predictions).
There are several built-in RapidMiner processes to perform sampling: sample,
sample (bootstrapping), sample (stratified), sample (model-based), and sam-
ple (Kennard-stone). Specific details about these techniques are well
described in the software help. Only the bootstrapping method will be
remarked on here because it is a common sampling technique.
Bootstrapping works by sampling repeatedly within a base dataset with
replacement. So, when this operator is used to generate new samples, one
FIGURE 15.16
Unbalanced data and the resulting accuracy.15.5 Sampling and Missing Value Tools 511may see repeated or nonunique examples. There is the option of specifying
an absolute sample size or a relative sample size and RapidMiner will ran-
domly pick examples from the base data set with replacement to build a new
bootstrapped example set.
This section will be closed with a brief description of missing value handling
options available in RapidMiner. The basic operator is called Replace Missing
Values . This operator provides several alternative ways to replace missing
values: minimum, maximum, average, zero, none, and a user-specified value.
There is no median value option. Basically, all missing values in a given col-
umn (attribute) are replaced by whatever option is chosen. A better way to
treat missing values is to use the Impute Missing Values operator. This operator
changes the attribute with missing values to a label or target variable, and
trains models to determine the relationship between this label variable and
other attributes so that it may then be predicted.
15.6 OPTIMIZATION TOOLS5
Recall that in Chapter 4: Classification, on decision trees, there was an
opportunity to specify parameters to build a decision tree for the credit risk
FIGURE 15.17
Rebalanced data and resulting improvement in class recall.
5Readers may skip this section if completely new to RapidMiner and return to it after developing
some familiarity with the tool and data science in general.512 CHAPTER 15: Getting Started with RapidMinerexample (Section 4.1.2, step 3) but default values were simply used. Similar
situations arose when building a support vector machine model (Section 4.6.3)
or logistic regression model (Section 5.2.3), where the default model parameter
values were simply chosen. When a model evaluation was run, the performance
of the model is usually an indicator as to whether the right parameter combina-
tions were chosen for the model.6But what if one is not happy with the model
accuracy (or its r-squared value)? Can it be improved? How?
RapidMiner provides several unique operators that will allow one to discover
and choose the best combination of parameters for pretty much all of the
available operators that need parameter specifications. The fundamental prin-
ciple on which this works is the concept of a nested operator. A nested opera-
tor was first encountered in Section 4.1.2, step 2 —the Split Validation
operator. Another nested operator was also described in Section 14.5 in the
discussion on wrapper-style feature selection methods. The basic idea is to
iteratively change the parameters for a learner until some stated performance
criteria are met. The Optimize operator performs two tasks: it determines
what values to set for the selected parameters for each iteration, and when to
stop the iterations. RapidMiner provides three basic methods to set parame-
ter values: grid search, greedy search, and an evolutionary search (also known
as genetic) method. Each method will not be described in detail, but a high-
level comparison between them will be done and when each approach
would be applicable will be mentioned.
To demonstrate the working of an optim ization process, consider a simple
model: a polynomial function ( Fig. 15.18 ). Specifically, for a function
y5f(x)5x61x327x223x11 one wishes to find the minimum value
of y within a given domain of x. This is of course the simplest form of
optimization —to select an interval of values for x where y is minimum. As
seen in the functional plot, for x in [ 21.5, 2], there are two minima: a local
minimum of y 524.33, x 521.3 and a global minimum of y 527.96,
x51.18. It will be demonstrated how to use RapidMiner to search for these
minima using the Optimize operators. As mentioned before, the optimization
happens in a nested operator, what is placed inside the optimizer will be
discussed first before discussing the optimizer itself.
The nested process itself, also called the inner process, is quite simple as seen
inFig. 15.19 A:Generate Data randomly generates values for x between an
upper bound and a lower bound (see Fig. 15.19 B).
6Normally one can ’t judge from just one performance estimate whether the right parameters were
chosen. Multiple performance values and their dependency on the parameter values would need to be
examined to infer that the right/optimal parameter values were chosen.15.6 Optimization Tools 513Generate Attributes will calculate y for each value of x in this interval.
Performance (Extract Performance) will store the minimum value of y within
each interval. This operator has to be configured as shown on the right of
Fig. 15.19 A in order to ensure that the correct performance is optimized. In
this case, y is selected as the attribute that has to be minimized. The Rename ,
Select Attributes , and Logoperators are plugged in to keep the process focused
on only two variables and to track the progress of optimization.
This nested process can be inserted into any of the available Optimize
Parameters operators. It will be described how one can do this with the
Optimize Parameters (Grid) operator first. In this exercise, the interval [ lower
bound, upper bound ] is basically being optimized so that the objective of mini-
mizing the function y 5f(x) can be achieved. As seen in the function plot,
the entire domain of x has to be traversed in small enough interval sizes so
that the exact point at which y hits a global minimum can be found.
The grid search optimizer simply moves this interval window across the
entire domain and stops the iterations after all the intervals are explored
(Fig. 15.20 ). Clearly it is an exhaustive but inefficient search method. To set
this process up, simply insert the inner process inside the outer Optimize
Parameters (Grid) operator and select the attributes upper bound and attributes
lower bound parameters from the Generate Data operator. To do this, click on
theEdit Parameter Settings option for the optimizer, select Generate Data under
theOperators tab of the dialog box, and further select attributes_upper_bound
andattributes_lower_bound under the Parameters tab ( Fig. 15.21 ).
A local
minimum
xy–2 –1 02
0
–2
–4
–6
–8
–1012
Global minimum,
y = –7.96 at x = 1.18
FIGURE 15.18
A simple polynomial function to demonstrate optimization.514 CHAPTER 15: Getting Started with RapidMinerOne would need to provide ranges for the grid search for each of these
parameters. In this case, the lower bound is set to go from 21.5 to21 and
the upper bound to go from 0 to 1.5 in steps of 10. So the first interval (or
window) will be x5[21.5, 0], the second one will be [ 21.45, 0] and so on
until the last window, which will be [ 21, 1.5] for a total of 121 iterations.
The Optimize Performance (Grid) search will evaluate yfor each of these
windows and store the minimum yin each iteration. The iterations will
only stop after all 121 intervals are evaluated, but the final output will indi-
cate the window that resulted in the smallest minimum y. The plot in
Fig. 15.22 shows the progress of the iterations. Each point in the chart
FIGURE 15.19
(A) The inner process that is nested inside an optimization loop. (B) Configuration of the generated data.15.6 Optimization Tools 515xy–2 –1 02
0
–2
–4
–6
–8
–1012
FIGURE 15.20
Searching for an optimum within a fixed window that slides across.
FIGURE 15.21
Configuring the grid search optimizer.516 CHAPTER 15: Getting Started with RapidMinercorresponds to the lowest value of yevaluated by the expression within a
given interval. The local minimum of y524.33 @ x521.3 is found at the
very first iteration. This corresponds to the window [ 21.5, 0]. If the grid had
not spanned the entire domain [ 21.5, 1.5], the optimizer would have
reported the local minimum as the best performance. This is one of the main
disadvantages of a grid search method.
The other disadvantage is the number of redundant iterations. Looking at the
plot above, one can see that the global minimum was reached by about the
90th iteration. In fact, for iteration 90, yminimum 527.962, whereas, the final
reported lowest yminimum was27.969 (iteration 113), which is only about
0.09% better. Depending on the tolerances, the computations could have
been terminated earlier. But a grid search does not allow early terminations
and there end up running nearly 30 extra iterations. Clearly as the number of
optimization parameters increase, this would become a significant cost.
TheOptimize Parameters (Quadratic) operator is next applied to the inner pro-
cess. Quadratic search is based on a greedy search methodology. A greedy
methodology is an optimization algorithm that makes a locally optimal deci-
sion at each step ( Ahuja, 2000; Bahmani, 2013 ). While the decision may be
locally optimal at the current step, it may not necessarily be the best for all
future steps. k-Nearest neighbor is one good example of a greedy algorithm.
FIGURE 15.22
Progression of the grid search optimization.15.6 Optimization Tools 517In theory, greedy algorithms will only yield local optima, but in special cases,
they can also find globally optimal solutions. Greedy algorithms are best
suited to find approximate solutions to difficult problems. This is because
they are less computationally intense and tend to operate over a large dataset
quickly. Greedy algorithms are by nature typically biased toward coverage of
a large number of cases or a quick payback in the objective function.
In this case, the performance of the quadratic optimizer is marginally worse
than a grid search requiring about 100 shots to hit the global minimum
(compared to 90 for a grid), as seen in Fig. 15.23 . It also seems to suffer
from some of the same problems encountered in the grid search.
Finally the last available option: Optimize Parameters (Evolutionary) will
be employed. Evolutionary (or genetic) algorithms are often more appro-
priate than a grid search or a greedy se a r c ha n dl e a dt ob e t t e rr e s u l t s ,T h i s ,
is because they cover a wider variety of the search space through mutation
and can iterate onto good minima through cross-over of successful models
based on the success criteria. As seen in the progress of iterations in
Fig. 15.24 , the global optimum was hit without getting stuck initially at a
FIGURE 15.23
Progression of the quadratic greedy search optimization.518 CHAPTER 15: Getting Started with RapidMinerlocal minimum —right from the first few iterations it can be seen that the
neighborhood of the lowest point has been approached. The evolutionary
method is particularly useful if one does not initially know the domain of
the functions, unlike in this case where they are known. It is evident that it
takes far fewer steps to get to the global minimum with a high degree of
confidence —about 18 iterations as oppose d to 90 or 100. Key concepts to
understanding this algorithm are mutation and cross-over ,b o t ho fw h i c ha r e
possible to control using the RapidMi ner GUI. More technical details of
how the algorithm works are beyond the scope of this book and some
excellent resources are listed at the end of this chapter ( Weise, 2009 ).
To summarize, there are three optimization algorithms available in
RapidMiner all of which are nested operators. The best application of optimi-
zation is for the selection of modeling parameters, for example, split size,
leaf size, or splitting criteria in a decision tree model. One builds their
machine learning process as usual and inserts this process or nests it inside
of the optimizer. By using the Edit Parameter Settings ...control button, one
can select the parameters of any of the inner process operators (for example
aDecision Tree ,W-Logistic, orSVM ) and define ranges to sweep. Grid search
FIGURE 15.24
Progression of the genetic search optimization.15.6 Optimization Tools 519is an exhaustive search process for finding the right settings, but is expensive
and cannot guarantee a global optimum. Evolutionary algorithms are
extremely flexible and fast and are usually the best choice for optimizing
machine learning models in RapidMiner.
15.7 INTEGRATION WITH R
R is a popular programmable open source statistical package that can execute
scripts. The RapidMiner process can invoke R, send data, process it in R and
receive back the data for further processing, modeling, and visualization in
RapidMiner. RapidMiner offers an operator Execute R to invoke R and run
the script that is contained in the operator. The script can be edited under
theEdit Text parameter. Fig. 15.25 shows a process with the R operator and a
sample script.
The script contains a function rm_main that can accept the datasets con-
nected to the input port of the Execute R operator. Similarly, the return part
FIGURE 15.25
Integration with R.520 CHAPTER 15: Getting Started with RapidMinerof the function sends the dataframe (dataset in R) back to the output port of
the operator.
15.8 CONCLUSION
As with other chapters in this book, the RapidMiner process explained and
developed in this discussion can be accessed from the companion site of the
book at www.IntroDataScience.com . The RapidMiner process (*.rmp files)
can be downloaded to the computer and can be imported to RapidMiner
from File .Import Process. The data files can be imported from File .
Import Data.
This chapter provided a high-level view of the main tools that one would
need to become familiar with in building data science models using
RapidMiner. Firstly the basic graphical user interface for the program was
introduced. Then options by which data can be brought into and exported
out of RapidMiner were discussed. An overview of the data visualization
methods that are available within the tool were provided, because quite nat-
urally, the next step of any data science process after ingesting the data is to
understand in a descriptive sense the nature of the data. Tools were then
introduced that allow one to transform and reshape the data by changing the
type of incoming data and restructuring them in different tabular forms to
make subsequent analysis easier. Tools that would allow us to resample
available data and account for any missing values were also introduced.
Once familiar with these essential data preparation options, it is possible to
apply any of the appropriate algorithms described in the earlier chapters for
analysis. Optimization operators were also introduced that allow one to fine-
tune their machine learning algorithms so that an optimized and good qual-
ity model can be developed to extract any insights.
With this overview, one can go back to any of the earlier chapters to learn
about a specific technique and understand how to use RapidMiner to build
models using that machine learning algorithm.
References
Ahuja, R. O. (2000). A greedy genetic algorithm for quadratic assignment problem. Computers
and Operations Research ,27(10), 917 /C0934.
Bahmani, S.R. (2013). Greedy sparsity-constrained optimization. Statistical Machine Learning ,14,
807/C0841.
Mierswa, I.W. (2006). YALE: Rapid prototyping for complex data mining tasks. Association for
computing machinery /C0Knowledge discovery in databases (pp. 935 /C0940).
Weise, T. (2009). Global optimization algorithms /C0Theory and application .,http://www.it-weise.de/ ..References 521Comparison of Data Science Algorithms
Classification : Predicting a categorical target variable
Algorithm Description Model Input Output Pros Cons Use Cases
Decision
treesPartitions the data
into smaller
subsets where
each subset
contains (mostly)responses of one
class (either “yes ”
or“no”)A set of rules
to partition a
data set
based on the
values of thedifferent
predictorsNo
restrictions on
variable type
for predictorsThe label
cannot be
numeric. It
must be
categoricalIntuitive to explain
to nontechnical
business users.
Normalizing
predictors is notnecessaryTends to overfit
the data. Small
changes in input
data can yield
substantiallydifferent trees.
Selecting the right
parameters can bechallengingMarketing
segmentation,
fraud detection
Rule
inductionModels the
relationshipbetween input and
output by deducing
simple “IF/THEN ”
rules from a data
setA set of
organizedrules that
contain an
antecedent(inputs) and
consequent
(output class)No
restrictions.Accepts
categorical,
numeric, andbinary inputsPrediction
of targetvariable,
which is
categoricalModel can be easily
explained tobusiness usersDivides the data
set in rectilinearfashionManufacturing,
applicationswhere
description of
model isnecessaryEasy to deploy in
almost any tools
and applications
k-Nearest
neighborsA lazy learner
where no model isgeneralized. Any
new unknown data
point is comparedagainst similar
known data points
in the training setEntire
training dataset is the
modelNo
restrictions.However, the
distance
calculationswork better
with numeric
data. Data
needs to be
normalizedPrediction
of targetvariable,
which is
categoricalRequires very little
time to build themodel. Handles
missing attributes in
the unknownrecord gracefully.
Works with
nonlinearrelationshipsThe deployment
runtime andstorage
requirements will
be expensiveImage
processing,applications
where slower
response timeis acceptableArbitrary selection
of value of k
No description of
the model
Naïve
BayesianPredicts the output
class based on the
Bayes ’theorem by
calculating classconditional
probability and
prior probabilityA lookup
table of
probabilities
andconditional
probabilities
for eachattribute with
an output
classNo
restrictions.
However, the
probabilitycalculation
works better
withcategorical
attributesPrediction
of
probability
for all classvalues,
along with
the winningclassTime required to
model and deploy
is minimumTraining data set
needs to be
representative
sample ofpopulation and
needs to have
completecombinations of
input and output.
Attributes need tobe independentSpam
detections, text
mining
Great algorithm for
benchmarking.
Strong statisticalfoundation
Continued
523Regression : Predicting a numeric target variableContinued
Algorithm Description Model Input Output Pros Cons Use Cases
Artificial
neural
networksA computational
and mathematical
model inspired by
the biologicalnervous system.
The weights in the
network learn to
reduce the error
between actualand predictionA network
topology of
layers and
weights toprocess
input dataAll attributes
should be
numericPrediction
of target
(label)
variable,which is
categoricalGood at modeling
nonlinear
relationships. Fast
response time indeploymentNo easy way to
explain the inner
working of the
modelImage
recognition,
fraud
detection,quick response
time
applicationsRequires
preprocessingdata. Cannot
handle missing
attributes
Support
vectormachinesBoundary
detection algorithmthat identifies/
defines multi-
dimensionalboundaries
separating data
points belonging todifferent classesThe model is
a vectorequation that
allows one to
classify newdata points
into different
regions(classes)All attributes
should benumericPrediction
of target(label)
variable,
which canbe
categorical
or numericExtremely robust
against overfitting.Small changes to
input data do not
affect boundaryand, thus, do not
yield different
results. Good athandling nonlinear
relationshipsComputational
performanceduring training
phase can be
slow. This may becompounded by
the effort needed
to optimizeparameter
combinationsOptical
characterrecognition,
fraud
detection,modeling
“black-swan ”
events
Ensemble
modelsLeverages wisdom
of the crowd.
Employs a numberof independent
models to make a
prediction andaggregates the final
predictionA meta-
model with
individualbase models
and an
aggregatorSuperset of
restrictions
from the basemodel usedPrediction
for all class
values witha winning
classReduces the
generalization error.
Takes differentsearch space into
considerationAchieving model
independence is
trickyMost of the
practical
classifiers areensembleDifficult to explain
the inner working
of the model
Algorithm Description Model Input Output Pros Cons Use Case
Linear
regressionThe classical
predictive modelthat expresses the
relationship between
inputs and an outputparameter in the
form of an equationThe model consists of
coefficients for eachinput predictor and their
statistical significance. A
bias (intercept) may beoptionalAll
attributesshould be
numericThe label
may benumeric
or
binominalThe workhorse of
most predictivemodeling
techniques. Easy
to use andexplain to
nontechnical
business usersCannot handle
missing data.Categorical data
are not directly
usable, butrequire
transformation
into numericPretty much
anyscenario
that requires
predicting acontinuous
numeric
value
Logistic
regressionTechnically, this is a
classification
method. But
structurally it is
similar to linearregressionThe model consists of
coefficients for each
input predictor that
relate to the “logit. ”
Transforming the logitinto probabilities of
occurrence (of each
class) completes themodelAll
attributes
should be
numericThe label
may only
be
binominalOne of the most
common
classification
methods.
ComputationallyefficientCannot handle
missing data. Not
intuitive when
dealing with a
large number ofpredictorsMarketing
scenarios
(e.g., will
click or not
click), anygeneral two-
class
problem524 Comparison of Data Science AlgorithmsAssociation analysis : Unsupervised process for finding relationships between
items
Algorithm Description Model Input Output Pros Cons Use Case
FP-Growth
and AprioriMeasures
the strengthof co-
occurrence
between oneitem with
anotherFinds simple,
easy tounderstand
rules like
{Milk,Diaper}-
{Beer}Transactions
format withitems in the
columns and
transactionsin the rowsList of
relevantrules
developed
from thedata setUnsupervised
approach withminimal user
inputs. Easy
to understandrulesRequires
preprocessingif input is of
different formatRecommendation
engines, cross-selling, and
content
suggestions
Clustering : An unsupervised process for finding meaningful groups in data
Algorithm Description Model Input Output Pros Cons Use case
k-Means Data set is
divided into
kclusters by
finding k
centroidsAlgorithm
finds k
centroidsand all the
data points
are assignedto the
nearest
centroid,which forms
a clusterNo
restrictions.
However,the distance
calculations
work betterwith
numeric
data. Datashould be
normalizedData set is
appended by
one of the k
cluster labelsSimple to
implement.
Can beused for
dimension
reductionSpecification of
kis arbitrary
and may notfind natural
clusters.
Sensitive tooutliersCustomer
segmentation,
anomalydetection,
applications where
globular clusteringis natural
DBSCAN Identifies
clusters as a
high-density
areasurrounded
by low-
density
areasList of
clusters and
assigned
data points.Default
cluster 0
contains
noise pointsNo
restrictions.
However,
the distancecalculations
work better
with
numeric
data. Datashould be
normalizedCluster labels
based on
identified
clustersFinds the
natural
clusters of
any shape.No need to
mention
number of
clustersSpecification of
density
parameters. A
bridge betweentwo clusters
can merge the
cluster. Cannot
cluster varying
density datasetApplications
where clusters are
nonglobular
shapes and whenthe prior number
of natural
groupings is not
known
Self-
organizing
mapsA visual
clustering
techniquewith rootsfrom neural
networks
andprototype
clusteringA two-
dimensional
lattice where
similar datapoints are
arranged
next to eachotherNo
restrictions.
However,
the distancecalculations
work better
withnumeric
data. Data
should benormalizedNo explicit
clusters
identified.
Similar datapoints occupy
either the
same cell orare placed
next to each
other in theneighborhoodA visual way
to explain
the clusters.
Reducesmulti-
dimensional
data to twodimensionsNumber of
centroids
(topology) is
specified bythe user. Does
not find natural
clusters in thedataDiverse
applications
including visual
data exploration,content
suggestions, and
dimensionreductionComparison of Data Science Algorithms 525Anomaly detection : Supervised and unsupervised techniques to find outliers in data
Deep learning : Training using multiple layers of representation of dataAlgorithm Description Model Input Output Pros Cons Use Case
Distance /C0based Outlier
identified
based ondistance to
kth nearest
neighborAll data points
are assigned
a distancescore based
on nearest
neighborAccepts both
numeric and
categoricalattributes.
Normalization is
required sincedistance is
calculatedEvery data point
has a distance
score. The higherthe distance, the
more likely the
data point is anoutlierEasy to
implement.
Works wellwith
numeric
attributesSpecification of
kis arbitraryFraud
detection,
preprocessingtechnique
Density-based Outlier is
identified
based ondata points in
low-density
regionsAll data points
are assigned
a densityscore based
on the
neighborhoodAccepts both
numeric and
categoricalattributes.
Normalization is
required sincedensity is
calculatedEvery data point
has a density
score. The lowerthe density, the
more likely the
data point is anoutlierEasy to
implement.
Works wellwith
numeric
attributesSpecification of
distance
parameter bythe user.
Inability to
identify varyingdensity regionsFraud
detection,
preprocessingtechnique
Local outlier
factorOutlier is
identified
based on
calculation ofrelative
density in the
neighborhoodAll data points
as assigned a
relative
density scorebased on the
neighborhoodAccepts both
numeric and
categorical
attributes.Normalization is
required since
density iscalculatedEvery data point
has a density
score. The lower
the relative density,t h em o r el i k e l yt h e
data point is an
outlierCan handle
the varying
density
scenarioSpecification of
distance
parameter bythe userFraud
detection,
preprocessing
technique
Layer Type Description Input Output Pros Cons Use Cases
Convolutional Based on
the conceptof applying filters to
incoming two-
dimensionalrepresentation
of data, such
as images. Machinelearning is used to
automatically
determinethe correct weights
for the filters.A tensor of
typically three ormore dimensions.
Two of the
dimensionscorrespond to the
image while a third
is sometimes usedfor color/channel
encoding.Typically the output
of convolutionallayer is flattened
and passed
through a dense orfully connected
layer which usually
terminates in asoftmax output
layer.Very powerful
and generalpurpose
network. The
number ofweights to be
learned in the
conv layer isnot very high.For most practical
classificationproblems,
conv layers have to
be coupled withdense layers which
result in a large
number of weightsto be trained and
thus lose any speed
advantages ofa pure conv layer.Classify almost any
data where spatialinformation is
highly correlated
such as images.Even audio data
can be converted
into images (usingfourier transforms)
and classified via
conv nets.
Recurrent Just as conv
nets are specialized
for analyzing spatially
correlated data,recurrent nets are
specialized for
temporally correlateddata: sequences.
The data can be
sequences ofnumbers, audio
signals, or even
imagesA sequence of any
type (time series,
text, speech, etc).RNNs can process
sequences and
output other
sequences (manyto many), or output
a fixed tensor
(many to one).Unlike other
types of neural
networks,
RNNs have norestriction that
the input shape
of the data beof fixed
dimension.RNNs suffer from
vanishing (or
exploding) gradients
when thesequences are very
long. RNNs are also
not amenable tomany stacked
layers due to the
same reasons.Forecasting time
series, natural
language
processingsituations such as
machine
translation, imagecaptioning.526 Comparison of Data Science AlgorithmsRecommenders : Finding the user ’s preference of an item
Time series forecasting : Predicting future value of a variableAlgorithm Description Assumption Input Output Pros Cons Use Case
Collaborative
Filtering -
neighborhoodbasedFind a cohort of
users who
provided similarratings. Derive the
outcome rating
from the cohort
usersSimilar users
or items have
similar likesRatings
matrix with
user-itempreferences.Completed
ratings
matrixThe only input
needed is the
ratings matrixCold start
problem for new
users and itemseCommerce,
music, new
connectionrecommendationsDomain agnostic Computation
grows linearlywith the number
of items and
users
Collaborative
Filtering -
Latent matrix
factorizationDecompose the
user-item matrix
into two matrices(P and Q) with
latent factors. Fill
the blank values in
the ratings matrix
by dot product ofPa n dQUser ’s
preference of
an item canbe better
explained by
their
preference of
an item ’s
character
(inferred)Ratings
matrix with
user-itempreferences.Completed
ratings
matrixWorks in sparse
matrixCannot explain
why the
prediction ismadeContent
recommendations
More accurate
than
neighborhoodbased
collaborative
filtering
Content-
based filteringAbstract the
features of the
item and build
item profile. Usethe item profile to
evaluate the user
preference for theattributes in the
item profileRecommend
items similar
to those the
user liked inthe pastUser-item
rating matrix
and Item
profileCompleted
ratings
matrixAddresses cold
start problem for
new itemsRequires item
profile data setMusic
recommendation
from Pandoraand CiteSeer ’s
citation indexingCan provide
explanations on
why therecommendation
is madeRecommenders
are domain
specific
Content-
based -
Supervisedlearning
modelsA personalized
classification or
regression modelfor every single
user in the
system. Learn aclassifier based
on user likes or
dislikes of an itemand its
relationship with
item attributesEvery time a
user prefers
an item, it is avote of
preference for
itemattributesUser-item
rating matrix
and ItemprofileCompleted
ratings
matrixEvery user has a
separate model
and could beindependently
customized.
HyperpersonalizationStorage and
computational
timeeCommerce,
content, and
connectionrecommendations
Algorithm Description Model Input Output Pros Cons Use Case
Decomposition Decompose the time
series into trend,seasonality, and noise.
Forecast the
componentsModels for the
individualcomponentsHistorical
valueForecasted
valueIncreased
understandingof the time
series by
visualizing thecomponentsAccuracy
depends onthe models
used for
componentsApplication
where theexplanation of
components is
important
ContinuedComparison of Data Science Algorithms 527Feature selection : Selection of most important attributesContinued
Algorithm Description Model Input Output Pros Cons Use Case
Exponential
smoothingThe future value is the
function of past
observationsLearn the
parameters of
the smoothing
equation fromthe historical
dataHistorical
valueForecasted
valueApplies to wide
range of time
series with or
without trend orseasonalityMultiple
seasonality in
the data
make themodels
cumbersomeCases where
trend or
seasonality is
not evident
ARIMA
(autoregressive
integratedmoving
average)The future value is the
function of auto
correlated past datapoints and the moving
average of the
predictionsParameter
values for (p,d,
q), AR, and MAcoefficientsHistorical
valueForecasted
valueForms a
statistical
baseline formodel accuracyThe optimal
p,d,q value is
unknown tobegin withApplies on
almost all types
of time seriesdata
Windowing-
based machinelearningCreate cross-sectional
data set with timelagged inputsMachine
learning modelslike regression,
neural
networks, etc.Historical
valueForecasted
valueUses any
machinelearning
approaches on
the cross-sectional dataThe
windowingsize, horizon,
and skip
values arearbitraryApplies to user
cases wherethe time series
has trend and/
or seasonality
Algorithm Description Model Input Output Pros Cons Use Case
PCA
(principal
component
analysis)
filter-basedcombines
the most
important
attributes
into a fewernumber of
transformed
attributesEach principal
component is
a function of
attributes in
the datasetNumerical
attributesNumerical
attributes
(reduced
set). Does
not reallyrequire a
labelEfficient way to
extract predictors that
are uncorrelated to
each other. Helps to
apply Pareto principlein identifying attributes
with highest varianceSensitive to scaling
effects, i.e., requires
normalization of
attribute values before
application. Focus onvariance sometimes
results in selecting
noisy attributesMost
numeric-
valued data
sets require
dimensionreduction
Info gain
(filter-based)Selecting
attributes
based onrelevance to
the target or
labelSimilar to
decision tree
modelNo
restrictions
on variabletype for
predictorsData sets
require a
label. Canonly be
applied on
data setswith
nominal
labelSame as decision
treesSame as decision
treesApplications
for feature
selectionwhere target
variable is
categoricalor numeric
Chi-square
(filter-based)Selecting
attributesbased on
relevance to
the target orlabelUses the chi-
square test ofindependence
to relate
predictors tolabelCategorical
(polynominal)attributesData sets
require alabel. Can
only be
applied ondata sets
with a
nominallabelExtremely robust. A
fast and efficientscheme to identify
which categorical
variables to select fora predictive modelSometimes difficult to
interpretApplications
for featureselection
where all
variables are
categorical
Continued528 Comparison of Data Science AlgorithmsContinued
Algorithm Description Model Input Output Pros Cons Use Case
Forward
Selection
(wrapper-
based)Selecting
attributes
based on
relevance tothe target or
labelWorks in
conjunction
with modeling
methods suchas regressionAll attributes
should be
numericThe label
may be
numeric or
binominalMulticollinearity
problems can be
avoided. Speeds up
the training phase ofthe modeling processOnce a variable is
added to the set, it is
never removed in
subsequent iterationseven if its influence on
the target diminishesData sets
with a large
number of
inputvariables
where
feature
selection is
required
Backward
elimination
(wrapper-based)Selecting
attributes
based onrelevance to
the target or
labelWorks in
conjunction
with modelingmethods such
as regressionAll attributes
should be
numericThe label
may be
numeric orbinominalMulticollinearity
problems can be
avoided. Speeds upthe training phase of
the modeling processNeed to begin with a
full model, which can
sometimes becomputationally
intensiveData sets
with few
inputvariables
where
featureselection is
requiredComparison of Data Science Algorithms 529About the Authors
VIJAY KOTU
Vijay Kotu is Vice President of Analytics at ServiceNow. He leads the
implementation of large-scale data platforms and services to support the
company's enterprise business. He has led analytics organizations for over adecade with focus on data strategy, business intelligence, machine learning,
experimentation, engineering, enterprise adoption, and building analytics
talent. Prior to joining ServiceNow, he was Vice President of Analytics atYahoo. He worked at Life Technologies and Adteractive where he led
marketing analytics, created algorithms to optimize online purchasing
behavior, and developed data platforms to manage marketing campaigns. Heis a member of the Association of Computing Machinery and a member of
the Advisory Board at RapidMiner.
BALA DESHPANDE, PHD
Dr. Deshpande has extensive experience in working with companies ranging
from startups to Fortune 5 in fields ranging from automotive, aerospace,retail, food, and manufacturing verticals delivering business analysis; design-
ing and developing custom data products for implementing business intelli-
gence, data science, and predictive analytics solutions. He was the Founder ofSimaFore, a predictive analytics consulting company which was acquired by
Soliton Inc., a provider of testing solutions for the semiconductor industry.
He was also the Founding Co-chair of the annual Predictive Analytics World-Manufacturing conference. In his professional career he has worked with
Ford Motor Company on their product development, with IBM at their IBM
Watson Center of Competence, and with Domino ’s Pizza at their data sci-
ence and artificial intelligence groups. He has a Ph.D. from Carnegie Mellon
and an MBA from Ross School of Business, Michigan.
531Index
Note: Page numbers followed by “b”“f”and “t”refer to boxes, figures and tables, respectively.
A
“ABCs ”of DL, 324
Accuracy, 83, 266, 266 t
ACF chart. SeeAutoCorrelation
Function chart (ACF chart)
Activation function, 320, 320 f
“Actual Class ”, 267
AdaBoost, 157 /C0158, 158 f, 159 f
Additive time series, 401 /C0402, 403 f
Advanced statistical techniques,
450/C0451
Aggregate operator, 404AGI. SeeArtificial general intelligence
(AGI)
AI.SeeArtificial intelligence (AI)
AI Winter (1960s /C02006), 310 /C0311
RapidMiner XOR example, 311 f
Algorithms, 14 /C018
Andrews curves, 61 /C063, 62 f
Andrews plot, 61
ANNs. SeeArtificial neural networks
(ANNs)
Anomaly detection, 13 t,1 7/C018, 372,
447/C0453, 526
causes of outliers, 448 /C0449
LOF technique, 460 /C0464
outlier detection
using data science, 451 /C0453
density-based, 457 /C0460
distance-based, 453 /C0457
using statistical methods,
450/C0451
Anscombe ’s quartet, 48, 49 f
Append operator, 508Apply forecast operator, 424 /C0425
Apply Model operator, 364, 372,
382, 418Apriori algorithm, 206 /C0211. See also
Naïve Bayesian algorithm; FP-Growth algorithm
frequent itemset generation using,
207f
, 208/C0211, 208 f
rule generation, 209 /C0211
Apriori algorithm, 525
Area under curve (AUC), 83 /C084,
263, 266 /C0268
ARIMA. SeeAuto Regressive
Integrated Moving Average
(ARIMA)
Artificial general intelligence (AGI),
314
Artificial intelligence (AI), 2 /C04, 3f,
307
to engineering, 308 b
models using deep learning, 335spring and summer of, 314 /C0315
Artificial neural networks (ANNs),
124/C0135, 316, 523
implementation, 130 /C0132
performance vector, 134 f
works, 128 /C0130
Assembling known ratings, 349Assimilation, 35 /C036
Association analysis, 11, 13 t, 16,
35/C036, 351, 525
Apriori algorithm, 206 /C0211
concepts of mining association
rules, 201 /C0205
FP-Growth algorithm, 211 /C0219
Association analysis, 348
Association rules
creation, 217 /C0218, 218 f, 219 f
learning, 199
Attributes, 23, 374, 505 /C0506and examples, 495
independence, 120naming for De-Pivot, 382understanding relationship
between, 64
weighting, 467
AUC. SeeArea under curve (AUC)
Australian Beer Production time
series dataset, 412
Auto Regressive Integrated Moving
Average (ARIMA), 398 /C0399
algorithm, 528
ARIMA( p,d,q) model, 423 /C0424
trainer, 424
Autocorrelation, 398 /C0
399, 419 /C0420
AutoCorrelation Function chart (ACF
chart), 420
Autoencoders, 334, 337 f
Automatic iterative methods, 6
Automatic Multilayer Perceptron
(AutoMLP), 130 /C0131
Autoregressive integrated moving
average, 418 /C0425
AutoRegressive models, 420 /C0421
differentiation, 422implementation, 424 /C0425
moving average of error, 423
stationary data, 421 /C0422
Average method, 408Average pooling, 327 /C0328
Axon, 125 /C0126
B
Backpropagation, 128, 313, 316, 329
need for, 321 /C0322
Backward elimination, 484 /C0489,
529
533Bagging technique, 154 /C0155
Balanced accuracy, 511Balanced dataset, 509
Bayes ’theorem, 113
predicting outcome, 115 /C0117
Bayesian belief network, 120
BI.SeeBusiness intelligence (BI)
Biased matrix factorization (BMF),
371/C0372
Binning technique, 27
Binomial
classification, 273operator, 463 /C0464
variables, 196
Biological neurons, 125 b
Bivariate plots, 502 /C0503
Blog authors gender prediction,
294/C0304
data preparation, 295 /C0297
applying trained models to
testing data, 302 /C0303
builds model, 298 /C0302
identifying key features,
297/C0298
preparing test data for model
application, 302
gathering unstructured data, 295raw data for blog classification
study, 296 t
training and testing predictive
models, 301 f
BMF. SeeBiased matrix factorization
(BMF)
Boosting, 156 /C0157
Bootstrap aggregating technique,
154/C0155
Bootstrapping, 154 /C0155
Boston housing dataset
attributes, 171 t
sample view, 171 t
Bot, 448
Box whisker plot, 50 /C051
Bubble chart, 57 /C058, 59 f
Business intelligence (BI), 4, 7
C
CAE. SeeComputer-aided
engineering (CAE)
Cartesian space, 54
Categorical data, 43Causation, 24
Center-based clustering.
SeePrototype-based clusteringCenter-based density, 239 /C0240
Centered cosine
coefficient metric, 359 /C0360
similarity, 356 /C0357
Central data point, 46Central point for each attribute, 63
Central tendency measure, 44 /C045
Centroid clustering. SeePrototype-
based clustering
Centroid prototype, 226 /C0227
Chi square-based filtering, 467 /C0468,
480/C0483
converting golf example set into
nominal values for, 481 f
process to rank attributes of Golf
dataset by, 482 f
Chi-square algorithm, 529
Chi-square test, 120
Class conditional probability
calculation, 115
of humidity, outlook, and wind,
116t
of temperature, 115 t
Class label. SeeOutput variable
Class selection, 91
Classic golf dataset, 69 t
Classical decomposition, 403 /C0404
Classification, 13 t,1 4/C016
model, 353, 383 /C0384, 523
performance, 264, 271
tasks, 29techniques, 11, 452
trees. SeeDecision trees
Click fraud detection in online
advertising, 449 b
Clustering, 11, 13 t, 16, 221, 452,
525/C0526
k-means clustering, 226 /C0238
working, 227 /C0234
implementation, 234 /C0238
DBSCAN clustering, 238 /C0247
working, 240 /C0243
implementation, 240 /C0243
self-organizing map (SOM),
247/C0259
working, 249 /C0252
implementation, 252 /C0259
for object reduction, 223
to reduce dimensionality,
222/C0223
techniques, 447
types, 223 /C0225
Cold start problem, 349, 358Collaborative filtering, 351 /C0373, 354 f
neighborhood-based methods,
354/C0366
Collaborative filtering algorithm,
527
Comma-separated values (CSV),
78/C079, 497 /C0498, 498 f
Competitive SOMs, 248 /C0249
Computer-aided engineering (CAE),
308/C0309, 308 f
Confidence
of response, 267rule, 203 /C0204
Confusion matrix, 263 /C0266,
273/C0274
Consumer Price Index (CPI),
505/C0506
Content recommendation, 345 /C0346
Content-based filtering, 352 /C0353,
373/C0389, 374 f
algorithm, 527
Content-based recommendation
engines, 388
Content-based recommenders, 378,
389
dataset, 378
implementation, 378
Predicted rating using content-
based filtering, 383 f
recommender process using
content-based filtering, 379 f
supervised learning models,
383/C0389
“Contextually adaptive ”system, 315
Contingency tables, 120, 120 t, 481
Continuous attributes, 118 /C0119
Golf dataset with, 119 t
mean and deviation, 119 t
Continuous data, 42
Convex hull, 139, 139 f
Conviction rule, 204 /C0205
Convolution, 324 /C0325, 325 f, 326 f
combining convolution with
activation function, 329 f
multiple filters of convolution,
330f
Convolutional neural networks, 330 f
convolution, 324 /C0325
dense layer, 331, 331 f
dropout layer, 331, 334 f
Correlation, 24
Correlation, 47 /C048, 47 f
Cosine similarity, 107, 364534 IndexCost function, 329 /C0330
CPI. SeeConsumer Price Index (CPI)
Crawl Web operator, 291
Credit rating, 77
Credit scoring, 77Cross Industry Standard Process for
Data Mining (CRISP-DM),
19/C020, 20 f
Cross selling, 200 b
Cross-entropy cost function, 318
Cross-sectional data, 395 /C0396, 397 f
CSV. SeeComma-separated values
(CSV)
Cycle, 401
D
DARPA, 314Data
cleansing practices, 25
engineering, 7errors, 448exploration, 25, 39, 502
datasets, 40 /C043
descriptive statistics, 43 /C048
objectives, 40
roadmap, 63 /C064
importing and exporting tools,
497/C0501
point, 23
preparation, 25 /C029, 40, 77 /C081,
94, 108, 121, 131, 141, 144,172/C0173, 193, 215 /C0216,
363/C0364, 378 /C0379,
504/C0505
feature selection, 28missing values, 26 /C027
outliers, 27
transformation, 27
quality, 25sampling, 28 /C029
series, 407
set or data table, 495splitting, 68 /C073, 70 f
transformation tools, 504 /C0509
types and conversion, 27
understanding, 40warehouses, 25
Data mining, 1, 4, 199
algorithm comparison
anomaly detection, 526association analysis, 525
classification, 523clustering, 525 /C0526
deep learning, 526feature selection, 529
recommenders, 527
regression, 524time series forecasting, 528
framework, 19 /C020
Data science, 1 /C07, 5f
algorithms, 12, 26 /C027
case for, 8 /C09
complex questions, 9
dimensions, 8 /C09
volume, 8
classification, 10 /C011, 10 f
getting started with, 12 /C018
outlier detection using, 451 /C0453
process, 19, 467, 501
application, 34 /C036
data mining process, 21 f
prior knowledge, 21 /C024
knowledge, 36 /C037
modeling, 29 /C034, 29 f
reducing uncertainty, 67 b
tasks, 40
and examples, 13 t
Data visualization, 48 /C063, 450
high-dimensional data, 60 /C063
multivariate visualization, 53 /C059
tools, 501 /C0503, 501 f
bivariate plots, 502 /C0503
finishing data import, 500 f
metadata visible under statistics
tab, 502 f
multivariate plots, 503univariate plots, 502
univariate visualization, 50 /C053
Data-driven approaches, 407 /C0413.
See also Model-driven
forecasting methods
exponential smoothing, 409 /C0412
simple forecasting methods,
407/C0409
Data-driven forecasting methods,
398
Datasets, 23, 24 t,4 0/C043, 361 /C0362,
378, 385
attributes, 501
dividing into training and testing
samples, 81
MovieLens datasets, 362 t
organization, 63
preparation, 336types of data, 42 /C043
categorical or nominal data, 43numeric or continuous data, 42
Davies/C0Bouldin index, 234
DBSCAN. SeeDensity-Based Spatial
Clustering of Applications
with Noise (DBSCAN)
DBSCAN clustering, 238 /C0247.
See also k -Means clustering
algorithm, 525 /C0526
implementation, 234 /C0238
clustering operator and
parameters, 244
data preparation, 244
evaluation, 244
execution and interpretation,
245/C0247
optimizing parameters, 242
varying densities, 242 /C0
243
working principle, 240 /C0243
classification of data points,
241/C0242
clustering, 242defining epsilon and MinPoints,
241
De-Pivot operator, 382, 508
Decision trees, 66 /C087, 74 f,7 5f,8 2f
algorithm, 523approach, 15
for Golf data, 72 f
for Golf dataset, 76 f,9 0f
implementation, 73 /C086
model, 81 /C084
operator, 152path, 66 /C067
works, 66 /C073
Decision-making model, 34
Decomposed data, forecasting using,
406/C0407
Decomposition algorithm, 528
Deep learning (DL), 11, 17, 307,
337f, 339 f.See also Machine
learning (ML)
AI Winter (1960s /C02006),
310/C0311
algorithm, 526convolutional neural networks,
324/C0331, 330 f
deep architectures
AI models using, 335autoencoders, 334, 337 f
RNN, 332 /C0334, 333 fIndex 535Deep learning (DL) ( Continued )
implementation, 335 /C0341
applying model, 340 /C0341
dataset preparation, 336
modeling, 338 /C0340
results, 341
Mid-Winter Thaw (1980s),
311/C0314
spring and summer of AI,
314/C0315
systems, 315
working principle, 315 /C0335
adding hidden layers and need
for backpropagation,
321/C0322
gradient descent, 317 /C0321
regression models as neural
networks, 316 /C0317, 317 f
softmax, 323 /C0324
Define, Measure, Analyze, Improve,
and Control (DMAIC), 19 /C020
Demographic attributes, 271
Dendrite, 125 /C0126
Dendrogram, 225Dense information comprehension,
48
Dense layer, 331, 331 f
Density chart, 58 /C059, 59 f
Density-based algorithm, 526
Density-based outlier detection, 452,
457/C0460. See also Distance-
based outlier detection
implementation, 459 /C0460
data preparation, 459detect outlier operator,
459/C0460
execution and interpretation,
460
working principle, 458 /C0459
Density-Based Spatial Clustering of
Applications with Noise
(DBSCAN), 225 /C0226
Density-clustering, 225
algorithm, 238 /C0239
mode, 246 f
visual output, 246 f
Descriptive Analytics Technique,
293/C0294
Descriptive data science, 11Descriptive modeling, 20 /C021
Descriptive statistics, 6 /C07, 25, 39,
43/C048, 46 f
multivariate exploration, 46 /C048univariate exploration, 44 /C045
Deviation, 45
chart, 60 /C061, 62 f
Dichotomization process, 504
Dimension reduction method, 467,
475/C0477
Dimensional slicing, 7, 63
Dimensionality, curse of, 28Direct marketing (DM), 263 b
Direct method, 89 /C090
Directed data science, 10
Discretization, 505
by binning operator, 505by frequency operator, 505
operator, 506 f
by size operator, 505by user specification, 505
Distance-based algorithms, 451, 526
Distance-based outlier detection,
453/C0457. See also Density-
based outlier detection
implementation, 454 /C0457
data preparation, 456detect outlier operator,
456/C0457
execution and interpretation,
457
working principle, 454
Distribution chart, 52 /C053, 55 f
Distribution model, 225
Distribution-based clustering.
SeeModel-based clustering
Distribution-based outlier, 452
DL.SeeDeep learning (DL)
DM. SeeDirect marketing (DM)
DMAIC. SeeDefine, Measure,
Analyze, Improve, and
Control (DMAIC)
Document, 285
clustering, 222
matrix, 106 /C0107
vector, 106 /C0107, 285 /C0286
“Dot product ”formulation, 141
Dropout
layer, 331, 334 f
prediction, 149 b
E
Eigenvalue analysis of covariance
matrix, 472
Empty clusters, 233
Ensemble learners, 15, 148 /C0161,
149f, 156 fachieving conditions for ensemble
modeling, 151 /C0152
data mining process, 153 f
implementation, 152 /C0160
AdaBoost, 157 /C0158
boosting, 156 /C0157
bootstrap aggregating or
bagging, 154 /C0155
ensemble by voting, 152 /C0153
random forest, 159 /C0160
wisdom of crowd, 148 /C0149
works, 150 /C0152
Ensemble model, 28 /C029, 33/C034,
523
Entertainment recommendation, 346
Entire dataset, 495Entropy, 67, 67 f,8 1/C083
Euclidean distance, 103 /C0104,
228/C0230
Evaluation, 350
framework, 19 /C020
of model, 32 /C033
Exclusive partitioning clusters, 223Execute Roperator, 412, 427
Explanatory modeling, 20 /C021
Exploratory data analysis. SeeData
exploration
Exploratory visualization, 7Exponential smoothing, 407,
409/C0412
algorithm, 528Holt-Winters ’three-parameter, 412
Holt ’s two-parameter, 411 /C0412
Extract Example Set, 436Extracting meaningful patterns, 4 /C05
F
Fast brute-force approach, 468
Feature selection method, 165 /C0166,
174, 467, 468 b
Chi-square-based filtering,
480/C0483
classification, 468 /C0470
information theory-based filtering,
477/C0480
PCA, 470 /C0477
taxonomy of, 469 f
wrapper-type feature selection,
483/C0489
Features, 374
selection, 11, 18, 28, 529
Filter bubble, 350 /C0351
Filter Example operator, 436536 IndexFilter Examples Range operator, 172
Filter model, 469Filter-based algorithm, 529
Filter-type methods, 467 /C0468
Filtering, 382
prospect, 77 /C086
First order differencing, 422
Forecast(ing), 17, 407
errors, 407, 439 /C0441
Forward selection, 484, 529
FP.SeeFrequent pattern (FP)
FP-Growth algorithm, 211 /C0219, 525
data science process, 216 f
frequent itemset generation,
214/C0215
generating FP-tree, 211 /C0215
implementation, 215 /C0219
FP-Growth operator, 217
FP-Tree, 211, 212 f, 213 f, 214 f
Frequent itemset
generation, 214 /C0215
using Apriori principle, 207 f,
208/C0211, 208 f
support calculation, 209 t
Frequent pattern (FP), 205
Fully connected layers, 330, 331 f
Function-fitting approach, 165 /C0166
futureMonths process, 434 /C0436
Fuzzy clusters, 224
G
Gain ratio, 83, 477 /C0478
GAN. SeeGenerative adversarial
network (GAN)
Gaussian distribution. SeeNormal
distribution
GDP. SeeGross domestic product
(GDP)
Gender prediction of blog authors,
294/C0304
Generalization, 4 /C05
Generate attributes, 178 /C0179
Generate Data operator, 514Generative adversarial network
(GAN), 335
Genre attributes, 380 /C0382
Get Pages operator, 291
Gini index, 67, 83
Global baseline, 366
matrix factorization, 366 /C0373
Golf dataset with modified
temperature and humidity
attributes, 114 tGPUs. SeeGraphics processing units
(GPUs)
Gradient descent technique,
169/C0170, 316 /C0321
Graphical user interface (GUI),
492/C0493
GUI-driven application, 496
launch view of RapidMiner 6.0,
492f
Graphics processing units (GPUs),
314
Greedy methodology, 517 /C0518
Gross domestic product (GDP),
248/C0249
GUI. SeeGraphical user interface
(GUI)
H
“Handcrafted knowledge ”systems,
315
Hidden layers, 321 /C0322, 438
combining multiple logistic
regression models, 322 f
Hierarchical clusters, 223, 225High-dimensional data visualization,
60/C063
Andrews curves, 61 /C063, 62 f
deviation chart, 60 /C061, 62 f
parallel chart, 60, 61 f
Histogram, 40, 50, 51 f
class-stratified, 52 f
Home price prediction, 166 b
Hybrid recommenders, 389 /C0390,
390f
Hyperplane, 135 /C0136, 136 f, 140 f
Hypothesis testing, 7
I
ID3. SeeIterative Dichotomizer 3
(ID3)
Identifiers, 24
IDF. SeeInverse document frequency
(IDF)
Impute Missing Values operator, 512
Independent variables, 397
Info gain algorithm, 529Information gain, 83, 477 /C0478
information gain /C0based filtering
method, 467 /C0468
Information theory-based filtering,
477/C0480
Integer, 42
Internet of things, 90Interpretation framework, 19 /C020
Interpreting results, 40Inverse document frequency (IDF),
284
Iris dataset, 40 /C042, 46
and descriptive statistics, 44 t
Iris setosa ,4 0/C041
Iris versicolor ,4 0/C041, 41 f
Iris virginica ,4 0/C041
Item k-NN recommender process,
363/C0364, 363 f, 365 f
Item profile
building, 374 /C0375, 375 t, 377 t
user profile computation,
375/C0383, 376 f, 378 t
preparation, 379 /C0382
Item-based collaborative filtering,
359/C0361
normalized ratings and similarity,
360t
transposed ratings matrix, 359 t
Item-based neighborhood method,
351/C0352
Itemsets, 202 /C0205, 206 f
Iterative Dichotomizer 3 (ID3), 68
Iterative process, 22
J
Jaccard similarity, 106 /C0107
Join operator, 404, 425, 508
Joint entropy, 73
K
k-means clustering, 16, 226 /C0238.
See also DBSCAN clustering
algorithm, 525 /C0526
evaluation of clusters, 233 /C0234
implementation, 234 /C0238
special cases, 232 /C0233
working principle, 227 /C0234
calculating new centroids, 231
data points assignment,
228/C0230, 230 f
initiate centroids, 228
repeating assignment and
calculating new centroids, 232
termination, 232
k-medoids clustering process,
290/C0291
k-nearest neighbor ( k-NN), 6,
98/C0111, 110 f, 454
algorithm, 26 /C027, 223, 523
data mining process for, 109 fIndex 537k-nearest neighbor ( k-NN)
(Continued )
implementation, 108 /C0110
performance vector for, 110 f
works, 100 /C0107
Keras extension for RapidMiner, 336
Keras operator, 338 /C0339, 339 f
Keyword clustering, 290 /C0294
apply clustering technique,
293/C0294
data preparation, 292
gathering unstructured data,
291/C0292
Knowledge discovery, 1, 4 /C05
Kohonen networks, 248
L
L1-norm regularization, 184L2-norm regularization, 183, 184 f
Label, 24Laplace correction, 118Lasso regression, 184
Latent factors, 354
model, 352, 366 /C0367, 367 f
Latent matrix factorization
algorithm, 527
Learn-One-Rule technique, 91 /C093
Learning. See also Deep learning (DL)
algorithms, 6, 30 /C032
regression model, 32 f
perceptron, 309 /C0310
process, 485rate, 319 /C0320
function, 256
Level, 401Lexical substitution process,
286/C0288
LibSVM model, 303Lift
charts, 263
curves, 268 /C0270, 271 t, 272 f, 277 f,
278f
rule, 204
Linear regression, 9, 16, 185, 186 f.
See also Logistic regression
algorithm, 524checkpoints to ensuring regression
model validity, 180 /C0185
implementation, 172 /C0179
line, 182 f
model, 9, 27, 165 /C0166, 415, 416 f
operator, 173 f, 174, 487
technique, 31works, 167 /C0171
Linearly non-separable dataset,
144/C0147
Linearly separable dataset, 141 /C0144
Lloyd ’s algorithm, 227
Lloyd/C0Forgy algorithm, 227
Local outlier factor technique (LOF
technique), 460 /C0464
algorithm, 526implementation, 462 /C0464
data preparation, 462 /C0463
detect outlier operator, 463results interpretation, 463 /C0464
LOF technique. SeeLocal outlier
factor technique (LOF
technique)
Logistic regression, 16, 185 /C0196.
See also Linear regression
algorithm, 524finding sigmoid curve, 188 /C0190
growth of logistic regression
applications, 186 f
implementation, 193 /C0195
model, 316 /C0317
points for logistic regression
modeling, 196
setting up RapidMiner process,
194f
works, 187 /C0192
Logit function, 189, 196
Loop operator, 438LSTAT, 177 /C0178
M
m-lag first order differencing, 422
MA(q) model. SeeMoving Average
with qlags model (MA(q)
model)
Machine breakdowns, predicting and
preventing, 90 b
Machine learning (ML), 1 /C04, 3f.
See also Deep learning (DL)
algorithms, 2 /C04, 9, 313
methods, 429 /C0438
lagged inputs and target, 429 f
neural network autoregressive,
436/C0438
windowing, 430 /C0436
ML-based prediction model, 350
systems, 315
Macro in RapidMiner, 434 /C0436
MAE. SeeMean absolute error (MAE)
Mahalanobis distance, 450 /C0451MAPE. SeeMean absolute percentage
error (MAPE)
Market basket analysis, 11, 199 /C0200
Marketing, 222
MASE. SeeMean absolute scaled
error (MASE)
Matrix factorization, 354, 366 /C0373,
371f, 372 f
decomposition of ratings matrix
into latent factor matrices,
368f
implementation, 370
Max pooling, 327 /C0328, 328 f
Mean, 44
Mean absolute error (MAE), 350,
441/C0442
Mean absolute percentage error
(MAPE), 442
Mean absolute scaled error (MASE),
442/C0443
Median, 44
Median value (MEDV), 171, 485
Medoid, 290 /C0291
MEDV. SeeMedian value (MEDV)
Meta learning, 148
MetaCost operator, 86, 194 /C0195,
195f
Metadata, 494Mid-Winter Thaw (1980s),
311/C0314
Mining. See also Text mining
association rules concepts,
201/C0205
itemsets, 202 /C0205
rule generation process, 205
process, 282 /C0283
Missing values, 26 /C027
Mixture of Gaussians, 225ML. SeeMachine learning (ML)
MLP. SeeMulti-layer perceptron
(MLP)
MLR. SeeMultiple linear regression
(MLR)
Mode, 44
Model Combiner operator, 390Model evaluation
confusion matrix, 263 /C0266
DM, 263 b
implementation, 271 /C0276
data preparation, 271 /C0273
evaluation, 273
execution and interpretation,
273/C0276538 Indexmodeling operator and
parameters, 273
lift curves, 268 /C0270, 271 t, 272 f
ROC curves and AUC, 266 /C0268
Model-based clustering, 225
Model-driven forecasting methods,
398/C0399, 413 /C0429. See also
Data-driven approaches
autoregressive integrated moving
average, 418 /C0425
global and local patterns, 415 f
regression, 415
implementation in RapidMiner,
417/C0418
with seasonality, 415 /C0418
seasonal ARIMA, 426 /C0429
seasonal attributes, 417 f
Modeling, 5, 29 /C034, 29 f, 338/C0340
ensemble, 33 /C034
evaluation of model, 32 /C033
learning algorithms, 30 /C032
process, 20 /C021
training and testing datasets, 30
Moore ’s Law, 1
MovieLens
datasets, 362 t
ratings matrix dataset, 378
Moving average
of error, 423
smoothing, 408
Moving Average with qlags model
(MA(q) model), 423
Multi-layer perceptron (MLP), 311
Multicollinearity, 468
Multiple linear regression (MLR), 170
Multiplicative time series, 401 /C0402,
403f
Multivariate exploration, 46 /C048
central data point, 46
correlation, 47 /C048, 47 f
Multivariate plots, 503
Multivariate visualization, 53 /C059
bubble chart, 57 /C058, 59 f
density chart, 58 /C059, 59 f
scatter matrix, 56 /C057, 58 f
scatter multiple, 55 /C056, 57 f
scatterplot, 54 /C055, 56 f
N
n-grams models, 289
Naïve Bayesian algorithm, 15,
111/C0124, 123 f, 523. See also
Apriori algorithm; FP-Growth
algorithmalgorithm, 468
data mining process, 122 f
distribution table output, 123 f
implementation, 121 /C0122
works, 113 /C0120
Naïve Bayesian operator, 121, 273
Naive forecast, 442
Naïve method, 407
Natural language processing method
(NLP method), 282 b
Neighborhood methods, 351 /C0352,
360
Neighborhood users, deducing rating
from, 357 /C0358
Neighborhood-based method,
354/C0366
dataset, 361 /C0362
implementation steps, 362 /C0364
Nested operator, 81
Nested process, 513
Neural net, 130 /C0131
Neural network autoregressive model
(NNAR model), 436 /C0438,
438f
Neural networks, 15. See also
Artificial neural networks
(ANNs)
models, 26 /C027
regression models as, 316 /C0317,
317f
Neuron, 125 /C0126, 248
NLP method. SeeNatural language
processing method (NLP
method)
NNAR model. SeeNeural network
autoregressive model (NNAR
model)
Noise, 401, 404
Nominal
to binominal operator, 504
data, 43
to numerical operator, 504
Non-domain-focused program,
282
Non-systematic component,
397/C0398
Nonlinear optimization techniques,
190
Normal distribution, 45, 52 /C053
Normalized ratings matrix, 356 t
Numeric data, 42
Numerical
to binominal operator, 504
to polynominal operator, 505O
Observation, 407
Online advertising, click fraud
detection in, 449 b
Online analytical processing (OLAP),
7
Operators, 78, 495 /C0496
Optical character recognition, 127 b
Optimization tools, 512 /C0520, 515 f
configuring grid search optimizer,
516f
progression
of genetic search optimization,
519f
of grid search optimization,
517f
of quadratic greedy search
optimization, 518 f
searching for optimum within
fixed window that slides
across, 516 f
simple polynomial function to
demonstrate optimization,
514f
Optimize Parameters operator,
424/C0425
Outliers, 27, 233, 447
causes, 448 /C0449
detection using data science,
451/C0453
detection using statistical methods,
450/C0451
watch out for, 64
Output variable, 10
Overfitting, 32 /C033, 71, 84 /C085, 181
Overlapping clusters, 223
P
“Padding ”process, 327, 327 f
Parallel axis, 60
Parallel chart, 60, 61 f
PCAPrincipal component analysis
(PCA)
Pearson correlation, 356 /C0357
coefficient, 47 /C048
coefficient metric, 359 /C0360
Pearson similarity measure, 364
Perceptron, 125, 309 /C0311, 310 f
learning rule, 309 /C0311
Performance
criterion, 232 /C0233
evaluation, 382
operator, 382
Pivot data, 63Index 539Pivot operator, 507 /C0508, 508 f
PMML. SeePredictive Model Markup
Language (PMML)
Polynomial regression, 415
Polynominal data type, 43Post-processing, 233
Post-pruning, 72 /C073
Precision, 265, 266 t
“Predicted Class ”, 267
Predictive analytics, 1
Predictive Model Markup Language
(PMML), 34 /C035
Predictive modeling, 20 /C021
Predictor variables, 397
Preprocessing
DBSCAN clustering, 238 /C0247
framework, 19 /C020
k-Means clustering, 226 /C0238
Principal component analysis (PCA),
18, 456, 467, 470 /C0477, 471 f,
529
implementation, 472 /C0477
data preparation, 474execution and interpretation,
474/C0477
PCA operator, 474
interpreting RapidMiner output
for, 478 f
working principle, 470 /C0472
breakfast cereals dataset for
dimension reduction, 473 t
Prior knowledge, 21 /C024
causation vs. correlation, 24
data, 23 /C024, 24 t
objective, 22subject area, 22 /C023
Prior probability calculation, 115
Probabilistic clusters. SeeFuzzy
clusters
Probability mass function, 150
Product recommendations, 345
Prototype-based clustering, 224 /C0225
and boundaries, 229 f
Proximity measure, 102 /C0107
Pruning process, 72 /C073
Q
Quartile, 50 /C051, 53 f
class-stratified quartile plot, 54 f
R
R software package
integration with, 520 /C0521script for Holt-Winters ’forecasting,
413
Random forest, 159 /C0160, 160 f
Random walk
with drift, 424model, 424
Randomized algorithms, 12
Range, 45RapidMiner, 12 /C013, 271, 284,
290/C0291, 404, 454, 475, 485,
491
data
importing and exporting tools,
497/C0501
transformation tools, 504 /C0509
visualization tools, 501 /C0503
implementation in, 417 /C0418
integration with R, 520 /C0521
optimization tools, 512 /C0520
process, 74, 86, 253, 256,
371/C0373, 382, 427
for rule induction, 96 f
for tree to rules operator, 97 f
sampling and missing value tools,
509/C0512
Studio, 491
user interface and terminology,
492/C0497
wrapper function logic used by,
487f
Rating prediction, 350
operator, 364, 372, 382
Ratings matrix, 347 /C0349, 347 t, 355 t,
377t
assemble known ratings, 349evaluation, 350
rating prediction, 350
Raw German Credit Data view, 78 t
Read CSV operator, 497 /C0498
Read Excel operator, 78, 79 f
Rebalance sub-process, 511
Recall, 265 /C0266, 266 t
Receiver operator characteristic curve
(ROC curve), 83 /C084, 263,
266/C0268
classifier performance data needed
for building, 268 t
Recommendation engines, 11, 13 t,
17, 343 /C0348, 347 f, 351/C0353
applications, 345 /C0346
balance, 350 /C0351
building item profile, 374 /C0375,
375t, 377 tcollaborative filtering, 353 /C0373,
354f
comparison, 392 t
content-based filtering, 373 /C0389,
374f
content-based recommenders, 378
global baseline, 366
hybrid recommenders, 389 /C0390,
390f
item-based collaborative filtering,
359/C0361
matrix factorization, 370, 371 f,
372f
neighborhood based method, 361
ratings matrix, 348 /C0349
supervised learning models, 385taxonomy, 351 f
user-based collaborative filtering,
355, 360 /C0361
Recommendation model, 389 /C0390
Recommenders, 527
modeling, 382
Rectified linear unit (RELU),
320/C0321
Recurrent neural networks (RNN),
332/C0334, 333 f
Regression, 13 t, 165, 168 f, 415, 524
methods, 11, 485, 512 /C0513
linear regression, 166 /C0185
logistic regression, 185 /C0196
predicting home prices, 166 b
model, 353, 489
activation function to regression
“network ”, 318 f
as neural networks, 316 /C0317,
317f
predictive models, 399
with seasonality, 415 /C0418
tasks, 29trees, 66
Regularization, 181, 369 /C0370
Reinforcement learning (RL), 315Relevance, 265, 266 t
RELU. SeeRectified linear unit
(RELU)
Remember operator, 436Rename operator, 178 /C0179
Repeated Incremental Pruning to
Produce Error Reductionapproach (RIPPER approach),91
Replace (Dictionary) operator,
78/C079540 IndexReplace missing values operator, 512
Repository, 494Representative models, 5
Retrieve operator, 172
Ridge regression, 183RIPPER approach. SeeRepeated
Incremental Pruning to
Produce Error Reductionapproach (RIPPER approach)
RL.SeeReinforcement learning (RL)
RMSE. SeeRoot mean square error
(RMSE)
RNN. SeeRecurrent neural networks
(RNN)
Roadmap for data exploration,
63/C064
ROC curve. SeeReceiver operator
characteristic curve (ROC
curve)
Root mean square error (RMSE),
350, 442
Rule development, 91 /C092
Rule generation, 205, 209 /C0211
Rule induction, 15, 87 /C098
algorithm, 523
approaches to developing rule set,
89/C090
implementation, 94 /C098
model, 388, 388 f
modeling operator, 94 /C095
works, 91 /C093
Rule Model window, 95
Rule set development, 93
S
Sample, Explore, Modify, Model, and
Assess (SEMMA), 19 /C020
Sampling, 28, 121Sampling and missing value tools,
509/C0512
decision trees on well-balanced
data, 510 f
rebalanced data and resulting
improvement in class recall,
512f
snapshot of imbalanced dataset,
510f
unbalanced data and resulting
accuracy, 511 f
Scaled error, 442Scatter
charts, 39
matrix, 56 /C057, 58 fmultiple, 55 /C056, 57 f
Scatterplot, 54 /C055, 56 f
Seasonal and Trend decomposition
using Loess (STL), 405
Seasonal ARIMA, 426 /C0429, 427 f
forecast using, 428 f
implementation, 427 /C0429
Seasonal differencing, 422Seasonal dummy variables, 415Seasonal Extraction in ARIMA Time
Series (SEATS), 405
Seasonal index, 412Seasonal Naive method, 407 /C0408
Seasonality, 401, 404
regression with, 415 /C0418
seasonally adjusted time series,
406/C0407
SEATS. SeeSeasonal Extraction in
ARIMA Time Series (SEATS)
Second order differencing, 422Segmenting customer records, 226 b
Select attribute operator, 363 /C0364
Self-organizing maps (SOMs), 16,
225/C0226, 247 /C0259, 248 f
algorithm, 525 /C0526
implementation, 252 /C0259
data preparation, 255execution and interpretation,
256
location coordinates, 259
SOM modeling operator and
parameters, 255 /C0256
working principle, 249 /C0252
assignment of data objects, 250centroid update, 250 /C0252
initialize centroids, 249
mapping new data object, 252
termination, 252topology specification, 249
SEMMA. SeeSample, Explore,
Modify, Model, and Assess
(SEMMA)
Sensitivity, 265, 266 t
Sequential covering technique,
89/C090
Session grouping, 222Set Role operator, 178 /C0179,
363/C0364, 371, 382
SGD. SeeStochastic Gradient Descent
(SGD)
Shuffle operator, 172
Sigmoid curve, logistic regression
finding, 188 /C0190Sigmoid transformation, 320
Simple forecasting methods,
407/C0409
average method, 408
moving average smoothing, 408Naïve method, 407
seasonal Naive method,
407/C0408
weighted moving average
smoothing, 408 /C0409
Simple matching coefficient (SMC),
106
Softmax, 323 /C0324, 323 f
SOMs. SeeSelf-organizing maps
(SOMs)
Spam email, predicting and filtering,
112b
Specificity, 265, 266 t
Split Data operator, 94, 363 /C0364
Split Validation operator, 81, 513Spread measure, 45
Spread of each attribute, 63
Spreadsheet programs, 398 /C0399
SSE. SeeSum of squared errors (SSE)
Stacking model, 153
Standard deviation, 45
Standard machine learning
techniques, 429 /C0430
Stationary data, 421 /C0422
Statistical methods, outlier detection
using, 450 /C0451
Statistics, 39
machine learning, and computing
combination, 6
Stemming process, 288 /C0289
STL. SeeSeasonal and Trend
decomposition using Loess
(STL)
Stochastic Gradient Descent (SGD),
370
Stop word filtering, 286
Stopping behavior, 485 /C0487
Stratified sampling, 28 /C029, 81
Strict partitioning clusters.
SeeExclusive partitioning
clusters
“
Stride ”, 327, 328 f
Subject matter expertise, 6
Sum of squared errors (SSE), 181,
231
Supervised classification, 399, 453
Supervised data science, 10
algorithms, 447Index 541Supervised learning models, 353,
383/C0389, 527
dataset, 385
implementation, 385
classification process for one
user in system, 385 f
converting item profile to
classification training set,387f
item profile with class label, 383 t
personalized decision tree, 384 f
Supervised model (SVM), 281Supervised techniques, 10Support count, 208 /C0209
Support rule, 203
Support vector machines (SVMs), 15,
135/C0147, 137 f, 144 f, 195
algorithm, 523
concept and terminology,
135/C0138
dataset to demonstrating, 142 t
implementation, 141 /C0147
works, 138 /C0140
Surface plot, 503SVM. SeeSupervised model (SVM)
SVMs. SeeSupport vector machines
(SVMs)
Synonymy, 365 /C0366
Systematic components, 397 /C0398
T
Target variable. SeeOutput variable
TDM. SeeTerm document matrix
(TDM)
Technical integration, 34 /C035
Tensorflow Playground (TF
Playground), 311, 312 f
Term document matrix (TDM),
285/C0286, 287 t
Term filtering, 286 /C0288
Term frequency /C0inverse document
frequency (TF /C0IDF),
283/C0285
Test dataset evaluation, 33 t
Testing datasets, 30, 30 t,3 1f
Text analytics, 281Text mining, 9, 11, 17, 281 /C0283,
380/C0382
high-level process, 283 f
implementation, 290 /C0304
keyword clustering, 290 /C0294
predicting gender of blog
authors, 294 /C0304sequence of preprocessing steps,
289t
working principle, 283 /C0290
term frequency /C0inverse
document frequency,283/C0285
terminology, 285 /C0290
TF Playground. SeeTensorflow
Playground (TF Playground)
TF/C0IDF. SeeTerm frequency /C0inverse
document frequency
(TF/C0IDF)
Three-parameter exponential
smoothing, 412
Time periods, 407
Time series, 395
analysis, 395, 400 f
data-driven approaches, 407 /C0413
decomposition, 397 /C0398,
400/C0407, 402 f
classical, 403
/C0404
forecasting using decomposed
data, 406 /C0407
implementation, 404 /C0407
process for, 405 f
forecasting, 11, 13 t, 395, 528
demand of product, 399 b
taxonomy, 397 /C0399, 398 f
implementation, 412 /C0413
R script for Holt-Winters ’
forecasting, 413
machine learning methods,
429/C0438
model-driven forecasting methods,
413/C0429
of monthly antidiabetic drug sales,
396f
performance evaluation, 439 /C0443
MAE, 441 /C0442
MAPE, 442
MASE, 442 /C0443
RMSE, 442validation dataset, 439 /C0443
Timeout, 434 /C0436
Token, 285Tokenization, 285Topology, 249
Traditional statistical analysis
approaches, 9
Tragic example, 190 /C0192
Trainable parameters, 324
Training
data, 2 /C04datasets, 30, 30 t,3 1f
incomplete training set, 117 /C0118
perceptron, 309 /C0310
Transformation, 19 /C020, 27
data transformation tools,
504/C0509
sigmoid, 320
unit, 320
Tree-to-rules operator, 96 /C098
Trend, 401, 404
Truth tables. SeeConfusion matrix
Two-parameter exponential
smoothing, 411 /C0412
U
Unbalance, sub-process, 509
Undirected data science, 10Unit transformation, 320
Univariate exploration, 44 /C045
central tendency measure, 44 /C045
spread measure, 45
Univariate plots, 502
Univariate visualization, 50 /C053
distribution chart, 52 /C053, 55 f
histogram, 50, 51 f
quartile, 50 /C051, 53 f
Universal approximator, 126 /C0127
Unseen dataset, applying model to,
195
Unseen test data, application to,
178/C0179
Unsupervised anomaly detection,
447
Unsupervised data science, 10
Unsupervised techniques, 11User interface
GUI, 492 /C0493
terminology, 493 /C0497, 493 f, 494 f
attributes and examples, 495 f
operator for building decision
tree, 496 f
process automatically translated
to XML document, 497 f
User k-NN recommender process,
363/C0364
User profile
approach, 352 /C0353, 374
computation, 375 /C0383, 376 f, 378 t
User-based collaborative filtering,
355, 360 /C0361
deducing rating from
neighborhood users,
357/C0358542 Indexidentifying similar users, 355 /C0357
User-based neighborhood method,
351/C0352, 358
User-item interaction, 348 /C0349, 370
User-to-user similarity matrix, 357 t
Utility matrix, 347
V
Validation
dataset, 30, 439 /C0443
techniques, 263
Variable, 40
binomial, 196independent, 397
output, 10
predictor, 397seasonal dummy, 415
Variance, 45
Visual model of SOM, 257 /C0258Visualization, 39, 48. See also Data
visualization
distribution of each attribute, 63
relationship between attributes, 64
style, 257tools, 502
Voronoi partitions, 227, 228 f
Vote operator, 152 /C0153, 154 f
W
Web analytics, 222
Weight adjustment, 129 /C0130
Weighted moving average
smoothing, 408 /C0409
Windowing, 399, 430 /C0436
forecast generation in loop,
434/C0436
implementation, 432
model training, 432, 434
set up, 432 /C0433windowing-based machine
learning, 528
Wizard-style functionality, 492 /C0493
Wrapper-type feature selection,
483/C0489
backward elimination, 485 /C0489,
488f, 489 f
sample view of Boston housing
dataset, 486 t
wrapper function logic used by
RapidMiner, 487 f
Wrapper-type methods, 467 /C0468
X
X11, 405
XML code, 496 /C0497
Y
Yet Another Learning Environment
(YALE), 491Index 543Praise
“A wonderful book to take on the broad topic of Data Science. It covers all
the relevant algorithms from the classical to the newly reborn to the latest
innovations. The content is presented in a way that is both accessible andtangible by describing practical applications in recommendation engines,
time series forecasting, and the like. The book provides the vocabulary and
taxonomy to discern the right approach for each application. ”
Eric Colson
Chief Algorithms Officer, Stitch Fix.
“If you are business leader, a data practitioner or just someone who is
curious about how Data Science and AI work in practice, then this book is a
must read. I received tremendous value when I read through the book the
first time to get a discipline overview, and I also find myself referring backto specific chapters time and time again as a reference guide. The content is
well presented and the book provides practical examples to illustrate
important concepts. ”
David Dowhan
CEO, TruSignal.
“This book is an important work to help analytics teams rapidly bridge the
data science skills gap. The hands-on application of advanced analytics and
machine learning will enable readers to competently solve high value use
cases. ”
Peter Lee
CEO, RapidMiner.
“Kotu and Deshpande have delivered a comprehensive and accessible a-to-z
on data science. The book enables leadership teams from organizations of
all sizes to grasp and implement data science principles and approaches to
unlock the value of their data. ”
Jonathan Murray
CEO, myVR.
545“This book is a must have for you and your organization to align and learn
the best methods to exploit the best competitive advantages you have: your
own data. Data science provides a method to understand, activate, and make
prescriptive decisions and recommendations about your data. This bookprovides a level of baseline for the organization; to understand and drive a
pathway on techniques used to do just that. ”
Sy Fahimi
Operating Partner, Symphony Technology Group.
“AI and Data Science is at a critical inflection point for businesses; the
transformation potential not seen since the infancy of the Internet. Thosewho fail to embrace this transformation will be disrupted. Kotu and
Deshpande have created a comprehensive and powerful manifest for both
the practitioner and the business leader seeking to understand the potential.This is your basis to Play to win with Data Science & AI ”
Andrew J. Walter
VP, IT and Commercial Services (ret), P&G.
“Only marketers living under a rock would deny the paramount importance
of data science now and in the future. Authors of this book do a fantastic jobof both explaining the fundamentals of data science and also providing
practical tips on how to make your data work for your business. ”
David Rodnitzky
CEO, 3Q Digital.
“A great resource for anyone interested in learning more about data
analysis and associated data science techniques. A deeper dive on machinelearning, association analysis and other emerging analytical tools is
available for the serious practitioners. All in all, a great tool and a handy
reference for those interested in data science in general. ”
Badal Choudari
SVP, Products, Symantec.
“Data science is challenging, transforming and re-inventing every business
sector. This comprehensive and hands-on guide delivers the conceptual
framework and the concrete examples that make data science accessible
and realizable in any organization ”
Meri Gruber
Co-founder, Decision Management Solutions.
“This book provides the quick start into practical data analysis. Without
overburdening the reader with theory, the book covers all standard data
mining topics, ranging from data preparation over classification and546 Praiseregression to deep learning and time series forecasting. Showing ready-to-
use configurations in RapidMiner, readers can directly apply the methods to
their problems at hand, allowing for a quick transition from theory to
practice. ”
Prof. Heiko Paulheim
Professor, Data and Web Science Group, University of Mannheim.
“There is a lot being written about how data science will transform our
world, but for most of us, it can be hard to get beneath the superficial
excitement or fear that these claims elicit and what they will mean to our
everyday lives. This book accomplishes the rare task of investigating thesemechanics, not just for experts, but for non-technicians whose lives and
livelihoods will be impacted by data science. Whether you ’re a CEO or a
salesman or a stock clerk, reading this book will allow you to understandwhere we ’re headed, and thrive in the data science revolution. ”
Charlie Stack
Head of Data & Analytics Practice, Spencer Stuart.
“This book does an amazing job of clearly organizing, explaining and then
presenting a practical implementation for various data science techniques.As a business executive, this book has enabled me to better understand,
what previously seemed like a foreign language. In today ’s highly
competitive digital environment I highly recommend adding this to yourmust-read list. ”
Cindy Brown
Chief Revenue Officer, ViralGains.
“From our operating rooms, to our schools, financial markets, concert halls
and art galleries; data science and machine learning have disrupted
traditional processes and expanded the borders that previously served asour limitations. This book democratizes the concepts of Data Science for
readers across all job types and skill sets and arms them with the
necessary concepts and tools to begin applying them to their daily lives. ”
Eoin Ryan
Head of Investor Relations, Naspers.
“From business leaders and emerging executives seeking to better
understand the concepts of data science, and how to use this field to better
understand their business and make better decisions; to emerging data
scientists seeking a stronger foundation in this field and a better sense ofhow to apply its concepts and techniques in the real world, this text is a
touchstone resource. ”
Jeff Russakaw
CEO, Boosted.Praise 547“Whether you are a company driving disruption or undergoing disruption in
your industry, deriving data-driven business insights via application of Data
Science, AI/ ML technologies for making the right strategic decisions is very
foundational. This book breaks down the concepts of Data Science fluidlyand couples them with very practical implementation tips. A must read for
the novice as well as experts in this field. ”
Milind Wagle
Chief Information Officer, Equinix.548 Praise