Data Science and Machine Learning
Mathematical and Statistical Methods
Dirk P. Kroese, Zdravko I. Botev, Thomas Taimre, Radislav Vaisman
30th October 2023To my wife and daughters: Lesley, Elise, and Jessica
‚ÄîDPK
To Sarah, SoÔ¨Åa, and my parents
‚ÄîZIB
To my grandparents: Arno, Harry, Juta, and Maila
‚ÄîTT
To Valerie
‚ÄîRVCONTENTS
Preface xiii
Notation xvii
1 Importing, Summarizing, and Visualizing Data 1
1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Structuring Features According to Type . . . . . . . . . . . . . . . . . . 3
1.3 Summary Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.4 Summary Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.5 Visualizing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.5.1 Plotting Qualitative Variables . . . . . . . . . . . . . . . . . . . . 9
1.5.2 Plotting Quantitative Variables . . . . . . . . . . . . . . . . . . . 9
1.5.3 Data Visualization in a Bivariate Setting . . . . . . . . . . . . . . 12
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2 Statistical Learning 19
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.2 Supervised and Unsupervised Learning . . . . . . . . . . . . . . . . . . . 20
2.3 Training and Test Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.4 Tradeo s in Statistical Learning . . . . . . . . . . . . . . . . . . . . . . 31
2.5 Estimating Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
2.5.1 In-Sample Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
2.5.2 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . . . . 37
2.6 Modeling Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
2.7 Multivariate Normal Models . . . . . . . . . . . . . . . . . . . . . . . . 44
2.8 Normal Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
2.9 Bayesian Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
3 Monte Carlo Methods 67
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
3.2 Monte Carlo Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
3.2.1 Generating Random Numbers . . . . . . . . . . . . . . . . . . . 68
3.2.2 Simulating Random Variables . . . . . . . . . . . . . . . . . . . 69
3.2.3 Simulating Random Vectors and Processes . . . . . . . . . . . . . 74
3.2.4 Resampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
3.2.5 Markov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . 78
3.3 Monte Carlo Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
viiviii Contents
3.3.1 Crude Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . 85
3.3.2 Bootstrap Method . . . . . . . . . . . . . . . . . . . . . . . . . . 88
3.3.3 Variance Reduction . . . . . . . . . . . . . . . . . . . . . . . . . 92
3.4 Monte Carlo for Optimization . . . . . . . . . . . . . . . . . . . . . . . . 96
3.4.1 Simulated Annealing . . . . . . . . . . . . . . . . . . . . . . . . 96
3.4.2 Cross-Entropy Method . . . . . . . . . . . . . . . . . . . . . . . 100
3.4.3 Splitting for Optimization . . . . . . . . . . . . . . . . . . . . . . 103
3.4.4 Noisy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . 105
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
4 Unsupervised Learning 121
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
4.2 Risk and Loss in Unsupervised Learning . . . . . . . . . . . . . . . . . . 122
4.3 Expectation‚ÄìMaximization (EM) Algorithm . . . . . . . . . . . . . . . . 128
4.4 Empirical Distribution and Density Estimation . . . . . . . . . . . . . . . 131
4.5 Clustering via Mixture Models . . . . . . . . . . . . . . . . . . . . . . . 135
4.5.1 Mixture Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
4.5.2 EM Algorithm for Mixture Models . . . . . . . . . . . . . . . . . 137
4.6 Clustering via Vector Quantization . . . . . . . . . . . . . . . . . . . . . 142
4.6.1 K-Means . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
4.6.2 Clustering via Continuous Multiextremal Optimization . . . . . . 146
4.7 Hierarchical Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
4.8 Principal Component Analysis (PCA) . . . . . . . . . . . . . . . . . . . 153
4.8.1 Motivation: Principal Axes of an Ellipsoid . . . . . . . . . . . . . 153
4.8.2 PCA and Singular Value Decomposition (SVD) . . . . . . . . . . 155
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
5 Regression 167
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
5.2 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
5.3 Analysis via Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . 171
5.3.1 Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . 171
5.3.2 Model Selection and Prediction . . . . . . . . . . . . . . . . . . . 172
5.3.3 Cross-Validation and Predictive Residual Sum of Squares . . . . . 173
5.3.4 In-Sample Risk and Akaike Information Criterion . . . . . . . . . 175
5.3.5 Categorical Features . . . . . . . . . . . . . . . . . . . . . . . . 177
5.3.6 Nested Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
5.3.7 Coe cient of Determination . . . . . . . . . . . . . . . . . . . . 181
5.4 Inference for Normal Linear Models . . . . . . . . . . . . . . . . . . . . 182
5.4.1 Comparing Two Normal Linear Models . . . . . . . . . . . . . . 183
5.4.2 ConÔ¨Ådence and Prediction Intervals . . . . . . . . . . . . . . . . 186
5.5 Nonlinear Regression Models . . . . . . . . . . . . . . . . . . . . . . . . 188
5.6 Linear Models in Python . . . . . . . . . . . . . . . . . . . . . . . . . . 191
5.6.1 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
5.6.2 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
5.6.3 Analysis of Variance (ANOV A) . . . . . . . . . . . . . . . . . . 195Contents ix
5.6.4 ConÔ¨Ådence and Prediction Intervals . . . . . . . . . . . . . . . . 198
5.6.5 Model Validation . . . . . . . . . . . . . . . . . . . . . . . . . . 198
5.6.6 Variable Selection . . . . . . . . . . . . . . . . . . . . . . . . . . 199
5.7 Generalized Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . 204
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
6 Regularization and Kernel Methods 215
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
6.2 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
6.3 Reproducing Kernel Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . 222
6.4 Construction of Reproducing Kernels . . . . . . . . . . . . . . . . . . . . 224
6.4.1 Reproducing Kernels via Feature Mapping . . . . . . . . . . . . . 224
6.4.2 Kernels from Characteristic Functions . . . . . . . . . . . . . . . 225
6.4.3 Reproducing Kernels Using Orthonormal Features . . . . . . . . 227
6.4.4 Kernels from Kernels . . . . . . . . . . . . . . . . . . . . . . . . 229
6.5 Representer Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
6.6 Smoothing Cubic Splines . . . . . . . . . . . . . . . . . . . . . . . . . . 235
6.7 Gaussian Process Regression . . . . . . . . . . . . . . . . . . . . . . . . 238
6.8 Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
7 ClassiÔ¨Åcation 251
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
7.2 ClassiÔ¨Åcation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
7.3 ClassiÔ¨Åcation via Bayes‚Äô Rule . . . . . . . . . . . . . . . . . . . . . . . 257
7.4 Linear and Quadratic Discriminant Analysis . . . . . . . . . . . . . . . . 259
7.5 Logistic Regression and Softmax ClassiÔ¨Åcation . . . . . . . . . . . . . . 266
7.6 K-Nearest Neighbors ClassiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . 268
7.7 Support Vector Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
7.8 ClassiÔ¨Åcation with Scikit-Learn . . . . . . . . . . . . . . . . . . . . . . . 277
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
8 Decision Trees and Ensemble Methods 287
8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
8.2 Top-Down Construction of Decision Trees . . . . . . . . . . . . . . . . . 289
8.2.1 Regional Prediction Functions . . . . . . . . . . . . . . . . . . . 290
8.2.2 Splitting Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
8.2.3 Termination Criterion . . . . . . . . . . . . . . . . . . . . . . . . 292
8.2.4 Basic Implementation . . . . . . . . . . . . . . . . . . . . . . . . 294
8.3 Additional Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . 298
8.3.1 Binary Versus Non-Binary Trees . . . . . . . . . . . . . . . . . . 298
8.3.2 Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . 298
8.3.3 Alternative Splitting Rules . . . . . . . . . . . . . . . . . . . . . 298
8.3.4 Categorical Variables . . . . . . . . . . . . . . . . . . . . . . . . 299
8.3.5 Missing Values . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
8.4 Controlling the Tree Shape . . . . . . . . . . . . . . . . . . . . . . . . . 300
8.4.1 Cost-Complexity Pruning . . . . . . . . . . . . . . . . . . . . . . 303x Contents
8.4.2 Advantages and Limitations of Decision Trees . . . . . . . . . . . 304
8.5 Bootstrap Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
8.6 Random Forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
8.7 Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
9 Deep Learning 323
9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
9.2 Feed-Forward Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 326
9.3 Back-Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
9.4 Methods for Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
9.4.1 Steepest Descent . . . . . . . . . . . . . . . . . . . . . . . . . . 334
9.4.2 Levenberg‚ÄìMarquardt Method . . . . . . . . . . . . . . . . . . . 335
9.4.3 Limited-Memory BFGS Method . . . . . . . . . . . . . . . . . . 336
9.4.4 Adaptive Gradient Methods . . . . . . . . . . . . . . . . . . . . . 338
9.5 Examples in Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
9.5.1 Simple Polynomial Regression . . . . . . . . . . . . . . . . . . . 340
9.5.2 Image ClassiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . 344
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
A Linear Algebra and Functional Analysis 355
A.1 Vector Spaces, Bases, and Matrices . . . . . . . . . . . . . . . . . . . . . 355
A.2 Inner Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
A.3 Complex Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . 361
A.4 Orthogonal Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
A.5 Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . 363
A.5.1 Left- and Right-Eigenvectors . . . . . . . . . . . . . . . . . . . . 364
A.6 Matrix Decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
A.6.1 (P)LU Decomposition . . . . . . . . . . . . . . . . . . . . . . . 368
A.6.2 Woodbury Identity . . . . . . . . . . . . . . . . . . . . . . . . . 370
A.6.3 Cholesky Decomposition . . . . . . . . . . . . . . . . . . . . . . 373
A.6.4 QR Decomposition and the Gram‚ÄìSchmidt Procedure . . . . . . . 375
A.6.5 Singular Value Decomposition . . . . . . . . . . . . . . . . . . . 376
A.6.6 Solving Structured Matrix Equations . . . . . . . . . . . . . . . . 379
A.7 Functional Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
A.8 Fourier Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
A.8.1 Discrete Fourier Transform . . . . . . . . . . . . . . . . . . . . . 392
A.8.2 Fast Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . 394
B Multivariate Di erentiation and Optimization 397
B.1 Multivariate Di erentiation . . . . . . . . . . . . . . . . . . . . . . . . . 397
B.1.1 Taylor Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . 400
B.1.2 Chain Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
B.2 Optimization Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
B.2.1 Convexity and Optimization . . . . . . . . . . . . . . . . . . . . 403
B.2.2 Lagrangian Method . . . . . . . . . . . . . . . . . . . . . . . . . 406
B.2.3 Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407Contents xi
B.3 Numerical Root-Finding and Minimization . . . . . . . . . . . . . . . . . 408
B.3.1 Newton-Like Methods . . . . . . . . . . . . . . . . . . . . . . . 409
B.3.2 Quasi-Newton Methods . . . . . . . . . . . . . . . . . . . . . . . 411
B.3.3 Normal Approximation Method . . . . . . . . . . . . . . . . . . 413
B.3.4 Nonlinear Least Squares . . . . . . . . . . . . . . . . . . . . . . 414
B.4 Constrained Minimization via Penalty Functions . . . . . . . . . . . . . . 415
C Probability and Statistics 421
C.1 Random Experiments and Probability Spaces . . . . . . . . . . . . . . . 421
C.2 Random Variables and Probability Distributions . . . . . . . . . . . . . . 422
C.3 Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
C.4 Joint Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
C.5 Conditioning and Independence . . . . . . . . . . . . . . . . . . . . . . . 428
C.5.1 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . 428
C.5.2 Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
C.5.3 Expectation and Covariance . . . . . . . . . . . . . . . . . . . . 429
C.5.4 Conditional Density and Conditional Expectation . . . . . . . . . 431
C.6 Functions of Random Variables . . . . . . . . . . . . . . . . . . . . . . . 431
C.7 Multivariate Normal Distribution . . . . . . . . . . . . . . . . . . . . . . 434
C.8 Convergence of Random Variables . . . . . . . . . . . . . . . . . . . . . 439
C.9 Law of Large Numbers and Central Limit Theorem . . . . . . . . . . . . 445
C.10 Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
C.11 Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
C.12 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
C.12.1 Method of Moments . . . . . . . . . . . . . . . . . . . . . . . . 455
C.12.2 Maximum Likelihood Method . . . . . . . . . . . . . . . . . . . 456
C.13 ConÔ¨Ådence Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
C.14 Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
D Python Primer 463
D.1 Getting Started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
D.2 Python Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
D.3 Types and Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
D.4 Functions and Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
D.5 Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
D.6 Flow Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471
D.7 Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472
D.8 Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473
D.9 Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475
D.10 NumPy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
D.10.1 Creating and Shaping Arrays . . . . . . . . . . . . . . . . . . . . 478
D.10.2 Slicing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480
D.10.3 Array Operations . . . . . . . . . . . . . . . . . . . . . . . . . . 480
D.10.4 Random Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 482
D.11 Matplotlib . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483
D.11.1 Creating a Basic Plot . . . . . . . . . . . . . . . . . . . . . . . . 483xii Contents
D.12 Pandas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
D.12.1 Series and DataFrame . . . . . . . . . . . . . . . . . . . . . . . . 485
D.12.2 Manipulating Data Frames . . . . . . . . . . . . . . . . . . . . . 487
D.12.3 Extracting Information . . . . . . . . . . . . . . . . . . . . . . . 488
D.12.4 Plotting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490
D.13 Scikit-learn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490
D.13.1 Partitioning the Data . . . . . . . . . . . . . . . . . . . . . . . . 491
D.13.2 Standardization . . . . . . . . . . . . . . . . . . . . . . . . . . . 491
D.13.3 Fitting and Prediction . . . . . . . . . . . . . . . . . . . . . . . . 492
D.13.4 Testing the Model . . . . . . . . . . . . . . . . . . . . . . . . . . 492
D.14 System Calls, URL Access, and Speed-Up . . . . . . . . . . . . . . . . . 493
Bibliography 495
Index 503PREFACE
In our present world of automation, cloud computing, algorithms, artiÔ¨Åcial intelligence,
and big data, few topics are as relevant as data science andmachine learning . Their recent
popularity lies not only in their applicability to real-life questions, but also in their natural
blending of many di erent disciplines, including mathematics, statistics, computer science,
engineering, science, and Ô¨Ånance.
To someone starting to learn these topics, the multitude of computational techniques
and mathematical ideas may seem overwhelming. Some may be satisÔ¨Åed with only learn-
ing how to use o -the-shelf recipes to apply to practical situations. But what if the assump-
tions of the black-box recipe are violated? Can we still trust the results? How should the
algorithm be adapted? To be able to truly understand data science and machine learning it
is important to appreciate the underlying mathematics and statistics, as well as the resulting
algorithms.
The purpose of this book is to provide an accessible, yet comprehensive, account of
data science and machine learning. It is intended for anyone interested in gaining a better
understanding of the mathematics and statistics that underpin the rich variety of ideas and
machine learning algorithms in data science. Our viewpoint is that computer languages
come and go, but the underlying key ideas and algorithms will remain forever and will
form the basis for future developments.
Before we turn to a description of the topics in this book, we would like to say a
few words about its philosophy. This book resulted from various courses in data science
and machine learning at the Universities of Queensland and New South Wales, Australia.
When we taught these courses, we noticed that students were eager to learn not only how
to apply algorithms but also to understand how these algorithms actually work. However,
many existing textbooks assumed either too much background knowledge (e.g., measure
theory and functional analysis) or too little (everything is a black box), and the information
overload from often disjointed and contradictory internet sources made it more di cult for
students to gradually build up their knowledge and understanding. We therefore wanted to
write a book about data science and machine learning that can be read as a linear story,
with a substantial ‚Äúbackstory‚Äù in the appendices. The main narrative starts very simply and
builds up gradually to quite an advanced level. The backstory contains all the necessary
xiiixiv Preface
background, as well as additional information, from linear algebra and functional analysis
(Appendix A), multivariate di erentiation and optimization (Appendix B), and probability
and statistics (Appendix C). Moreover, to make the abstract ideas come alive, we believe
it is important that the reader sees actual implementations of the algorithms, directly trans-
lated from the theory. After some deliberation we have chosen Python as our programming
language. It is freely available and has been adopted as the programming language of
choice for many practitioners in data science and machine learning. It has many useful
packages for data manipulation (often ported from R) and has been designed to be easy to
program. A gentle introduction to Python is given in Appendix D.
To keep the book manageable in size we had to be selective in our choice of topics.
Important ideas and connections between various concepts are highlighted via keywords
keywords and page references (indicated by a +) in the margin. Key deÔ¨Ånitions and theorems are
highlighted in boxes. Whenever feasible we provide proofs of theorems. Finally, we place
great importance on notation . It is often the case that once a consistent and concise system
of notation is in place, seemingly di cult ideas suddenly become obvious. We use di er-
ent fonts to distinguish between di erent types of objects. Vectors are denoted by letters in
boldface italics, x;X, and matrices by uppercase letters in boldface roman font, A;K. We
also distinguish between random vectors and their values by using upper and lower case
letters, e.g., X(random vector) and x(its value or outcome). Sets are usually denoted by
calligraphic letters G;H. The symbols for probability and expectation are PandE, respect-
ively. Distributions are indicated by sans serif font, as in BinandGamma ; exceptions are
the ubiquitous notations NandUfor the normal and uniform distributions. A summary of
the most important symbols and abbreviations is given on Pages xvii‚Äìxxi. +xvii
Data science provides the language and techniques necessary for understanding and
dealing with data. It involves the design, collection, analysis, and interpretation of nu-
merical data, with the aim of extracting patterns and other useful information. Machine
learning, which is closely related to data science, deals with the design of algorithms and
computer resources to learn from data. The organization of the book follows roughly the
typical steps in a data science project: Gathering data to gain information about a research
question; cleaning, summarization, and visualization of the data; modeling and analysis of
the data; translating decisions about the model into decisions and predictions about the re-
search question. As this is a mathematics and statistics oriented book, most emphasis will
be on modeling and analysis.
We start in Chapter 1 with the reading, structuring, summarization, and visualization
of data using the data manipulation package pandas in Python. Although the material
covered in this chapter requires no mathematical knowledge, it forms an obvious starting
point for data science: to better understand the nature of the available data. In Chapter 2, we
introduce the main ingredients of statistical learning . We distinguish between supervised
andunsupervised learning techniques, and discuss how we can assess the predictive per-
formance of (un)supervised learning methods. An important part of statistical learning is
themodeling of data. We introduce various useful models in data science including linear,
multivariate Gaussian, and Bayesian models. Many algorithms in machine learning and
data science make use of Monte Carlo techniques, which is the topic of Chapter 3. Monte
Carlo can be used for simulation, estimation, and optimization. Chapter 4 is concerned
with unsupervised learning, where we discuss techniques such as density estimation, clus-
tering, and principal component analysis. We then turn our attention to supervised learningPreface xv
in Chapter 5, and explain the ideas behind a broad class of regression models. Therein, we
also describe how Python‚Äôs statsmodels package can be used to deÔ¨Åne and analyze linear
models. Chapter 6 builds upon the previous regression chapter by developing the power-
ful concepts of kernel methods and regularization, which allow the fundamental ideas of
Chapter 5 to be expanded in an elegant way, using the theory of reproducing kernel Hilbert
spaces. In Chapter 7, we proceed with the classiÔ¨Åcation task, which also belongs to the
supervised learning framework, and consider various methods for classiÔ¨Åcation, including
Bayes classiÔ¨Åcation, linear and quadratic discriminant analysis, K-nearest neighbors, and
support vector machines. In Chapter 8 we consider versatile methods for regression and
classiÔ¨Åcation that make use of tree structures. Finally, in Chapter 9, we consider the work-
ings of neural networks and deep learning, and show that these learning algorithms have a
simple mathematical interpretation. An extensive range of exercises is provided at the end
of each chapter.
Python code and data sets for each chapter can be downloaded from the GitHub site:
https://github.com/DSML-book
Acknowledgments
Some of the Python code for Chapters 1 and 5 was adapted from [73]. We thank Benoit
Liquet for making this available, and Lauren Jones for translating the R code into Python.
We thank all who through their comments, feedback, and suggestions have contributed
to this book, including Qibin Duan, Luke Taylor, R√©mi Mouzayek, Harry Goodman, Bryce
StansÔ¨Åeld, Ryan Tongs, Dillon Steyl, Bill Rudd, Nan Ye, Christian Hirsch, Chris van der
Heide, Sarat Moka, Aapeli Vuorinen, Joshua Ross, Giang Nguyen, and the anonymous
referees. David Grubbs deserves a special accollade for his professionalism and attention
to detail in his role as Editor for this book.
The book was test-run during the 2019 Summer School of the Australian Mathemat-
ical Sciences Institute . More than 80 bright upper-undergraduate (Honours) students used
the book for the course Mathematical Methods for Machine Learning , taught by Zdravko
Botev. We are grateful for the valuable feedback that they provided.
Our special thanks go out to Robert Salomone, Liam Berry, Robin Carrick, and Sam
Daley, who commented in great detail on earlier versions of the entire book and wrote and
improved our Python code. Their enthusiasm, perceptiveness, and kind assistance have
been invaluable.
Of course, none of this work would have been possible without the loving support,
patience, and encouragement from our families, and we thank them with all our hearts.
This book was Ô¨Ånancially supported by the Australian Research Council Centre of
Excellence for Mathematical &Statistical Frontiers , under grant number CE140100049.
Dirk Kroese, Zdravko Botev,
Thomas Taimre, and Radislav Vaisman
Brisbane and SydneyxviNOTATION
We could, of course, use any notation we want; do not laugh at notations;
invent them, they are powerful. In fact, mathematics is, to a large extent, in-
vention of better notations.
Richard P. Feynman
We have tried to use a notation system that is, in order of importance, simple, descript-
ive, consistent, and compatible with historical choices. Achieving all of these goals all of
the time would be impossible, but we hope that our notation helps to quickly recognize
the type or ‚ÄúÔ¨Çavor‚Äù of certain mathematical objects (vectors, matrices, random vectors,
probability measures, etc.) and clarify intricate ideas.
We make use of various typographical aids, and it will be beneÔ¨Åcial for the reader to
be aware of some of these.
¬àBoldface font is used to indicate composite objects, such as column vectors x=
[x1;:::; xn]>and matrices X=[xi j]. Note also the di erence between the upright bold
font for matrices and the slanted bold font for vectors.
¬àRandom variables are generally speciÔ¨Åed with upper case roman letters X;Y;Zand their
outcomes with lower case letters x;y;z. Random vectors are thus denoted in upper case
slanted bold font: X=[X1;:::; Xn]>.
¬àSets of vectors are generally written in calligraphic font, such as X, but the set of real
numbers uses the common blackboard bold font R. Expectation and probability also use
the latter font.
¬àProbability distributions use a sans serif font, such as BinandGamma . Exceptions to
this rule are the ‚Äústandard‚Äù notations NandUfor the normal and uniform distributions.
¬àWe often omit brackets when it is clear what the argument is of a function or operator.
For example, we prefer EX2toE[X2].
xviixviii Notation
¬àWe employ color to emphasize that certain words refer to a dataset ,function , or
package in Python. All code is written in typewriter font. To be compatible with past
notation choices, we introduced a special blue symbol Xfor the model (design) matrix of
a linear model.
¬àImportant notation such as T,g,gis often deÔ¨Åned in a mnemonic way, such as Tfor
‚Äútraining‚Äù, gfor ‚Äúguess‚Äù, gfor the ‚Äústar‚Äù (that is, optimal) guess, and `for ‚Äúloss‚Äù.
¬àWe will occasionally use a Bayesian notation convention in which the same symbol is
used to denote di erent (conditional) probability densities. In particular, instead of writing
fX(x) and fXjY(xjy) for the probability density function (pdf) of Xand the conditional pdf
ofXgiven Y, we simply write f(x) and f(xjy). This particular style of notation can be of
great descriptive value, despite its apparent ambiguity.
General font/notation rules
x scalar
x vector
X random vector
X matrix
X set
bx estimate or approximation
xoptimal
x average
Common mathematical symbols
8 for all
9 there exists
/ is proportional to
? is perpendicular to
 is distributed as
iid,iid are independent and identically distributed as
approx: is approximately distributed as
rf gradient of f
r2f Hessian of f
f2Cpfhas continuous derivatives of order p
 is approximately
' is asymptotically
 is much smaller than
 direct sumNotation xix
 elementwise product
\ intersection
[ union
:=,=: is deÔ¨Åned as
a:s: ! converges almost surely to
d ! converges in distribution to
P ! converges in probability to
Lp ! converges in Lp-norm to
kk Euclidean norm
dxe smallest integer larger than x
bxc largest integer smaller than x
x+ maxfx;0g
Matrix/vector notation
A>,x>transpose of matrix Aor vector x
A 1inverse of matrix A
A+pseudo-inverse of matrix A
A >inverse of matrix A>or transpose of A 1
A0 matrix Ais positive deÔ¨Ånite
A0 matrix Ais positive semideÔ¨Ånite
dim(x) dimension of vector x
det(A) determinant of matrix A
jAj absolute value of the determinant of matrix A
tr(A) trace of matrix A
Reserved letters and words
C set of complex numbers
d di erential symbol
E expectation
e the number 2 :71828:::
f probability density (discrete or continuous)
g prediction function
1fAgor1Aindicator function of set A
i the square root of  1
` risk: expected lossxx Notation
Loss loss function
ln (natural) logarithm
N set of natural numbers f0;1;:::g
O big-O order symbol: f(x)=O(g(x)) ifjf(x)j6g(x) for some constant as
x!a
o little-o order symbol: f(x)=o(g(x)) if f(x)=g(x)!0 as x!a
P probability measure
 the number 3 :14159:::
R set of real numbers (one-dimensional Euclidean space)
Rnn-dimensional Euclidean space
R+ positive real line: [0 ;1)
 deterministic training set
T random training set
X model (design) matrix
Z set of integersf:::; 1;0;1;:::g
Probability distributions
Ber Bernoulli
Beta beta
Bin binomial
Exp exponential
Geom geometric
Gamma gamma
F Fisher‚ÄìSnedecor F
N normal or Gaussian
Pareto Pareto
Poi Poisson
t Student‚Äôs t
U uniform
Abbreviations and acronyms
cdf cumulative distribution function
CMC crude Monte Carlo
CE cross-entropy
EM expectation‚Äìmaximization
GP Gaussian process
KDE Kernel density estimate /estimatorNotation xxi
KL Kullback‚ÄìLeibler
KKT Karush‚ÄìKuhn‚ÄìTucker
iid independent and identically distributed
MAP maximum a posteriori
MCMC Markov chain Monte Carlo
MLE maximum likelihood estimator /estimate
OOB out-of-bag
PCA principal component analysis
pdf probability density function (discrete or continuous)
SVD singular value decompositionxxiiCHAPTER1
IMPORTING , SUMMARIZING ,AND
VISUALIZING DATA
This chapter describes where to Ô¨Ånd useful data sets, how to load them into Python,
and how to (re)structure the data. We also discuss various ways in which the data can
be summarized via tables and Ô¨Ågures. Which type of plots and numerical summaries
are appropriate depends on the type of the variable(s) in play. Readers unfamiliar with
Python are advised to read Appendix D Ô¨Årst.
1.1 Introduction
Data comes in many shapes and forms, but can generally be thought of as being the result
of some random experiment ‚Äî an experiment whose outcome cannot be determined in
advance, but whose workings are still subject to analysis. Data from a random experiment
are often stored in a table or spreadsheet. A statistical convention is to denote variables ‚Äî
often called features features ‚Äî as columns and the individual items (or units) as rows . It is useful
to think of three types of columns in such a spreadsheet:
1. The Ô¨Årst column is usually an identiÔ¨Åer or index column, where each unit /row is
given a unique name or ID.
2. Certain columns (features) can correspond to the design of the experiment, specify-
ing, for example, to which experimental group the unit belongs. Often the entries in
these columns are deterministic ; that is, they stay the same if the experiment were to
be repeated.
3. Other columns represent the observed measurements of the experiment. Usually,
these measurements exhibit variability ; that is, they would change if the experiment
were to be repeated.
There are many data sets available from the Internet and in software packages. A well-
known repository of data sets is the Machine Learning Repository maintained by the Uni-
versity of California at Irvine (UCI), found at https://archive.ics.uci.edu/ .
12 1.1. Introduction
These data sets are typically stored in a CSV (comma separated values) format, which
can be easily read into Python. For example, to access the abalone data set from this web-
site with Python, download the Ô¨Åle to your working directory, import the pandas package
via
import pandas as pd
and read in the data as follows:
abalone = pd.read_csv( 'abalone.data ',header = None)
It is important to add header = None , as this lets Python know that the Ô¨Årst line of the
CSV does not contain the names of the features, as it assumes so by default. The data set
was originally used to predict the age of abalone from physical measurements, such as
shell weight and diameter.
Another useful repository of over 1000 data sets from various packages in the R pro-
gramming language, collected by Vincent Arel-Bundock, can be found at:
https://vincentarelbundock.github.io/Rdatasets/datasets.html .
For example, to read Fisher‚Äôs famous iris data set from R‚Äôs datasets package into Py-
thon, type:
urlprefix = 'https://vincentarelbundock.github.io/Rdatasets/csv/ '
dataname = 'datasets/iris.csv '
iris = pd.read_csv(urlprefix + dataname)
Theiris data set contains four physical measurements (sepal /petal length /width) on
50 specimens (each) of 3 species of iris: setosa, versicolor, and virginica. Note that in this
case the headers are included. The output of read_csv is aDataFrame object, which is
pandas ‚Äôs implementation of a spreadsheet; see Section D.12.1. The DataFrame method +485
head gives the Ô¨Årst few rows of the DataFrame , including the feature names. The number
of rows can be passed as an argument and is 5 by default. For the iris DataFrame , we
have:
iris.head()
Unnamed: 0 Sepal.Length ... Petal.Width Species
0 1 5.1 ... 0.2 setosa
1 2 4.9 ... 0.2 setosa
2 3 4.7 ... 0.2 setosa
3 4 4.6 ... 0.2 setosa
4 5 5.0 ... 0.2 setosa
[5 rows x 6 columns]
The names of the features can be obtained via the columns attribute of the DataFrame
object, as in iris.columns . Note that the Ô¨Årst column is a duplicate index column, whose
name (assigned by pandas ) is'Unnamed: 0 '. We can drop this column and reassign the
iris object as follows:
iris = iris.drop( 'Unnamed: 0 ',1)Chapter 1. Importing, Summarizing, and Visualizing Data 3
The data for each feature (corresponding to its speciÔ¨Åc name) can be accessed by using
Python‚Äôs slicing notation []. For example, the object iris['Sepal.Length'] contains
the 150 sepal lengths.
The Ô¨Årst three rows of the abalone data set from the UCI repository can be found as
follows:
abalone.head(3)
0 1 2 3 4 5 6 7 8
0 M 0.455 0.365 0.095 0.5140 0.2245 0.1010 0.150 15
1 M 0.350 0.265 0.090 0.2255 0.0995 0.0485 0.070 7
2 F 0.530 0.420 0.135 0.6770 0.2565 0.1415 0.210 9
Here, the missing headers have been assigned according to the order of the natural
numbers. The names should correspond to Sex, Length, Diameter, Height, Whole weight,
Shucked weight, Viscera weight, Shell weight, and Rings, as described in the Ô¨Åle with the
name abalone.names on the UCI website. We can manually add the names of the features
to the DataFrame by reassigning the columns attribute, as in:
abalone.columns = [ 'Sex','Length ','Diameter ','Height ',
'Whole weight ','Shucked weight ','Viscera weight ','Shell weight ',
'Rings ']
1.2 Structuring Features According to Type
We can generally classify features as either quantitative or qualitative. Quantitative Quantitative features
possess ‚Äúnumerical quantity‚Äù, such as height, age, number of births, etc., and can either be
continuous ordiscrete . Continuous quantitative features take values in a continuous range
of possible values, such as height, voltage, or crop yield; such features capture the idea
that measurements can always be made more precisely. Discrete quantitative features have
a countable number of possibilities, such as a count.
In contrast, qualitative qualitative features do not have a numerical meaning, but their possible
values can be divided into a Ô¨Åxed number of categories, such as fM,Fgfor gender orfblue,
black, brown, green gfor eye color. For this reason such features are also called categorical categorical .
A simple rule of thumb is: if it does not make sense to average the data, it is categorical.
For example, it does not make sense to average eye colors. Of course it is still possible to
represent categorical data with numbers, such as 1 =blue, 2 =black, 3 =brown, but such
numbers carry no quantitative meaning. Categorical features are often called factors factors .
When manipulating, summarizing, and displaying data, it is important to correctly spe-
cify the type of the variables (features). We illustrate this using the nutrition_elderly
data set from [73], which contains the results of a study involving nutritional measure-
ments of thirteen features (columns) for 226 elderly individuals (rows). The data set can be
obtained from:
http://www.biostatisticien.eu/springeR/nutrition_elderly.xls .
Excel Ô¨Åles can be read directly into pandas via the read_excel method:4 1.2. Structuring Features According to Type
xls = 'http://www.biostatisticien.eu/springeR/nutrition_elderly.xls '
nutri = pd.read_excel(xls)
This creates a DataFrame object nutri . The Ô¨Årst three rows are as follows:
pd.set_option( 'display.max_columns ', 8) # to fit display
nutri.head(3)
gender situation tea ... cooked_fruit_veg chocol fat
0 2 1 0 ... 4 5 6
1 2 1 1 ... 5 1 4
2 2 1 0 ... 2 5 4
[3 rows x 13 columns]
You can check the type (or structure) of the variables via the info method of nutri .
nutri.info()
<class 'pandas.core.frame.DataFrame '>
RangeIndex: 226 entries , 0 to 225
Data columns (total 13 columns):
gender 226 non-null int64
situation 226 non-null int64
tea 226 non-null int64
coffee 226 non-null int64
height 226 non-null int64
weight 226 non-null int64
age 226 non-null int64
meat 226 non-null int64
fish 226 non-null int64
raw_fruit 226 non-null int64
cooked_fruit_veg 226 non-null int64
chocol 226 non-null int64
fat 226 non-null int64
dtypes: int64(13)
memory usage: 23.0 KB
All 13 features in nutri are (at the moment) interpreted by Python as quantitative
variables, indeed as integers, simply because they have been entered as whole numbers.
The meaning of these numbers becomes clear when we consider the description of the
features, given in Table 1.2. Table 1.1 shows how the variable types should be classiÔ¨Åed.
Table 1.1: The feature types for the data frame nutri .
Qualitative gender ,situation ,fat
meat ,fish ,raw_fruit ,cooked_fruit_veg ,chocol
Discrete quantitative tea,coffee
Continuous quantitative height ,weight ,age
Note that the categories of the qualitative features in the second row of Table 1.1, meat ,
. . . ,chocol have a natural order. Such qualitative features are sometimes called ordinal , inChapter 1. Importing, Summarizing, and Visualizing Data 5
Table 1.2: Description of the variables in the nutritional study [73].
Feature Description Unit orCoding
gender Gender 1 =Male; 2 =Female
situation Family status1=Single
2=Living with spouse
3=Living with family
4=Living with someone else
tea Daily consumption of tea Number of cups
coffee Daily consumption of co ee Number of cups
height Height cm
weight Weight (actually: mass) kg
age Age at date of interview Years
meat Consumption of meat0=Never
1=Less than once a week
2=Once a week
3=2‚Äì3 times a week
4=4‚Äì6 times a week
5=Every day
fish Consumption of Ô¨Åsh As in meat
raw_fruit Consumption of raw fruits As in meat
cooked_fruit_vegConsumption of cookedAs in meatfruits and vegetables
chocol Consumption of chocolate As in meat
fat1=Butter
2=Margarine
3=Peanut oil
Type of fat used 4 =SunÔ¨Çower oil
for cooking 5 =Olive oil
6=Mix of vegetable oils (e.g., Isio4)
7=Colza oil
8=Duck or goose fat
contrast to qualitative features without order, which are called nominal . We will not make
such a distinction in this book.
We can modify the Python value and type for each categorical feature, using the
replace andastype methods. For categorical features, such as gender , we can replace
the value 1 with 'Male 'and 2 with 'Female ', and change the type to 'category 'as
follows.
DICT = {1: 'Male ', 2: 'Female '}# dictionary specifies replacement
nutri[ 'gender '] = nutri[ 'gender '].replace(DICT).astype( 'category ')
The structure of the other categorical-type features can be changed in a similar way.
Continuous features such as height should have type float :
nutri[ 'height '] = nutri[ 'height '].astype( float )6 1.3. Summary Tables
We can repeat this for the other variables (see Exercise 2) and save this modiÔ¨Åed data
frame as a CSV Ô¨Åle, by using the pandas method to_csv .
nutri.to_csv( 'nutri.csv ',index=False)
1.3 Summary Tables
It is often useful to summarize a large spreadsheet of data in a more condensed form. A
table of counts or a table of frequencies makes it easier to gain insight into the underlying
distribution of a variable, especially if the data are qualitative. Such tables can be obtained
with the methods describe andvalue_counts .
As a Ô¨Årst example, we load the nutri DataFrame , which we restructured and saved
(see previous section) as 'nutri.csv ', and then construct a summary for the feature
(column) 'fat'.
nutri = pd.read_csv( 'nutri.csv ')
nutri[ 'fat'].describe()
count 226
unique 8
top sunflower
freq 68
Name: fat, dtype: object
We see that there are 8 di erent types of fat used and that sunÔ¨Çower has the highest
count, with 68 out of 226 individuals using this type of cooking fat. The method
value_counts gives the counts for the di erent fat types.
nutri[ 'fat'].value_counts()
sunflower 68
peanut 48
olive 40
margarine 27
Isio4 23
butter 15
duck 4
colza 1
Name: fat, dtype: int64
Column labels are also attributes of a DataFrame , and nutri.fat , for example, is
exactly the same object as nutri[ 'fat'].Chapter 1. Importing, Summarizing, and Visualizing Data 7
It is also possible to use crosstab tocross tabulate between two or more variables,cross tabulategiving a contingency table :
pd.crosstab(nutri.gender , nutri.situation)
situation Couple Family Single
gender
Female 56 7 78
Male 63 2 20
We see, for example, that the proportion of single men is substantially smaller than the
proportion of single women in the data set of elderly people. To add row and column totals
to a table, use margins=True .
pd.crosstab(nutri.gender , nutri.situation , margins=True)
situation Couple Family Single All
gender
Female 56 7 78 141
Male 63 2 20 85
All 119 9 98 226
1.4 Summary Statistics
In the following, x=[x1;:::; xn]>is a column vector of nnumbers. For our nutri data,
the vector xcould, for example, correspond to the heights of the n=226 individuals.
Thesample mean sample mean ofx, denoted by x, is simply the average of the data values:
x=1
nnX
i=1xi:
Using the mean method in Python for the nutri data, we have, for instance:
nutri[ 'height '].mean()
163.96017699115043
Thep-sample quantile sample quantile (0<p<1) of xis a value xsuch that at least a fraction pof the
data is less than or equal to xand at least a fraction 1  pof the data is greater than or equal
tox. The sample median sample median is the sample 0 :5-quantile. The p-sample quantile is also called
the 100p percentile . The 25, 50, and 75 sample percentiles are called the Ô¨Årst, second,
and third quartiles quartiles of the data. For the nutri data they are obtained as follows.
nutri[ 'height '].quantile(q=[0.25,0.5,0.75])
0.25 157.0
0.50 163.0
0.75 170.08 1.5. Visualizing Data
The sample mean and median give information about the location of the data, while the
distance between sample quantiles (say the 0.1 and 0.9 quantiles) gives some indication of
thedispersion (spread) of the data. Other measures for dispersion are the sample range ,sample rangemax ixi min ixi, the sample variancesample variance
s2=1
n 1nX
i=1(xi x)2; (1.1)
and the sample standard deviation s =p
s2. For the nutri data, the range (in cm) is:sample
standard
deviation
+455nutri[ 'height '].max() - nutri[ 'height '].min()
48.0
The variance (in cm2) is:
round (nutri[ 'height '].var(), 2) # round to two decimal places
81.06
And the standard deviation can be found via:
round (nutri[ 'height '].std(), 2)
9.0
We already encountered the describe method in the previous section for summarizing
qualitative features, via the most frequent count and the number of unique elements. When
applied to a quantitative feature, it returns instead the minimum, maximum, mean, and the
three quartiles. For example, the 'height 'feature in the nutri data has the following
summary statistics.
nutri[ 'height '].describe()
count 226.000000
mean 163.960177
std 9.003368
min 140.000000
25\% 157.000000
50\% 163.000000
75\% 170.000000
max 188.000000
Name: height , dtype: float64
1.5 Visualizing Data
In this section we describe various methods for visualizing data. The main point we would
like to make is that the way in which variables are visualized should always be adapted to
the variable types; for example, qualitative data should be plotted di erently from quantit-
ative data.Chapter 1. Importing, Summarizing, and Visualizing Data 9
For the rest of this section, it is assumed that matplotlib.pyplot ,pandas , and
numpy , have been imported in the Python code as follows.
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
1.5.1 Plotting Qualitative Variables
Suppose we wish to display graphically how many elderly people are living by themselves,
as a couple, with family, or other. Recall that the data are given in the situation column
of our nutri data. Assuming that we already restructured the data , as in Section 1.2, we + 3
can make a barplot of the number of people in each category via the plt.bar function ofbarplotthe standard matplotlib plotting library. The inputs are the x-axis positions, heights, and
widths of each bar respectively.
width = 0.35 # the width of the bars
x = [0, 0.8, 1.6] # the bar positions on x-axis
situation_counts=nutri[ 'situation '].value_counts()
plt.bar(x, situation_counts , width , edgecolor = 'black ')
plt.xticks(x, situation_counts.index)
plt.show()
Couple Single Family0255075100125
Figure 1.1: Barplot for the qualitative variable 'situation '.
1.5.2 Plotting Quantitative Variables
We now present a few useful methods for visualizing quantitative data, again using the
nutri data set. We will Ô¨Årst focus on continuous features (e.g., 'age') and then add some
speciÔ¨Åc graphs related to discrete features (e.g., 'tea'). The aim is to describe the variab-
ility present in a single feature. This typically involves a central tendency, where observa-
tions tend to gather around, with fewer observations further away. The main aspects of the
distribution are the location (or center) of the variability, the spread of the variability (how
far the values extend from the center), and the shape of the variability; e.g., whether or not
values are spread symmetrically on either side of the center.10 1.5. Visualizing Data
1.5.2.1 Boxplot
Aboxplot can be viewed as a graphical representation of the Ô¨Åve-number summary ofboxplotthe data consisting of the minimum, maximum, and the Ô¨Årst, second, and third quartiles.
Figure 1.2 gives a boxplot for the 'age'feature of the nutri data.
plt.boxplot(nutri[ 'age'],widths=width ,vert=False)
plt.xlabel( 'age')
plt.show()
Thewidths parameter determines the width of the boxplot, which is by default plotted
vertically. Setting vert=False plots the boxplot horizontally, as in Figure 1.2.
65
 70
 75
 80
 85
 90
age
1
Figure 1.2: Boxplot for 'age'.
The box is drawn from the Ô¨Årst quartile ( Q1) to the third quartile ( Q3). The vertical line
inside the box signiÔ¨Åes the location of the median. So-called ‚Äúwhiskers‚Äù extend to either
side of the box. The size of the box is called the interquartile range : IQR =Q3 Q1. The
left whisker extends to the largest of (a) the minimum of the data and (b) Q1 1:5 IQR.
Similarly, the right whisker extends to the smallest of (a) the maximum of the data and
(b)Q3+1:5 IQR. Any data point outside the whiskers is indicated by a small hollow dot,
indicating a suspicious or deviant point (outlier). Note that a boxplot may also be used for
discrete quantitative features.
1.5.2.2 Histogram
Ahistogram is a common graphical representation of the distribution of a quantitativehistogramfeature. We start by breaking the range of the values into a number of bins orclasses .
We tally the counts of the values falling in each bin and then make the plot by drawing
rectangles whose bases are the bin intervals and whose heights are the counts. In Python
we can use the function plt.hist . For example, Figure 1.3 shows a histogram of the 226
ages in nutri , constructed via the following Python code.
weights = np.ones_like(nutri.age)/nutri.age.count()
plt.hist(nutri.age,bins=9,weights=weights ,facecolor= 'cyan ',
edgecolor= 'black ', linewidth=1)
plt.xlabel( 'age')
plt.ylabel( 'Proportion of Total ')
plt.show()Chapter 1. Importing, Summarizing, and Visualizing Data 11
Here 9 bins were used. Rather than using raw counts (the default), the vertical axis
here gives the percentage in each class, deÔ¨Åned bycount
total. This is achieved by choosing the
‚Äúweights‚Äù parameter to be equal to the vector with entries 1 =266, with length 226. Various
plotting parameters have also been changed.
65
 70
 75
 80
 85
 90
age
0.00
0.05
0.10
0.15
0.20Proportion of Total
Figure 1.3: Histogram of 'age'.
Histograms can also be used for discrete features, although it may be necessary to
explicitly specify the bins and placement of the ticks on the axes.
1.5.2.3 Empirical Cumulative Distribution Function
The empirical cumulative distribution function , denoted by Fn, is a step function whichempirical
cumulative
distribution
functionjumps an amount k=nat observation values, where kis the number of tied observations
at that value. For observations x1;:::; xn,Fn(x) is the fraction of observations less than or
equal to x, i.e.,
Fn(x)=number of xi6x
n=1
nnX
i=11fxi6xg; (1.2)
where 1denotes the indicator indicator function; that is, 1fxi6xgis equal to 1 when xi6xand 0
otherwise. To produce a plot of the empirical cumulative distribution function we can use
theplt.step function. The result for the age data is shown in Figure 1.4. The empirical
cumulative distribution function for a discrete quantitative variable is obtained in the same
way.
x = np.sort(nutri.age)
y = np.linspace(0,1, len(nutri.age))
plt.xlabel( 'age')
plt.ylabel( 'Fn(x) ')
plt.step(x,y)
plt.xlim(x. min(),x. max())
plt.show()12 1.5. Visualizing Data
65
 70
 75
 80
 85
 90
age
0.0
0.2
0.4
0.6
0.8
1.0Fn(x)
Figure 1.4: Plot of the empirical distribution function for the continuous quantitative fea-
ture'age'.
1.5.3 Data Visualization in a Bivariate Setting
In this section, we present a few useful visual aids to explore relationships between two
features. The graphical representation will depend on the type of the two features.
1.5.3.1 Two-way Plots for Two Categorical Variables
Comparing barplots for two categorical variables involves introducing subplots to the Ô¨Åg-
ure. Figure 1.5 visualizes the contingency table of Section 1.3, which cross-tabulates the
family status (situation) with the gender of the elderly people. It simply shows two barplots
next to each other in the same Ô¨Ågure.
Couple
 Family
 Single
0
20
40
60
80Counts
Male
Female
Figure 1.5: Barplot for two categorical variables.Chapter 1. Importing, Summarizing, and Visualizing Data 13
The Ô¨Ågure was made using the seaborn package, which was speciÔ¨Åcally designed to
simplify statistical visualization tasks.
import seaborn as sns
sns.countplot(x= 'situation ', hue = 'gender ', data=nutri ,
hue_order = [ 'Male ','Female '], palette = [ 'SkyBlue ','Pink '],
saturation = 1, edgecolor= 'black ')
plt.legend(loc= 'upper center ')
plt.xlabel( '')
plt.ylabel( 'Counts ')
plt.show()
1.5.3.2 Plots for Two Quantitative Variables
We can visualize patterns between two quantitative features using a scatterplot scatterplot . This can be
done with plt.scatter . The following code produces a scatterplot of 'weight 'against
'height 'for the nutri data.
plt.scatter(nutri.height , nutri.weight , s=12, marker= 'o')
plt.xlabel( 'height ')
plt.ylabel( 'weight ')
plt.show()
140
 150
 160
 170
 180
 190
height
40
50
60
70
80
90weight
Figure 1.6: Scatterplot of 'weight 'against 'height '.
The next Python code illustrates that it is possible to produce highly sophisticated scat-
ter plots, such as in Figure 1.7. The Ô¨Ågure shows the birth weights (mass) of babies whose
mothers smoked (blue triangles) or not (red circles). In addition, straight lines were Ô¨Åtted to
the two groups, suggesting that birth weight decreases with age when the mother smokes,
but increases when the mother does not smoke! The question is whether these trends are
statistically signiÔ¨Åcant or due to chance. We will revisit this data set later on in the book. +19914 1.5. Visualizing Data
urlprefix = 'https://vincentarelbundock.github.io/Rdatasets/csv/ '
dataname = 'MASS/birthwt.csv '
bwt = pd.read_csv(urlprefix + dataname)
bwt = bwt.drop( 'Unnamed: 0 ',1) #drop unnamed column
styles = {0: [ 'o','red'], 1: [ '^','blue ']}
for kinstyles:
grp = bwt[bwt.smoke==k]
m,b = np.polyfit(grp.age, grp.bwt, 1) # fit a straight line
plt.scatter(grp.age, grp.bwt, c=styles[k][1], s=15, linewidth=0,
marker = styles[k][0])
plt.plot(grp.age, m*grp.age + b, '-', color=styles[k][1])
plt.xlabel( 'age')
plt.ylabel( 'birth weight (g) ')
plt.legend([ 'non-smokers ','smokers '],prop={ 'size ':8},
loc=(0.5,0.8))
plt.show()
10
 15
 20
 25
 30
 35
 40
 45
 50
age
0
1000
2000
3000
4000
5000
6000birth weight (g)
non-smokers
smokers
Figure 1.7: Birth weight against age for smoking and non-smoking mothers.
1.5.3.3 Plots for One Qualitative and One Quantitative Variable
In this setting, it is interesting to draw boxplots of the quantitative feature for each level
of the categorical feature. Assuming the variables are structured correctly, the function
plt.boxplot can be used to produce Figure 1.8, using the following code:
males = nutri[nutri.gender == 'Male ']
females = nutri[nutri.gender == 'Female ']
plt.boxplot([males.coffee ,females.coffee],notch=True ,widths
=(0.5,0.5))
plt.xlabel( 'gender ')
plt.ylabel( 'coffee ')
plt.xticks([1,2],[ 'Male ','Female '])
plt.show()Chapter 1. Importing, Summarizing, and Visualizing Data 15
Male
 Female
gender
0
1
2
3
4
5coffee
Figure 1.8: Boxplots of a quantitative feature 'coffee 'as a function of the levels of a
categorical feature 'gender '. Note that we used a di erent, ‚Äúnotched‚Äù, style boxplot this
time.
Further Reading
The focus in this book is on the mathematical and statistical analysis of data, and for the
rest of the book we assume that the data is available in a suitable form for analysis. How-
ever, a large part of practical data science involves the cleaning of data; that is, putting
it into a form that is amenable to analysis with standard software packages. Standard Py-
thon modules such as numpy andpandas can be used to reformat rows, rename columns,
remove faulty outliers, merge rows, and so on. McKinney, the creator of pandas , gives
many practical case studies in [84]. E ective data visualization techniques are beautifully
illustrated in [65].
Exercises
Before you attempt these exercises, make sure you have up-to-date versions of the relevant
Python packages, speciÔ¨Åcally matplotlib ,pandas , and seaborn . An easy way to ensure
this is to update packages via the Anaconda Navigator, as explained in Appendix D.
1. Visit the UCI Repository https://archive.ics.uci.edu/ . Read the description of
the data and download the Mushroom data set agaricus-lepiota.data . Using pandas ,
read the data into a DataFrame called mushroom , via read_csv .
(a) How many features are in this data set?
(b) What are the initial names and types of the features?
(c) Rename the Ô¨Årst feature (index 0) to 'edibility 'and the sixth feature (index 5) to
'odor '[Hint: the column names in pandas are immutable; so individual columns
cannot be modiÔ¨Åed directly. However it is possible to assign the entire column names
list via mushroom.columns = newcols . ]16 Exercises
(d) The 6th column lists the various odors of the mushrooms: encoded as 'a','c', . . . .
Replace these with the names 'almond ','creosote ', etc. (categories correspond-
ing to each letter can be found on the website). Also replace the 'edibility 'cat-
egories 'e'and'p'with 'edible 'and'poisonous '.
(e) Make a contingency table cross-tabulating 'edibility 'and'odor '.
(f) Which mushroom odors should be avoided, when gathering mushrooms for consump-
tion?
(g) What proportion of odorless mushroom samples were safe to eat?
2. Change the type and value of variables in the nutri data set according to Table 1.2 and
save the data as a CSV Ô¨Åle. The modiÔ¨Åed data should have eight categorical features, three
Ô¨Çoats, and two integer features.
3. It frequently happens that a table with data needs to be restructured before the data can
be analyzed using standard statistical software. As an example, consider the test scores in
Table 1.3 of 5 students before and after specialized tuition.
Table 1.3: Student scores.
Student Before After
1 75 85
2 30 50
3 100 100
4 50 52
5 60 65
This is not in the standard format described in Section 1.1. In particular, the student scores
are divided over two columns, whereas the standard format requires that they are collected
in one column, e.g., labelled 'Score '. Reformat (by hand) the table in standard format,
using three features:
¬à'Score ', taking continuous values,
¬à'Time ', taking values 'Before 'and'After ',
¬à'Student ', taking values from 1 to 5.
Useful methods for reshaping tables in pandas aremelt ,stack , and unstack .
4. Create a similar barplot as in Figure 1.5, but now plot the corresponding proportions of
males and females in each of the three situation categories. That is, the heights of the bars
should sum up to 1 for both barplots with the same 'gender' value. [Hint: seaborn does
not have this functionality built in, instead you need to Ô¨Årst create a contingency table and
usematplotlib.pyplot to produce the Ô¨Ågure.]
5. The iris data set, mentioned in Section 1.1, contains various features, including +2
'Petal.Length 'and'Sepal.Length ', of three species of iris: setosa, versicolor, and
virginica.Chapter 1. Importing, Summarizing, and Visualizing Data 17
(a) Load the data set into a pandas DataFrame object.
(b) Using matplotlib.pyplot , produce boxplots of 'Petal.Length 'for each the
three species, in one Ô¨Ågure.
(c) Make a histogram with 20 bins for 'Petal.Length '.
(d) Produce a similar scatterplot for 'Sepal.Length 'against 'Petal.Length 'to that
of the left plot in Figure 1.9. Note that the points should be colored according to the
'Species' feature as per the legend in the right plot of the Ô¨Ågure.
(e) Using the kdeplot method of the seaborn package, reproduce the right plot of
Figure 1.9, where kernel density plots for 'Petal.Length 'are given. +131
1
 2
 3
 4
 5
 6
 7
Petal.Length
5
6
7
8Sepal.Length
2
 4
 6
 8
Petal.Length
0.0
0.5
1.0
1.5
2.0
2.5Density
setosa
versicolor
virginica
Figure 1.9: Left: scatterplot of 'Sepal.Length 'against 'Petal.Length '. Right: kernel
density estimates of 'Petal.Length 'for the three species of iris.
6. Import the data set EuStockMarkets from the same website as the iris data set above.
The data set contains the daily closing prices of four European stock indices during the
1990s, for 260 working days per year.
(a) Create a vector of times (working days) for the stock prices, between 1991.496 and
1998.646 with increments of 1 /260.
(b) Reproduce Figure 1.10. [Hint: Use a dictionary to map column names (stock indices)
to colors.]18 Exercises
1991
 1992
 1993
 1994
 1995
 1996
 1997
 1998
 1999
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
DAX
SMI
CAC
FTSE
Figure 1.10: Closing stock indices for various European stock markets.
7. Consider the KASANDR data set from the UCI Machine Learning Repository, which can
be downloaded from
https://archive.ics.uci.edu/ml/machine-learning-databases/00385/de
.tar.bz2 .
This archive Ô¨Åle has a size of 900Mb, so it may take a while to download. Uncompressing
the Ô¨Åle (e.g., via 7-Zip) yields a directory decontaining two large CSV Ô¨Åles: test_de.csv
andtrain_de.csv , with sizes 372Mb and 3Gb, respectively. Such large data Ô¨Åles can still
be processed e ciently in pandas , provided there is enough memory. The Ô¨Åles contain
records of user information from Kelkoo web logs in Germany as well as meta-data on
users, o ers, and merchants. The data sets have 7 attributes and 1919561 and 15844717
rows, respectively. The data sets are anonymized via hex strings.
(a) Load train_de.csv into a pandas DataFrame object de, using
read_csv( 'train_de.csv ', delimiter = '\t').
If not enough memory is available, load test_de.csv instead. Note that entries are
separated here by tabs, not commas. Time how long it takes for the Ô¨Åle to load, using
thetime package. (It took 38 seconds for train_de.csv to load on one of our
computers.)
(b) How many unique users and merchants are in this data set?
8. Visualizing data involving more than two features requires careful design, which is often
more of an art than a science.
(a) Go to Vincent Arel-Bundocks‚Äôs website (URL given in Section 1.1) and read the
Orange data set into a pandas DataFrame object called orange . Remove its Ô¨Årst
(unnamed) column.
(b) The data set contains the circumferences of 5 orange trees at various stages in their
development. Find the names of the features.
(c) In Python, import seaborn and visualize the growth curves (circumference against
age) of the trees, using the regplot andFacetGrid methods.CHAPTER2
STATISTICAL LEARNING
The purpose of this chapter is to introduce the reader to some common concepts
and themes in statistical learning. We discuss the di erence between supervised and
unsupervised learning, and how we can assess the predictive performance of supervised
learning. We also examine the central role that the linear and Gaussian properties play
in the modeling of data. We conclude with a section on Bayesian learning. The required
probability and statistics background is given in Appendix C.
2.1 Introduction
Although structuring and visualizing data are important aspects of data science, the main
challenge lies in the mathematical analysis of the data. When the goal is to interpret the
model and quantify the uncertainty in the data, this analysis is usually referred to as stat-
istical learning . In contrast, when the emphasis is on making predictions using large-scalestatistical
learning data, then it is common to speak about machine learning ordata mining .
machine
learning
data miningThere are two major goals for modeling data: 1) to accurately predict some future
quantity of interest, given some observed data, and 2) to discover unusual or interesting
patterns in the data. To achieve these goals, one must rely on knowledge from three im-
portant pillars of the mathematical sciences.
Function approximation. Building a mathematical model for data usually means under-
standing how one data variable depends on another data variable. The most natural
way to represent the relationship between variables is via a mathematical function or
map. We usually assume that this mathematical function is not completely known,
but can be approximated well given enough computing power and data. Thus, data
scientists have to understand how best to approximate and represent functions using
the least amount of computer processing and memory.
Optimization. Given a class of mathematical models, we wish to Ô¨Ånd the best possible
model in that class. This requires some kind of e cient search or optimization pro-
cedure. The optimization step can be viewed as a process of Ô¨Åtting or calibrating
a function to observed data. This step usually requires knowledge of optimization
algorithms and e cient computer coding or programming.
1920 2.2. Supervised and Unsupervised Learning
Probability and Statistics. In general, the data used to Ô¨Åt the model is viewed as a realiz-
ation of a random process or numerical vector, whose probability law determines the
accuracy with which we can predict future observations. Thus, in order to quantify
the uncertainty inherent in making predictions about the future, and the sources of er-
ror in the model, data scientists need a Ô¨Årm grasp of probability theory and statistical
inference.
2.2 Supervised and Unsupervised Learning
Given an input or feature feature vector x, one of the main goals of machine learning is to predict
an output or response response variable y. For example, xcould be a digitized signature and ya
binary variable that indicates whether the signature is genuine or false. Another example is
where xrepresents the weight and smoking habits of an expecting mother and ythe birth
weight of the baby. The data science attempt at this prediction is encoded in a mathematical
function g, called the prediction function prediction
function, which takes as an input xand outputs a guess g(x)
fory(denoted by by, for example). In a sense, gencompasses all the information about the
relationship between the variables xandy, excluding the e ects of chance and randomness
in nature.
Inregression problems, the response variable ycan take any real value. In contrast,regressionwhen ycan only lie in a Ô¨Ånite set, say y2f0;:::; c 1g, then predicting yis conceptually
the same as classifying the input xinto one of ccategories, and so prediction becomes a
classiÔ¨Åcation classification problem.
We can measure the accuracy of a prediction bywith respect to a given response yby
using some loss function loss function Loss( y;by). In a regression setting the usual choice is the squared-
error loss ( y by)2. In the case of classiÔ¨Åcation, the zero‚Äìone (also written 0‚Äì1) loss function
Loss( y;by)=1fy,bygis often used, which incurs a loss of 1 whenever the predicted class
byis not equal to the class y. Later on in this book, we will encounter various other useful
loss functions, such as the cross-entropy and hinge loss functions (see, e.g., Chapter 7).
The word error is often used as a measure of distance between a ‚Äútrue‚Äù object yand
some approximation bythereof. If yis real-valued, the absolute error jy byjand the
squared error ( y by)2are both well-established error concepts, as are the norm ky byk
and squared norm ky byk2for vectors. The squared error ( y by)2is just one example
of a loss function.
It is unlikely that any mathematical function gwill be able to make accurate predictions
for all possible pairs ( x;y) one may encounter in Nature. One reason for this is that, even
with the same input x, the output ymay be di erent, depending on chance circumstances
or randomness. For this reason, we adopt a probabilistic approach and assume that each
pair ( x;y) is the outcome of a random pair ( X;Y) that has some joint probability density
f(x;y). We then assess the predictive performance via the expected loss, usually called the
risk risk , for g:
`(g)=ELoss( Y;g(X)): (2.1)
For example, in the classiÔ¨Åcation case with zero‚Äìone loss function the risk is equal to the
probability of incorrect classiÔ¨Åcation: `(g)=P[Y,g(X)]. In this context, the predictionChapter 2. Statistical Learning 21
function gis called a classiÔ¨Åer classifier . Given the distribution of ( X;Y) and any loss function, we
can in principle Ô¨Ånd the best possible g:=argmingELoss( Y;g(X)) that yields the smallest
risk`:=`(g). We will see in Chapter 7 that in the classiÔ¨Åcation case with y2f0;:::; c 1g +251
and`(g)=P[Y,g(X)], we have
g(x)=argmax
y2f0;:::;c 1gf(yjx);
where f(yjx)=P[Y=yjX=x] is the conditional probability of Y=ygiven X=x.
As already mentioned, for regression the most widely-used loss function is the squared-
error loss. In this setting, the optimal prediction function gis often called the regression
function . The following theorem speciÔ¨Åes its exact form.regression
function
Theorem 2.1: Optimal Prediction Function for Squared-Error Loss
For the squared-error loss Loss( y;by)=(y by)2, the optimal prediction function gis
equal to the conditional expectation of Ygiven X=x:
g(x)=E[YjX=x]:
Proof: Letg(x)=E[YjX=x]. For any function g, the squared-error risk satisÔ¨Åes
E(Y g(X))2=E[(Y g(X)+g(X) g(X))2]
=E(Y g(X))2+2E[(Y g(X))(g(X) g(X))]+E(g(X) g(X))2
>E(Y g(X))2+2E[(Y g(X))(g(X) g(X))]
=E(Y g(X))2+2Ef(g(X) g(X))E[Y g(X)jX]g:
In the last equation we used the tower property. By the deÔ¨Ånition of the conditional expect- +431
ation, we have E[Y g(X)jX]=0. It follows that E(Y g(X))2>E(Y g(X))2, showing
thatgyields the smallest squared-error risk. 
One consequence of Theorem 2.1 is that, conditional on X=x, the (random) response
Ycan be written as
Y=g(x)+"(x); (2.2)
where"(x) can be viewed as the random deviation of the response from its conditional
mean at x. This random deviation satisÔ¨Åes E"(x)=0. Further, the conditional variance of
the response Yatxcan be written as Var"(x)=v2(x) for some unknown positive function
v. Note that, in general, the probability distribution of "(x) is unspeciÔ¨Åed.
Since, the optimal prediction function gdepends on the typically unknown joint distri-
bution of ( X;Y), it is not available in practice. Instead, all that we have available is a Ô¨Ånite
number of (usually) independent realizations from the joint density f(x;y). We denote this
sample byT=f(X1;Y1);:::; (Xn;Yn)gand call it the training set training set (Tis a mnemonic for
training) with nexamples. It will be important to distinguish between a random training
setTand its (deterministic) outcome f(x1;y1);:::; (xn;yn)g. We will use the notation for
the latter. We will also add the subscript ninnwhen we wish to emphasize the size of the
training set.
Our goal is thus to ‚Äúlearn‚Äù the unknown gusing the nexamples in the training set T.
Let us denote by gTthe best (by some criterion) approximation for gthat we can construct22 2.2. Supervised and Unsupervised Learning
fromT. Note that gTis a random function. A particular outcome is denoted by g. It is
often useful to think of a teacher‚Äìlearner metaphor, whereby the function gTis alearner learner
who learns the unknown functional relationship g:x7!yfrom the training data T. We
can imagine a ‚Äúteacher‚Äù who provides nexamples of the true relationship between the
output Yiand the input Xifori=1;:::; n, and thus ‚Äútrains‚Äù the learner gTto predict the
output of a new input X, for which the correct output Yis not provided by the teacher (is
unknown).
The above setting is called supervised learning supervised
learning, because one tries to learn the functional
relationship between the feature vector xand response yin the presence of a teacher who
provides nexamples. It is common to speak of ‚Äúexplaining‚Äù or predicting yon the basis of
x, where xis a vector of explanatory variables explanatory
variables.
An example of supervised learning is email spam detection. The goal is to train the
learner gTto accurately predict whether any future email, as represented by the feature
vector x, is spam or not. The training data consists of the feature vectors of a number
of dierent email examples as well as the corresponding labels (spam or not spam). For
instance, a feature vector could consist of the number of times sales-pitch words like ‚Äúfree‚Äù,
‚Äúsale‚Äù, or ‚Äúmiss out‚Äù occur within a given email.
As seen from the above discussion, most questions of interest in supervised learning
can be answered if we know the conditional pdf f(yjx), because we can then in principle
work out the function value g(x).
In contrast, unsupervised learning unsupervised
learningmakes no distinction between response and explan-
atory variables, and the objective is simply to learn the structure of the unknown distribu-
tion of the data. In other words, we need to learn f(x). In this case the guess g(x) is an
approximation of f(x) and the risk is of the form
`(g)=ELoss( f(X);g(X)):
An example of unsupervised learning is when we wish to analyze the purchasing be-
haviors of the customers of a grocery shop that has a total of, say, a hundred items on sale.
A feature vector here could be a binary vector x2f0;1g100representing the items bought
by a customer on a visit to the shop (a 1 in the k-th position if a customer bought item
k2f1;:::; 100gand a 0 otherwise). Based on a training set =fx1;:::; xng, we wish to
Ô¨Ånd any interesting or unusual purchasing patterns. In general, it is di cult to know if an
unsupervised learner is doing a good job, because there is no teacher to provide examples
of accurate predictions.
The main methodologies for unsupervised learning include clustering ,principal com-
ponent analysis , and kernel density estimation , which will be discussed in Chapter 4. +121
In the next three sections we will focus on supervised learning. The main super-
vised learning methodologies are regression andclassiÔ¨Åcation , to be discussed in detail in
Chapters 5 and 7. More advanced supervised learning techniques, including reproducing +167
+251 kernel Hilbert spaces ,tree methods , and deep learning , will be discussed in Chapters 6, 8,
and 9.Chapter 2. Statistical Learning 23
2.3 Training and Test Loss
Given an arbitrary prediction function g, it is typically not possible to compute its risk `(g)
in (2.1). However, using the training sample T, we can approximate `(g) via the empirical
(sample average) risk
`T(g)=1
nnX
i=1Loss( Yi;g(Xi)); (2.3)
which we call the training loss training loss . The training loss is thus an unbiased estimator of the risk
(the expected loss) for a prediction function g, based on the training data.
To approximate the optimal prediction function g(the minimizer of the risk `(g)) we
Ô¨Årst select a suitable collection of approximating functions Gand then take our learner to
be the function in Gthat minimizes the training loss; that is,
gG
T=argmin
g2G`T(g): (2.4)
For example, the simplest and most useful Gis the set of linear functions of x; that is, the
set of all functions g:x7!>xfor some real-valued vector .
We suppress the superscript Gwhen it is clear which function class is used. Note that
minimizing the training loss over all possible functions g(rather than over all g2G) does
not lead to a meaningful optimization problem, as any function gfor which g(Xi)=Yifor
alligives minimal training loss. In particular, for a squared-error loss, the training loss will
be 0. Unfortunately, such functions have a poor ability to predict new (that is, independent
fromT) pairs of data. This poor generalization performance is called overÔ¨Åtting overfitting .
By choosing ga function that predicts the training data exactly (and is, for example,
0 otherwise), the squared-error training loss is zero. Minimizing the training loss is
not the ultimate goal!
The prediction accuracy of new pairs of data is measured by the generalization risk generalization
riskof
the learner. For a Ô¨Åxed training set it is deÔ¨Åned as
`(gG
)=ELoss( Y;gG
(X)); (2.5)
where ( X;Y) is distributed according to f(x;y). In the discrete case the generalization risk
is therefore: `(gG
)=P
x;yLoss( y;gG
(x))f(x;y) (replace the sum with an integral for the
continuous case). The situation is illustrated in Figure 2.1, where the distribution of ( X;Y)
is indicated by the red dots. The training set (points in the shaded regions) determines a
Ô¨Åxed prediction function shown as a straight line. Three possible outcomes of ( X;Y) are
shown (black dots). The amount of loss for each point is shown as the length of the dashed
lines. The generalization risk is the average loss over all possible pairs ( x;y), weighted by
the corresponding f(x;y).24 2.3. Training and Test Loss
xx xyy
y
Figure 2.1: The generalization risk for a Ô¨Åxed training set is the weighted-average loss over
all possible pairs ( x;y).
For a random training setT, the generalization risk is thus a random variable that
depends onT(andG). If we average the generalization risk over all possible instances of
T, we obtain the expected generalization risk expected
generalization
risk:
E`(gG
T)=ELoss( Y;gG
T(X)); (2.6)
where ( X;Y) in the expectation above is independent of T. In the discrete case, we have
E`(gG
T)=P
x;y;x1;y1;:::;xn;ynLoss( y;gG
(x))f(x;y)f(x1;y1)f(xn;yn). Figure 2.2 gives an il-
lustration.
y
xy
y
x x
Figure 2.2: The expected generalization risk is the weighted-average loss over all possible
pairs ( x;y) and over all training sets.
For any outcome of the training data, we can estimate the generalization risk without
bias by taking the sample average
`T0(gG
) :=1
n0n0X
i=1Loss( Y0
i;gG
(X0
i)); (2.7)
wheref(X0
1;Y0
1);:::; (X0
n0;Y0
n0)g=:T0is a so-called test sample test sample . The test sample is com-
pletely separate from T, but is drawn in the same way as T; that is, via independent draws
from f(x;y), for some sample size n0. We call the estimator (2.7) the test loss test loss . For a ran-
dom training setTwe can deÔ¨Åne `T0(gG
T) similarly. It is then crucial to assume that Tis
independent ofT0. Table 2.1 summarizes the main deÔ¨Ånitions and notation for supervised
learning.Chapter 2. Statistical Learning 25
Table 2.1: Summary of deÔ¨Ånitions for supervised learning.
x Fixed explanatory (feature) vector.
X Random explanatory (feature) vector.
y Fixed (real-valued) response.
Y Random response.
f(x;y) Joint pdf of XandY, evaluated at ( x;y).
f(yjx) Conditional pdf of Ygiven X=x, evaluated at y.
orn Fixed training data f(xi;yi);i=1;:::; ng.
TorTn Random training data f(Xi;Yi);i=1;:::; ng.
X Matrix of explanatory variables, with nrows x>
i;i=1;:::; n
and dim( x) feature columns; one of the features may be the
constant 1.
y Vector of response variables ( y1;:::; yn)>.
g Prediction (guess) function.
Loss( y;by) Loss incurred when predicting response ywithby.
`(g) Risk for prediction function g; that is,ELoss( Y;g(X)).
gOptimal prediction function; that is, argming`(g).
gGOptimal prediction function in function class G; that is,
argming2G`(g).
`(g) Training loss for prediction function g; that is, the sample av-
erage estimate of `(g) based on a Ô¨Åxed training sample .
`T(g) The same as `(g), but now for a random training sample T.
gG
org The learner : argming2G`(g). That is, the optimal prediction
function based on a Ô¨Åxed training set and function class G.
We suppress the superscript Gif the function class is implicit.
gG
TorgT The learner, where we have replaced with a random training
setT.
To compare the predictive performance of various learners in the function class G, as
measured by the test loss, we can use the same Ô¨Åxed training set and test set 0for all
learners. When there is an abundance of data, the ‚Äúoverall‚Äù data set is usually (randomly)
divided into a training and test set, as depicted in Figure 2.3. We then use the training data
to construct various learners gG1;gG2;:::, and use the test data to select the best (with the
smallest test loss) among these learners. In this context the test set is called the validation
set validation set . Once the best learner has been chosen, a third ‚Äútest‚Äù set can be used to assess the
predictive performance of the best learner. The training, validation, and test sets can again
be obtained from the overall data set via a random allocation. When the overall data set
is of modest size, it is customary to perform the validation phase (model selection) on the
training set only, using cross-validation. This is the topic of Section 2.5.2. + 3726 2.3. Training and Test Loss

	
	  
	 

	
	
Figure 2.3: Statistical learning algorithms often require the data to be divided into training
and test data. If the latter is used for model selection, a third set is needed for testing the
performance of the selected model.
We next consider a concrete example that illustrates the concepts introduced so far.
Example 2.1 (Polynomial Regression) In what follows, it will appear that we have ar-
bitrarily replaced the symbols x;g;Gwith u;h;H, respectively. The reason for this switch
of notation will become clear at the end of the example.
The data (depicted as dots) in Figure 2.4 are n=100 points ( ui;yi);i=1;:::; ndrawn
from iid random points ( Ui;Yi);i=1;:::; n, where thefUigare uniformly distributed on
the interval (0 ;1) and, given Ui=ui, the random variable Yihas a normal distribution with
expectation 10 140ui+400u2
i 250u3
iand variance `=25. This is an example of a
polynomial regression model polynomial
regression
model. Using a squared-error loss, the optimal prediction function
h(u)=E[YjU=u] is thus
h(u)=10 140u+400u2 250u3;
which is depicted by the dashed curve in Figure 2.4.
0.0
 0.2
 0.4
 0.6
 0.8
 1.0
u
10
0
10
20
30
40h*(u)
data points
true
Figure 2.4: Training data and the optimal polynomial prediction function h.Chapter 2. Statistical Learning 27
To obtain a good estimate of h(u) based on the training set =f(ui;yi);i=1;:::; ng,
we minimize the outcome of the training loss (2.3):
`(h)=1
nnX
i=1(yi h(ui))2; (2.8)
over a suitable set Hof candidate functions. Let us take the set Hpof polynomial functions
inuof order p 1:
h(u) :=1+2u+3u2++pup 1(2.9)
forp=1;2;:::and parameter vector =[1;2;:::; p]>. This function class contains the
best possible h(u)=E[YjU=u] for p>4. Note that optimization over Hpis a parametric
optimization problem, in that we need to Ô¨Ånd the best . Optimization of (2.8) over Hpis
not straightforward, unless we notice that (2.9) is a linear function in. In particular, if we
map each feature uto a feature vector x=[1;u;u2;:::; up 1]>, then the right-hand side of
(2.9) can be written as the function
g(x)=x>;
which is linear in x(as well as). The optimal h(u) inHpforp>4 then corresponds
to the function g(x)=x>in the setGpof linear functions from RptoR, where=
[10; 140;400; 250;0;:::; 0]>. Thus, instead of working with the set Hpof polynomial
functions we may prefer to work with the set Gpof linear functions. This brings us to a
very important idea in statistical learning:
Expand the feature space to obtain a linear prediction function.
Let us now reformulate the learning problem in terms of the new explanatory (feature)
variables xi=[1;ui;u2
i;:::; up 1
i]>,i=1;:::; n. It will be convenient to arrange these
feature vectors into a matrix Xwith rows x>
1;:::; x>
n:
X=2666666666666666641u1u2
1 up 1
1
1u2u2
2 up 1
2:::::::::::::::
1unu2
n up 1
n377777777777777775: (2.10)
Collecting the responses fyiginto a column vector y, the training loss (2.3) can now be
written compactly as
1
nky Xk2: (2.11)
To Ô¨Ånd the optimal learner (2.4) in the class Gpwe need to Ô¨Ånd the minimizer of (2.11):
b=argmin
ky Xk2; (2.12)
which is called the ordinary least-squares ordinary
least -squaressolution. As is illustrated in Figure 2.5, to Ô¨Ånd b,
we choose Xbto be equal to the orthogonal projection of yonto the linear space spanned
by the columns of the matrix X; that is, Xb=Py, where Pis the projection matrix projection
matrix.28 2.3. Training and Test Loss
Span( X)XŒ≤X/hatwideŒ≤y
Figure 2.5: Xbis the orthogonal projection of yonto the linear space spanned by the
columns of the matrix X.
According to Theorem A.4, the projection matrix is given by +362
P=X X+; (2.13)
where the pnmatrix X+in (2.13) is the pseudo-inverse ofX. IfXhappens to be of full +360
pseudo -inverse column rank (so that none of the columns can be expressed as a linear combination of the
+356 other columns), then X+=(X>X) 1X>.
In any case, from Xb=PyandPX=X, we can see that bsatisÔ¨Åes the normal
equations normal
equations:
X>X=X>Py=(PX)>y=X>y: (2.14)
This is a set of linear equations, which can be solved very fast and whose solution can be
written explicitly as:
b=X+y: (2.15)
Figure 2.6 shows the trained learners for various values of p:
hHp
(u)=gGp
(x)=x>b
0.0
 0.2
 0.4
 0.6
 0.8
 1.0
u
10
0
10
20
30
40hp(u)
data points
true
p= 2, underfit
p= 4, correct
p= 16, overfit
Figure 2.6: Training data with Ô¨Åtted curves for p=2;4, and 16. The true cubic polynomial
curve for p=4 is also plotted (dashed line).Chapter 2. Statistical Learning 29
We see that for p=16 the Ô¨Åtted curve lies closer to the data points, but is further away
from the dashed true polynomial curve, indicating that we overÔ¨Åt. The choice p=4 (the
true cubic polynomial) is much better than p=16, or indeed p=2 (straight line).
Each function class Gpgives a di erent learner gGp
,p=1;2;:::. To assess which is
better, we should not simply take the one that gives the smallest training loss. We can
always get a zero training loss by taking p=n, because for any set of npoints there exists
a polynomial of degree n 1 that interpolates all points!
Instead, we assess the predictive performance of the learners using the test loss (2.7),
computed from a test data set. If we collect all n0test feature vectors in a matrix X0and
the corresponding test responses in a vector y0, then, similar to (2.11), the test loss can be
written compactly as
`0(gGp
)=1
n0ky0 X0bk2;
wherebis given by (2.15), using the training data.
Figure 2.7 shows a plot of the test loss against the number of parameters in the vector
; that is, p. The graph has a characteristic ‚Äúbath-tub‚Äù shape and is at its lowest for p=4,
correctly identifying the polynomial order 3 for the true model. Note that the test loss, as
an estimate for the generalization risk (2.7), becomes numerically unreliable after p=16
(the graph goes down, where it should go up). The reader may check that the graph for
the training loss exhibits a similar numerical instability for large p, and in fact fails to
numerically decrease to 0 for large p, contrary to what it should do in theory. The numerical
problems arise from the fact that for large pthe columns of the (Vandermonde) matrix X
are of vastly di erent magnitudes and so Ô¨Çoating point errors quickly become very large.
Finally, observe that the lower bound for the test loss is here around 21, which corres-
ponds to an estimate of the minimal (squared-error) risk `=25.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
Number of parameters p
20
40
60
80
100
120
140
160Test loss
Figure 2.7: Test loss as function of the number of parameters pof the model.
This script shows how the training data were generated and plotted in Python:30 2.3. Training and Test Loss
polyreg1.py
import numpy as np
from numpy.random import rand , randn
from numpy.linalg import norm , solve
import matplotlib.pyplot as plt
def generate_data(beta , sig, n):
u = np.random.rand(n, 1)
y = (u ** np.arange(0, 4)) @ beta + sig * np.random.randn(n, 1)
return u, y
np.random.seed(12)
beta = np.array([[10, -140, 400, -250]]).T
n = 100
sig = 5
u, y = generate_data(beta , sig, n)
xx = np.arange(np. min(u), np. max(u)+5e-3, 5e-3)
yy = np.polyval(np.flip(beta), xx)
plt.plot(u, y, '.', markersize=8)
plt.plot(xx, yy, '--',linewidth=3)
plt.xlabel(r '$u$')
plt.ylabel(r '$h^*(u)$ ')
plt.legend([ 'data points ','true '])
plt.show()
The following code, which imports the code above, Ô¨Åts polynomial models with p=
1;:::; K=18 parameters to the training data and plots a selection of Ô¨Åtted curves, as
shown in Figure 2.6.
polyreg2.py
from polyreg1 import *
max_p = 18
p_range = np.arange(1, max_p + 1, 1)
X = np.ones((n, 1))
betahat , trainloss = {}, {}
for pinp_range: # p is the number of parameters
ifp > 1:
X = np.hstack((X, u**(p-1))) # add column to matrix
betahat[p] = solve(X.T @ X, X.T @ y)
trainloss[p] = (norm(y - X @ betahat[p])**2/n)
p = [2, 4, 16] # select three curves
#replot the points and true line and store in the list "plots"
plots = [plt.plot(u, y, 'k.', markersize=8)[0],
plt.plot(xx, yy, 'k--',linewidth=3)[0]]
# add the three curves
for iinp:
yy = np.polyval(np.flip(betahat[i]), xx)
plots.append(plt.plot(xx, yy)[0])Chapter 2. Statistical Learning 31
plt.xlabel(r '$u$')
plt.ylabel(r '$h^{\mathcal{H}_p}_{\tau}(u)$ ')
plt.legend(plots ,( 'data points ','true ','$p=2$, underfit ',
'$p=4$, correct ','$p=16$, overfit '))
plt.savefig( 'polyfitpy.pdf ',format ='pdf')
plt.show()
The last code snippet which imports the previous code, generates the test data and plots the
graph of the test loss, as shown in Figure 2.7.
polyreg3.py
from polyreg2 import *
# generate test data
u_test , y_test = generate_data(beta , sig, n)
MSE = []
X_test = np.ones((n, 1))
for pinp_range:
ifp > 1:
X_test = np.hstack((X_test , u_test**(p-1)))
y_hat = X_test @ betahat[p] # predictions
MSE.append(np. sum((y_test - y_hat)**2/n))
plt.plot(p_range , MSE, 'b', p_range , MSE, 'bo')
plt.xticks(ticks=p_range)
plt.xlabel( 'Number of parameters $p$ ')
plt.ylabel( 'Test loss ')
2.4 Tradeoffs in Statistical Learning
The art of machine learning in the supervised case is to make the generalization risk (2.5)
or expected generalization risk (2.6) as small as possible, while using as few computational
resources as possible. In pursuing this goal, a suitable class Gof prediction functions has
to be chosen. This choice is driven by various factors, such as
¬àthe complexity of the class (e.g., is it rich enough to adequately approximate, or even
contain, the optimal prediction function g?),
¬àthe ease of training the learner via the optimization program (2.4),
¬àhow accurately the training loss (2.3) estimates the risk (2.1) within class G,
¬àthe feature types (categorical, continuous, etc.).
As a result, the choice of a suitable function class Gusually involves a tradeo between
conÔ¨Çicting factors. For example, a learner from a simple class Gcan be trained very32 2.4. Tradeoffs in Statistical Learning
quickly, but may not approximate gvery well, whereas a learner from a rich class G
that contains gmay require a lot of computing resources to train.
To better understand the relation between model complexity, computational simplicity,
and estimation accuracy, it is useful to decompose the generalization risk into several parts,
so that the tradeo s between these parts can be studied. We will consider two such decom-
positions: the approximation‚Äìestimation tradeo and the bias‚Äìvariance tradeo .
We can decompose the generalization risk (2.5) into the following three components:
`(gG
)=`|{z}
irreducible risk+`(gG) `
|     {z     }
approximation error+`(gG
) `(gG)|          {z          }
statistical error; (2.16)
where`:=`(g) is the irreducible risk irreducible risk andgG:=argming2G`(g) is the best learner within
classG. No learner can predict a new response with a smaller risk than `.
The second component is the approximation error approximation
error; it measures the di erence between
the irreducible risk and the best possible risk that can be obtained by selecting the best
prediction function in the selected class of functions G. Determining a suitable class Gand
minimizing `(g) over this class is purely a problem of numerical and functional analysis,
as the training data are not present. For a Ô¨Åxed Gthat does not contain the optimal g, the
approximation error cannot be made arbitrarily small and may be the dominant component
in the generalization risk. The only way to reduce the approximation error is by expanding
the classGto include a larger set of possible functions.
The third component is the statistical (estimation) error statistical
(estimation )
error. It depends on the training
setand, in particular, on how well the learner gG
estimates the best possible prediction
function, gG, within classG. For any sensible estimator this error should decay to zero (in
probability or expectation) as the training size tends to inÔ¨Ånity. +439
The approximation‚Äìestimation tradeo  approximation ‚Äì
estimation
tradeoffpits two competing demands against each
other. The Ô¨Årst is that the class Ghas to be simple enough so that the statistical error is
not too large. The second is that the class Ghas to be rich enough to ensure a small approx-
imation error. Thus, there is a tradeo between the approximation and estimation errors.
For the special case of the squared-error loss, the generalization risk is equal to `(gG
)=
E(Y gG
(X))2; that is, the expected squared error1between the predicted value gG
(X)
and the response Y. Recall that in this case the optimal prediction function is given by
g(x)=E[YjX=x]. The decomposition (2.16) can now be interpreted as follows.
1. The Ô¨Årst component, `=E(Y g(X))2, is the irreducible error , as no prediction
function will yield a smaller expected squared error.
2. The second component, the approximation error `(gG) `(g), is equal to E(gG(X) 
g(X))2. We leave the proof (which is similar to that of Theorem 2.1) as an exercise;
see Exercise 2. Thus, the approximation error (deÔ¨Åned as a risk di erence) can here
be interpreted as the expected squared error between the optimal predicted value and
the optimal predicted value within the class G.
3. For the third component, the statistical error, `(gG
) `(gG) there is no direct inter-
pretation as an expected squared error unlessGis the class of linear functions; that
is,g(x)=x>for some vector . In this case we can write (see Exercise 3) the
statistical error as `(gG
) `(gG)=E(gG
(X) gG(X))2.
1Colloquially called mean squared error .Chapter 2. Statistical Learning 33
Thus, when using a squared-error loss, the generalization risk for a linear class Gcan
be decomposed as:
`(gG
)=E(gG
(X) Y)2=`+E(gG(X) g(X))2
|                 {z                 }
approximation error+E(gG
(X) gG(X))2
|                  {z                  }
statistical error: (2.17)
Note that in this decomposition the statistical error is the only term that depends on the
training set.
Example 2.2 (Polynomial Regression (cont.)) We continue Example 2.1. Here G=
Gpis the class of linear functions of x=[1;u;u2;:::; up 1]>, and g(x)=x>. Condi-
tional on X=xwe have that Y=g(x)+"(x), with"(x)N(0;`), where`=E(Y 
g(X))2=25 is the irreducible error. We wish to understand how the approximation and
statistical errors behave as we change the complexity parameter p.
First, we consider the approximation error. Any function g2G pcan be written as
g(x)=h(u)=1+2u++pup 1=[1;u;:::; up 1];
and so g(X) is distributed as [1 ;U;:::; Up 1], where UU(0;1). Similarly, g(X) is
distributed as [1 ;U;U2;U3]. It follows that an expression for the approximation error
is:R1
0
[1;u;:::; up 1] [1;u;u2;u3]2du:To minimize this error, we set the gradient
with respect to to zero and obtain the plinear equations +397
R1
0
[1;u;:::; up 1] [1;u;u2;u3]
du=0;
R1
0
[1;u;:::; up 1] [1;u;u2;u3]
udu=0;
:::
R1
0
[1;u;:::; up 1] [1;u;u2;u3]
up 1du=0:
Let
Hp=Z1
0[1;u;:::; up 1]>[1;u;:::; up 1] du
be the pp Hilbert matrix Hilbert matrix , which has ( i;j)-th entry given byR1
0ui+j 2du=1=(i+j 1).
Then, the above system of linear equations can be written as Hp=eH, where eHis the
p4 upper left sub-block of Hepandep=maxfp;4g. The solution, which we denote by p,
is:
p=8>>>>>>>><>>>>>>>>:65
6; p=1;
[ 20
3;35]>; p=2;
[ 5
2;10;25]>; p=3;
[10; 140;400; 250;0;:::; 0]>;p>4:(2.18)
Hence, the approximation error E
gGp(X) g(X)2is given by
Z1
0
[1;u;:::; up 1]p [1;u;u2;u3]2du=8>>>>>>>><>>>>>>>>:32225
252127:9;p=1;
1625
6325:8; p=2;
625
2822:3; p=3;
0; p>4:(2.19)34 2.4. Tradeoffs in Statistical Learning
Notice how the approximation error becomes smaller as pincreases. In this particular
example the approximation error is in fact zero for p>4. In general, as the class of ap-
proximating functions Gbecomes more complex, the approximation error goes down.
Next, we illustrate the typical behavior of the statistical error. Since g(x)=x>b, the
statistical error can be written as
Z1
0
[1;:::; up 1](b p)2du=(b p)>Hp(b p): (2.20)
Figure 2.8 illustrates the decomposition (2.17) of the generalization risk for the same train-
ing set that was used to compute the test loss in Figure 2.7. Recall that test loss gives an
estimate of the generalization risk, using independent test data. Comparing the two Ô¨Ågures,
we see that in this case the two match closely. The global minimum of the statistical error is
approximately 0 :28, with minimizer p=4. Since the approximation error is monotonically
decreasing to zero, p=4 is also the global minimizer of the generalization risk.
0 2 4 6 8 10 12 14 16 18050100150approximation error
statistical error
irreducible error
generalization risk
Figure 2.8: The generalization risk for a particular training set is the sum of the irreducible
error, the approximation error, and the statistical error. The approximation error decreases
to zero as pincreases, whereas the statistical error has a tendency to increase after p=4.
Note that the statistical error depends on the estimate b, which in its turn depends on
the training set . We can obtain a better understanding of the statistical error by consid-
ering its expected behavior; that is, averaged over many training sets. This is explored in
Exercise 11.
Using again a squared-error loss, a second decomposition (for general G) starts from
`(gG
)=`+`(gG
) `(g);
where the statistical error and approximation error are combined. Using similar reasoning
as in the proof of Theorem 2.1, we have
`(gG
)=E(gG
(X) Y)2=`+E
gG
(X) g(X)2=`+ED2(X;);Chapter 2. Statistical Learning 35
where D(x;) :=gG
(x) g(x). Now consider the random variable D(x;T) for a random
training setT. The expectation of its square is:
E
gG
T(x) g(x)2=ED2(x;T)=(ED(x;T))2+VarD(x;T)
=(EgG
T(x) g(x))2
|                 {z                 }
pointwise squared bias+VargG
T(x)|     {z     }
pointwise variance: (2.21)
If we view the learner gG
T(x) as a function of a random training set, then the pointwise
squared bias pointwise
squared biasterm is a measure for how close gG
T(x) is on average to the true g(x),
whereas the pointwise variance term measures the deviation of gG
T(x) from its expectedpointwise
variance valueEgG
T(x). The squared bias can be reduced by making the class of functions Gmore
complex. However, decreasing the bias by increasing the complexity often leads to an in-
crease in the variance term. We are thus seeking learners that provide an optimal balance
between the bias and variance, as expressed via a minimal generalization risk. This is called
thebias‚Äìvariance tradeo  bias‚Äìvariance
tradeoff.
Note that the expected generalization risk (2.6) can be written as `+ED2(X;T), where
XandTare independent. It therefore decomposes as
E`(gG
T)=`+E(E[gG
T(X)jX] g(X))2
|                           {z                           }
expected squared bias+E[Var[gG
T(X)jX]]|                 {z                 }
expected variance: (2.22)
2.5 Estimating Risk
The most straightforward way to quantify the generalization risk (2.5) is to estimate it via
the test loss (2.7). However, the generalization risk depends inherently on the training set,
and so di erent training sets may yield signiÔ¨Åcantly di erent estimates. Moreover, when
there is a limited amount of data available, reserving a substantial proportion of the data
for testing rather than training may be uneconomical. In this section we consider di erent
methods for estimating risk measures which aim to circumvent these di culties.
2.5.1 In-Sample Risk
We mentioned that, due to the phenomenon of overÔ¨Åtting, the training loss of the learner,
`(g) (for simplicity, here we omit Gfrom gG
), is not a good estimate of the generalization
risk`(g) of the learner. One reason for this is that we use the same data for both training
the model and assessing its risk. How should we then estimate the generalization risk or
expected generalization risk?
To simplify the analysis, suppose that we wish to estimate the average accuracy of the
predictions of the learner gat the nfeature vectors x1;:::; xn(these are part of the training
set). In other words, we wish to estimate the in-sample risk in-sample risk of the learner g:
`in(g)=1
nnX
i=1ELoss( Y0
i;g(xi)); (2.23)
where each response Y0
iis drawn from f(yjxi), independently. Even in this simpliÔ¨Åed set-
ting, the training loss of the learner will be a poor estimate of the in-sample risk. Instead, the36 2.5. Estimating Risk
proper way to assess the prediction accuracy of the learner at the feature vectors x1;:::; xn,
is to draw new response values Y0
if(yjxi);i=1;:::; n, that are independent from the
responses y1;:::; ynin the training data, and then estimate the in-sample risk of gvia
1
nnX
i=1Loss( Y0
i;g(xi)):
For a Ô¨Åxed training set , we can compare the training loss of the learner with the
in-sample risk. Their di erence,
op=`in(g) `(g);
is called the optimism (of the training loss), because it measures how much the training
loss underestimates (is optimistic about) the unknown in-sample risk. Mathematically, it is
simpler to work with the expected optimism expected
optimism:
E[opTjX1=x1;:::; Xn=xn]=:EXopT;
where the expectation is taken over a random training set T, conditional on Xi=xi;
i=1;:::; n. For ease of notation, we have abbreviated the expected optimism to EXopT,
whereEXdenotes the expectation operator conditional on Xi=xi;i=1;:::; n. As in Ex-
ample 2.1, the feature vectors are stored as the rows of an npmatrix X. It turns out that the
expected optimism for various loss functions can be expressed in terms of the (conditional)
covariance between the observed and predicted response.
Theorem 2.2: Expected Optimism
For the squared-error loss and 0‚Äì1 loss with 0‚Äì1 response, the expected optimism is
EXopT=2
nnX
i=1CovX(gT(xi);Yi): (2.24)
Proof: In what follows, all expectations are taken conditional on X1=x1;:::; Xn=xn.
LetYibe the response for xiand let bYi=gT(xi) be the predicted value. Note that the latter
depends on Y1;:::; Yn. Also, let Y0
ibe an independent copy of Yifor the same xi, as in
(2.23). In particular, Y0
ihas the same distribution as Yiand is statistically independent of
allfYjg, including Yi, and therefore is also independent of bYi. We have
EXopT=1
nnX
i=1EXh
(Y0
i bYi)2 (Yi bYi)2i
=2
nnX
i=1EXh
(Yi Y0
i)bYii
=2
nnX
i=1
EX[YibYi] EXYiEXbYi
=2
nnX
i=1CovX(bYi;Yi):
The proof for the 0‚Äì1 loss with 0‚Äì1 response is left as Exercise 4. 
In summary, the expected optimism indicates how much, on average, the training loss
deviates from the expected in-sample risk. Since the covariance of independent random
variables is zero, the expected optimism is zero if the learner gTis statistically independent
from the responses Y1;:::; Yn.Chapter 2. Statistical Learning 37
Example 2.3 (Polynomial Regression (cont.)) We continue Example 2.2, where the
components of the response vector Y=[Y1;:::; Yn]>are independent and normally distrib-
uted with variance `=25 (the irreducible error) and expectations EXYi=g(xi)=x>
i,
i=1;:::; n. Using the formula (2.15) for the least-squares estimator b, the expected op-
timism (2.24) is
2
nnX
i=1CovX
x>
ib;Yi
=2
ntr
CovX
Xb;Y
=2
ntr CovX XX+Y;Y
=2tr(XX+CovX(Y;Y))
n=2`tr(XX+)
n=2`p
n:
In the last equation we used the cyclic property of the trace (Theorem A.1): tr( XX+)= +357
tr(X+X)=tr(Ip), assuming that rank( X)=p. Therefore, an estimate for the in-sample risk
(2.23) is:
b`in(g)=`(g)+2`p=n; (2.25)
where we have assumed that the irreducible risk `is known. Figure 2.9 shows that this
estimate is very close to the test loss from Figure 2.7. Hence, instead of computing the test
loss to assess the best model complexity p, we could simply have minimized the training
loss plus the correction term 2 `p=n. In practice, `also has to be estimated somehow.
2 4 6 8 10 12 14 16 18050100150
Figure 2.9: In-sample risk estimate b`in(g) as a function of the number of parameters pof
the model. The test loss is superimposed as a blue dashed curve.
2.5.2 Cross-Validation
In general, for complex function classes G, it is very di cult to derive simple formulas of
the approximation and statistical errors, let alone for the generalization risk or expected
generalization risk. As we saw, when there is an abundance of data, the easiest way to
assess the generalization risk for a given training set is to obtain a test set 0and evaluate
the test loss (2.7). When a su ciently large test set is not available but computational + 24
resources are cheap, one can instead gain direct knowledge of the expected generalization
risk via a computationally intensive method called cross-validation cross -validation .38 2.5. Estimating Risk
The idea is to make multiple identical copies of the data set, and to partition each copy
into di erent training and test sets, as illustrated in Figure 2.10. Here, there are four copies
of the data set (consisting of response and explanatory variables). Each copy is divided into
a test set (colored blue) and training set (colored pink). For each of these sets, we estimate
the model parameters using only training data and then predict the responses for the test
set. The average loss between the predicted and observed responses is then a measure for
the predictive power of the model.

	

  






Figure 2.10: An illustration of four-fold cross-validation, representing four copies of the
same data set. The data in each copy is partitioned into a training set (pink) and a test
set (blue). The darker columns represent the response variable and the lighter ones the
explanatory variables.
In particular, suppose we partition a data set Tof size nintoK folds folds C1;:::;CKof sizes
n1;:::; nK(hence, n1++nK=n). Typically nkn=K,k=1;:::; K.
Let`Ckbe the test loss when using Ckas test data and all remaining data, denoted T k,
as training data. Each `Ckis an unbiased estimator of the generalization risk for training set
T k; that is, for `(gT k).
TheK-fold cross-validation K-fold
cross -validationloss is the weighted average of these risk estimators:
CV K=KX
k=1nk
n`Ck(gT k)
=1
nKX
k=1X
i2CkLoss( gT k(xi);yi)
=1
nnX
i=1Loss( gT (i)(xi);yi);
where the function :f1;:::; ng 7! f 1;:::; Kgindicates to which of the Kfolds each
of the nobservations belongs. As the average is taken over varying training sets fT kg, it
estimates the expected generalization risk E`(gT), rather than the generalization risk `(g)
for the particular training set .
Example 2.4 (Polynomial Regression (cont.)) For the polynomial regression ex-
ample, we can calculate a K-fold cross-validation loss with a nonrandom partitioning of the
training set using the following code, which imports the previous code for the polynomial
regression example. We omit the full plotting code.Chapter 2. Statistical Learning 39
polyregCV.py
from polyreg3 import *
K_vals = [5, 10, 100] # number of folds
cv = np.zeros(( len(K_vals), max_p)) # cv loss
X = np.ones((n, 1))
for pinp_range:
ifp > 1:
X = np.hstack((X, u**(p-1)))
j = 0
for KinK_vals:
loss = []
for kin range (1, K+1):
# integer indices of test samples
test_ind = ((n/K)*(k-1) + np.arange(1,n/K+1) -1).astype( 'int')
train_ind = np.setdiff1d(np.arange(n), test_ind)
X_train , y_train = X[train_ind , :], y[train_ind , :]
X_test , y_test = X[test_ind , :], y[test_ind]
# fit model and evaluate test loss
betahat = solve(X_train.T @ X_train , X_train.T @ y_train)
loss.append(norm(y_test - X_test @ betahat) ** 2)
cv[j, p-1] = sum(loss)/n
j += 1
# basic plotting
plt.plot(p_range , cv[0, :], 'k-.')
plt.plot(p_range , cv[1, :], 'r')
plt.plot(p_range , cv[2, :], 'b--')
plt.show()
2
 4
 6
 8
 10
 12
 14
 16
 18
Number of parameters p
50
100
150
200
250
300K-fold cross-validation loss
K=5
K=10
K=100
Figure 2.11: K-fold cross-validation for the polynomial regression example.40 2.6. Modeling Data
Figure 2.11 shows the cross-validation loss for K2f5;10;100g. The case K=100 cor-
responds to the leave-one-out cross-validation , which can be computed more e cientlyleave -one-out
cross -validation using the formula in Theorem 5.1.
+174
2.6 Modeling Data
The Ô¨Årst step in any data analysis is to model model the data in one form or another. For example,
in an unsupervised learning setting with data represented by a vector x=[x1;:::; xp]>, a
very general model is to assume that xis the outcome of a random vector X=[X1;:::; Xp]>
with some unknown pdf f. The model can then be reÔ¨Åned by assuming a speciÔ¨Åc form of
f.
When given a sequence of such data vectors x1;:::; xn, one of the simplest models is to
assume that the corresponding random vectors X1;:::; Xnareindependent and identically
distributed (iid) . We write +429
X1;:::; XniidforX1;:::; XniidDist;
to indicate that the random vectors form an iid sample from a sampling pdf for sampling
distribution Dist. This model formalizes the notion that the knowledge about one variable
does not provide extra information about another variable. The main theoretical use of
independent data models is that the joint density of the random vectors X1;:::; Xnis simply
theproduct of the marginal ones; see Theorem C.1. SpeciÔ¨Åcally, +429
fX1;:::;Xn(x1;:::; xn)=f(x1)f(xn):
In most models of this kind, our approximation or model for the sampling distribution is
speciÔ¨Åed up to a small number of parameters. That is, g(x) is of the form g(xj) which
is known up to some parameter vector . Examples for the one-dimensional case ( p=1)
include the N(;2);Bin(n;p), and Exp() distributions. See Tables C.1 and C.2 for other +425
common sampling distributions.
Typically, the parameters are unknown and must be estimated from the data. In a non-
parametric setting the whole sampling distribution would be unknown. To visualize the
underlying sampling distribution from outcomes x1;:::; xnone can use graphical repres-
entations such as histograms, density plots, and empirical cumulative distribution func-
tions, as discussed in Chapter 1. +11
If the order in which the data were collected (or their labeling) is not informative or
relevant, then the joint pdf of X1;:::; XnsatisÔ¨Åes the symmetry:
fX1;:::;Xn(x1;:::; xn)=fX1;:::;Xn(x1;:::; xn) (2.26)
for any permutation 1;:::; nof the integers 1 ;:::; n. We say that the inÔ¨Ånite sequence
X1;X2;:::isexchangeable exchangeable if this permutational invariance (2.26) holds for any Ô¨Ånite subset
of the sequence. As we shall see in Section 2.9 on Bayesian learning, it is common to
assume that the random vectors X1;:::; Xnare a subset of an exchangeable sequence and
thus satisfy (2.26). Note that while iid random variables are exchangeable, the converse is
not necessarily true. Thus, the assumption of an exchangeable sequence of random vectors
is weaker than the assumption of iid random vectors.Chapter 2. Statistical Learning 41
Figure 2.12 illustrates the modeling tradeo s. The keywords within the triangle repres-
ent various modeling paradigms. A few keywords have been highlighted, symbolizing their
importance in modeling. The speciÔ¨Åc meaning of the keywords does not concern us here,
but the point is there are many models to choose from, depending on what assumptions are
made about the data.
Figure 2.12: Illustration of the modeling dilemma. Complex models are more generally
applicable, but may be di cult to analyze. Simple models may be highly tractable, but
may not describe the data accurately. The triangular shape signiÔ¨Åes that there are a great
many speciÔ¨Åc models but not so many generic ones.
On the one hand, models that make few assumptions are more widely applicable, but at
the same time may not be very mathematically tractable or provide insight into the nature
of the data. On the other hand, very speciÔ¨Åc models may be easy to handle and interpret, but
may not match the data very well. This tradeo between the tractability and applicability of
the model is very similar to the approximation‚Äìestimation tradeo described in Section 2.4.
In the typical unsupervised setting we have a training set =fx1;:::; xngthat is viewed
as the outcome of niid random variables X1;:::; Xnfrom some unknown pdf f. The ob-
jective is then to learn or estimate ffrom the Ô¨Ånite training data. To put the learning in
a similar framework as for supervised learning discussed in the preceding Sections 2.3‚Äì
2.5, we begin by specifying a class of probability density functions Gp:=fg(j);2g,
whereis a parameter in some subset ofRp. We now seek the best ginGpto minimize
some risk. Note that Gpmay not necessarily contain the true feven for very large p.
We stress that our notation g(x) has a di erent meaning in the supervised and unsu-
pervised case. In the supervised case, gis interpreted as a prediction function for a
response y; in the unsupervised setting, gis an approximation of a density f.
For each xwe measure the discrepancy between the true model f(x) and the hypothes-
ized model g(xj) using the loss function
Loss( f(x);g(xj))=lnf(x)
g(xj)=lnf(x) lng(xj):42 2.6. Modeling Data
The expected value of this loss (that is, the risk) is thus
`(g)=Elnf(X)
g(Xj)=Z
f(x) lnf(x)
g(xj)dx: (2.27)
The integral in (2.27) provides a fundamental way to measure the distance between two
densities and is called the Kullback‚ÄìLeibler (KL) divergence2Kullback ‚Äì
Leibler
divergencebetween fandg(j). Note
that the KL divergence is not symmetric in fandg(j). Moreover, it is always greater
than or equal to 0 (see Exercise 15) and equal to 0 when f=g(j).
Using similar notation as for the supervised learning setting in Table 2.1, deÔ¨Åne gGpas
the global minimizer of the risk in the class Gp; that is, gGp=argming2Gp`(g). If we deÔ¨Åne
=argmin
ELoss( f(X);g(Xj))=argmin
Z lnf(x) lng(xj)f(x) dx
=argmax
Z
f(x) lng(xj) dx=argmax
Elng(Xj);
then gGp=g(j) and learning gGpis equivalent to learning (or estimating) . To learn
from a training set =fx1;:::; xngwe then minimize the training loss,
1
nnX
i=1Loss( f(xi);g(xij))= 1
nnX
i=1lng(xij)+1
nnX
i=1lnf(xi);
giving:
bn:=argmax
1
nnX
i=1lng(xij): (2.28)
As the logarithm is an increasing function, this is equivalent to
bn:=argmax
nY
i=1g(xij);
whereQn
i=1g(xij) is the likelihood of the data; that is, the joint density of the fXigeval-
uated at the points fxig. We therefore have recovered the classical maximum likelihood
estimate of.maximum
likelihood
estimate
+456When the risk `(g(j)) is convex in over a convex set , we can Ô¨Ånd the maximum
likelihood estimator by setting the gradient of the training loss to zero; that is, we solve
 1
nnX
i=1S(xij)=0;
where S(xj) :=@lng(xj)
@is the gradient of ln g(xj) with respect to and is often called
thescore score .
Example 2.5 (Exponential Model) Suppose we have the training data n=fx1;:::; xng,
which is modeled as a realization of npositive iid random variables: X1;:::; Xniidf(x).
We select the class of approximating functions Gto be the parametric class fg:g(xj)=
2Sometimes called cross-entropy distance.Chapter 2. Statistical Learning 43
exp( x);x>0; > 0g. In other words, we look for the best gGwithin the family of
exponential distributions with unknown parameter >0. The likelihood of the data is
nY
i=1g(xij)=nY
i=1exp( xi)=exp( nxn+nln)
and the score is S(xj)= x+ 1. Thus, maximizing the likelihood with respect to is the
same as maximizing  nxn+nlnor solving Pn
i=1S(xij)=n=xn  1=0. In other
words, the solution to (2.28) is the maximum likelihood estimate bn=1=xn.
In a supervised setting, where the data is represented by a vector xof explanatory
variables and a response y, the general model is that ( x;y) is an outcome of ( X;Y)f
for some unknown f. And for a training sequence ( x1;y1);:::; (xn;yn) the default model
assumption is that ( X1;Y1);:::; (Xn;Yn)iidf. As explained in Section 2.2, the analysis
primarily involves the conditional pdf f(yjx) and in particular (when using the squared-
error loss) the conditional expectation g(x)=E[YjX=x]. The resulting representation
(2.2) allows us to then write the response at X=xas a function of the feature xplus an
error term: Y=g(x)+"(x).
This leads to the simplest and most important model for supervised learning, where we
choose a linear classGof prediction or guess functions and assume that it is rich enough
to contain the true g. If we further assume that, conditional on X=x, the error term "
does not depend on x, that is,E"=0 andVar"=2, then we obtain the following model.
DeÔ¨Ånition 2.1: Linear Model
In alinear model linear model the response Ydepends on a p-dimensional explanatory variable
x=[x1;:::; xp]>via the linear relationship
Y=x>+"; (2.29)
whereE"=0 andVar"=2.
Note that (2.29) is a model for a single pair ( x;Y). The model for the training set
f(xi;Yi)gis simply that each YisatisÔ¨Åes (2.29) (with x=xi) and that thefYigare independ-
ent. Gathering all responses in the vector Y=[Y1;:::; Yn]>, we can write
Y=X+"; (2.30)
where"=["1;:::;" n]>is a vector of iid copies of "andXis the so-called model matrix model matrix ,
with rows x>
1;:::; x>
n. Linear models are fundamental building blocks of statistical learning
algorithms. For this reason, a large part of Chapter 5 is devoted to linear regression models. +167
Example 2.6 (Polynomial Regression (cont.)) For our running Example 2.1, we see + 26
that the data is described by a linear model of the form (2.30), with model matrix Xgiven
in (2.10).44 2.7. Multivariate Normal Models
Before we discuss a few other models in the following sections, we would like to em-
phasize a number of points about modeling.
¬àAny model for data is likely to be wrong . For example, real data (as opposed to
computer-generated data) are often assumed to come from a normal distribution,
which is never exactly true. However, an important advantage of using a normal
distribution is that it has many nice mathematical properties, as we will see in Sec-
tion 2.7.
¬àMost data models depend on a number of unknown parameters, which need to be
estimated from the observed data.
¬àAny model for real-life data needs to be checked for suitability. An important cri-
terion is that data simulated from the model should resemble the observed data, at
least for a certain choice of model parameters.
Here are some guidelines for choosing a model. Think of the data as a spreadsheet or
data frame, as in Chapter 1, where rows represent the data units and the columns the data
features (variables, groups).
¬àFirst establish the type of the features (quantitative, qualitative, discrete, continuous,
etc.).
¬àAssess whether the data can be assumed to be independent across rows or columns.
¬àDecide on the level of generality of the model. For example, should we use a simple
model with a few unknown parameters or a more generic model that has a large
number of parameters? Simple speciÔ¨Åc models are easier to Ô¨Åt to the data (low es-
timation error) than more general models, but the Ô¨Åt itself may not be accurate (high
approximation error). The tradeo s discussed in Section 2.4 play an important role
here.
¬àDecide on using a classical (frequentist) or Bayesian model. Section 2.9 gives a short
introduction to Bayesian learning. +47
2.7 Multivariate Normal Models
A standard model for numerical observations x1;:::; xn(forming, e.g., a column in a
spreadsheet or data frame) is that they are the outcomes of iid normal random variables
X1;:::; XniidN(;2):
It is helpful to view a normally distributed random variable as a simple transformation
of a standard normal random variable. To wit, if Zhas a standard normal distribution, then
X=+Zhas aN(;2) distribution. The generalization to ndimensions is discussed
in Appendix C.7. We summarize the main points: Let Z1;:::; ZniidN(0;1). The pdf of +434
Z=[Z1;:::; Zn]>(that is, the joint pdf of Z1;:::; Zn) is given by
fZ(z)=nY
i=11p
2e 1
2z2
i=(2) n
2e 1
2z>z;z2Rn: (2.31)Chapter 2. Statistical Learning 45
We write ZN(0;In) and say that Zhas a standard normal distribution in Rn. Let
X=+BZ (2.32)
for some mnmatrix Bandm-dimensional vector . Then Xhas expectation vector and
covariance matrix =BB>; see (C.20) and (C.21). This leads to the following deÔ¨Ånition. +432
DeÔ¨Ånition 2.2: Multivariate Normal Distribution
Anm-dimensional random vector Xthat can be written in the form (2.32) for some
m-dimensional vector andmnmatrix B, with ZN(0;In), is said to have a
multivariate normal multivariate
normalormultivariate Gaussian distribution with mean vector and
covariance matrix =BB>. We write XN(;).
Them-dimensional density of a multivariate normal distribution has a very similar form
to the density of the one-dimensional normal distribution and is given in the next theorem.
We leave the proof as an exercise; see Exercise 5. + 59
Theorem 2.3: Density of a Multivariate Random Vector
LetXN(;), where the mmcovariance matrix is invertible. Then Xhas pdf
fX(x)=1p(2)mjje 1
2(x )> 1(x );x2Rm: (2.33)
Figure 2.13 shows the pdfs of two bivariate (that is, two-dimensional) normal distribu-
tions. In both cases the mean vector is =[0;0]>and the variances (the diagonal elements
of) are 1. The correlation coe cients (or, equivalently here, the covariances) are respect-
ively%=0 and%=0:8.
00.1
20.2
0
-22 0 -2
00.1
20.2
0
-22 0 -2
Figure 2.13: Pdfs of bivariate normal distributions with means zero, variances 1, and cor-
relation coe cients 0 (left) and 0 :8 (right).46 2.8. Normal Linear Models
The main reason why the multivariate normal distribution plays an important role in
data science and machine learning is that it satisÔ¨Åes the following properties, the details
and proofs of which can be found in Appendix C.7: +434
1. Ane combinations are normal.
2. Marginal distributions are normal.
3. Conditional distributions are normal.
2.8 Normal Linear Models
Normal linear models combine the simplicity of the linear model with the tractability of
the Gaussian distribution. They are the principal model for traditional statistics, and include
the classic linear regression and analysis of variance models.
DeÔ¨Ånition 2.3: Normal Linear Model
In a normal linear model normal linear
modelthe response Ydepends on a p-dimensional explanatory
variable x=[x1;:::; xp]>, via the linear relationship
Y=x>+"; (2.34)
where"N(0;2).
Thus, a normal linear model is a linear model (in the sense of DeÔ¨Ånition 2.1) with
normal error terms. Similar to (2.30), the corresponding normal linear model for the whole
training setf(xi;Yi)ghas the form
Y=X+"; (2.35)
where Xis the model matrix comprised of rows x>
1;:::; x>
nand"N(0;2In). Con-
sequently, Ycan be written as Y=X+Z, where ZN(0;In), so that YN(X;2In).
It follows from (2.33) that its joint density is given by +45
g(yj;2;X)=(22) n
2e 1
22jjy Xjj2: (2.36)
Estimation of the parameter can be performed via the least-squares method, as discussed
in Example 2.1. An estimate can also be obtained via the maximum likelihood method.
This simply means Ô¨Ånding the parameters 2andthat maximize the likelihood of the
outcome y, given by the right-hand side of (2.36). It is clear that for every value of 2
the likelihood is maximal when ky Xk2is minimal. As a consequence, the maximum
likelihood estimate for is the same as the least-squares estimate (2.15). We leave it as an
exercise (see Exercise 18) to show that the maximum likelihood estimate of 2is equal to +63
c2=ky Xbk2
n; (2.37)
wherebis the maximum likelihood estimate (least squares estimate in this case) of .Chapter 2. Statistical Learning 47
2.9 Bayesian Learning
In Bayesian unsupervised learning, we seek to approximate the unknown joint density
f(x1;:::; xn) of the training data Tn=fX1;:::; Xngvia a joint pdf of the form
Z0BBBBB@nY
i=1g(xij)1CCCCCAw() d; (2.38)
where g(j) belongs to a family of parametric densities Gp:=fg(j);2g(viewed
as a family of pdfs conditional on a parameter in some set Rp) and w() is a pdf
that belongs to a (possibly di erent) family of densities W p. Note how the joint pdf (2.38)
satisÔ¨Åes the permutational invariance (2.26) and can thus be useful as a model for training
data which is part of an exchangeable sequence of random variables.
Following standard practice in a Bayesian context, instead of writing fX(x) and
fXjY(xjy) for the pdf of Xand the conditional pdf of Xgiven Y, one simply writes
f(x) and f(xjy). IfYis a di erent random variable, its pdf (at y) is thus denoted by
f(y).
Thus, we will use the same symbol gfor di erent (conditional) approximating probab-
ility densities and ffor the di erent (conditional) true and unknown probability densities.
Using Bayesian notation, we can write g(j)=Qn
i=1g(xij) and thus the approximating
joint pdf (2.38) can then be written asR
g(j)w() dand the true unknown joint pdf as
f()=f(x1;:::; xn).
OnceGpandW pare speciÔ¨Åed, selecting an approximating function g(x) of the form
g(x)=Z
g(xj)w() d
is equivalent to selecting a suitable wfromW p. Similar to (2.27), we can use the Kullback‚Äì
Leibler risk to measure the discrepancy between the proposed approximation (2.38) and the
true f():
`(g)=Elnf(T)R
g(Tj)w() d=Z
f() lnf()R
g(j)w() dd: (2.39)
The main di erence with (2.27) is that since the training data is not necessarily iid (it may
be exchangeable, for example), the expectation must be with respect to the joint density of + 40
T, not with respect to the marginal f(x) (as in the iid case).
Minimizing the training loss is equivalent to maximizing the likelihood of the training
data; that is, solving the optimization problem
max
w2W pZ
g(j)w() d;
where the maximization is over an appropriate class W pof density functions that is be-
lieved to result in the smallest KL risk.48 2.9. Bayesian Learning
Suppose that we have a rough guess, denoted w0(), for the best w2W pthat min-
imizes the Kullback‚ÄìLeibler risk. We can always increase the resulting likelihood L0:=R
g(j)w0() dby instead using the density w1() :=w0()g(j)=L0, giving a likeli-
hood L1:=R
g(j)w1() d. To see this, write L0andL1as expectations with respect to
w0. In particular, we can write
L0=Ew0g(j) and L1=Ew1g(j)=Ew0g2(j)=L0:
It follows that
L1 L0=1
L0Ew0h
g2(j) L2
0i
=1
L0Varw0[g(j)]>0: (2.40)
We may thus expect to obtain better predictions using w1instead of w0, because w1has
taken into account the observed data and increased the likelihood of the model. In fact,
if we iterate this process (see Exercise 20) and create a sequence of densities w1;w2;:::
such that wt()_wt 1()g(j), then wt() concentrates more and more of its probability
mass at the maximum likelihood estimator b(see (2.28)) and in the limit equals a (degen-
erate) point-mass pdf at b. In other words, in the limit we recover the maximum likelihood
method: g(x)=g(xjb). Thus, unless the class of densities W pis restricted to be non-
degenerate, maximizing the likelihood as much as possible leads to a degenerate choice
forw().
In many situations, the maximum likelihood estimate g(jb) is either not an ap-
propriate approximation to f() (see Example 2.9), or simply fails to exist (see Exer-
cise 10 in Chapter 4). In such cases, given an initial non-degenerate guess w0()=g(), +161
one can obtain a more appropriate and non-degenerate approximation to f() by taking
w()=w1()_g(j)g() in (2.38), giving the following Bayesian learner of f(x):
g(x) :=Z
g(xj)g(j)g()R
g(j#)g(#) d#d; (2.41)
whereR
g(j#)g(#) d#=g(). Using Bayes‚Äô formula for probability densities, +428
g(j)=g(j)g()
g(); (2.42)
we can write w1()=g(j). With this notation, we have the following deÔ¨Ånitions.
DeÔ¨Ånition 2.4: Prior, Likelihood, and Posterior
LetandGp:=fg(j);2gbe the training set and family of approximating
functions.
¬àA pdf g() that reÔ¨Çects our a priori beliefs about is called the prior prior pdf.
¬àThe conditional pdf g(j) is called the likelihood likelihood .
¬àInference about is given by the posterior posterior pdfg(j), which is proportional
to the product of the prior and the likelihood:
g(j)/g(j)g():Chapter 2. Statistical Learning 49
Remark 2.1 (Early Stopping) Bayes iteration is an example of an ‚Äúearly stopping‚Äù
heuristic for maximum likelihood optimization, where we exit after only one step. As ob-
served above, if we keep iterating, we obtain the maximum likelihood estimate (MLE). In
a sense the Bayes rule provides a regularization of the MLE. Regularization is discussed in
more detail in Chapter 6; see also Example 2.9. The early stopping rule is also of beneÔ¨Åt
in regularization; see Exercise 20 in Chapter 6.
On the one hand, the initial guess g() conveys the a priori (prior to training the
Bayesian learner) information about the optimal density in W pthat minimizes the KL risk.
Using this prior g(), the Bayesian approximation to f(x) is the prior predictive density prior predictive
density:
g(x)=Z
g(xj)g() d:
On the other hand, the posterior pdf conveys improved knowledge about this optimal dens-
ity inW pafter training with . Using the posterior g(j), the Bayesian learner of f(x) is
theposterior predictive density posterior
predictive
density:
g(x)=g(xj)=Z
g(xj)g(j) d;
where we have assumed that g(xj;)=g(xj); that is, the likelihood depends on only
through the parameter .
The choice of the prior is typically governed by two considerations:
1. the prior should be simple enough to facilitate the computation or simulation of the
posterior pdf;
2. the prior should be general enough to model ignorance of the parameter of interest.
Priors that do not convey much knowledge of the parameter are said to be uninformat-
ive. The uniform or Ô¨Çatprior in Example 2.9 (to follow) is frequently used.uninformative
prior
For the purpose of analytical and numerical computations, we can view as a ran-
dom vector with prior density g(), which after training is updated to the posterior
density g(j).
The above thinking allows us to write g(xj)_R
g(xj)g(j)g() d, for example,
thus ignoring any constants that do not depend on the argument of the densities.
Example 2.7 (Normal Model) Suppose that the training data T=fX1;:::; Xngis
modeled using the likelihood g(xj) that is the pdf of
XjN(;2);
where:=[;2]>. Next, we need to specify the prior distribution of to complete
the model. We can specify prior distributions for and2separately and then take their
product to obtain the prior for vector (assuming independence). A possible prior distri-
bution foris
N(;2): (2.43)50 2.9. Bayesian Learning
It is typical to refer to any parameters of the prior density as hyperparameters hyperparamet -
ersof the
Bayesian model. Instead of giving directly a prior for 2(or), it turns out to be con-
venient to give the following prior distribution to 1 =2:
1
2Gamma (;): (2.44)
The smaller andare, the less informative is the prior. Under this prior, 2is said to have
aninverse gamma inverse gamma3distribution. If 1 =ZGamma (;), then the pdf of Zis proportional
to exp ( =z)=z+1(Exercise 19). The Bayesian posterior is then given by: +63
g(;2j)_g()g(2)g(j;2)
_exp(
 ( )2
22)
expn
 =2o
(2)+1expn
 P
i(xi )2=(22)o
(2)n=2
_(2) n=2  1exp(
 ( )2
22 
2 ( xn)2+S2
n
22=n)
;
where S2
n:=1
nP
ix2
i x2
n=1
nP
i(xi xn)2is the (scaled) sample variance. All inference
about (;2) is then represented by the posterior pdf. To facilitate computations it is helpful
to Ô¨Ånd out if the posterior belongs to a recognizable family of distributions. For example,
the conditional pdf of given2andis
g(j2;)_exp(
 ( )2
22 ( xn)2
22=n)
;
which after simpliÔ¨Åcation can be recognized as the pdf of
(j2;)N
nxn+(1 n);  n2=n
; (2.45)
where we have deÔ¨Åned the weight parameter: n:=n
2.
1
2+n
2
:We can then see that the
posterior mean E[j2;]=nxn+(1 n)is a weighted linear combination of the prior
meanand the sample average xn. Further, as n!1 , the weight n!1 and thus the
posterior mean approaches the maximum likelihood estimate xn.
It is sometimes possible to use a prior g() that is not a bona Ô¨Åde probability density, in the
sense thatR
g() d=1, as long as the resulting posterior g(j)_g(j)g() is a proper
pdf. Such a prior is called an improper prior improper prior .
Example 2.8 (Normal Model (cont.)) An example of an improper prior is obtained
from (2.43) when we let !1 (the larger is, the more uninformative is the prior).
Then, g()_1 is a Ô¨Çat prior, butR
g() d=1, making it an improper prior. Neverthe-
less, the posterior is a proper density, and in particular the conditional posterior of ( j2;)
simpliÔ¨Åes to
(j2;)N
xn;2=n
;
3Reciprocal gamma distribution would have been a better name.Chapter 2. Statistical Learning 51
because the weight parameter ngoes to 1 as !1 . The improper prior g()_1 also
allows us to simplify the posterior marginal for 2:
g(2j)=Z
g(;2j) d_(2) (n 1)=2  1exp(
 +nS2
n=2
2)
;
which we recognize as the density corresponding to
1
2Gamma 
+n 1
2; +n
2S2
n!
:
In addition to g()_1, we can also use an improper prior for 2. If we take the limit !0
and!0 in (2.44), then we also obtain the improper prior g(2)_1=2(or equivalently
g(1=2)_1=2). In this case, the posterior marginal density for 2implies that:
nS2
n
22
n 1
and the posterior marginal density for implies that:
 xn
Sn=p
n 1tn 1: (2.46)
In general, deriving a simple formula for the posterior density of is either impossible
or too tedious. Instead, the Monte Carlo methods in Chapter 3 can be used to simulate
(approximately) from the posterior for the purposes of inference and prediction.
One way in which a distributional result such as (2.46) can be useful is in the construc-
tion of a 95% credible interval credible
intervalIfor the parameter ; that is, an interval Isuch that the
probabilityP[2Ij] is equal to 0 :95. For example, the symmetric 95% credible interval
is
I="
xn Snp
n 1;xn+Snp
n 1#
;
whereis the 0:975-quantile of the tn 1distribution. Note that the credible interval is
not a random object and that the parameter is interpreted as a random variable with a
distribution. This is unlike the case of classical conÔ¨Ådence intervals, where the parameter
is nonrandom, but the interval is (the outcome of) a random object. +457
As a generalization of the 95% Bayesian credible interval we can deÔ¨Åne a 1  credible
region credible region , which is any setRsatisfying
P[2Rj]=Z
2Rg(j) d>1 : (2.47)52 2.9. Bayesian Learning
Example 2.9 (Bayesian Regularization of Maximum Likelihood) Consider model-
ing the number of deaths during birth in a maternity ward. Suppose that the hospital data
consists of=fx1;:::; xng, with xi=1 if the i-th baby has died during birth and xi=0
otherwise, for i=1;:::; n. A possible Bayesian model for the data is U(0;1) (uniform
prior) with ( X1;:::; Xnj)iidBer(). The likelihood is therefore
g(j)=nY
i=1xi(1 )1 xi=s(1 )n s;
where s=x1++xnis the total number of deaths. Since g()=1, the posterior pdf is
g(j)_s(1 )n s; 2[0;1];
which is the pdf of the Beta (s+1;n s+1) distribution. The normalization constant is
(n+1)n
s
. The posterior pdf is shown in Figure 2.14 for ( s;n)=(0;100). It is not di cult
Figure 2.14: Posterior pdf for , with n=100 and s=0.
to see that the maximum a posteriori maximum a
posteriori(MAP) estimate of (the mode or maximizer of the
posterior density) is
argmax
g(j)=s
n;
which agrees with the maximum likelihood estimate. Figure 2.14 also shows that the left
one-sided 95% credible interval for is [0;0:0292], where 0 :0292 is the 0.95 quantile
(rounded) of the Beta (1;101) distribution.
Observe that when ( s;n)=(0;100) the maximum likelihood estimate b=0 infers that
deaths at birth are not possible. We know that this inference is wrong ‚Äî the probability of
death can never be zero, it is simply (and fortunately) too small to be inferred accurately
from a sample size of n=100. In contrast to the maximum likelihood estimate, the pos-
terior mean E[j]=(s+1)=(n+2) is not zero for ( s;n)=(0;100) and provides the more
reasonable point estimate of 0 :0098 for the probability of death.Chapter 2. Statistical Learning 53
In addition, while computing a Bayesian credible interval poses no conceptual di -
culties, it is not simple to derive a conÔ¨Ådence interval for the maximum likelihood estimate
ofb, because the likelihood as a function of is not di erentiable at =0. As a result of
this lack of smoothness, the usual conÔ¨Ådence intervals based on the normal approximation
cannot be used.
We now return to the unsupervised learning setting of Section 2.6, but consider this
from a Bayesian perspective. Recall from (2.39) that the Kullback‚ÄìLeibler risk for an ap-
proximating function gis
`(g)=Z
f(0
n)[lnf(0
n) lng(0
n)] d0
n;
where0
ndenotes the test data. SinceR
f(0
n) lnf(0
n) d0
nplays no role in minimizing the
risk, we consider instead the cross-entropy risk , deÔ¨Åned as +122
`(g)= Z
f(0
n) lng(0
n) d0
n:
Note that the smallest possible cross-entropy risk is `
n= R
f(0
n) lnf(0
n) d0
n. The expec-
ted generalization risk of the Bayesian learner can then be decomposed as
E`(gTn)=`
n+Z
f(0
n) lnf(0
n)
Eg(0
njTn)d0
n
|                            {z                            }
‚Äúbias‚Äù component+EZ
f(0
n) lnEg(0
njTn)
g(0
njTn)d0
n
|                               {z                               }
‚Äúvariance‚Äù component;
where gTn(0
n)=g(0
njTn)=R
g(0
nj)g(jTn) dis the posterior predictive density after
observingTn.
Assuming that the sets TnandT0
nare comprised of 2 niid random variables with density
f, we can show (Exercise 23) that the expected generalization risk simpliÔ¨Åes to
E`(gTn)=Elng(Tn) Elng(T2n); (2.48)
where g(n) and g(2n) are the prior predictive densities of nand2n, respectively.
Letn=argmaxg(jTn) be the MAP estimator of :=argmaxElng(Xj). As-
suming that nconverges to (with probability one) and1
nElng(Tnjn)=Elng(Xj)+
O(1=n), we can use the following large-sample approximation of the expected generaliza-
tion risk.
Theorem 2.4: Approximating the Bayesian Cross-Entropy Risk
Forn!1 , the expected cross-entropy generalization risk satisÔ¨Åes:
E`(gTn)' Elng(Tn) p
2lnn; (2.49)
where (with pthe dimension of the parameter vector andnthe MAP estimator):
Elng(Tn)'Elng(Tnjn) p
2lnn: (2.50)54 2.9. Bayesian Learning
Proof: To show (2.50), we apply Theorem C.21 to lnR
e nrn()g() d, where +450
rn() := 1
nlng(Tnj)= 1
nnX
i=1lng(Xij)a:s: ! Elng(Xj)=:r()<1:
This gives (with probability one)
lnZ
g(Tnj)g() d' nr() p
2ln(n):
Taking expectations on both sides and using nr()=nE[rn(n)]+O(1), we deduce (2.50).
To demonstrate (2.49), we derive the asymptotic approximation of Elng(T2n) by repeating
the argument for (2.50), but replacing nwith 2 n, where necessary. Thus, we obtain:
Elng(T2n)' 2nr() p
2ln(2n):
Then, (2.49) follows from the identity (2.48). 
The results of Theorem 2.4 have two major implications for model selection and assess-
ment. First, (2.49) suggests that  lng(Tn) can be used as a crude (leading-order) asymp-
totic approximation to the expected generalization risk for large nand Ô¨Åxed p. In this
context, the prior predictive density g(Tn) is usually called the model evidence model evidence ormarginal
likelihood for the classGp. Since the integralR
g(Tnj)g() dis rarely available in closed
form, the exact computation of the model evidence is typically not feasible and may require
Monte Carlo estimation methods. +78
Second, when the model evidence is di cult to compute via Monte Carlo methods or
otherwise, (2.50) suggests that we can use the following large-sample approximation:
 2Elng(Tn)' 2 lng(Tnjn)+pln(n): (2.51)
The asymptotic approximation on the right-hand side of (2.51) is called the Bayesian in-
formation criterion Bayesian
information
criterion(BIC). We prefer the class Gpwith the smallest BIC. The BIC is typic-
ally used when the model evidence is di cult to compute and nis suciently larger than
p. For a Ô¨Åxed p, and as nbecomes larger and larger, the BIC becomes a more and more
accurate estimator of  2Elng(Tn). Note that the BIC approximation is valid even when the
true density f<Gp. The BIC provides an alternative to the Akaike information criterion
(AIC) for model selection. However, while the BIC approximation does not assume that +126
the true model fbelongs to the parametric class under consideration, the AIC assumes
that f2G p. Thus, the AIC is merely a heuristic approximation based on the asymptotic
approximations in Theorem 4.1.
Although the above Bayesian theory has been presented in an unsupervised learn-
ing setting, it can be readily extended to the supervised case. We only need to relabel
the training setTn. In particular, when (as is typical for regression models) the train-
ing responses Y1;:::; Ynare considered as random variables but the corresponding fea-
ture vectors x1;:::; xnare viewed as being Ô¨Åxed, then Tnis the collection of random re-
sponsesfY1;:::; Yng. Alternatively, we can simply identify Tnwith the response vector
Y=[Y1;:::; Yn]>. We will adopt this notation in the next example.Chapter 2. Statistical Learning 55
Example 2.10 (Polynomial Regression (cont.)) Consider Example 2.2 once again, but
now in a Bayesian framework, where the prior knowledge on ( 2;) is speciÔ¨Åed by
g(2)=1=2andj2N(0;2D), and Dis a (matrix) hyperparameter. Let :=
(X>X+D 1) 1. Then the posterior can be written as:
g(;2jy)=exp
 ky Xk2
22
(22)n=2exp
 >D 1
22
(22)p=2jDj1=21
2,
g(y)
=(2) (n+p)=2 1
(2)(n+p)=2jDj1=2exp 
 k 1=2( )k2
22 (n+p+2)2
22!,
g(y);
where:=X>yand2:=y>(I XX>)y=(n+p+2) are the MAP estimates of and
2, and g(y) is the model evidence for Gp:
g(y)="
g(;2;y) dd2
=jj1=2
(2)n=2jDj1=2Z1
0exp
 (n+p+2)2
22
(2)n=2+1d2
=jj1=2 (n=2)
jDj1=2((n+p+2)2)n=2:
Therefore, based on (2.49), we have
2E`(gTn)' 2 lng(y)=nlnh
(n+p+2)2i
 2 ln (n=2)+lnjDj lnjj:
On the other hand, the minus of the log-likelihood of Ycan be written as
 lng(yj;2)=ky Xk2
22+n
2ln(22)
=k 1=2( )k2
22+(n+p+2)2
22+n
2ln(22):
Therefore, the BIC approximation (2.51) is
 2 lng(yj;2)+(p+1) ln( n)=n[ln(22)+1]+(p+1) ln( n)+(p+2); (2.52)
where the extra ln( n) term in ( p+1) ln( n) is due to the inclusion of 2in=(2;).
Figure 2.15 shows the model evidence and its BIC approximation, where we used a hyper-
parameter D=104Ipfor the prior density of . We can see that both approximations
exhibit a pronounced minimum at p=4, thus identifying the true polynomial regression
model. Compare the overall qualitative shape of the cross-entropy risk estimate with the
shape of the square-error risk estimate in Figure 2.11.56 2.9. Bayesian Learning
123456789 1 0600650700750800
Figure 2.15: The BIC and marginal likelihood used for model selection.
It is possible to give the model complexity parameter pa Bayesian treatment, in which
we deÔ¨Åne a prior density on the set of all models under consideration. For example, let
g(p);p=1;:::; mbe a prior density on mcandidate models. Treating the model com-
plexity index pas an additional parameter to 2Rp, and applying Bayes‚Äô formula, the
posterior for ( ;p) can be written as:
g(;pj)=g(jp;)g(pj)
=g(j;p)g(jp)
g(jp)|               {z               }
posterior of given model pg(jp)g(p)
g()|        {z        }
posterior of model p:
The model evidence for a Ô¨Åxed pis now interpreted as the prior predictive density of ,
conditional on the model p:
g(jp)=Z
g(j;p)g(jp) d;
and the quantity g()=Pm
p=1g(jp)g(p) is interpreted as the marginal likelihood of all the
mcandidate models. Finally, a simple method for model selection is to pick the index bp
with the largest posterior probability:
bp=argmax
pg(pj)=argmax
pg(jp)g(p):
Example 2.11 (Polynomial Regression (cont.)) Let us revisit Example 2.10 by giving
the parameter p=1;:::; m, with m=10, a Bayesian treatment. Recall that we used the
notation=yin that example. We assume that the prior g(p)=1=mis Ô¨Çat and uninform-
ative so that the posterior is given by
g(pjy)_g(yjp)=jj1=2 (n=2)
jDj1=2((n+p+2)2)n=2;Chapter 2. Statistical Learning 57
where all quantities in g(yjp) are computed using the Ô¨Årst pcolumns of X. Figure 2.16
shows the resulting posterior density g(pjy). The Ô¨Ågure also shows the posterior density
bg(yjp)P10
p=1bg(yjp);where
bg(yjp) :=exp 
 n[ln(22)+1]+(p+1) ln( n)+(p+2)
2!
is derived from the BIC approximation (2.52). In both cases, there is a clear maximum at
p=4, suggesting that a third-degree polynomial is the most appropriate model for the
data.
1 2 3 4 5 6 7 8 9 1000.20.40.60.81
Figure 2.16: Posterior probabilities for each polynomial model of degree p 1.
Suppose that we wish to compare two models, say model p=1 and model p=2.
Instead of computing the posterior g(pj) explicitly, we can compare the posterior odds
ratio:
g(p=1j)
g(p=2j)=g(p=1)
g(p=2)g(jp=1)
g(jp=2)|        {z        }
Bayes factor B1j2:
This gives rise to the Bayes factor Bayes factor Bijj, whose value signiÔ¨Åes the strength of the evidence
in favor of model iover model j. In particular Bijj>1 means that the evidence in favor for
model iis larger.
Example 2.12 (Savage‚ÄìDickey Ratio) Suppose that we have two models. Model p=
2 has a likelihood g(j;;p=2), depending on two parameters. Model p=1 has the
same functional form for the likelihood but now is Ô¨Åxed to some (known) 0; that
is,g(j;p=1)=g(j;=0;p=2). We also assume that the prior information on 58 Exercises
for model 1 is the same as that for model 2, conditioned on =0. That is, we assume
g(jp=1)=g(j=0;p=2). As model 2 contains model 1 as a special case, the latter
is said to be nested inside model 2. We can formally write (see also Exercise 26):
g(jp=1)=Z
g(j;p=1)g(jp=1) d
=Z
g(j;=0;p=2)g(j=0;p=2) d
=g(j=0;p=2)=g(;=0jp=2)
g(=0jp=2):
Hence, the Bayes factor simpliÔ¨Åes to
B1j2=g(jp=1)
g(jp=2)=g(;=0jp=2)
g(=0jp=2)
g(jp=2)=g(=0j;p=2)
g(=0jp=2):
In other words, B1j2is the ratio of the posterior density to the prior density of , evaluated at
=0and both under the unrestricted model p=2. This ratio of posterior to prior densities
is called the Savage‚ÄìDickey density ratio Savage ‚ÄìDickey
density ratio.
Whether to use a classical (frequentist) or Bayesian model is largely a question of con-
venience. Classical inference is useful because it comes with a huge repository of ready-
to-use results, and requires no (subjective) prior information on the parameters. Bayesian
models are useful because the whole theory is based on the elegant Bayes‚Äô formula, and
uncertainty in the inference (e.g., conÔ¨Ådence intervals) can be quantiÔ¨Åed much more nat-
urally (e.g., credible intervals). A usual practice is to ‚ÄúBayesify‚Äù a classical model, simply
by adding some prior information on the parameters.
Further Reading
A popular textbook on statistical learning is [55]. Accessible treatments of mathematical
statistics can be found, for example, in [69], [74], and [124]. More advanced treatments
are given in [10], [25], and [78]. A good overview of modern-day statistical inference
is given in [36]. Classical references on pattern classiÔ¨Åcation and machine learning are
[12] and [35]. For advanced learning theory including information theory and Rademacher
complexity, we refer to [28] and [109]. An applied reference for Bayesian inference is [46].
For a survey of numerical techniques relevant to computational statistics, see [90].
Exercises
1. Suppose that the loss function is the piecewise linear function
Loss( y;by)=(by y)++(y by)+; ;> 0;
where c+is equal to cifc>0, and zero otherwise. Show that the minimizer of the risk
`(g)=ELoss( Y;g(X)) satisÔ¨Åes
P[Y<g(x)jX=x]=
+:
In other words, g(x) is the=(+) quantile of Y, conditional on X=x.Chapter 2. Statistical Learning 59
2. Show that, for the squared-error loss, the approximation error `(gG) `(g) in (2.16), is
equal toE(gG(X) g(X))2. [Hint: expand `(gG)=E(Y g(X)+g(X) gG(X))2.]
3. SupposeGis the class of linear functions. A linear function evaluated at a feature xcan
be described as g(x)=>xfor some parameter vector of appropriate dimension. Denote
gG(x)=x>GandgG
(x)=x>b. Show that
E
gG
(X) g(X)2=E
X>b X>G2+E
X>G g(X)2:
Hence, deduce that the statistical error in (2.16) is `(gG
) `(gG)=E(gG
(X) gG(X))2.
4. Show that formula (2.24) holds for the 0‚Äì1 loss with 0‚Äì1 response.
5. Let Xbe an n-dimensional normal random vector with mean vector and covariance
matrix , where the determinant of is non-zero. Show that Xhas joint probability density
fX(x)=1p(2)njje 1
2(x )> 1(x );x2Rn:
6. Letb=A+y. Using the deÔ¨Åning properties of the pseudo-inverse, show that for any +360
2Rp,
kAb yk6kA yk:
7. Suppose that in the polynomial regression Example 2.1 we select the linear class of
functionsGpwith p>4. Then, g2G pand the approximation error is zero, because
gGp(x)=g(x)=x>, where=[10; 140;400; 250;0;:::; 0]>2Rp. Use the tower
property to show that the learner g(x)=x>bwithb=X+y, assuming rank( X)>4, is +431
unbiased unbiased :
EgT(x)=g(x):
8. (Exercise 7 continued.) Observe that the learner gTcan be written as a linear combina-
tion of the response variable: gT(x)=x>X+Y. Prove that for any learner of the form x>Ay,
where A2Rpnis some matrix and that satisÔ¨Åes EX[x>AY]=g(x), we have
VarX[x>X+Y]6VarX[x>AY];
where the equality is achieved for A=X+. This is called the Gauss‚ÄìMarkov inequality Gauss‚ÄìMarkov
inequality.
Hence, using the Gauss‚ÄìMarkov inequality deduce that for the unconditional variance:
VargT(x)6Var[x>AY]:
Deduce that A=X+also minimizes the expected generalization risk.
9. Consider again the polynomial regression Example 2.1. Use the fact that EXb=X+h(u),
where h(u)=E[YjU=u]=[h(u1);:::; h(un)]>, to show that the expected in-sample
risk is:
EX`in(gT)=`+kh(u)k2 kXX+h(u)k2
n+`p
n:
Also, use Theorem C.2 to show that the expected statistical error is: +430
EX(b )>Hp(b )=`tr(X+(X+)>Hp)+(X+h(u) )>Hp(X+h(u) ):60 Exercises
10. Consider the setting of the polynomial regression in Example 2.2. Use Theorem C.19
to prove that +449
pn(bn p)d !N
0; `H 1
p+H 1
pMpH 1
p
; (2.53)
where Mp:=E[XX>(g(X) gGp(X))2] is the matrix with ( i;j)-th entry:
Z1
0ui+j 2(hHp(u) h(u))2du;
andH 1
pis the pp inverse Hilbert matrix inverse Hilbert
matrixwith ( i;j)-th entry:
( 1)i+j(i+j 1) p+i 1
p j! p+j 1
p i! i+j 2
i 1!2
:
Observe that Mp=0forp>4, so that the matrix Mpterm is due to choosing a restrictive
classGpthat does not contain the true prediction function.
11. In Example 2.2 we saw that the statistical error can be expressed (see (2.20)) as
Z1
0
[1;:::; up 1](b p)2du=(b p)>Hp(b p):
By Exercise 10 the random vector Zn:=pn(bn p) has asymptotically a multivariate
normal distribution with mean vector 0and covariance matrix V:=`H 1
p+H 1
pMpH 1
p.
Use Theorem C.2 to show that the expected statistical error is asymptotically +430
E(b p)>Hp(b p)'`p
n+tr(MpH 1
p)
n;n!1: (2.54)
Plot this large-sample approximation of the expected statistical error and compare it with
the outcome of the statistical error.
We note a subtle technical detail: In general, convergence in distribution does not imply
convergence in Lp-norm (see Example C.6), and so here we have implicitly assumed that +442
kZnkd !Dist:)kZnkL2 !constant : =lim n"1EkZnk.
12. Consider again Example 2.2. The result in (2.53) suggests that Eb!pasn!1 ,
wherepis the solution in the class Gpgiven in (2.18). Thus, the large-sample approxim-
ation of the pointwise bias of the learner gGp
T(x)=x>batx=[1;:::; up 1]>is
EgGp
T(x) g(x)'[1;:::; up 1]p [1;u;u2;u3];n!1:
Use Python to reproduce Figure 2.17, which shows the (large-sample) pointwise squared
bias of the learner for p2f1;2;3g. Note how the bias is larger near the endpoints u=0
andu=1. Explain why the areas under the curves correspond to the approximation errors.Chapter 2. Statistical Learning 61
0 0.2 0.4 0.6 0.8 1050100150200250
Figure 2.17: The large-sample pointwise squared bias of the learner for p=1;2;3. The
bias is zero for p>4.
13. For our running Example 2.2 we can use (2.53) to derive a large-sample approximation
of the pointwise variance of the learner gT(x)=x>bn. In particular, show that for large n
VargT(x)'`x>H 1
px
n+x>H 1
pMpH 1
px
n;n!1: (2.55)
Figure 2.18 shows this (large-sample) variance of the learner for di erent values of the
predictor uand model index p. Observe that the variance ultimately increases in pand that
it is smaller at u=1=2 than closer to the endpoints u=0 or u=1. Since the bias is also
9
7
5
1
2
0.05
3
34
0.5 10.95
Figure 2.18: The pointwise variance of the learner for various pairs of pandu.
larger near the endpoints, we deduce that the pointwise mean squared error (2.21) is larger
near the endpoints of the interval [0 ;1] than near its middle. In other words, the error is
much smaller in the center of the data cloud than near its periphery.62 Exercises
14. Let h:x7!Rbe a convex function and let Xbe a random variable. Use the subgradi-
ent deÔ¨Ånition of convexity to prove Jensen‚Äôs inequality : +403
Jensen ‚Äôs
inequality Eh(X)>h(EX): (2.56)
15. Using Jensen‚Äôs inequality, show that the Kullback‚ÄìLeibler divergence between prob-
ability densities fandgis always positive; that is,
Elnf(X)
g(X)>0;
where Xf.
16. The purpose of this exercise is to prove the following Vapnik‚ÄìChernovenkis bound Vapnik ‚Äì
Chernovenkis
bound: for
anyÔ¨Ånite classG(containing only a Ô¨Ånite number jGjof possible functions) and a general
bounded loss function, l6Loss6u, the expected statistical error is bounded from above
according to:
E`(gG
Tn) `(gG)6(u l)p2 ln(2jGj)pn: (2.57)
Note how this bound conveniently does not depend on the distribution of the training set
Tn(which is typically unknown), but only on the complexity (i.e., cardinality) of the class
G. We can break up the proof of (2.57) into the following four parts:
(a) For a general function class G, training setT, risk function `, and training loss `T,
we have, by deÔ¨Ånition, `(gG)6`(g) and`T(gG
T)6`T(g) for all g2G. Show that
`(gG
T) `(gG)6sup
g2Gj`T(g) `(g)j+`T(gG) `(gG);
where we used the notation sup (supremum) for the least upper bound. Since
E`T(g)=E`(g), we obtain, after taking expectations on both sides of the inequal-
ity above:
E`(gG
T) `(gG)6Esup
g2Gj`T(g) `(g)j:
(b) If Xis a zero-mean random variable taking values in the interval [ l;u], then the fol-
lowing Hoeding‚Äôs inequality Hoeffding ‚Äôs
inequalitystates that the moment generating function satisÔ¨Åes
EetX6exp t2(u l)2
8!
;t2R: (2.58)
Prove this result by using the fact that the line segment joining points ( l;exp(tl)) and
(u;exp(tu)) bounds the convex function x7!exp(tx) for x2[l;u]; that is:
etx6etlu x
u l+etux l
u l;x2[l;u]:
(c) Let Z1;:::; Znbe (possibly dependent and non-identically distributed) zero-mean ran-
dom variables with moment generating functions that satisfy Eexp(tZk)6exp(t22=2)
for all kand some parameter . Use Jensen‚Äôs inequality (2.56) to prove that for any +427Chapter 2. Statistical Learning 63
t>0,
Emax
kZk=1
tEln max
ketZk61
tlnn+t2
2:
From this derive that
Emax
kZk6p
2 lnn:
Finally, show that this last inequality implies that
Emax
kjZkj6p
2 ln(2 n): (2.59)
(d) Returning to the objective of this exercise, denote the elements of Gbyg1;:::; gjGj,
and let Zk=`Tn(gk) `(gk). By part (a) it is su cient to bound Emax kjZkj. Show that
thefZkgsatisfy the conditions of (c) with =(u l)=pn. For this you will need to
apply part (b) to the random variable Loss( g(X);Y) `(g), where ( X;Y) is a generic
data point. Now complete the proof of (2.57).
17. Consider the problem in Exercise 16a above. Show that
j`T(gG
T) `(gG)j62 sup
g2Gj`T(g) `(g)j+`T(gG) `(gG):
From this, conclude:
Ej`T(gG
T) `(gG)j62Esup
g2Gj`T(g) `(g)j:
The last bound allows us to assess how close the training loss `T(gG
T) is to the optimal risk
`(gG) within classG.
18. Show that for the normal linear model YN(X;2In), the maximum likelihood es-
timator of2is identical to the method of moments estimator (2.37).
19. Let XGamma (;). Show that the pdf of Z=1=Xis equal to
(z)  1e (z) 1
 ();z>0:
20. Consider the sequence w0;w1;:::, where w0=g() is a non-degenerate initial guess
andwt()_wt 1()g(j);t>1. We assume that g(j) is not the constant function (with
respect to) and that the maximum likelihood value
g(jb)=max
g(j)<1
exists (is bounded). Let
lt:=Z
g(j)wt() d:
Show thatfltgis a strictly increasing and bounded sequence. Hence, conclude that its limit
isg(jb).64 Exercises
21. Consider the Bayesian model for =fx1;:::; xngwith likelihood g(j) such that
(X1;:::; Xnj)iidN(;1) and prior pdf g() such that N(;1) for some hyperpara-
meter. DeÔ¨Åne a sequence of densities wt();t>2 via wt()_wt 1()g(j), start-
ing with w1()=g(). Let atand btdenote the mean and precision4ofunder the
posterior gt(j)_g(j)wt(). Show that gt(j) is a normal density with precision
bt=bt 1+n;b0=1 and mean at=(1 t)at 1+txn;a0=, wheret:=n=(bt 1+n).
Hence, deduce that gt(j) converges to a degenerate density with a point-mass at xn.
22. Consider again Example 2.8, where we have a normal model with improper prior
g()=g(;2)_1=2. Show that the prior predictive pdf is an improper density g(x)_1,
but that the posterior predictive density is
g(xj)_ 
1+(x xn)2
(n+1)S2
n! n=2
:
Deduce thatX xn
Snp(n+1)=(n 1)tn 1.
23. Assuming that X1;:::; Xniidf, show that (2.48) holds and that `
n= nElnf(X).
24. Suppose that =fx1;:::; xngare observations of iid continuous and strictly positive
random variables, and that there are two possible models for their pdf. The Ô¨Årst model
p=1 is
g(xj;p=1)=exp( x)
and the second p=2 is
g(xj;p=2)= 2
!1=2
exp 
 x2
2!
:
For both models, assume that the prior for is a gamma density
g()=bt
 (t)t 1exp( b);
with the same hyperparameters bandt. Find a formula for the Bayes factor, g(jp=
1)=g(jp=2), for comparing these models.
25. Suppose that we have a total of mpossible models with prior probabilities g(p);p=
1;:::; m. Show that the posterior probability of model g(pj) can be expressed in terms of
all the p(p 1) Bayes factors:
g(p=ij)=0BBBBBB@1+X
j,ig(p=j)
g(p=i)Bjji1CCCCCCA 1
:
4The precision is the reciprocal of the variance.Chapter 2. Statistical Learning 65
26. Given the data =fx1;:::; xng, suppose that we use the likelihood ( Xj)N(;2)
with parameter =(;2)>and wish to compare the following two nested models.
(a) Model p=1, where2=2
0is known and this is incorporated via the prior
g(jp=1)=g(j2;p=1)g(2jp=1)=1p
2e ( x0)2
22(2 2
0):
(b) Model p=2, where both mean and variance are unknown with prior
g(jp=2)=g(j2)g(2)=1p
2e ( x0)2
22bt(2) t 1e b=2
 (t):
Show that the prior g(jp=1) can be viewed as the limit of the prior g(jp=2) when
t!1 andb=t2
0. Hence, conclude that
g(jp=1)=lim
t!1
b=t2
0g(jp=2)
and use this result to calculate B1j2. Check that the formula for B1j2agrees with the Savage‚Äì
Dickey density ratio:
g(jp=1)
g(jp=2)=g(2=2
0j)
g(2=2
0);
where g(2j) and g(2) are the posterior and prior, respectively, under model p=2.66CHAPTER3
MONTE CARLO METHODS
Many algorithms in machine learning and data science make use of Monte Carlo
techniques. This chapter gives an introduction to the three main uses of Monte Carlo
simulation: to (1) simulate random objects and processes in order to observe their beha-
vior, (2) estimate numerical quantities by repeated sampling, and (3) solve complicated
optimization problems through randomized algorithms.
3.1 Introduction
BrieÔ¨Çy put, Monte Carlo simulation Monte Carlo
simulationis the generation of random data by means of a com-
puter. These data could arise from simple models, such as those described in Chapter 2,
or from very complicated models describing real-life systems, such as the positions of
vehicles on a complex road network, or the evolution of security prices in the stock mar-
ket. In many cases, Monte Carlo simulation simply involves random sampling from certain
probability distributions. The idea is to repeat the random experiment that is described by
the model many times to obtain a large quantity of data that can be used to answer questions
about the model. The three main uses of Monte Carlo simulation are:
Sampling. Here the objective is to gather information about a random object by observing
many realizations of it. For instance, this could be a random process that mimics the
behavior of some real-life system such as a production line or telecommunications
network. Another usage is found in Bayesian statistics, where Markov chains are
often used to sample from a posterior distribution. + 48
Estimation. In this case the emphasis is on estimating certain numerical quantities related
to a simulation model. An example is the evaluation of multidimensional integrals
via Monte Carlo techniques. This is achieved by writing the integral as the expecta-
tion of a random variable, which is then approximated by the sample mean. Appeal-
ing to the Law of Large Numbers guarantees that this approximation will eventually +446
converge when the sample size becomes large.
Optimization. Monte Carlo simulation is a powerful tool for the optimization of complic-
ated objective functions. In many applications these functions are deterministic and
6768 3.2. Monte Carlo Sampling
randomness is introduced artiÔ¨Åcially in order to more e ciently search the domain of
the objective function. Monte Carlo techniques are also used to optimize noisy func-
tions, where the function itself is random; for example, when the objective function
is the output of a Monte Carlo simulation.
The Monte Carlo method dramatically changed the way in which statistics is used in
today‚Äôs analysis of data. The ever-increasing complexity of data requires radically di erent
statistical models and analysis techniques from those that were used 20 to 100 years ago.
By using Monte Carlo techniques, the data analyst is no longer restricted to using basic
(and often inappropriate) models to describe data. Now, any probabilistic model that can
be simulated on a computer can serve as the basis for statistical analysis. This Monte Carlo
revolution has had an impact on both Bayesian and frequentist statistics. In particular, in
frequentist statistics, Monte Carlo methods are often referred to as resampling techniques.
An important example is the well-known bootstrap method [37], where statistical quantit-
ies such as conÔ¨Ådence intervals and P-values for statistical tests can simply be determined
by simulation without the need of a sophisticated analysis of the underlying probability
distributions; see, for example, [69] for basic applications. The impact on Bayesian statist-
ics has been even more profound, through the use of Markov chain Monte Carlo (MCMC)
techniques [87, 48]. MCMC samplers construct a Markov process which converges in dis-
tribution to a desired (often high-dimensional) density. This convergence in distribution
justiÔ¨Åes using a Ô¨Ånite run of the Markov process as an approximate random realization
from the target density. The MCMC approach has rapidly gained popularity as a versat-
ile heuristic approximation, partly due to its simple computer implementation and inbuilt
mechanism to tradeo between computational cost and accuracy; namely, the longer one
runs the Markov process, the better the approximation. Nowadays, MCMC methods are
indispensable for analyzing posterior distributions for inference and model selection; see
also [50, 99].
The following three sections elaborate on these three uses of Monte Carlo simulation
in turn.
3.2 Monte Carlo Sampling
In this section we describe a variety of Monte Carlo sampling methods, from the building
block of simulating uniform random numbers to MCMC samplers.
3.2.1 Generating Random Numbers
At the heart of any Monte Carlo method is a random number generator : a procedure thatrandom number
generator produces a stream of uniform random numbers on the interval (0,1). Since such numbers
are usually produced via deterministic algorithms, they are not truly random. However, for
most applications all that is required is that such pseudo-random numbers are statistically
indistinguishable from genuine random numbers U1;U2;:::that are uniformly distributed
on the interval (0,1) and are independent of each other; we write U1;U2;:::iidU(0;1).
For example, in Python the rand method of the numpy.random module is widely used for
this purpose.Chapter 3. Monte Carlo Methods 69
Most random number generators at present are based on linear recurrence relations.
One of the most important random number generators is the multiple-recursive generatormultiple -
recursive
generator(MRG) of order k , which generates a sequence of integers Xk;Xk+1;:::via the linear recur-
rence
Xt=(a1Xt 1++akXt k) mod m;t=k;k+1;::: (3.1)
for some modulus m andmultipliersfai;i=1;:::; kg. Here ‚Äúmod‚Äù refers to the modulo op-modulus
multipliers eration: nmod mis the remainder when nis divided by m. The recurrence is initialized by
specifying k‚Äúseeds‚Äù, X0;:::; Xk 1. To yield fast algorithms, all but a few of the multipliers
should be 0. When mis a large integer, one can obtain a stream of pseudo-random numbers
Uk;Uk+1;:::between 0 and 1 from the sequence Xk;Xk+1;:::, simply by setting Ut=Xt=m.
It is also possible to set a small modulus, in particular m=2. The output function for such
modulo 2 generators is then typically of the formmodulo 2
generators
Ut=wX
i=1Xtw+i 12 i
for some w6k, e.g., w=32 or 64. Examples of modulo 2 generators are the feedback shift
register generators, the most popular of which are the Mersenne twisters ; see, for example,feedback shift
register
Mersenne
twisters[79] and [83]. MRGs with excellent statistical properties can be implemented e ciently
by combining several simpler MRGs and carefully choosing their respective moduli and
multipliers. One of the most successful is L‚ÄôEcuyer‚Äôs MRG32k3a generator; see [77]. From
now on, we assume that the reader has a sound random number generator available.
3.2.2 Simulating Random Variables
Simulating a random variable Xfrom an arbitrary (that is, not necessarily uniform) distri-
bution invariably involves the following two steps:
1. Simulate uniform random numbers U1;:::; Ukon (0;1) for some k=1;2;:::.
2. Return X=g(U1;:::; Uk), where gis some real-valued function.
The construction of suitable functions gis as much of an art as a science. Many
simulation methods may be found, for example, in [71] and the accompanying website
www.montecarlohandbook.org . Two of the most useful general procedures for gen-
erating random variables are the inverse-transform method and the acceptance‚Äìrejection
method. Before we discuss these, we show one possible way to simulate standard normal
random variables. In Python we can generate standard normal random variables via the
randn method of the numpy.random module.
Example 3.1 (Simulating Standard Normal Random Variables) IfXandYare in-
dependent standard normally distributed random variables (that is, X;YiidN(0;1)), then
their joint pdf is
f(x;y)=1
2e 1
2(x2+y2);(x;y)2R2;
which is a radially symmetric function. In Example C.2 we see that, in polar coordin- +433
ates, the angle that the random vector [ X;Y]>makes with the positive x-axis is U(0;2)70 3.2. Monte Carlo Sampling
distributed (as would be expected from the radial symmetry) and the radius Rhas pdf
fR(r)=re r2=2;r>0. Moreover, Randare independent. We will see shortly, in Ex-
ample 3.4, that Rhas the same distribution asp
 2 lnUwith UU(0;1). So, to sim- +72
ulate X;YiidN(0;1), the idea is to Ô¨Årst simulate Randindependently and then return
X=Rcos() and Y=Rsin() as a pair of independent standard normal random variables.
This leads to the Box‚ÄìMuller approach for generating standard normal random variables.
Algorithm 3.2.1: Normal Random Variable Simulation: Box‚ÄìMuller Approach
output: Independent standard normal random variables XandY.
1Simulate two independent random variables, U1andU2, from U(0;1).
2X ( 2 lnU1)1=2cos(2U2)
3Y ( 2 lnU1)1=2sin(2U2)
4return X;Y
Once a standard normal number generator is available, simulation from any n-
dimensional normal distribution N(;) is relatively straightforward. The Ô¨Årst step is to
Ô¨Ånd an nnmatrix Bthat decomposes into the matrix product BB>. In fact there exist
many such decompositions. One of the more important ones is the Cholesky decomposition ,Cholesky
decomposition which is a special case of the LU decomposition; see Section A.6.1 for more information
+368 on such decompositions. In Python, the function cholesky ofnumpy.linalg can be used
to produce such a matrix B.
Once the Cholesky factorization is determined, it is easy to simulate XN(;) as,
by deÔ¨Ånition, it is the a ne transformation +BZof an n-dimensional standard normal
random vector.
Algorithm 3.2.2: Normal Random Vector Simulation
input:;
output: XN(;)
1Determine the Cholesky factorization =BB>.
2Simulate Z=[Z1;:::; Zn]>by drawing Z1;:::; ZniidN(0;1).
3X +BZ
4return X
Example 3.2 (Simulating from a Bivariate Normal Distribution) The Python code
below draws N=1000 iid samples from the two bivariate ( n=2) normal pdfs in Fig-
ure 2.13. The resulting point clouds are given in Figure 3.1. +45
bvnormal.py
import numpy as np
from numpy.random import randn
import matplotlib.pyplot as plt
N = 1000
r = 0.0 #change to 0.8 for other plot
Sigma = np.array([[1, r], [r, 1]])Chapter 3. Monte Carlo Methods 71
B = np.linalg.cholesky(Sigma)
x = B @ randn(2,N)
plt.scatter([x[0,:]],[x[1,:]], alpha =0.4, s = 4)
2
 0
 2
3
2
1
0
1
2
3
2
 0
 2
3
2
1
0
1
2
3
Figure 3.1: 1000 realizations of bivariate normal distributions with means zero, variances
1, and correlation coe cients 0 (left) and 0 :8 (right).
In some cases, the covariance matrix has special structure which can be exploited to
create even faster generation algorithms, as illustrated in the following example.
Example 3.3 (Simulating Normal Vectors in O(n2)Time) Suppose that the random
vector X=[X1;:::; Xn]>represents the values at times t0+k,k=0;:::; n 1 of a zero-
mean Gaussian process (X(t);t>0) that is weakly stationary , meaning that Cov(X(s);X(t)) +238
depends only on t s. Then clearly the covariance matrix of X, say An, is a symmetric Toep-
litz matrix. Suppose for simplicity that VarX(t)=1. Then the covariance matrix is in fact +379
a correlation matrix, and will have the following structure:
An:=266666666666666666666666641 a1::: an 2an 1
a1 1::: an 2
:::::::::::::::
an 2::::::a1
an 1an 2 a1 137777777777777777777777775:
Using the Levinson‚ÄìDurbin algorithm we can compute a lower diagonal matrix Lnand
a diagonal matrix DninO(n2) time such that LnAnL>
n=Dn; see Theorem A.14. If we +383
simulate ZnN(0;In), then the solution Xof the linear system:
LnX=D1=2
nZn
has the desired distribution N(0;An). The linear system is solved in O(n2) time via forward
substitution.72 3.2. Monte Carlo Sampling
3.2.2.1 Inverse-Transform Method
LetXbe a random variable with cumulative distribution function (cdf) F. Let F 1denote
the inverse1ofFandUU(0;1). Then,
P[F 1(U)6x]=P[U6F(x)]=F(x): (3.2)
This leads to the following method to simulate a random variable Xwith cdf F:
Algorithm 3.2.3: Inverse-Transform Method
input: Cumulative distribution function F.
output: Random variable Xdistributed according to F.
1Generate UfromU(0;1).
2X F 1(U)
3return X
The inverse-transform method works both for continuous and discrete distribu-
tions. After importing numpy asnp, simulating numbers 0 ;:::; k 1 according to
probabilities p0;:::; pk 1can be done via np.min(np.where(np.cumsum(p) >
np.random.rand())) , where pis the vector of the probabilities.
Example 3.4 (Example 3.1 (cont.)) One remaining issue in Example 3.1 was how to
simulate the radius Rwhen we only know its density fR(r)=re r2=2;r>0. We can use the
inverse-transform method for this, but Ô¨Årst we need to determine its cdf. The cdf of Ris,
by integration of the pdf,
FR(r)=1 e 1
2r2;r>0;
and its inverse is found by solving u=FR(r) in terms of r, giving
F 1
R(u)=p
 2 ln(1 u);u2(0;1):
Thus Rhas the same distribution asp 2 ln(1 U), with UU(0;1). Since 1 Ualso has
aU(0;1) distribution, Rhas also the same distribution asp
 2 lnU.
3.2.2.2 Acceptance‚ÄìRejection Method
The acceptance‚Äìrejection method is used to sample from a ‚Äúdi cult‚Äù probability density
function (pdf) f(x) by generating instead from an ‚Äúeasy‚Äù pdf g(x) satisfying f(x)6C g(x)
for some constant C>1 (for example, via the inverse-transform method), and then ac-
cepting or rejecting the drawn sample with a certain probability. Algorithm 3.2.4 gives the
pseudo-code.
The idea of the algorithm is to generate uniformly a point ( X;Y) under the graph of the
function Cg, by Ô¨Årst drawing Xgand then YU(0;Cg(X)). If this point lies under the
graph of f, then we accept Xas a sample from f; otherwise, we try again. The e ciency
of the acceptance‚Äìrejection method is usually expressed in terms of the probability of
acceptance, which is 1 =C.
1Every cdf has a unique inverse function deÔ¨Åned by F 1(u)=inffx:F(x)>ug. If, for each u, the
equation F(x)=uhas a unique solution x, this deÔ¨Ånition coincides with the usual interpretation of the
inverse function.Chapter 3. Monte Carlo Methods 73
Algorithm 3.2.4: Acceptance‚ÄìRejection Method
input: Pdfgand constant Csuch that Cg(x)>f(x) for all x.
output: Random variable Xdistributed according to pdf f.
1found false
2while not found do
3 Generate Xfrom g.
4 Generate UfromU(0;1) independently of X.
5 Y UCg (X)
6 ifY6f(X)then found true
7return X
Example 3.5 (Simulating Gamma Random Variables) Simulating random variables
from a Gamma (;) distribution is generally done via the acceptance‚Äìrejection method.
Consider, for example, the Gamma distribution with =1:3 and=5:6. Its pdf, +425
f(x)=x 1e x
 ();x>0;
where  is the gamma function  () :=R1
0e xx 1dx,>0, is depicted by the blue solid
curve in Figure 3.2.
0 0.5 1 1.5 2012345
Figure 3.2: The pdf gof the Exp(4) distribution multiplied by C=1:2 dominates the pdf f
of the Gamma (1:3;5:6) distribution.
This pdf happens to lie completely under the graph of Cg(x), where C=1:2 and
g(x)=4 exp( 4x);x>0 is the pdf of the exponential distribution Exp(4). Hence, we
can simulate from this particular Gamma distribution by accepting or rejecting a sample
from the Exp(4) distribution according to Step 6 of Algorithm 3.2.4. Simulating from the +425
Exp(4) distribution can be done via the inverse-transform method: simulate UU(0;1)
and return X= ln(U)=4. The following Python code implements Algorithm 3.2.4 for this
example.74 3.2. Monte Carlo Sampling
accrejgamma.py
from math import exp, gamma , log
from numpy.random import rand
alpha = 1.3
lam = 5.6
f = lambda x: lam**alpha * x**(alpha -1) * exp(-lam*x)/gamma(alpha)
g = lambda x: 4*exp(-4*x)
C = 1.2
found = False
while not found:
x = - log(rand())/4
ifC*g(x)*rand() <= f(x):
found = True
print (x)
3.2.3 Simulating Random Vectors and Processes
Techniques for generating random vectors and processes are as diverse as the class of
random processes themselves; see, for example, [71]. We highlight a few general scenarios.
When X1;:::; Xnareindependent random variables with pdfs fi,i=1;:::; n, so that
their joint pdf is f(x)=f1(x1)fn(xn), the random vector X=[X1;:::; Xn]>can be +429
simply simulated by drawing each component Xifiindividually ‚Äî for example, via the
inverse-transform method or acceptance‚Äìrejection.
Fordependent components X1;:::; Xn, we can, as a consequence of the product rule of
probability, represent the joint pdf f(x) as +431
f(x)=f(x1;:::; xn)=f1(x1)f2(x2jx1)fn(xnjx1;:::; xn 1); (3.3)
where f1(x1) is the marginal pdf of X1and fk(xkjx1;:::; xk 1) is the conditional pdf of Xk
given X1=x1;X2=x2;:::; Xk 1=xk 1. Provided the conditional pdfs are known, one can
generate Xby Ô¨Årst generating X1, then, given X1=x1, generate X2from f2(x2jx1), and so
on, until generating Xnfrom fn(xnjx1;:::; xn 1).
The latter method is particularly applicable for generating Markov chains. Recall from
Section C.10 that a Markov chain is a stochastic process fXt;t=0;1;2;:::gthat satisÔ¨Åes +451
Markov chain theMarkov property ; meaning that for all tandsthe conditional distribution of Xt+sgiven
Xu;u6t, is the same as that of Xt+sgiven only Xt. As a result, each conditional density
ft(xtjx1;:::; xt 1) can be written as a one-step transition density q t(xtjxt 1); that is, the
probability density to go from state xt 1to state xtin one step. In many cases of interest
the chain is time-homogeneous , meaning that the transition density qtdoes not depend on
t. Such Markov chains can be generated sequentially , as given in Algorithm 3.2.5.Chapter 3. Monte Carlo Methods 75
Algorithm 3.2.5: Simulate a Markov Chain
input: Number of steps N, initial pdf f0, transition density q.
1Draw X0from the initial pdf f0.
2fort=1toNdo
3 Draw Xtfrom the distribution corresponding to the density q(jXt 1)
4return X0;:::; XN
Example 3.6 (Markov Chain Simulation) For time-homogeneous Markov chains
with a discrete state space, we can visualize the one-step transitions by means of a trans-
ition graph transition
graph, where arrows indicate possible transitions between states and the labels de-
scribe the corresponding probabilities. Figure 3.3 shows (on the left) the transition graph
of the Markov chain fXt;t=0;1;2;:::gwith state spacef1;2;3;4gand one-step transition
matrix
P=2666666666666640 0:2 0:5 0:3
0:5 0 0:5 0
0:3 0:7 0 0
0:1 0 0 0 :9377777777777775:
1 2
3 40.2
0.70.5
0.90.50.3
0.50.3
0.1
0 20 40 60 80 1001234
Figure 3.3: The transition graph (left) and a typical path (right) of the Markov chain.
In the same Ô¨Ågure (on the right) a typical outcome (path) of the Markov chain is
shown. The path was simulated using the Python program below. In this implementation
the Markov chain always starts in state 1. We will revisit Markov chains, and in particular
Markov chains with continuous state spaces, in Section 3.2.5. + 78
MCsim.py
import numpy as np
import matplotlib.pyplot as plt
n = 101
P = np.array([[0, 0.2, 0.5, 0.3],
[0.5, 0, 0.5, 0],
[0.3, 0.7, 0, 0],
[0.1, 0, 0, 0.9]])
x = np.array(np.ones(n, dtype= int))
x[0] = 0
for tin range (0,n-1):76 3.2. Monte Carlo Sampling
x[t+1] = np. min(np.where(np.cumsum(P[x[t],:]) >
np.random.rand()))
x = x + 1 #add 1 to all elements of the vector x
plt.plot(np.array( range (0,n)),x, 'o')
plt.plot(np.array( range (0,n)),x, '--')
plt.show()
3.2.4 Resampling
The idea behind resampling is very simple: an iid sample :=fx1;:::; xngfrom someresamplingunknown cdf Frepresents our best knowledge of Fif we make no further a priori as-
sumptions about it. If it is not possible to simulate more samples from F, the best way to
‚Äúrepeat‚Äù the experiment is to resample from the original data by drawing from the empir-
ical cdf Fn; see (1.2). That is, we draw each xiwith equal probability and repeat this N +11
times, according to Algorithm 3.2.6 below. As we draw here ‚Äúwith replacement‚Äù, multiple
instances of the original data points may occur in the resampled data.
Algorithm 3.2.6: Sampling from an Empirical Cdf.
input: Original iid sample x1;:::; xnand sample size N.
output: Iid sample X
1;:::; X
Nfrom the empirical cdf.
1fort=1toNdo
2 Draw UU(0;1)
3 SetI dnUe
4 SetX
t xI
5return X
1;:::; X
N
In Step 3,dnUereturns the ceiling ofnU; that is, it is the smallest integer larger than
or equal to nU. Consequently, Iis drawn uniformly at random from the set of indices
f1;:::; ng.
By sampling from the empirical cdf we can thus (approximately) repeat the experiment
that gave us the original data as many times as we like. This is useful if we want to assess
the properties of certain statistics obtained from the data. For example, suppose that the
original data gave the statistic t(). By resampling we can gain information about the
distribution of the corresponding random variable t(T).
Example 3.7 (Quotient of Uniforms) LetU1;:::; Un;V1;:::; Vnbe iidU(0;1) random
variables and deÔ¨Åne Xi=Ui=Vi,i=1;:::; n. Suppose we wish to investigate the distribu-
tion of the sample median eXand sample mean Xof the (random) data T:=fX1;:::; Xng.
Since we know the model for Texactly, we can generate a large number, Nsay, of inde-
pendent copies of it, and for each of these copies evaluate the sample medians eX1;:::;eXN
and sample means X1;:::; XN. For n=100 and N=1000 the empirical cdfs might look
like the left and right curves in Figure 3.4, respectively. Contrary to what you might have
expected, the distributions of the sample median and sample mean do not match at all. The
sample median is quite concentrated around 1, whereas the distribution of the sample mean
is much more spread out.Chapter 3. Monte Carlo Methods 77
0 1 2 3 4 5 6 700.51
Figure 3.4: Empirical cdfs of the medians of the resampled data (left curve) and sample
means (right curve) of the resampled data.
Instead of sampling completely new data, we could also reuse the original data by
resampling them via Algorithm 3.2.6. This gives independent copies eX
1;:::;eX
Nand
X
1;:::; X
N, for which we can again plot the empirical cdf. The results will be similar
to the previous case. In fact, in Figure 3.4 the cdf of the resampled sample medians and
sample means are plotted. The corresponding Python code is given below. The essential
point of this example is that resampling of data can greatly add to the understanding of the
probabilistic properties of certain measurements on the data, even if the underlying model
is not known . See Exercise 12 for a further investigation of this example. +116
quotunif.py
import numpy as np
from numpy.random import rand , choice
import matplotlib.pyplot as plt
from statsmodels.distributions.empirical_distribution import ECDF
n = 100
N = 1000
x = rand(n)/rand(n) # data
med = np.zeros(N)
ave = np.zeros(N)
for iin range (0,N):
s = choice(x, n, replace=True) # resampled data
med[i] = np.median(s)
ave[i] = np.mean(s)
med_cdf = ECDF(med)
ave_cdf = ECDF(ave)
plt.plot(med_cdf.x, med_cdf.y)
plt.plot(ave_cdf.x, ave_cdf.y)
plt.show()78 3.2. Monte Carlo Sampling
3.2.5 Markov Chain Monte Carlo
Markov chain Monte Carlo (MCMC) is a Monte Carlo sampling technique for (approxim-Markov chain
Monte Carlo ately) generating samples from an arbitrary distribution ‚Äî often referred to as the target
target distribution. The basic idea is to run a Markov chain long enough such that its limiting
distribution is close to the target distribution. Often such a Markov chain is constructed to
be reversible, so that the detailed balance equations (C.43) can be used. Depending on the +453
starting position of the Markov chain, the initial random variables in the Markov chain may
have a distribution that is signiÔ¨Åcantly di erent from the target (limiting) distribution. The
random variables that are generated during this burn-in period burn -in period are often discarded. The
remaining random variables form an approximate anddependent sample from the target
distribution.
In the next two sections we discuss two popular MCMC samplers: the Metropolis‚Äì
Hastings sampler and the Gibbs sampler.
3.2.5.1 Metropolis‚ÄìHastings Sampler
The Metropolis‚ÄìHastings sampler [87] is similar to the acceptance‚Äìrejection method in +72
that it simulates a trial state, which is then accepted or rejected according to some random
mechanism. SpeciÔ¨Åcally, suppose we wish to sample from a target pdf f(x), where xtakes
values in some d-dimensional set. The aim is to construct a Markov chain fXt;t=0;1;:::g
in such a way that its limiting pdf is f. Suppose the Markov chain is in state xat time t. A
transition of the Markov chain from state xis carried out in two phases. First a proposal proposal
state Yis drawn from a transition density q(jx). This state is accepted as the new state,
with acceptance probability acceptance
probability
(x;y)=min(f(y)q(xjy)
f(x)q(yjx);1)
; (3.4)
or rejected otherwise. In the latter case the chain remains in state x. The algorithm just
described can be summarized as follows.
Algorithm 3.2.7: Metropolis‚ÄìHastings Sampler
input: Initial state X0, sample size N, target pdf f(x), proposal function q(yjx).
output: X1;:::; XN(dependent), approximately distributed according to f(x).
1fort=0toN 1do
2 Draw Yq(yjXt) // draw a proposal
3 (Xt;Y) // acceptance probability as in (3.4)
4 Draw UU(0;1)
5 ifU6then Xt+1 Y
6 elseXt+1 Xt
7return X1;:::; XN
The fact that the limiting distribution of the Metropolis‚ÄìHastings Markov chain is equal
to the target distribution (under general conditions) is a consequence of the following result.Chapter 3. Monte Carlo Methods 79
Theorem 3.1: Local Balance for the Metropolis‚ÄìHastings Sampler
The transition density of the Metropolis‚ÄìHastings Markov chain satisÔ¨Åes +453 the de-
tailed balance equations.
Proof: We prove the theorem for the discrete case only. Because a transition of the
Metropolis‚ÄìHastings Markov chain consists of two steps, the one-step transition probabil-
ity to go from xtoyis not q(yjx) but
eq(yjx)=8>><>>:q(yjx)(x;y); ify,x;
1 P
z,xq(zjx)(x;z);ify=x:(3.5)
We thus need to show that
f(x)eq(yjx)=f(y)eq(xjy) for all x;y: (3.6)
With the acceptance probability as in (3.4), we need to check (3.6) for three cases:
(a)x=y,
(b)x,yandf(y)q(xjy)6f(x)q(yjx), and
(c)x,yandf(y)q(xjy)>f(x)q(yjx).
Case (a) holds trivially. For case (b), (x;y)=f(y)q(xjy)=(f(x)q(yjx)) and(y;x)=1.
Consequently,
eq(yjx)=f(y)q(xjy)=f(x) and eq(xjy)=q(xjy);
so that (3.6) holds. Similarly, for case (c) we have (x;y)=1 and(y;x)=f(x)q(yjx)=
(f(y)q(xjy)). It follows that,
eq(yjx)=q(yjx) and eq(xjy)=f(x)q(yjx)=f(y);
so that (3.6) holds again. 
Thus if the Metropolis‚ÄìHastings Markov chain is ergodic, then its limiting pdf is f(x). +452
A fortunate property of the algorithm, which is important in many applications, is that in
order to evaluate the acceptance probability (x;y) in (3.4), one only needs to know the
target pdf f(x)up to a constant ; that is f(x)=cf(x) for some known function f(x) but
unknown constant c.
The e ciency of the algorithm depends of course on the choice of the proposal trans-
ition density q(yjx). Ideally, we would like q(yjx) to be ‚Äúclose‚Äù to the target f(y), irre-
spective of x. We discuss two common approaches.
1. Choose the proposal transition density q(yjx) independent of x; that is, q(yjx)=
g(y) for some pdf g(y). An MCMC sampler of this type is called an independence
sampler independence
sampler. The acceptance probability is thus
(x;y)=min(f(y)g(x)
f(x)g(y);1)
:80 3.2. Monte Carlo Sampling
2. If the proposal transition density is symmetric (that is, q(yjx)=q(xjy)), then the
acceptance probability has the simple form
(x;y)=min(f(y)
f(x);1)
; (3.7)
and the MCMC algorithm is called a random walk sampler . A typical example israndom walk
sampler when, for a given current state x, the proposal state Yis of the form Y=x+Z,
where Zis generated from some spherically symmetric distribution, such as N(0;I).
We now give an example illustrating the second approach.
Example 3.8 (Random Walk Sampler) Consider the two-dimensional pdf
f(x1;x2)=ce 1
4p
x2
1+x2
2
sin
2q
x2
1+x2
2
+1
; 2<x1<2; 2<x2<2;(3.8)
where cis an unknown normalization constant. The graph of this pdf (unnormalized) is
depicted in the left panel of Figure 3.5.
-6 -4 -2 0 2 4 6-6-4-20246
Figure 3.5: Left panel: the two-dimensional target pdf. Right panel: points from the random
walk sampler are approximately distributed according to the target pdf.
The following Python program implements a random walk sampler to (approximately)
draw N=104dependent samples from the pdf f. At each step, given a current state x,
a proposal Yis drawn from the N(x;I) distribution. That is, Y=x+Z, with Zbivariate
standard normal. We see in the right panel of Figure 3.5 that the sampler works correctly.
The starting point for the Markov chain is chosen as (0 ;0). Note that the normalization
constant cis never required to be speciÔ¨Åed in the program.
rwsamp.py
import numpy as np
import matplotlib.pyplot as plt
from numpy import pi, exp, sqrt , sin
from numpy.random import rand , randnChapter 3. Monte Carlo Methods 81
N = 10000
a = lambda x: -2*pi < x
b = lambda x: x < 2*pi
f = lambda x1, x2: exp(-sqrt(x1**2+x2**2)/4)*(
sin(2*sqrt(x1**2+x2**2))+1)*a(x1)*b(x1)*a(x2)*b(x2)
xx = np.zeros((N,2))
x = np.zeros((1,2))
for iin range (1,N):
y = x + randn(1,2)
alpha = np.amin((f(y[0][0],y[0][1])/f(x[0][0],x[0][1]) ,1))
r = rand() < alpha
x = r*y + (1-r)*x
xx[i,:] = x
plt.scatter(xx[:,0], xx[:,1], alpha =0.4,s =2)
plt.axis( 'equal ')
plt.show()
3.2.5.2 Gibbs Sampler
The Gibbs sampler Gibbs sampler [48] uses a somewhat di erent methodology from the Metropolis‚Äì
Hastings algorithm and is particularly useful for generating n-dimensional random vectors.
The key idea of the Gibbs sampler is to update the components of the random vector
one at a time, by sampling them from conditional pdfs. Thus, Gibbs sampling can be
advantageous if it is easier to sample from the conditional distributions than from the joint
distribution.
SpeciÔ¨Åcally, suppose that we wish to sample a random vector X=[X1;:::; Xn]>ac-
cording to a target pdf f(x). Let f(xijx1;:::; xi 1;xi+1;:::; xn) represent the conditional
pdf2of the i-th component, Xi, given the other components x1;:::; xi 1;xi+1;:::; xn. The
Gibbs sampling algorithm is as follows.
Algorithm 3.2.8: Gibbs Sampler
input: Initial point X0, sample size N, and target pdf f.
output: X1;:::; XNapproximately distributed according to f.
1fort=0toN 1do
2 Draw Y1from the conditional pdf f(y1jXt;2;:::; Xt;n).
3 fori=2tondo
4 Draw Yifrom the conditional pdf f(yijY1;:::; Yi 1;Xt;i+1;:::; Xt;n).
5 Xt+1 Y
6return X1;:::; XN
There exist many variants of the Gibbs sampler, depending on the steps required to
update XttoXt+1‚Äî called the cycle of the Gibbs algorithm. In the algorithm above, thecycle
2In this section we employ a Bayesian notation style, using the same letter ffor di erent (conditional)
densities.82 3.2. Monte Carlo Sampling
cycle consists of Steps 2‚Äì5, in which the components are updated in a Ô¨Åxed order 1 !2!
! n. For this reason Algorithm 3.2.8 is also called the systematic Gibbs sampler .systematic
Gibbs sampler In the random-order Gibbs sampler , the order in which the components are updated
random -order
Gibbs samplerin each cycle is a random permutation of f1;:::; ng(see Exercise 9). Other modiÔ¨Åcations
+115are to update the components in blocks (i.e., several at the same time), or to update only
a random selection of components. The variant where in each cycle only a single random
component is updated is called the random Gibbs sampler . In the reversible Gibbs samplerrandom Gibbs
sampler
reversible
Gibbs samplera cycle consists of the coordinate-wise updating 1 !2!! n 1!n!n 1!
! 2!1. In all cases, except for the systematic Gibbs sampler, the resulting Markov
chainfXt;t=1;2;:::gisreversible and hence its limiting distribution is precisely f(x).+452Unfortunately, the systematic Gibbs Markov chain is not reversible and so the detailed
balance equations are not satisÔ¨Åed. However, a similar result holds, due to Hammersley and
Cliord, under the so-called positivity condition : if at a point x=(x1;:::; xn) all marginal
densities f(xi)>0;i=1;:::; n, then the joint density f(x)>0.
Theorem 3.2: Hammersley‚ÄìCli ord Balance for the Gibbs Sampler
Letq1!n(yjx) denote the transition density of the systematic Gibbs sampler, and let
qn!1(xjy) be the transition density of the reverse move, in the order n!n 1!
! 1. Then, if the positivity condition holds,
f(x)q1!n(yjx)=f(y)qn!1(xjy): (3.9)
Proof: For the forward move we have:
q1!n(yjx)=f(y1jx2;:::; xn)f(y2jy1;x3;:::; xn)f(ynjy1;:::; yn 1);
and for the reverse move:
qn!1(xjy)=f(xnjy1;:::; yn 1)f(xn 1jy1;:::; yn 2;xn)f(x1jx2;:::; xn):
Consequently,
q1!n(yjx)
qn!1(xjy)=nY
i=1f(yijy1;:::; yi 1;xi+1;:::; xn)
f(xijy1;:::; yi 1;xi+1;:::; xn)
=nY
i=1f(y1;:::; yi;xi+1;:::; xn)
f(y1;:::; yi 1;xi;:::; xn)
=f(y)Qn 1
i=1f(y1;:::; yi;xi+1;:::; xn)
f(x)Qn
j=2f(y1;:::; yj 1;xj;:::; xn)
=f(y)Qn 1
i=1f(y1;:::; yi;xi+1;:::; xn)
f(x)Qn 1
j=1f(y1;:::; yj;xj+1;:::; xn)=f(y)
f(x):
The result follows by rearranging the last identity. The positivity condition ensures that we
do not divide by 0 along the line. 
Intuitively, the long-run proportion of transitions x!yfor the ‚Äúforward move‚Äù chain
is equal to the long-run proportion of transitions y!xfor the ‚Äúreverse move‚Äù chain.Chapter 3. Monte Carlo Methods 83
To verify that the Markov chain X0;X1;:::for the systematic Gibbs sampler indeed has
limiting pdf f(x), we need to check that the global balance equations (C.42) hold. By +452
integrating (in the continuous case) both sides in (3.9) with respect to x, we see that indeed
Z
f(x)q1!n(yjx) dx=f(y):
Example 3.9 (Gibbs Sampler for the Bayesian Normal Model) Gibbs samplers are
often applied in Bayesian statistics, to sample from the posterior pdf. Consider for instance
the Bayesian normal model + 50
f(;2)=1=2
(xj;2)N(1;2I):
Here the prior for ( ;2) isimproper . improper prior That is, it is not a pdf in itself, but by obstinately
applying Bayes‚Äô formula it does yield a proper posterior pdf. In some sense this prior
conveys the least amount of information about and2. Following the same procedure as
in Example 2.8, we Ô¨Ånd the posterior pdf:
f(;2jx)/
2 n=2 1exp(
 1
2P
i(xi )2
2)
: (3.10)
Note thatand2here are the ‚Äúvariables‚Äù and xis a Ô¨Åxed data vector. To simulate samples
and2from (3.10) using the Gibbs sampler, we need the distributions of both ( j2;x)
and (2j;x). To Ô¨Ånd f(j2;x), view the right-hand side of (3.10) as a function of 
only, regarding 2as a constant. This gives
f(j2;x)/exp(
 n2 2P
ixi
22)
=exp(
 2 2x
2(2=n))
/exp(
 1
2( x)2
2=n)
: (3.11)
This shows that ( j2;x) has a normal distribution with mean xand variance 2=n.
Similarly, to Ô¨Ånd f(2j;x), view the right-hand side of (3.10) as a function of 2,
regardingas a constant. This gives
f(2j;x)/(2) n=2 1exp8>><>>: 1
2nX
i=1(xi )2=29>>=>>;; (3.12)
showing that ( 2j;x) has an inverse-gamma distribution with parameters n=2 and +425Pn
i=1(xi )2=2. The Gibbs sampler thus involves the repeated simulation of
(j2;x)N
x; 2=n
and (2j;x)InvGamma0BBBBB@n=2;nX
i=1(xi )2=21CCCCCA:
Simulating XInvGamma (;) is achieved by Ô¨Årst generating ZGamma (;) and
then returning X=1=Z.84 3.2. Monte Carlo Sampling
In our parameterization of the Gamma (;) distribution, is the rate parameter.
Many software packages instead use the scale parameter c=1=. Be aware of this
when simulating Gamma random variables.
The Python script below deÔ¨Ånes a small data set of size n=10 (which was randomly
simulated from a standard normal distribution), and implements the systematic Gibbs
sampler to simulate from the posterior distribution, using N=105samples.
gibbsamp.py
import numpy as np
import matplotlib.pyplot as plt
x = np.array([[-0.9472, 0.5401, -0.2166, 1.1890, 1.3170,
-0.4056, -0.4449, 1.3284, 0.8338, 0.6044]])
n=x.size
sample_mean = np.mean(x)
sample_var = np.var(x)
sig2 = np.var(x)
mu=sample_mean
N=10**5
gibbs_sample = np.array(np.zeros((N, 2)))
for kin range (N):
mu=sample_mean + np.sqrt(sig2/n)*np.random.randn()
V=np. sum((x-mu)**2)/2
sig2 = 1/np.random.gamma(n/2, 1/V)
gibbs_sample[k,:]= np.array([mu, sig2])
plt.scatter(gibbs_sample[:,0], gibbs_sample[:,1],alpha =0.1,s =1)
plt.plot(np.mean(x), np.var(x), 'wo')
plt.show()
-1 -0.5 0 0.5 1 1.5 2
700.511.5^f(7jx)
Figure 3.6: Left: approximate draws from the posterior pdf f(;2jx) obtained via the
Gibbs sampler. Right: estimate of the posterior pdf f(jx).Chapter 3. Monte Carlo Methods 85
The left panel of Figure 3.6 shows the ( ;2) points generated by the Gibbs sampler.
Also shown, via the white circle, is the point ( x;s2), where x=0:3798 is the sample mean
ands2=0:6810 the sample variance. This posterior point cloud visualizes the considerable
uncertainty in the estimates. By projecting the ( ;2) points onto the -axis ‚Äî that is,
by ignoring the 2values ‚Äî one obtains (approximate) samples from the posterior pdf
of; that is, f(jx). The right panel of Figure 3.6 shows a kernel density estimate (see
Section 4.4) of this pdf. The corresponding 0 :025 and 0:975 sample quantiles were found +134
to be 0:2054 and 0:9662, respectively, giving the 95% credible interval (  0:2054;0:9662)
for, which contains the true expectation 0. Similarly, an estimated 95% credible interval
for2is (0:3218;2:2485), which contains the true variance 1.
3.3 Monte Carlo Estimation
In this section we describe how Monte Carlo simulation can be used to estimate complic-
ated integrals, probabilities, and expectations. A number of variance reduction techniques
are introduced as well, including the recent cross-entropy method.
3.3.1 Crude Monte Carlo
The most common setting for Monte Carlo estimation is the following: Suppose we wish to
compute the expectation =EYof some (say continuous) random variable Ywith pdf f,
but the integral EY=R
y f(y) dyis dicult to evaluate. For example, if Yis a complicated
function of other random variables, it would be di cult to obtain an exact expression for
f(y). The idea of crude Monte Carlo ‚Äî sometimes abbreviated as CMC ‚Äî is to approx-crude Monte
Carlo imateby simulating many independent copies Y1;:::; YNofYand then take their sample
mean Yas an estimator of . All that is needed is an algorithm to simulate such copies.
By the Law of Large Numbers, Yconverges to asN!1 , provided the expectation +446
ofYexists. Moreover, by the Central Limit Theorem, Yapproximately has a N(;2=N) +447
distribution for large N, provided that the variance 2=VarY<1. This enables the con-
struction of an approximate (1  )conÔ¨Ådence interval for:confidence
interval 
Y z1 =2Sp
N;Y+z1 =2Sp
N!
; (3.13)
where Sis the sample standard deviation of the fYigandzdenotes the -quantile of the
N(0;1) distribution; see also Section C.13. Instead of specifying the conÔ¨Ådence interval, +457
one often reports only the sample mean and the estimated standard error :S=p
N, or theestimated
standard error estimated relative error :S=(Yp
N). The basic estimation procedure for independent data
estimated
relative erroris summarized in Algorithm 3.3.1 below.
It is often the case that the output Yis a function of some underlying random vector or
stochastic process; that is, Y=H(X), where His a real-valued function and Xis a random
vector or process. The beauty of Monte Carlo for estimation is that (3.13) holds regardless
of the dimension of X.86 3.3. Monte Carlo Estimation
Algorithm 3.3.1: Crude Monte Carlo for Independent Data
input: Simulation algorithm for Yf, sample size N, conÔ¨Ådence level 1  .
output: Point estimate and approximate (1  ) conÔ¨Ådence interval for =EY.
1Simulate Y1;:::; YNiidf.
2Y 1
NPN
i=1Yi
3S2 1
N 1PN
i=1(Yi Y)2
4return Yand the interval (3.13) .
Example 3.10 (Monte Carlo Integration) InMonte Carlo integration , simulation isMonte Carlo
integration used to evaluate complicated integrals. Consider, for example, the integral
=Z1
 1Z1
 1Z1
 1p
jx1+x2+x3je (x2
1+x2
2+x2
3)=2dx1dx2dx3:
DeÔ¨Åning Y=jX1+X2+X3j1=2(2)3=2, with X1;X2;X3iidN(0;1), we can write =EY.
Using the following Python program, with a sample size of N=106, we obtained an
estimate Y=17:031 with an approximate 95% conÔ¨Ådence interval (17 :017;17:046).
mcint.py
import numpy as np
from numpy import pi
c = (2*pi)**(3/2)
H = lambda x: c*np.sqrt(np. abs(np. sum(x,axis=1)))
N = 10**6
z = 1.96
x = np.random.randn(N,3)
y = H(x)
mY = np.mean(y)
sY = np.std(y)
RE = sY/mY/np.sqrt(N)
print ('Estimate = {:3.3f}, CI = ({:3.3f},{:3.3f}) '.format (
mY, mY*(1-z*RE), mY*(1+z*RE)))
Estimate = 17.031, CI = (17.017 ,17.046)
Example 3.11 (Example 2.1 (cont.)) We return to the bias‚Äìvariance tradeo in Ex-
ample 2.1. Figure 2.7 gives estimates of the (squared-error) generalization risk (2.5) as +26
+29
+23a function of the number of parameters in the model. But how accurate are these estim-
ates? Because we know in this case the exact model for the data, we can use Monte Carlo
simulation to estimate the generalization risk (for a Ô¨Åxed training set) and the expected
generalization risk (averaged over all training sets) precisely. All we need to do is repeat
the data generation, Ô¨Åtting, and validation steps many times and then take averages of the
results. The following Python code repeats 100 times:
1. Simulate the training set of size n=100.
2. Fit models up to size k=8.Chapter 3. Monte Carlo Methods 87
3. Estimate the test loss using a test set with the same sample size n=100.
Figure 3.7 shows that there is some variation in the test losses, due to the randomness in
both the training and test sets. To obtain an accurate estimate of the expected generalization
risk (2.6), take the average of the test losses. We see that for k68 the estimate in Figure 2.7
is close to the true expected generalization risk.
1
 2
 3
 4
 5
 6
 7
 8
Number of parameters p
25
50
75
100
125
150
175
200Test loss
Figure 3.7: Independent estimates of the test loss show some variability.
CMCtestloss.py
import numpy as np, matplotlib.pyplot as plt
from numpy.random import rand , randn
from numpy.linalg import solve
def generate_data(beta , sig, n):
u = rand(n, 1)
y = (u ** np.arange(0, 4)) @ beta + sig * randn(n, 1)
return u, y
beta = np.array([[10, -140, 400, -250]]).T
n = 100
sig = 5
betahat = {}
plt.figure(figsize=[6,5])
totMSE = np.zeros(8)
max_p = 8
p_range = np.arange(1, max_p + 1, 1)
for Nin range (0,100):88 3.3. Monte Carlo Estimation
u, y = generate_data(beta , sig, n) #training data
X = np.ones((n, 1))
for pinp_range:
ifp > 1:
X = np.hstack((X, u**(p-1)))
betahat[p] = solve(X.T @ X, X.T @ y)
u_test , y_test = generate_data(beta , sig, n) #test data
MSE = []
X_test = np.ones((n, 1))
for pinp_range:
ifp > 1:
X_test = np.hstack((X_test , u_test**(p-1)))
y_hat = X_test @ betahat[p] # predictions
MSE.append(np. sum((y_test - y_hat)**2/n))
totMSE = totMSE + np.array(MSE)
plt.plot(p_range , MSE, 'C0',alpha=0.1)
plt.plot(p_range ,totMSE/N, 'r-o')
plt.xticks(ticks=p_range)
plt.xlabel( 'Number of parameters $p$ ')
plt.ylabel( 'Test loss ')
plt.tight_layout()
plt.savefig( 'MSErepeatpy.pdf ',format ='pdf')
plt.show()
3.3.2 Bootstrap Method
The bootstrap method [37] combines CMC estimation with the resampling procedure of
Section 3.2.4. The idea is as follows: Suppose we wish to estimate a number via some +76
estimator Y=H(T), whereT:=fX1;:::; Xngis an iid sample from some unknown cdf
F. It is assumed that Ydoes not depend on the order of the fXig. To assess the quality (for
example, accuracy) of the estimator Y, one could draw independent replications T1;:::;TN
ofTand Ô¨Ånd sample estimates for quantities such as the variance VarY, the biasEY ,
and the mean squared error E(Y )2. However, it may be too time-consuming or simply
not feasible to obtain such replications. An alternative is to resample the original data.
To reiterate, given an outcome =fx1;:::; xngofT, we simulate an iid sample T:=
fX
1;:::; X
ngfrom the empirical cdf Fn, via Algorithm 3.2.6 (hence the resampling size is +76
N=nhere).
The rationale is that the empirical cdf Fnis close to the actual cdf Fand gets closer as
ngets larger. Hence, any quantities depending on F, such asEFg(Y), where gis a function,
can be approximated by EFng(Y). The latter is usually still di cult to evaluate, but it can
be simply estimated via CMC as
1
KKX
i=1g(Y
i);
where Y
1;:::; Y
Kare independent random variables, each distributed as Y=H(T). This
seemingly self-referent procedure is called bootstrapping ‚Äî alluding to Baron von M√ºn-bootstrappingChapter 3. Monte Carlo Methods 89
chausen, who pulled himself out of a swamp by his own bootstraps. As an example, the
bootstrap estimate of the expectation of Yis
cEY=Y=1
KKX
i=1Y
i;
which is simply the sample mean of fY
ig. Similarly, the bootstrap estimate for VarYis the
sample variance
[VarY=1
K 1KX
i=1(Y
i Y)2: (3.14)
Bootstrap estimators for the bias and MSE are Y Yand1
KPK
i=1(Y
i Y)2, respectively.
Note that for these estimators the unknown quantity is replaced with its original estimator
Y. ConÔ¨Ådence intervals can be constructed in the same fashion. We mention two variants:
thenormal method and the percentile method . In the normal method, a 1  conÔ¨Ådencenormal method
percentile
methodinterval for is given by
(Yz1 =2S);
where Sis the bootstrap estimate of the standard deviation of Y; that is, the square root
of (3.14). In the percentile method, the upper and lower bounds of the 1  conÔ¨Ådence
interval for are given by the 1  =2 and=2 quantiles of Y, which in turn are estimated
via the corresponding sample quantiles of the bootstrap sample fY
ig.
The following example illustrates the usefulness of the bootstrap method for ratio es-
timation and also introduces the renewal reward process model for data.
Example 3.12 (Bootstrapping the Ratio Estimator) A common scenario in stochastic
simulation is that the output of the simulation consists of independent pairs of data
(C1;R1);(C2;R2);:::, where each Cis interpreted as the length of a period of time ‚Äî a so-
called cycle ‚Äî and Ris the reward obtained during that cycle. Such a collection of random
variablesf(Ci;Ri)gis called a renewal reward process renewal
reward process. Typically, the reward Ridepends on
the cycle length Ci. Let Atbe the average reward earned by time t; that is, At=PNt
i=1Ri=t,
where Nt=maxfn:C1++Cn6tgcounts the number of complete cycles at time t. It
can be shown, see Exercise 20, that if the expectations of the cycle length and reward are +118
Ô¨Ånite, then Atconverges to the constant ER=EC. This ratio can thus be interpreted as the
long-run average reward long -run
average reward.
Estimation of the ratio ER=ECfrom data ( C1;R1);:::; (Cn;Rn) is easy: take the ratio
estimatorratio estimator
A=R
C:
However, this estimator Ais not unbiased and it is not obvious how to derive conÔ¨Ådence
intervals. Fortunately, the bootstrap method can come to the rescue: simply resample the
pairsf(Ci;Ri)g, obtain ratio estimators A
1;:::; A
K, and from these compute quantities of
interest such as conÔ¨Ådence intervals.
As a concrete example, let us return to the Markov chain in Example 3.6. Recall that + 75
the chain starts at state 1 at time 0. After a certain amount of time T1, the process returns
to state 1. The time steps 0 ;:::; T1 1 form a natural ‚Äúcycle‚Äù for this process, as from
time T1onwards the process behaves probabilistically exactly the same as when it started,90 3.3. Monte Carlo Estimation
independently ofX0;:::; XT1 1. Thus, if we deÔ¨Åne T0=0, and let Tibe the i-th time that
the chain returns to state 1, then we can break up the time interval into independent cycles
of lengths Ci=Ti Ti 1,i=1;2;:::. Now suppose that during the i-th cycle a reward
Ri=Ti 1X
t=Ti 1%t Ti 1r(Xt)
is received, where r(i) is some Ô¨Åxed reward for visiting state i2f1;2;3;4gand%2(0;1)
is a discounting factor. Clearly, f(Ci;Ri)gis a renewal reward process. Figure 3.8 shows the
outcomes of 1000 pairs ( C;R), using r(1)=4;r(2)=3;r(3)=10;r(4)=1, and%=0:9.
0 10 20 30 40 50 60 70
Cycle length0102030405060Reward
Figure 3.8: Each circle represents a (cycle length, reward) pair. The varying circle sizes
indicate the number of occurrences for a given pair. For example, (2,15.43) is the most
likely pair here, occurring 186 out of a 1000 times. It corresponds to the cycle path 1 !
3!2!1.
The long-run average reward is estimated as 2.50 for our data. But how accurate is this
estimate? Figure 3.9 shows a density plot of the bootstrapped ratio estimates, where we
independently resampled the data pairs 1000 times.
2.2
 2.4
 2.6
 2.8
long-run average reward
0
2
4density
Figure 3.9: Density plot of the bootstrapped ratio estimates for the Markov chain renewal
reward process.Chapter 3. Monte Carlo Methods 91
Figure 3.9 indicates that the true long-run average reward lies between 2.2 and 2.8
with high conÔ¨Ådence. More precisely, the 99% bootstrap conÔ¨Ådence interval (percentile
method) is here (2.27, 2.77). The following Python script spells out the procedure.
ratioest.py
import numpy as np, matplotlib.pyplot as plt, seaborn as sns
from numba import jit
np.random.seed(123)
n = 1000
P = np.array([[0, 0.2, 0.5, 0.3],
[0.5 ,0, 0.5, 0],
[0.3, 0.7, 0, 0],
[0.1, 0, 0, 0.9]])
r = np.array([4,3,10,1])
Corg = np.array(np.zeros((n,1)))
Rorg = np.array(np.zeros((n,1)))
rho=0.9
@jit() #for speed -up; see Appendix
def generate_cyclereward(n):
for iin range (n):
t=1
xreg = 1 #regenerative state (out of 1,2,3,4)
reward = r[0]
x= np.amin(np.argwhere(np.cumsum(P[xreg -1,:]) > np.random.
rand())) + 1
while x != xreg:
t += 1
reward += rho**(t-1)*r[x-1]
x = np.amin(np.where(np.cumsum(P[x-1,:]) > np.random.rand
())) + 1
Corg[i] = t
Rorg[i] = reward
return Corg , Rorg
Corg , Rorg = generate_cyclereward(n)
Aorg = np.mean(Rorg)/np.mean(Corg)
K = 5000
A = np.array(np.zeros((K,1)))
C = np.array(np.zeros((n,1)))
R = np.array(np.zeros((n,1)))
for iin range (K):
ind = np.ceil(n*np.random.rand(1,n)).astype( int)[0]-1
C = Corg[ind]
R = Rorg[ind]
A[i] = np.mean(R)/np.mean(C)
plt.xlabel( 'long -run average reward ')
plt.ylabel( 'density ')
sns.kdeplot(A.flatten(),shade=True)
plt.show()92 3.3. Monte Carlo Estimation
3.3.3 Variance Reduction
The estimation of performance measures in Monte Carlo simulation can be made more
ecient by utilizing known information about the simulation model. Variance reduction
techniques include antithetic variables, control variables, importance sampling, conditional
Monte Carlo, and stratiÔ¨Åed sampling; see, for example, [71, Chapter 9]. We shall only deal
with control variables and importance sampling here.
Suppose Yis the output of a simulation experiment. A random variable eY, obtained
from the same simulation run, is called a control variable control
variableforYifYandeYare correlated
(negatively or positively) and the expectation of eYis known. The use of control variables
for variance reduction is based on the following theorem. We leave its proof to Exercise 21.
+118
Theorem 3.3: Control Variable Estimation
LetY1;:::; YNbe the output of Nindependent simulation runs and let eY1;:::;eYNbe
the corresponding control variables, with EeYk=eknown. Let %Y;eYbe the correlation
coecient between each YkandeYk. For each2Rthe estimator
b(c)=1
NNX
k=1h
Yk eYk ei
(3.15)
is an unbiased estimator for =EY. The minimal variance of b(c)is
Varb(c)=1
N(1 %2
Y;eY)VarY; (3.16)
which is obtained for =%Y;eYq
VarY=VareY.
From (3.16) we see that, by using the optimal in (3.15), the variance of the control
variate estimator is a factor 1  %2
Y;eYsmaller than the variance of the crude Monte Carlo
estimator. Thus, if eYis highly correlated with Y, a signiÔ¨Åcant variance reduction can be
achieved. The optimal is usually unknown, but it can be easily estimated from the sample
covariance matrix of f(Yk;eYk)g. +456
In the next example, we estimate the multiple integral in Example 3.10 using control
variables.
Example 3.13 (Monte Carlo Integration (cont.)) +86 The random variable Y=jX1+X2+
X3j1=2(2)3=2is positively correlated with the random variable eY=X2
1+X2
2+X2
3, for the
same choice of X1;X2;X3iidN(0;1). AsEeY=Var(X1+X2+X3)=3, we can use it as a
control variable to estimate the expectation of Y. The following Python program is based
on Theorem 3.3. It imports the crude Monte Carlo sampling code from Example 3.10.Chapter 3. Monte Carlo Methods 93
mcintCV.py
from mcint import *
Yc = np. sum(x**2, axis=1) # control variable data
yc = 3 # true expectation of control variable
C = np.cov(y,Yc) # sample covariance matrix
cor = C[0][1]/np.sqrt(C[0][0]*C[1][1])
alpha = C[0][1]/C[1][1]
est = np.mean(y-alpha*(Yc-yc))
RECV = np.sqrt((1-cor**2)*C[0][0]/N)/est #relative error
print ('Estimate = {:3.3f}, CI = ({:3.3f},{:3.3f}), Corr = {:3.3f} '.
format (est, est*(1-z*RECV), est*(1+z*RECV),cor))
Estimate = 17.045, CI = (17.032,17.057), Corr = 0.480
A typical estimate of the correlation coe cient%Y;eYis 0.48, which gives a reduction of
the variance with a factor 1  0:4820:77 ‚Äî a simulation speed-up of 23% compared with
crude Monte Carlo. Although the gain is small in this case, due to the modest correlation
between YandeY, little extra work was required to achieve this variance reduction.
One of the most important variance reduction techniques is importance sampling importance
sampling. This
technique is especially useful for the estimation of very small probabilities. The standard
setting is the estimation of a quantity
=EfH(X)=Z
H(x)f(x) dx; (3.17)
where His a real-valued function and fthe probability density of a random vector X,
called the nominal pdf . The subscript fis added to the expectation operator to indicate thatnominal pdfit is taken with respect to the density f.
Letgbe another probability density such that g(x)=0 implies that H(x)f(x)=0.
Using the density gwe can represent as
=Z
H(x)f(x)
g(x)g(x) dx=Eg"
H(X)f(X)
g(X)#
: (3.18)
Consequently, if X1;:::; XNiidg, then
b=1
NNX
k=1H(Xk)f(Xk)
g(Xk)(3.19)
is an unbiased estimator of . This estimator is called the importance sampling estimatorimportance
sampling
estimatorandgis called the importance sampling density. The ratio of densities, f(x)=g(x), is called
thelikelihood ratio . The importance sampling pseudo-code is given in Algorithm 3.3.2.likelihood ratio94 3.3. Monte Carlo Estimation
Algorithm 3.3.2: Importance Sampling Estimation
input: Function H, importance sampling density gsuch that g(x)=0 for all xfor
which H(x)f(x)=0, sample size N, conÔ¨Ådence level 1  .
output: Point estimate and approximate (1  ) conÔ¨Ådence interval for
=EH(X), where Xf.
1Simulate X1;:::; XNiidgand let Yi=H(Xi)f(Xi)=g(Xi);i=1;:::; N.
2Estimateviab=Yand determine an approximate (1  ) conÔ¨Ådence interval as
I:= 
b z1 =2Sp
N;b+z1 =2Sp
N!
;
where zdenotes the -quantile of the N(0;1) distribution and Sis the sample
standard deviation of Y1;:::; YN.
3return band the intervalI.
Example 3.14 (Importance Sampling) Let us examine the workings of importance
sampling by estimating the area, say, under the graph of the function
M(x1;x2)=e 1
4p
x2
1+x2
2
sin
2q
x2
1+x2
2
+1
;(x1;x2)2R2: (3.20)
We saw a similar function in Example 3.8 (but note the di erent domain). A natural ap- +80
proach to estimate the area is to truncate the domain to the square [  b;b]2, for large enough
b, and to estimate the integral
b=Zb
 bZb
 b(2b)2M(x)|      {z      }
H(x)f(x) dx=EfH(X)
via crude Monte Carlo, where f(x)=1=(2b)2;x2[ b;b]2, is the pdf of the uniform distri-
bution on [ b;b]2. Here is the Python code which does just that.
impsamp1.py
import numpy as np
from numpy import exp, sqrt , sin, pi, log, cos
from numpy.random import rand
b = 1000
H = lambda x1, x2: (2*b)**2 * exp(-sqrt(x1**2+x2**2)/4)*(sin(2*sqrt(
x1**2+x2**2))+1)*(x1**2 + x2**2 < b**2)
f = 1/((2*b)**2)
N = 10**6
X1 = -b + 2*b*rand(N,1)
X2 = -b + 2*b*rand(N,1)
Z = H(X1,X2)
estCMC = np.mean(Z).item() # to obtain scalar
RECMC = np.std(Z)/estCMC/sqrt(N).item()
print ('CI = ({:3.3f},{:3.3f}), RE = {: 3.3f} '.format (estCMC*(1-1.96*
RECMC), estCMC *(1+1.96*RECMC),RECMC))
CI = (82.663,135.036), RE = 0.123Chapter 3. Monte Carlo Methods 95
For a truncation level of b=1000 and a sample size of N=106, a typical estimate is
108.8, with an estimated relative error of 0.123. We have two sources of error here. The
Ô¨Årst is the error in approximating byb. However, as the function Hdecays exponentially
fast, b=1000 is more than enough to ensure this error is negligible. The second type of
error is the statistical error, due to the estimation process itself. This can be quantiÔ¨Åed by
the estimated relative error, and can be reduced by increasing the sample size.
Let us now consider an importance sampling approach in which the importance
sampling pdf gis radially symmetric and decays exponentially in the radius, similar to the
function H. In particular, we simulate ( X1;X2) in a way akin to Example 3.1, by Ô¨Årst gen- + 69
erating a radius RExp() and an angle U(0;2), and then returning X1=Rcos()
andX2=Rsin(). By the Transformation Rule (Theorem C.4) we then have +433
g(x)=fR;(r;)1
r=e r1
21
r=e p
x2
1+x2
2
2q
x2
1+x2
2;x2R2nf0g:
The following code, which imports the one given above, implements the importance
sampling steps, using the parameter =0:1.
impsamp2.py
from impsamp1 import *
lam = 0.1;
g = lambda x1, x2: lam*exp(-sqrt(x1**2 + x2**2)*lam)/sqrt(x1**2 + x2
**2)/(2*pi);
U = rand(N,1); V = rand(N,1)
R = -log(U)/lam
X1 = R*cos(2*pi*V)
X2 = R*sin(2*pi*V)
Z = H(X1,X2)*f/g(X1,X2)
estIS = np.mean(Z).item() # obtain scalar
REIS = np.std(Z)/estIS/sqrt(N).item()
print ('CI = ({:3.3f},{:3.3f}), RE = {: 3.3f} '.format (estIS*(1-1.96*
REIS), estIS*(1+1.96*REIS),REIS))
CI = (100.723,101.077), RE = 0.001
A typical estimate is 100.90 with an estimated relative error of 1 10 4, which gives
a substantial variance reduction. In terms of approximate 95% conÔ¨Ådence intervals, we
have (82.7,135.0) in the CMC case versus (100.7,101.1) in the importance sampling case.
Of course, we could have reduced the truncation level bto improve the performance of
CMC, but then the approximation error might become more signiÔ¨Åcant. For the importance
sampling case, the relative error is hardly a ected by the threshold level, but does depend
on the choice of . We chosesuch that the decay rate is slower than the decay rate of the
function H, which is 0.25.
As illustrated in the above example, a main di culty in importance sampling is how to
choose the importance sampling distribution. A poor choice of gmay seriously a ect the
accuracy of both the estimate and the conÔ¨Ådence interval. The theoretically optimal choice96 3.4. Monte Carlo for Optimization
gfor the importance sampling density minimizes the variance of band is therefore the
solution to the functional minimization program
min
gVarg 
H(X)f(X)
g(X)!
: (3.21)
It is not di cult to show, see also Exercise 22, that if either H(x)>0 or H(x)60 for all +118
x, then the optimal importance sampling pdf optimal
importance
sampling pdfis
g(x)=H(x)f(x)
: (3.22)
Namely, in this case Vargb=Varg(H(X)f(X)=g(X))=Varg=0, so that the estimator b
isconstant under g. An obvious di culty is that the evaluation of the optimal importance
sampling density gis usually not possible, since g(x) in (3.22) depends on the unknown
quantity. Nevertheless, one can typically choose a good importance sampling density g
‚Äúclose‚Äù to the minimum variance density g.
One of the main considerations for choosing a good importance sampling pdf is that
the estimator (3.19) should have Ô¨Ånite variance. This is equivalent to the requirement
that
Eg"
H2(X)f2(X)
g2(X)#
=Ef"
H2(X)f(X)
g(X)#
<1: (3.23)
This suggests that gshould not have lighter tails than fand that, preferably, the
likelihood ratio, f=g, should be bounded.
3.4 Monte Carlo for Optimization
In this section we describe several Monte Carlo methods for optimization. Such random-
ized algorithms can be useful for solving optimization problems with many local optima
and complicated constraints, possibly involving a mix of continuous and discrete variables.
Randomized algorithms are also used to solve noisy optimization problems, in which the
objective function is unknown and has to be obtained via Monte Carlo simulation.
3.4.1 Simulated Annealing
Simulated annealing is a Monte Carlo technique for minimization that emulates the phys-Simulated
annealing ical state of atoms in a metal when the metal is heated up and then slowly cooled down.
When the cooling is performed very slowly, the atoms settle down to a minimum-energy
state. Denoting the state as xand the energy of a state as S(x), the probability distribution
of the (random) states is described by the Boltzmann pdf
f(x)/e S(x)
kT;x2X;
where kis Boltzmann‚Äôs constant and Tis the temperature.Chapter 3. Monte Carlo Methods 97
Going beyond the physical interpretation, suppose that S(x) is an arbitrary function to
be minimized, with xtaking values in some discrete or continuous set X. The Gibbs pdfGibbs pdfcorresponding to S(x) is deÔ¨Åned as
fT(x)=e S(x)
T
zT;x2X;
provided that the normalization constant zT:=P
xexp( S(x)=T) is Ô¨Ånite. Note that this
is simply the Boltzmann pdf with the Boltzmann constant kremoved. As T!0, the pdf
becomes more and more peaked around the set of global minimizers of S.
The idea of simulated annealing is to create a sequence of points X1;X2;:::that are ap-
proximately distributed according to pdfs fT1(x);fT2(x);:::, where T1;T2;::: is a sequence
of ‚Äútemperatures‚Äù that decreases (is ‚Äúcooled‚Äù) to 0 ‚Äî known as the annealing schedule . Ifannealing
schedule each Xtwere sampled exactly from fTt, then Xtwould converge to a global minimum of
S(x) asTt!0. However, in practice sampling is approximate and convergence to a global
minimum is not assured. A generic simulated annealing algorithm is as follows.
Algorithm 3.4.1: Simulated Annealing
input: Annealing schedule T0;T1;:::; , function S, initial value x0.
output: Approximations to the global minimizer xand minimum value S(x).
1SetX0 x0andt 1.
2while not stopping do
3 Approximately simulate Xtfrom fTt(x).
4 t t+1
5return Xt;S(Xt)
A popular annealing schedule is geometric cooling , where Tt=Tt 1,t=1;2;:::, forgeometric
cooling a given initial temperature T0and a cooling factor 2(0;1). Appropriate values for T0
cooling factor andare problem-dependent and this has traditionally required tuning on the part of the
user. A possible stopping criterion is to stop after a Ô¨Åxed number of iterations, or when the
temperature is ‚Äúsmall enough‚Äù.
Approximate sampling from a Gibbs distribution is most often carried out via Markov
chain Monte Carlo. For each iteration t, the Markov chain should theoretically run for a
large number of steps to accurately sample from the Gibbs pdf fTt. However, in practice,
one often only runs a single step of the Markov chain, before updating the temperature, as
in Algorithm 3.4.2 below.
To sample from a Gibbs distribution fT, this algorithm uses a random walk Metropolis‚Äì
Hastings sampler. From (3.7), the acceptance probability of a proposal yis thus + 80
(x;y)=min8>><>>:e 1
TS(y)
e 1
TS(x);19>>=>>;=minn
e 1
T(S(y) S(x));1o
:
Hence, if S(y)<S(x), then the proposal is aways accepted. Otherwise, the proposal is
accepted with probability exp(  1
T(S(y) S(x))).98 3.4. Monte Carlo for Optimization
Algorithm 3.4.2: Simulated Annealing with a Random Walk Sampler
input: Objective function S, starting state X0, initial temperature T0, number of
iterations N, symmetric proposal density q(yjx), constant.
output: Approximate minimizer and minimum value of S.
1fort=0toN 1do
2 Simulate a new state Yfrom the symmetric proposal q(yjXt).
3 ifS(Y)<S(Xt)then
4 Xt+1 Y
5 else
6 Draw UU(0;1).
7 ifU6e (S(Y) S(Xt))=Ttthen
8 Xt+1 Y
9 else
10 Xt+1 Xt
11 Tt+1 Tt
12return XNandS(XN)
Example 3.15 (Simulated Annealing for Minimization) Let us minimize the ‚Äúwig-
gly‚Äù function depicted in the bottom panel of Figure 3.10 and given by:
S(x)=8>><>>: e x2=100sin(13 x x4)5sin(1 3x2)2;if 26x62;
1; otherwise:
012345f(x) = e!S(x)=T
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2
x-101S(x)T= 1T= 0 :4T= 0 :2
Figure 3.10: Lower panel: the ‚Äúwiggly‚Äù function S(x). Upper panel: three (normalized)
Gibbs pdfs for temperatures T=1;0:4;0:2. As the temperature decreases, the Gibbs pdf
converges to the pdf that has all its mass concentrated at the minimizer of S.Chapter 3. Monte Carlo Methods 99
The function has many local minima and maxima, with a global minimum around 1.4.
The Ô¨Ågure also illustrates the relationship between Sand the (unnormalized) Gibbs pdf fT.
The following Python code implements a slight variant of Algorithm 3.4.2 where, in-
stead of stopping after a Ô¨Åxed number of iterations, the algorithm stops when the temper-
ature is lower than some threshold (here 10 3).
Instead of stopping after a Ô¨Åxed number Nof iterations or when the temperature
is low enough, it is useful to stop when consecutive function values are closer than
some distance "to each other, or when the best found function value has not changed
over a Ô¨Åxed number dof iterations.
For a ‚Äúcurrent‚Äù state x, the proposal state Yis here drawn from the N(x;0:52) distri-
bution. We use geometric cooling with decay parameter =0:999 and initial temperature
T0=1. We set the initial state to x0=0. Figure 3.11 depicts a realization of the sequence
of states xtfort=0;1;:::. After initially Ô¨Çuctuating wildly, the sequence settles down
to a value around 1.37, with S(1:37)= 0:92, corresponding to the global optimizer and
minimum, respectively.
simann.py
import numpy as np
import matplotlib.pyplot as plt
def wiggly(x):
y = -np.exp(x**2/100)*np.sin(13*x-x**4)**5*np.sin(1-3*x**2)**2
ind = np.vstack((np.argwhere(x<-2),np.argwhere(x>2)))
y[ind]= float ('inf')
return y
S = wiggly
beta = 0.999
sig = 0.5
T=1
x= np.array([0])
xx=[]
Sx=S(x)
while T>10**(-3):
T=beta*T
y = x+sig*np.random.randn()
Sy = S(y)
alpha = np.amin((np.exp(-(Sy-Sx)/T),1))
ifnp.random.uniform()<alpha:
x=y
Sx=Sy
xx=np.hstack((xx,x))
print ('minimizer = {:3.3f}, minimum ={:3.3f} '.format (x[0],Sx[0]))
plt.plot(xx)
plt.show()
minimizer = 1.365, minimum = -0.958100 3.4. Monte Carlo for Optimization
0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
number of iterations
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
2.0state
Figure 3.11: Typical states generated by the simulated annealing algorithm.
3.4.2 Cross-Entropy Method
Thecross-entropy (CE) method [103] is a simple Monte Carlo algorithm that can be usedcross -entropyfor both optimization and estimation.
The basic idea of the CE method for minimizing a function Son a setXis to deÔ¨Åne
a parametric family of probability densities ff(jv);v2Vg onXand to iteratively update
the parameter vso that f(jv) places more mass on states xthat have smaller Svalues than
on the previous iteration. In particular, the CE algorithm has two basic phases:
¬àSampling : Samples X1;:::; XNare drawn independently according to f(jv). The
objective function Sis evaluated at these points.
¬àUpdating : A new parameter v0is selected on the basis of those Xifor which S(Xi)6
for some level . ThesefXigform the elite sample set,E.elite sample
At each iteration the level parameter is chosen as the worst of the Nelite:=d%Ne
best performing samples, where %2(0;1) is the rarity parameter rarity
parameter‚Äî typically, %=0:1 or
%=0:01. The parameter vis updated as a smoothed average v0+(1 )v, where2(0;1)
is the smoothing parameter smoothing
parameterand
v0:=argmax
v2VX
X2Elnf(Xjv): (3.24)
The updating rule (3.24) is the result of minimizing the Kullback‚ÄìLeibler divergence
between the conditional density of Xf(xjv) given S(X)6, and f(x;v); see [103].
Note that (3.24) yields the maximum likelihood estimator (MLE) of vbased on the elite +456
samples. Hence, for many speciÔ¨Åc families of distributions, explicit solutions can be found.
An important example is where XN(;diag(2)); that is, Xhas independent GaussianChapter 3. Monte Carlo Methods 101
components. In this case, the mean vector and the vector of variances 2are simply
updated via the sample mean and sample variance of the elite samples. This is known as
normal updating . A generic CE procedure for minimization is given in Algorithm 3.4.3.normal
updating
Algorithm 3.4.3: Cross-Entropy Method for Minimization
input: Function S;initial sampling parameter v0, sample size N, rarity parameter
%, smoothing parameter .
output: Approximate minimum of Sand optimal sampling parameter v.
1Initialize v0, set Nelite d%Neandt 0.
2while a stopping criterion is not met do
3 t t+1
4 Simulate an iid sample X1;:::; XNfrom the density f(jvt 1).
5 Evaluate the performances S(X1);:::; S(XN) and sort them from smallest to
largest: S(1);:::; S(N).
6 Lettbe the sample %-quantile of the performances:
t S(Nelite): (3.25)
7 Determine the set of elite samples Et=fXi:S(Xi)6tg.
8 Letv0
tbe the MLE of the elite samples:
v0
t argmax
vX
X2Etlnf(Xjv): (3.26)
9 Update the sampling parameter as
vt v0
t+(1 )vt 1: (3.27)
10returnt,vt
The CE algorithm produces a sequence of pairs ( 1;v1);(2;v2);:::; such thattcon-
verges (approximately) to the minimal function value, and f(jvt) to a degenerate pdf that
(approximately) concentrates all its mass at a minimizer of S, ast!1 . A possible stop-
ping condition is to stop when the sampling distribution f(jvt) is su ciently close to a
degenerate distribution. For normal updating this means that the standard deviation is suf-
Ô¨Åciently small.
The output of the CE algorithm could also include the overall best function value
and corresponding solution.
In the following example, we minimize the same function as in Example 3.15, but + 97
instead use the CE algorithm.
Example 3.16 (Cross-Entropy Method for Minimization) In this case we take the
family of normal distributions fN(;2)gfor the sampling step (Step 4 of Algorithm 3.4.3),
starting with =0 and=3. The choice of the initial parameter is quite arbitrary, as long
asis large enough to sample a wide range of points. We take N=100 samples at each it-
eration, set%=0:1, and keep the Nelite=10=dN%esmallest ones as the elite samples. The
parameters andare then updated via the sample mean and sample standard deviation102 3.4. Monte Carlo for Optimization
of the elite samples. In this case we do not use any smoothing ( =1). In the following
Python code the 100 2 matrix Sxstores the x-values in the Ô¨Årst column and the func-
tion values in the second column. The rows of this matrix are sorted in ascending order
according to the function values, giving the matrix sortSx . The Ô¨Årst Nelite=10 rows of
this sorted matrix correspond to the elite samples and their function values. The updating
ofandis done in Lines 14 and 15. Figure 3.12 shows how the pdfs of the N(t;2
t)
sampling distributions degenerate to the point mass at the global minimizer 1 :366.
CEmethod.py
from simann import wiggly
import numpy as np
np.set_printoptions(precision=3)
mu, sigma = 0, 3
N, Nel = 100, 10
eps = 10**-5
S = wiggly
while sigma > eps:
X = np.random.randn(N,1)*sigma + np.array(np.ones((N,1)))*mu
Sx = np.hstack((X, S(X)))
sortSx = Sx[Sx[:,1].argsort(),]
Elite = sortSx[0:Nel ,:-1]
mu = np.mean(Elite , axis=0)
sigma = np.std(Elite , axis=0)
print ('S(mu)= {}, mu: {}, sigma: {}\n '.format (S(mu), mu, sigma))
S(mu)= [0.071], mu: [0.414], sigma: [0.922]
S(mu)= [0.063], mu: [0.81], sigma: [0.831]
S(mu)= [-0.033], mu: [1.212], sigma: [0.69]
S(mu)= [-0.588], mu: [1.447], sigma: [0.117]
S(mu)= [-0.958], mu: [1.366], sigma: [0.007]
S(mu)= [-0.958], mu: [1.366], sigma: [0.]
S(mu)= [-0.958], mu: [1.366], sigma: [3.535e-05]
S(mu)= [-0.958], mu: [1.366], sigma: [2.023e-06]
-2 -1 0 1 2 3x00.511.52f(x;7;<)5 
4 
2 
 1 iteration 03 
Figure 3.12: The normal pdfs of the Ô¨Årst six sampling distributions, truncated to the interval
[ 2;3]. The initial sampling distribution is N(0;32).Chapter 3. Monte Carlo Methods 103
3.4.3 Splitting for Optimization
Minimizing a function S(x),x2X is closely related to drawing a random sample from a
level set of the formfx2X :S(x)6g. Suppose Shas minimum value attained at x.level setAs long as>, this level set contains the minimizer. Moreover, if is close to, the
volume of this level set will be small. So, a randomly selected point from this set is expected
to be close to x. Thus, by gradually decreasing the level parameter , the level sets will
gradually shrink towards the set fxg. Indeed, the CE method was developed with exactly
this connection in mind; see, e.g., [102]. Note that the CE method employs a parametric
sampling distribution to obtain samples from the level sets (the elite samples). In [34]
anon-parametric sampling mechanism is introduced that uses an evolving collection of
particles. The resulting optimization algorithm, called splitting for continuous optimization splitting for
continuous
optimization(SCO), provides a fast and accurate way to optimize complicated continuous functions. The
details of SCO are given in Algorithm 3.4.4.
Algorithm 3.4.4: Splitting for Continuous Optimization (SCO)
input: Objective function S, sample size N, rarity parameter %, scale factor w,
bounded regionBX that is known to contain a global minimizer, and
maximum number of attempts MaxTry .
output: Final iteration number tand sequence ( Xbest;1;b1);:::; (Xbest;t;bt) of best
solutions and function values at each iteration.
1SimulateY0=fY1;:::; YNguniformly onB. Set t 0 and Nelite dN%e.
2while stopping condition is not satisÔ¨Åed do
3 Determine the Nelitesmallest values, S(1)66S(Nelite), offS(X);X2Y tg,
and store the corresponding vectors, X(1);:::; X(Nelite), inXt+1. Set bt+1 S(1)
andXbest;t+1 X(1).
4 Draw BiBernoulli (1
2),i=1;:::; Nelite, withPNelite
i=1Bi=Nmod Nelite.
5 fori=1toNelitedo
6 Ri j
N
Nelitek
+Bi // random splitting factor
7 Y X(i);Y0 Y
8 forj=1toRido
9 Draw I2f1;:::; Nelitegnfiguniformly and let i wjX(i) X(I)j.
10 Simulate a uniform permutation =(1;:::; n) of (1;:::; n).
11 fork=1tondo
12 forTry=1toMaxTry do
13 Y0(k) Y(k)+i(k)Z;ZN(0;1)
14 ifS(Y0)<S(Y)then Y Y0andbreak .
15 AddYtoYt+1
16 t t+1
17returnf(Xbest;k;bk);k=1;:::; tg
At iteration t=0, the algorithm starts with a population of particles Y0=fY1;:::; YNg
that are uniformly generated on some bounded region B, which is large enough to contain
a global minimizer. The function values of all particles in Y0are sorted, and the best104 3.4. Monte Carlo for Optimization
Nelite=dN%eform the elite particle set X1, exactly as in the CE method. Next, the elite
particles are ‚Äúsplit‚Äù into bN=Nelitecchildren particles, adding one extra child to some of
the elite particles to ensure that the total number of children is again N. The purpose of
Line 4 is to randomize which elite particles receive an extra child. Lines 8‚Äì15 describe
how the children of the i-th elite particle are generated. First, in Line 9, we select one
of the other elite particles uniformly at random. The same line deÔ¨Ånes an n-dimensional
vectoriwhose components are the absolute di erences between the vectors X(i)andX(I),
multiplied by a constant w. That is,
i=wjX(i) X(I)j:=w26666666666666664jX(i);1 X(I);1j
jX(i);2 X(I);2j
:::
jX(i);n X(I);nj37777777777777775:
Next, a uniform random permutation of (1;:::; n) is simulated (see Exercise 9). Lines +115
11‚Äì14 describe how, starting from a candidate child point Y, each coordinate of Yis re-
sampled, in the order determined by , by adding a standard normal random variable to
that component, multiplied by the corresponding component of i(Line 13). If the result-
ingY0has a function value that is less than that of Y, then the new candidate is accepted.
Otherwise, the same coordinate is tried again. If no improvement is found in MaxTry at-
tempts, the original component is retained. This process is performed for all elite samples,
to produce the Ô¨Årst-generation population Y1. The procedure is then repeated for iterations
t=1;2;:::, until some stopping criterion is met, e.g., when the best found function value
does not change for a number of consecutive iterations, or when the total number of func-
tion evaluations exceeds some threshold. The best found function value and corresponding
argument (particle) are returned at the conclusion of the algorithm.
The input variable MaxTry governs how much computational time is dedicated to up-
dating a component. In most cases we have encountered, the choices w=0:5 and MaxTry
=5 work well. Empirically, relatively high value for %work well, such as %=0:4;0:8, or
even%=1. The latter case means that at each stage t all samples fromYt 1carry over to
the elite setXt.
Example 3.17 (Test Problem 112) Hock and Schittkowski [58] provide a rich source
of test problems for multiextremal optimization. A challenging one is Problem 112, where
the goal is to Ô¨Ånd xso as to minimize the function
S(x)=10X
j=1xj 
cj+lnxj
x1++x10!
;
subject to the following set of constraints:
x1+2x2+2x3+x6+x10 2=0;
x4+2x5+x6+x7 1=0;
x3+x7+x8+2x9+x10 1=0;
xj>0:000001;j=1;:::; 10;
where the constants fcigare given in Table 3.1.Chapter 3. Monte Carlo Methods 105
Table 3.1: Constants for Test Problem 112.
c1= 6:089 c2= 17:164 c3= 34:054 c4= 5:914 c5= 24:721
c6= 14:986 c7= 24:100 c8= 10:708 c9= 26:662 c10= 22:179
The best known minimal value in [58] was  47:707579. In [89] a better solution was
found, 47:760765, using a genetic algorithm. The corresponding solution vector was
completely di erent from the one in [58]. A further improvement,  47:76109081, was
found in [70], using the CE method, giving a similar solution vector to that in [89]:
0.04067247 0.14765159 0.78323637 0.00141368 0.48526222
0.00069291 0.02736897 0.01794290 0.03729653 0.09685870
To obtain a solution with SCO, we Ô¨Årst converted this 10-dimensional problem into a
7-dimensional one by deÔ¨Åning the objective function
S7(y)=S(x);
where x2=y1;x3=y2;x5=y3;x6=y4;x7=y5;x9=y6;x10=y7, and
x1=2 (2y1+2y2+y4+x7);
x4=1 (2y3+y4+y5);
x8=1 (y2+y5+2y6+y7);
subject to x1;:::; x10>0:000001, where the fxigare taken as functions of the fyig. We then
adopted a penalty approach (see Section B.4) by adding a penalty function to the original +415
objective function:
eS7(y)=S(x)+100010X
i=1maxf (xi 0:000001);0g;
where, again, thefxigare deÔ¨Åned in terms of the fyigas above.
Optimizing this last function with SCO, we found, in less time than the other al-
gorithms, a slightly smaller function value:  47:761090859365858, with solution vector
0.040668102417464 0.147730393049955 0.783153291185250 0.001414221643059
0.485246633088859 0.000693172682617 0.027399339496606 0.017947274343948
0.037314369272343 0.096871356429511
in line with the earlier solutions.
3.4.4 Noisy Optimization
Innoisy optimization noisy
optimization, the objective function is unknown, but estimates of function val-
ues are available, e.g., via simulation. For example, to Ô¨Ånd an optimal prediction function
gin supervised learning, the exact risk `(g)=ELoss( Y;g(x)) is usually unknown and
only estimates of the risk are available. Optimizing the risk is thus typically a noisy op- + 20
timization problem. Noisy optimization features prominently in simulation studies where106 3.4. Monte Carlo for Optimization
the behavior of some system (e.g., vehicles on a road network) is simulated under certain
parameters (e.g., the lengths of the tra c light intervals) and the aim is to choose those
parameters optimally (e.g., to maximize the tra c throughput). For each parameter setting
the exact value for the objective function is unknown but estimates can be obtained via the
simulation.
In general, suppose the goal is to minimize a function S, where Sis unknown, but
an estimate of S(x) can be obtained for any choice of x2X. Because the gradient rSis
unknown, one cannot directly apply classical optimization methods. The stochastic approx-
imation method mimics the classical gradient descent method by replacing a deterministicstochastic
approximation gradient with an estimate crS(x).
A simple estimator for the i-th component ofrS(x) (that is,@S(u)=@xi), is the central
dierence estimator central
difference
estimatorbS(x+ei=2) bS(x ei=2)
; (3.28)
where eidenotes the i-th unit vector, and bS(x+ei=2) andbS(x ei=2) can be any estimators
ofS(x+ei=2) and S(x ei=2), respectively. The di erence parameter  >0 should be
small enough to reduce the bias of the estimator, but large enough to keep the variance of
the estimator small.
To reduce the variance in the estimator (3.28) it is important to have bS(x+ei=2)
andbS(x ei=2) positively correlated. This can for example be achieved by using
common random numbers common random
numbersin the simulation.
In direct analogy to gradient descent methods, the stochastic approximation method +412
produces a sequence of iterates, starting with some x12X, via
xt+1=xt tcrS(xt); (3.29)
where1;2;:::is a sequence of strictly positive step sizes. A generic stochastic approx-
imation algorithm for minimizing a function Sis thus as follows.
Algorithm 3.4.5: Stochastic Approximation
input: A mechanism to estimate any gradient rS(x) and step sizes 1;2;:::.
output: Approximate optimizer of S.
1Initialize x12X. Set t 1.
2while a stopping criterion is not met do
3 Obtain an estimated gradient crS(xt) ofSatxt.
4 Determine a step size t.
5 Setxt+1 xt tcrS(xt).
6 t t+1
7return xt
WhencrS(xt) is an unbiased estimator ofrS(xt) in (3.29) the stochastic approxima-
tion Algorithm 3.4.5 is referred to as the Robbins‚ÄìMonro algorithm. When Ô¨Ånite di er-Robbins ‚ÄìMonroences are used to estimate crS(xt), as in (3.28), the resulting algorithm is known as theChapter 3. Monte Carlo Methods 107
Kiefer‚ÄìWolfowitz algorithm. In Section 9.4.1 we will see how stochastic gradient descentKiefer ‚Äì
Wolfowitz is employed in deep learning to minimize the training loss, based on a ‚Äúminibatch‚Äù of
training data. +334
It can be shown [72] that, under certain regularity conditions on S, the sequence
x1;x2;:::converges to the true minimizer xwhen the step sizes decrease slowly enough
to 0; in particular, when
1X
t=1t=1and1X
t=12
t<1: (3.30)
In practice, one rarely uses step sizes that satisfy (3.30), as the convergence of the
sequence will be too slow to be of practical use.
An alternative approach to stochastic approximation is the stochastic counterpart stochastic
counterpartmethod, also called sample average approximation . It can be applied in situations where
the noisy objective function is of the form
S(x)=EeS(x;);x2X; (3.31)
whereis a random vector that can be simulated and eS(x;) can be evaluated exactly. The
idea is to replace the optimization of (3.31) with that of the sample average
bS(x)=1
NNX
i=1eS(x;i);x2X; (3.32)
where1;:::;Nare iid copies of . Note that bSis a deterministic function of xand so can
be optimized using any optimization algorithm. A solution to this sample average version
is taken to be an estimator of a solution xto the original problem (3.31).
Example 3.18 (Determining Good Importance Sampling Parameters) The selection
of good importance sampling parameters can be viewed as a stochastic optimization prob-
lem. Consider, for instance, the importance sampling estimator in Example 3.14. Recall + 94
that the nominal distribution is the uniform distribution on the square [  b;b]2, with pdf
fb(x)=1
(2b)2;x2[ b;b]2;
where bis large enough to ensure that bis close to; in that example, we chose b=1000.
The importance sampling pdf is
g(x)=fR;(r;)1
r=e r1
21
r=e p
x2
1+x2
2
2q
x2
1+x2
2;x=(x1;x2)2R2nf0g;
which depends on a free parameter . In the example we chose =0:1. Is this the best
choice? Maybe =0:05 or 0:2 would have resulted in a more accurate estimate. The im-
portant thing to realize is that the ‚Äúe ectiveness‚Äù of can be measured in terms of the
variance of the estimator bin (3.19), which is given by + 93108 3.4. Monte Carlo for Optimization
1
NVarg 
H(X)f(X)
g(X)!
=1
NEg"
H2(X)f2(X)
g2
(X)#
 2
N=1
NEf"
H2(X)f(X)
g(X)#
 2
N:
Hence, the optimal parameter minimizes the function S()=Ef[H2(X)f(X)=g(X)],
which is unknown, but can be estimated from simulation. To solve this stochastic minim-
ization problem, we Ô¨Årst use stochastic approximation. Thus, at each step of the algorithm,
the gradient of S() is estimated from realizations of bS()=H2(X)f(X)=g(X), where
Xfb. As in the original problem (that is, the estimation of ), the parameter bshould
be large enough to avoid any bias in the estimator of , but also small enough to en-
sure a small variance. The following Python code implements a particular instance of Al-
gorithm 3.4.5. For sampling from fbhere, we used b=100 instead of b=1000, as this will
improve the crude Monte Carlo estimation of , without noticeably a ecting the bias. The
gradient of S() is estimated in Lines 11‚Äì17, using the central di erence estimator (3.28).
Notice how for the S( =2) and S(+=2) the same random vector X=[X1;X2]>is used.
This signiÔ¨Åcantly reduces the variance of the gradient estimator; see also Exercise 23. The +118
step sizetshould be such that tcrS(xt)t. Given the large gradient here, we choose
0=10 7and decrease it each step by a factor of 0 :99. Figure 3.13 shows how the se-
quence0;1;:::decreases towards approximately 0 :125, which we take as an estimator
for the optimal importance sampling parameter .
stochapprox.py
import numpy as np
from numpy import pi
import matplotlib.pyplot as plt
b=100 # choose b large enough , but not too large
delta = 0.01
H = lambda x1, x2: (2*b)**2*np.exp(-np.sqrt(x1**2 + x2**2)/4)*(np.
sin(2*np.sqrt(x1**2+x2**2)+1))*(x1**2+x2**2<b**2)
f = 1/(2*b)**2
g = lambda x1, x2, lam: lam*np.exp(-np.sqrt(x1**2+x2**2)*lam)/np.
sqrt(x1**2+x2**2)/(2*pi)
beta = 10**-7 #step size very small , as the gradient is large
lam=0.25
lams = np.array([lam])
N=10**4
for iin range (200):
x1 = -b + 2*b*np.random.rand(N,1)
x2 = -b + 2*b*np.random.rand(N,1)
lamL = lam - delta/2
lamR = lam + delta/2
estL = np.mean(H(x1,x2)**2*f/g(x1, x2, lamL))
estR = np.mean(H(x1,x2)**2*f/g(x1, x2, lamR)) #use SAME x1,x2
gr = (estR -estL)/delta #gradient
lam = lam - gr*beta #gradient descend
lams = np.hstack((lams , lam))
beta = beta*0.99
lamsize= range (0, (lams.size))
plt.plot(lamsize , lams)
plt.show()Chapter 3. Monte Carlo Methods 109
0
 25
 50
 75
 100
 125
 150
 175
 200
steps
0.12
0.14
0.16
0.18
0.20
0.22
0.24
Figure 3.13: The stochastic optimization algorithm produces a sequence t;t=0;1;2;:::
that tends to an approximate estimate of the optimal importance sampling parameter 
0:125.
Next, we estimate using a stochastic counterpart approach. As the objective function
S() is of the form (3.31) (with taking the role of xandXthe role of), we obtain the
sample average
bS()=1
NNX
i=1H2(Xi)f(Xi)
g(Xi); (3.33)
where X1;:::; XNiidfb. Once the X1;:::; XNiidfbhave been simulated, bS() is a de-
terministic function of , which can be optimized by any means. We take the most basic
approach and simply evaluate the function for =0:01;0:02;:::; 0:3 and select the min-
imizingon this grid. The code is given below and Figure 3.14 shows bS() as a function
of. The minimum value found was 0 :60104for minimizer b=0:12, which is in accord-
ance with the value obtained via stochastic approximation. The sensitivity of this estimate
can be assessed from the graph: for a wide range of values (say from 0.04 to 0.15) bSstays
rather Ô¨Çat. So any of these values could be used in an importance sampling procedure to
estimate. However, very small values (less than 0.02) and large values (greater than 0.25)
should be avoided. Our original choice of =0:1 was therefore justiÔ¨Åed and we could not
have done much better.
stochcounterpart.py
from stochapprox import *
lams = np.linspace(0.01, 0.31, 1000)
res=[]
res = np.array(res)
for iin range (lams.size):
lam = lams[i]
np.random.seed(1)
g = lambda x1, x2: lam*np.exp(-np.sqrt(x1**2+x2**2)*lam)/np.sqrt
(x1**2+x2**2)/(2*pi)110 3.4. Monte Carlo for Optimization
X=-b+2*b*np.random.rand(N,1)
Y=-b+2*b*np.random.rand(N,1)
Z=H(X,Y)**2*f/g(X,Y)
estCMC = np.mean(Z)
res = np.hstack((res, estCMC))
plt.plot(lams , res)
plt.xlabel(r '$\lambda$ ')
plt.ylabel(r '$\hat{S}(\lambda)$ ')
plt.ticklabel_format(style= 'sci', axis= 'y', scilimits=(0,0))
plt.show()
0.00
 0.05
 0.10
 0.15
 0.20
 0.25
 0.30
0.5
1.0
1.5
2.0
2.5
3.0S( )1e4
Figure 3.14: The stochastic counterpart method replaces the unknown S() (that is, the
scaled variance of the importance sampling estimator) with its sample average, bS(). The
minimum value of bSis attained around =0:12.
A third method for stochastic optimization is the cross-entropy method. In particular,
Algorithm 3.4.3 can easily be modiÔ¨Åed to minimize noisy functions S(x)=EeS(x;), as +101
deÔ¨Åned in (3.31). The only change required in the algorithm is that every function value
S(x) be replaced by its estimate bS(x). Depending on the level of noise in the function, the
sample size Nmight have to be increased considerably.
Example 3.19 (Cross-Entropy Method for Noisy Optimization) To explore the use
of the CE method for noisy optimization, take the following noisy discrete optimization
problem. Suppose there is a ‚Äúblack box‚Äù that contains an unknown binary sequence of n
bits. If one feeds the black box any input vector, it will Ô¨Årst scramble the input by inde-
pendently Ô¨Çipping the bits (changing 0 to 1 and 1 to 0) with a probability and then return
the number of bits that do not match the true (unknown) binary sequence. This is illustrated
in Figure 3.15 for n=10.Chapter 3. Monte Carlo Methods 111
Figure 3.15: A noisy optimization function as a black box. The input to the black box is a
binary vector. Inside the black box the digits of the input vector are scrambled by Ô¨Çipping
bits with probability . The output is the number of bits of the scrambled vector that do not
match the true (unknown) binary vector.
Denoting by S(x) the true number of matching digits for a binary input vector x, the
black box thus returns a noisy estimate bS(x). The objective is to estimate the binary se-
quence inside the black box, by feeding it with many input vectors and observing their
output. Or, to put it in a di erent way, to minimize S(x) using bS(x) as a proxy. Since there
are 2npossible input vectors, it is infeasible to try all possible vectors xeven for moderate
n.
The following Python program implements the noisy function bS(x) for n=100. Each
input bit is Ô¨Çipped with a rather high probability =0:4, so that the output is a poor indic-
ator of how many bits actually match the true vector. This true vector has 1s at positions
1;:::; 50 and 0s at 51 ;:::; 100.
Snoisy.py
import numpy as np
def Snoisy(X): #takes a matrix
n = X.shape[1]
N = X.shape[0]
# true binary vector
xorg = np.hstack((np.ones((1,n//2)), np.zeros((1,n//2))))
theta = 0.4 # probability to flip the input
# storing the number of bits unequal to the true vector
s = np.zeros(N)
for iin range (0,N):
# determine which bits to flip
flip = (np.random.uniform(size=(n)) < theta).astype( int)
ind = flip >0
X[i][ind] = 1-X[i][ind]
s[i] = (X[i] != xorg). sum()
return s
The CE code below to optimize S(x) is quite similar to the continuous optimization
code in Example 3.16. However, instead of sampling iid random variables X1;:::; XNfrom +101
a normal distribution, we now sample iid binary vectors X1;:::; XNfrom a Ber(p) distribu-
tion. More precisely, given a row vector of probabilities p=[p1;:::; pn], we independently
simulate the components X1;:::; Xnof each binary vector Xaccording to XiBer(pi),
i=1;:::; n. After each iteration, the vector pis updated as the (vector) mean of the elite112 3.4. Monte Carlo for Optimization
samples. The sample size is N=1000 and the number of elite samples is 100. The compon-
ents of the initial sampling vector pare all equal to 1 =2; that is, the Xare initially uniformly
sampled from the set of all binary vectors of length n=100. At each subsequent iteration
the parameter vector is updated via the mean of the elite samples and evolves towards a
degenerate vector pwith only 1s and 0s. Sampling from such a Ber(p) distribution gives
an outcome x=p, which can be taken as an estimate for the minimizer of S; that is, the
true binary vector hidden in the black box. The algorithm stops when phas degenerated
suciently.
Figure 3.16 shows the evolution of the vector of probabilities p. This Ô¨Ågure may be
seen as the discrete analogue of Figure 3.12. We see that, despite the high noise, the CE
method is able to Ô¨Ånd the true state of the black box, and hence the minimum value of S.
00.51
00.51
00.51
00.51
00.51
0 10 20 30 40 50 60 70 80 90 10000.51
Figure 3.16: Evolution of the vector of probabilities p=[p1;:::; pn] towards the degener-
ate solution.Chapter 3. Monte Carlo Methods 113
CEnoisy.py
from Snoisy import Snoisy
import numpy as np
n = 100
rho = 0.1
N = 1000; Nel = int(N*rho); eps = 0.01
p = 0.5*np.ones(n)
i = 0
pstart = p
ps = np.zeros((1000,n))
ps[0] = pstart
pdist = np.zeros((1,1000))
while np.max(np.minimum(p,1-p)) > eps:
i += 1
X = (np.random.uniform(size=(N,n)) < p).astype( int)
X_tmp = np.array(X, copy=True)
SX = Snoisy(X_tmp)
ids = np.argsort(SX,axis=0)
Elite = X[ids[0:Nel],:]
p = np.mean(Elite ,axis=0)
ps[i] = p
print (p)
Further Reading
The article [68] explores why the Monte Carlo method is so important in today‚Äôs quantitat-
ive investigations. The Handbook of Monte Carlo Methods [71] provides a comprehensive
overview of Monte Carlo simulation that explores the latest topics, techniques, and real-
world applications. Popular books on simulation and the Monte Carlo method include [42],
[75], and [104]. A classic reference on random variable generation is [32]. Easy introduc-
tions to stochastic simulation are given in [49], [98], and [100]. More advanced theory
can be found in [5]. Markov chain Monte Carlo is detailed in [50] and [99]. The research
monograph on the cross-entropy method is [103] and a tutorial is provided in [30]. A range
of optimization applications of the CE method is given in [16]. Theoretical results on ad-
aptive tuning schemes for simulated annealing may be found, for example, in [111]. There
are several established ways for gradient estimation. These include the Ô¨Ånite di erence
method, inÔ¨Ånitesimal perturbation analysis, the score function method, and the method of
weak derivatives; see, for example, [51, Chapter 7].
Exercises
1. We can modify the Box‚ÄìMuller method in Example 3.1 to draw XandYuniformly + 69
on the unit disc,f(x;y)2R2:x2+y261g, in the following way: Independently draw
a radius Rand an angle U(0;2), and return X=Rcos();Y=Rsin(). The
question is how to draw R.
(a) Show that the cdf of Ris given by FR(r)=r2for 06r61 (with FR(r)=0 and114 Exercises
FR(r)=1 for r<0 and r>1, respectively).
(b) Explain how to simulate Rusing the inverse-transform method.
(c) Simulate 100 independent draws of [ X;Y]>according to the method described
above.
2. A simple acceptance‚Äìrejection method to simulate a vector Xin the unit d-ballfx2
Rd:kxk61gis to Ô¨Årst generate Xuniformly in the hyper cube [  1;1]dand then to
accept the point only if kXk61. Determine an analytic expression for the probability
of acceptance as a function of dand plot this for d=1;:::; 50.
3. Let the random variable Xhave pdf
f(x)=8>><>>:1
2x; 06x<1;
1
2; 16x65
2:
Simulate a random variable from f(x), using
(a) the inverse-transform method;
(b) the acceptance‚Äìrejection method, using the proposal density
g(x)=8
25x;06x65
2:
4. Construct simulation algorithms for the following distributions:
(a) The Weib (;) distribution, with cdf F(x)=1 e (x);x>0, where>0 and
>0.
(b) The Pareto (;) distribution, with pdf f(x)=(1+x) (+1);x>0, where
>0 and>0.
5. We wish to sample from the pdf
f(x)=xe x;x>0;
using acceptance‚Äìrejection with the proposal pdf g(x)=e x=2=2,x>0.
(a) Find the smallest Cfor which Cg(x)>f(x) for all x.
(b) What is the e ciency of this acceptance‚Äìrejection method?
6. Let [ X;Y]>be uniformly distributed on the triangle with corners (0 ;0);(1;2), and
( 1;1). Give the distribution of [ U;V]>deÔ¨Åned by the linear transformation
"U
V#
="1 2
3 4#"X
Y#
:
7. Explain how to generate a random variable from the extreme value distribution ,
which has cdf
F(x)=1 e exp(x 
); 1<x<1;(>0);
via the inverse-transform method.Chapter 3. Monte Carlo Methods 115
8. Write a program that generates and displays 100 random vectors that are uniformly
distributed within the ellipse
5x2+21x y+25y2=9:
[Hint: Consider generating uniformly distributed samples within the circle of radius
3 and use the fact that linear transformations preserve uniformity to transform the
circle to the given ellipse.]
9. Suppose that XiExp(i), independently, for all i=1;:::; n. Let=[1;:::;n]>
be the random permutation induced by the ordering X1<X2<<Xn;and
deÔ¨Åne Z1:=X1andZj:=Xj Xj 1forj=2;:::; n.
(a) Determine an nnmatrix Asuch that Z=AXand show that det( A)=1.
(b) Denote the joint pdf of Xandas
fX;(x;)=nY
i=1iexp  ixi1fx1<<xng;x>0;2P n;
wherePnis the set of all n! permutations of f1;:::; ng. Use the multivariate
transformation formula (C.22) to show that +432
fZ;(z;)=exp0BBBBB@ nX
i=1ziX
k>ik1CCCCCAnY
i=1i; z>0;2P n:
Hence, conclude that the probability mass function of the random permutation
is:
P[=]=nY
i=1iP
k>ik;2P n:
(c) Write pseudo-code to simulate a uniform random permutation 2P n; that is,
such thatP[=]=1
n!, and explain how this uniform random permutation
can be used to reshu e a training set n.
10. Consider the Markov chain with transition graph given in Figure 3.17, starting in
state 1.
Start0.51
0.80.9 0 .20.10.5
0.2
0.30.50.30.7
43
12 6
5
Figure 3.17: The transition graph for the Markov chain fXt;t=0;1;2;:::g.116 Exercises
(a) Construct a computer program to simulate the Markov chain, and show a real-
ization for N=100 steps.
(b) Compute the limiting probabilities that the Markov chain is in state 1,2,. . . ,6,
by solving the global balance equations (C.42). +452
(c) Verify that the exact limiting probabilities correspond to the average fraction
of times that the Markov process visits states 1,2,. . . ,6, for a large number of
steps N.
11. As a generalization of Example C.9, consider a random walk on an arbitrary undir- +453
ected connected graph with a Ô¨Ånite vertex set V. For any vertex v2V, letd(v) be
the number of neighbors of v‚Äî called the degree ofv. The random walk can jump to
each one of the neighbors with probability 1 =d(v) and can be described by a Markov
chain. Show that, if the chain is aperiodic , the limiting probability that the chain is
in state vis equal to d(v)=P
v02Vd(v0).
12. Let U;ViidU(0;1). The reason why in Example 3.7 the sample mean and sample +76
median behave very di erently is that E[U=V]=1, while the median of U=Vis
Ô¨Ånite. Show this, and compute the median. [Hint: start by determining the cdf of
Z=U=Vby writing it as an expectation of an indicator function.]
13. Consider the problem of generating samples from YGamma (2;10).
(a) Direct simulation: Let U1;U2iidU(0;1). Show that ln(U1)=10 ln(U2)=10
Gamma (2;10). [Hint: derive the distribution of  ln(U1)=10 and use Ex-
ample C.1.] +427
(b) Simulation via MCMC: Implement an independence sampler to simulate from
theGamma (2;10) target pdf
f(x)=100xe 10x;x>0;
using proposal transition density q(yjx)=g(y), where g(y) is the pdf of an
Exp(5) random variable. Generate N=500 samples, and compare the true cdf
with the empirical cdf of the data.
14. Let X=[X;Y]>be a random column vector with a bivariate normal distribution with
expectation vector =[1;2]>and covariance matrix
="1a
a4#
:
(a) What are the conditional distributions of ( YjX=x) and ( XjY=y)? [Hint: use
Theorem C.8.] +436
(b) Implement a Gibbs sampler to draw 103samples from the bivariate distribution
N(;) for a=0, 1, and 1:75, and plot the resulting samples.
15. Here the objective is to sample from the 2-dimensional pdf
f(x;y)=ce (xy+x+y);x>0;y>0;
for some normalization constant c, using a Gibbs sampler. Let ( X;Y)f.Chapter 3. Monte Carlo Methods 117
(a) Find the conditional pdf of Xgiven Y=y, and the conditional pdf of Ygiven
X=x.
(b) Write working Python code that implements the Gibbs sampler and outputs
1000 points that are approximately distributed according to f.
(c) Describe how the normalization constant ccould be estimated via Monte Carlo
simulation, using random variables X1;:::; XN;Y1;:::; YNiidExp(1).
16. We wish to estimate =R2
 2e x2=2dx=R
H(x)f(x) dxvia Monte Carlo simulation
using two di erent approaches: (1) deÔ¨Åning H(x)=4 e x2=2and fthe pdf of the
U[ 2;2] distribution and (2) deÔ¨Åning H(x)=p
21f 26x62gand fthe pdf of
theN(0;1) distribution.
(a) For both cases estimate via the estimator b
b=N 1NX
i=1H(Xi): (3.34)
Use a sample size of N=1000.
(b) For both cases estimate the relative error ofbusing N=100.
(c) Give a 95% conÔ¨Ådence interval for for both cases using N=100.
(d) From part (b), assess how large Nshould be such that the relative width of the
conÔ¨Ådence interval is less than 0 :01, and carry out the simulation with this N.
Compare the result with the true value of .
17. Consider estimation of the tail probability =P[X>] of some random variable X,
whereis large. The crude Monte Carlo estimator of is
b=1
NNX
i=1Zi; (3.35)
where X1;:::; XNare iid copies of XandZi=1fXi>g,i=1;:::; N.
(a) Show that bis unbiased; that is, Eb=.
(b) Express the relative error of b, i.e.,
RE=p
Varb
Eb;
in terms of Nand.
(c) Explain how to estimate the relative error of bfrom outcomes x1;:::; xNof
X1;:::; XN, and how to construct a 95% conÔ¨Ådence interval for .
(d) An unbiased estimator Zofis said to be logarithmically e cient if
lim
!1lnEZ2
ln2=1: (3.36)
Show that the CMC estimator (3.35) with N=1 is not logarithmically e cient.118 Exercises
18. One of the test cases in [70] involves the minimization of the Hougen function. Im-
plement a cross-entropy and a simulated annealing algorithm to carry out this optim-
ization task.
19. In the binary knapsack problem , the goal is to solve the optimization problem:
max
x2f0;1gnp>x;
subject to the constraints
Ax6c;
where pandwaren1 vectors of non-negative numbers, A=(ai j) is an mn
matrix, and cis an m1 vector. The interpretation is that xj=1 or 0 depending
on whether item jwith value pjis packed into the knapsack or not , j=1;:::; n;
The variable ai jrepresents the i-th attribute (e.g., volume, weight) of the j-th item.
Associated with each attribute is a maximal capacity, e.g., c1could be the maximum
volume of the knapsack, c2the maximum weight, etc.
Write a CE program to solve the Sento1.dat knapsack problem at http://peop
le.brunel.ac.uk/~mastjjb/jeb/orlib/files/mknap2.txt , as described in
[16].
20. Let ( C1;R1);(C2;R2);::: be a renewal reward process, with ER1<1and
EC1<1. Let At=PNt
i=1Ri=tbe the average reward at time t=1;2;:::, where
Nt=maxfn:Tn6tgand we have deÔ¨Åned Tn=Pn
i=1Cias the time of the n-th re-
newal.
(a) Show that Tn=na:s: !EC1asn!1 .
(b) Show that Nta:s: !1 ast!1 .
(c) Show that Nt=ta:s: !1=EC1ast!1 . [Hint: Use the fact that TNt6t6TNt+1for
allt=1;2;:::.]
(d) Show that
Ata:s: !ER1
EC1ast!1:
21. Prove Theorem 3.3. +92
22. Prove that if H(x)>0 the importance sampling pdf gin (3.22) gives the zero- +96
variance importance sampling estimator b=.
23. Let XandYbe random variables (not necessarily independent) and suppose we wish
to estimate the expected di erence=E[X Y]=EX EY.
(a) Show that if XandYarepositively correlated , the variance of X Yis smaller
than if XandYareindependent .
(b) Suppose now that Xand Yhave cdfs Fand G, respectively, and are
simulated via the inverse-transform method: X=F 1(U),Y=G 1(V), with
U;VU(0;1), not necessarily independent. Intuitively, one might expect thatChapter 3. Monte Carlo Methods 119
ifUandVare positively correlated, the variance of X Ywould be smaller than
ifUandVare independent. Show that this is not always the case by providing
a counter-example.
(c) Continuing (b), assume now that FandGare continuous. Show that the vari-
ance of X Yby taking common random numbers U =Vis no larger than
when UandVare independent. [Hint: Use the following lemma of Hoe ding
[41]: If ( X;Y) have joint cdf Hwith marginal cdfs of XandYbeing FandG,
respectively, then
Cov(X;Y)=Z1
 1Z1
 1(H(x;y) F(x)G(y))dxdy;
providedCov(X;Y) exists.]120CHAPTER4
UNSUPERVISED LEARNING
When there is no distinction between response and explanatory variables, unsu-
pervised methods are required to learn the structure of the data. In this chapter we
look at various unsupervised learning techniques, such as density estimation, cluster-
ing, and principal component analysis. Important tools in unsupervised learning in-
clude the cross-entropy training loss, mixture models, the Expectation‚ÄìMaximization
algorithm, and the Singular Value Decomposition.
4.1 Introduction
In contrast to supervised learning, where an ‚Äúoutput‚Äù (response) variable yis explained by
an ‚Äúinput‚Äù (explanatory) vector x, in unsupervised learning there is no response variable
and the overall goal is to extract useful information and patterns from the data, e.g., in
the form=fx1;:::; xngor as a matrix X>=[x1;:::; xn]. In essence, the objective of
unsupervised learning is to learn about the underlying probability distribution of the data.
We start in Section 4.2 by setting up a framework for unsupervised learning that is
similar to the framework used for supervised learning in Section 2.3. That is, we formulate + 23
unsupervised learning in terms of risk and loss minimization; but now involving the cross-
entropy risk, rather than the squared-error risk. In a natural way this leads to fundamental
learning concepts such as likelihood, Fisher information, and the Akaike information cri-
terion. Section 4.3 introduces the Expectation‚ÄìMaximization (EM) algorithm as a useful
method for maximizing likelihood functions when their solution cannot be found easily in
closed form.
If the data forms an iid sample from some unknown distribution, the ‚Äúempirical dis-
tribution‚Äù of the data provides valuable information about the unknown distribution. In
Section 4.4 we formalize the concept of the empirical distribution (a generalization of the
empirical cdf) and explain how we can produce an estimate of the underlying probability + 11
density function of the data using kernel density estimators.
Most unsupervised learning techniques focus on identifying certain traits of the under-
lying distribution, such as its local maximizers. A related idea is to partition the data into
clusters of points that are in some sense ‚Äúsimilar‚Äù to each other. In Section 4.5 we formu-
late the clustering problem in terms of a mixture model. In particular, the data are assumed +135
121122 4.2. Risk and Loss in Unsupervised Learning
to come from a mixture of (usually Gaussian) distributions, and the objective is to recover
the parameters of the mixture distributions from the data. The principal tool for parameter
estimation in mixture models is the EM algorithm.
Section 4.6 discusses a more heuristic approach to clustering, where the data are
grouped according to certain ‚Äúcluster centers‚Äù, whose positions are found by solving an
optimization problem. Section 4.7 describes how clusters can be constructed in a hierarch-
ical manner.
Finally, in Section 4.8 we discuss the unsupervised learning technique called Principal
Component Analysis (PCA), which is an important tool for reducing the dimensionality of
the data.
We will revisit various unsupervised learning techniques in subsequent chapters on su-
pervised learning. For example, cross-entropy training loss minimization will be important
in logistic regression (Section 5.7) and classiÔ¨Åcation (Chapter 7), and PCA can be used +204
+251 for variable selection and dimensionality reduction, to make models easier to train and
increase their predictive power; see e.g., Sections 6.8 and 7.4.
4.2 Risk and Loss in Unsupervised Learning
In unsupervised learning, the training data T:=fX1;:::; Xngonly consists of (what are
usually assumed to be) independent copies of a feature vector X; there is no response
data. Suppose our objective is to learn the unknown pdf fofXbased on an outcome
=fx1;:::; xngof the training data T. Conveniently, we can follow the same line of reas-
oning as for supervised learning, discussed in Sections 2.3‚Äì2.5. Table 4.1 gives a summary +23
of deÔ¨Ånitions for the case of unsupervised learning. Compare this with Table 2.1 for the
supervised case. +25
Similar to supervised learning, we wish to Ô¨Ånd a function g, which is now a probability
density (continuous or discrete), that best approximates the pdf fin terms of minimizing a
risk
`(g) :=ELoss( f(X);g(X)); (4.1)
where Loss is a loss function. In (2.27), we already encountered the Kullback‚ÄìLeibler risk
`(g) :=Elnf(X)
g(X)=Elnf(X) Elng(X): (4.2)
IfGis a class of functions that contains f, then minimizing the Kullback‚ÄìLeibler risk over
Gwill yield the (correct) minimizer f. Of course, the problem is that minimization of (4.2)
depends on f, which is generally not known. However, since the term Elnf(X) does not
depend on g, it plays no role in the minimization of the Kullback‚ÄìLeibler risk. By removing
this term, we obtain the cross-entropy risk cross -entropy
risk(for discrete Xreplace the integral with a sum):
`(g) := Elng(X)= Z
f(x) lng(x) dx: (4.3)
Thus, minimizing the cross-entropy risk (4.3) over all g2G, again gives the minimizer
f, provided that f2G. Unfortunately, solving (4.3) is also infeasible in general, as it stillChapter 4. Unsupervised Learning 123
Table 4.1: Summary of deÔ¨Ånitions for unsupervised learning.
x Fixed feature vector.
X Random feature vector.
f(x) Pdf of Xevaluated at the point x.
orn Fixed training data fxi;i=1;:::; ng.
TorTn Random training data fXi;i=1;:::; ng.
g Approximation of the pdf f.
Loss( f(x);g(x)) Loss incurred when approximating f(x) with g(x).
`(g) Risk for approximation function g; that is,ELoss( f(X);g(X)).
gGOptimal approximation function in function class G; that is,
argming2G`(g).
`(g) Training loss for approximation function (guess) g; that is,
the sample average estimate of `(g) based on a Ô¨Åxed training
sample.
`T(g) The same as `(g), but now for a random training sample T.
gG
org Thelearner : argming2G`(g). That is, the optimal approxima-
tion function based on a Ô¨Åxed training set and function class
G. We suppress the superscript Gif the function class is impli-
cit.
gG
TorgT The learner for a random training set T.
depends on f. Instead, we seek to minimize the cross-entropy training loss cross -entropy
training loss:
`(g) :=1
nnX
i=1Loss( f(xi);g(xi))= 1
nnX
i=1lng(xi) (4.4)
over the class of functions G, where=fx1;:::; xngis an iid sample from f. This optimiz-
ation is doable without knowing fand is equivalent to solving the maximization problem
max
g2GnX
i=1lng(xi): (4.5)
A key step in setting up the learning procedure is to select a suitable function class Gover
which to optimize. The standard approach is to parameterize gwith a parameter and let
Gbe the class of functions fg(j);2gfor some p-dimensional parameter set . For the
remainder of Section 4.2, we will be using this function class, as well as the cross-entropy
risk.
The function 7!g(xj) is called the likelihood function likelihood
function. It gives the likelihood of
the observed feature vector xunder g(j), as a function of the parameter . The natural
logarithm of the likelihood function is called the log-likelihood function and its gradient
with respect to is called the score function score function , denoted S(xj); that is,
S(xj) :=@lng(xj)
@=@g(xj)
@
g(xj): (4.6)124 4.2. Risk and Loss in Unsupervised Learning
The random score S(Xj), with Xg(j), is of particular interest. In many cases, its
expectation is equal to the zero vector ; namely,
ES(Xj)=Z@g(xj)
@
g(xj)g(xj) dx
=Z@g(xj)
@dx=@R
g(xj) dx
@=@1
@=0;(4.7)
provided that the interchange of di erentiation and integration is justiÔ¨Åed. This is true for
a large number of distributions, including the normal, exponential, and binomial distri-
butions. Notable exceptions are distributions whose support depends on the distributional
parameter; for example the U(0;) distribution.
It is important to see whether expectations are taken with respect to Xg(j) or
Xf. We use the expectation symbols EandEto distinguish the two cases.
From now on we simply assume that the interchange of di erentiation and integration
is permitted; see, e.g., [76] for su cient conditions. The covariance matrix of the random
score S(Xj) is called the Fisher information matrix Fisher
information
matrix, which we denote by ForF() to
show its dependence on . Since the expected score is 0, we have
F()=E[S(Xj)S(Xj)>]: (4.8)
A related matrix is the expected Hessian matrix of  lng(Xj): +398
H() :=E"
 @S(Xj)
@#
= E26666666666666666666664@2lng(Xj)
@21@2lng(Xj)
@1@2@2lng(Xj)
@1@p
@2lng(Xj)
@2@1@2lng(Xj)
@22@2lng(Xj)
@2@p::::::::::::
@2lng(Xj)
@p@1@2lng(Xj)
@p@2@2lng(Xj)
@2p37777777777777777777775: (4.9)
Note that the expectation here is with respect to Xf. It turns out that if f=g(j), the
two matrices are the same ; that is,
F()=H(); (4.10)
provided that we may swap the order of di erentiation and integration (expectation). This
result is called the information matrix equality information
matrix equality. We leave the proof as Exercise 1.
The matrices F() and H() play important roles in approximating the cross-entropy
risk for large n. To set the scene, let gG=g(j) be the minimizer of the cross-entropy
risk
r() := Elng(Xj):
We assume that r, as a function of , is well-behaved; in particular, that in the neighborhood
ofit is strictly convex and twice continuously di erentiable (this holds true, for example,
ifgis a Gaussian density). It follows that is a root ofES(Xj), because
0=@r()
@= @Elng(Xj)
@= E@lng(Xj)
@= ES(Xj);Chapter 4. Unsupervised Learning 125
again provided that the order of di erentiation and integration (expectation) can be
swapped. In the same way, H() is then the Hessian matrix of r. Let g(jbn) be the minim-
izer of the training loss
rTn() := 1
nnX
i=1lng(Xij);
whereTn=fX1;:::; Xngis a random training set. Let rbe the smallest possible cross-
entropy risk, taken over all functions; clearly, r= Elnf(X), where Xf. Similar to
the supervised learning case, we can decompose the generalization risk, `(g(jbn))=r(bn),
into
r(bn)=r+r() r
|     {z     }
approx. error+r(bn) r()|         {z         }
statistical error=r() Elng(Xj)
g(Xjbn):
The following theorem speciÔ¨Åes the asymptotic behavior of the components of the gener-
alization risk. In the proof we assume that bnP !asn!1 . +439
Theorem 4.1: Approximating the Cross-Entropy Risk
It holds asymptotically ( n!1 ) that
Er(bn) r()'tr
F()H 1()
=(2n); (4.11)
where
r()'ErTn(bn)+tr
F()H 1()
=(2n): (4.12)
Proof: A Taylor expansion of r(bn) aroundgives the statistical error +400
r(bn) r()=(bn )>@r()
@| {z }
=0+1
2(bn )>H(n)(bn ); (4.13)
wherenlies on the line segment between andbn. For large nwe may replace H(n) with
H() as, by assumption, bnconverges to . The matrix H() is positive deÔ¨Ånite because
r() is strictly convex at by assumption, and therefore invertible. It is important to realize
thatbnis in fact an M-estimator of . In particular, in the notation of Theorem C.19, we +449
have =S,A=H(), and B=F(). Consequently, by that same theorem,
pn(bn )d !N
0;H 1()F()H >()
: (4.14)
Combining (4.13) with (4.14), it follows from Theorem C.2 that asymptotically the +430
expected estimation error is given by (4.11).
Next, we consider a Taylor expansion of rTn() around bn:
rTn()=rTn(bn)+( bn)>@rTn(bn)
@|   {z   }
=0+1
2( bn)>HTn(n)( bn); (4.15)126 4.2. Risk and Loss in Unsupervised Learning
where HTn(n) := 1
nPn
i=1@S(Xijn)
@is the Hessian of rTn() at somenbetween bnand.
Taking expectations on both sides of (4.15), we obtain
r()=ErTn(bn)+1
2E( bn)>HTn(n)( bn):
Replacing HTn(n) with H() for large nand using (4.14), we have
nE( bn)>HTn(n)( bn) !tr
F()H 1()
;n!1:
Therefore, asymptotically as n!1 , we have (4.12). 
Theorem 4.1 has a number of interesting consequences:
1. Similar to Section 2.5.1, the training loss `Tn(gTn)=rTn(bn) tends to underestimate the +35
risk`(gG)=r(), because the training set Tnis used to both train g2G(that is, estimate
) and to estimate the risk. The relation (4.12) tells us that on average the training loss
underestimates the true risk by tr( F()H 1())=(2n).
2. Adding equations (4.11) and (4.12), yields the following asymptotic approximation to
the expected generalization risk:
Er(bn)'ErTn(bn)+1
ntr
F()H 1()
(4.16)
The Ô¨Årst term on the right-hand side of (4.16) can be estimated (without bias) via the
training loss rTn(bn). As for the second term, we have already mentioned that when the
true model f2G, then F()=H(). Therefore, when Gis deemed to be a su ciently
rich class of models parameterized by a p-dimensional vector , we may approximate the
second term as tr( F()H 1())=ntr(Ip)=n=p=n. This suggests the following heuristic
approximation to the (expected) generalization risk:
Er(bn)rTn(bn)+p
n: (4.17)
3. Multiplying both sides of (4.16) by 2 nand substituting tr
F()H 1()
p, we obtain
the approximation:
2n r(bn) 2nX
i=1lng(Xijbn)+2p: (4.18)
The right-hand side of (4.18) is called the Akaike information criterion Akaike
information
criterion(AIC). Just like
(4.17), the AIC approximation can be used to compare the di erence in generalization risk
of two or more learners. We prefer the learner with the smallest (estimated) generalization
risk.
Suppose that, for a training set T, the training loss rT() has a unique minimum point
bwhich lies in the interior of . IfrT() is a di erentiable function with respect to , then
we can Ô¨Ånd the optimal parameter bby solving
@rT()
@=1
nnX
i=1S(Xij)
|           {z           }
ST()=0:Chapter 4. Unsupervised Learning 127
In other words, the maximum likelihood estimate bforis obtained by solving the root of
the average score function, that is, by solving
ST()=0: (4.19)
It is often not possible to Ô¨Ånd bin an explicit form. In that case one needs to solve the
equation (4.19) numerically. There exist many standard techniques for root-Ô¨Ånding, e.g.,
viaNewton‚Äôs method (see Section B.3.1), whereby, starting from an initial guess 0, sub-Newton ‚Äôs
method
+409sequent iterates are obtained via the iterative scheme
t+1=t+H 1
T(t)ST(t);
where
HT() := @ST()
@=1
nnX
i=1 @S(Xij)
@
is the average Hessian matrix of f lng(Xij)gn
i=1. Under f=g(j), the expectation of
HT() is equal to the information matrix F(), which does not depend on the data. This
suggests an alternative iterative scheme, called Fisher‚Äôs scoring method Fisher ‚Äôs
scoring method:
t+1=t+F 1(t)ST(t); (4.20)
which is not only easier to implement (if the information matrix can be readily evaluated),
but also is more numerically stable.
Example 4.1 (Maximum Likelihood for the Gamma Distribution) We wish to ap-
proximate the density of the Gamma (;) distribution for some true but unknown para-
metersand, on the basis of a training set =fx1;:::; xngof iid samples from this
distribution. Choosing our approximating function g(j;) in the same class of gamma
densities,
g(xj;)=x 1e x
 ();x>0; (4.21)
with>0 and>0, we seek to solve (4.19). Taking the logarithm in (4.21), the log-
likelihood function is given by
l(xj;) :=ln ln ()+( 1) ln x x:
It follows that
S(;)=266664@
@l(xj;)
@
@l(xj;)377775="ln  ()+lnx

 x#
;
where is the derivative of ln  : the so-called digamma function digamma
function. Hence,
H(;)= E2666664@2
@2l(Xj;)@2
@@l(Xj;)
@2
@@l(Xj;)@2
@2l(Xj;)3777775= E"  0()1
1
 
2#
=" 0() 1

 1

2#
:
Fisher‚Äôs scoring method (4.20) can now be used to solve (4.19), with
S(;)="ln  ()+n 1Pn
i=1lnxi

 n 1Pn
i=1xi#
andF(;)=H(;).128 4.3. Expectation‚ÄìMaximization (EM) Algorithm
4.3 Expectation‚ÄìMaximization (EM) Algorithm
TheExpectation‚ÄìMaximization algorithm (EM) is a general algorithm for maximization of
complicated (log-)likelihood functions, through the introduction of auxiliary variables.
To simplify the notation in this section, we use a Bayesian notation system, where
the same symbol is used for di erent (conditional) probability densities.
As in the previous section, given independent observations =fx1;:::; xngfrom some
unknown pdf f, the objective is to Ô¨Ånd the best approximation to fin a function class
G=fg(j);2gby solving the maximum likelihood problem:
=argmax
2g(j); (4.22)
where g(j) :=g(x1j)g(xnj). The key element of the EM algorithm is the aug-
mentation of the data with a suitable vector of latent variables latent
variables,z, such that
g(j)=Z
g(;zj) dz:
The function 7!g(;zj) is usually referred to as the complete-data likelihood complete -data
likelihoodfunction.
The choice of the latent variables is guided by the desire to make the maximization of
g(;zj) much easier than that of g(j).
Suppose pdenotes an arbitrary density of the latent variables z. Then, we can write:
lng(j)=Z
p(z) lng(j) dz
=Z
p(z) ln g(;zj)=p(z)
g(zj;)=p(z)!
dz
=Z
p(z) ln g(;zj)
p(z)!
dz Z
p(z) ln g(zj;)
p(z)!
dz
=Z
p(z) ln g(;zj)
p(z)!
dz+D(p;g(j;)); (4.23)
whereD(p;g(j;)) is the Kullback‚ÄìLeibler divergence from the density ptog(j;). +42
SinceD>0, it follows that
lng(j)>Z
p(z) ln g(;zj)
p(z)!
dz=:L(p;)
for alland any density pof the latent variables. In other words, L(p;) is a lower bound
on the log-likelihood that involves the complete-data likelihood. The EM algorithm then
aims to increase this lower bound as much as possible by starting with an initial guess (0)
and then, for t=1;2;:::, solving the following two steps:
1.p(t)=argmaxpL(p;(t 1)),
2.(t)=argmax2L(p(t);).Chapter 4. Unsupervised Learning 129
The Ô¨Årst optimization problem can be solved explicitly. Namely, by (4.23), we have
that
p(t)=argmin
pD(p;g(j;(t 1)))=g(j;(t 1)):
That is, the optimal density is the conditional density of the latent variables given the data
and the parameter (t 1). The second optimization problem can be simpliÔ¨Åed by writing
L(p(t);)=Q(t)() Ep(t)lnp(t)(Z), where
Q(t)() :=Ep(t)lng(;Zj)
is the expected complete-data log-likelihood under Zp(t). Consequently, the maximiza-
tion ofL(p(t);) with respect to is equivalent to Ô¨Ånding
(t)=argmax
2Q(t)():
This leads to the following generic EM algorithm.
Algorithm 4.3.1: Generic EM Algorithm
input: Data, initial guess (0).
output: Approximation of the maximum likelihood estimate.
1t 1
2while a stopping criterion is not met do
3 Expectation Step : Find p(t)(z) :=g(zj;(t 1)) and compute the expectation
Q(t)() :=Ep(t)lng(;Zj): (4.24)
4 Maximization Step : Let(t) argmax2Q(t)().
5 t t+1
6return(t)
A possible stopping criterion is to stop when
lng(j(t)) lng(j(t 1))
lng(j(t))6"
for some small tolerance ">0.
Remark 4.1 (Properties of the EM Algorithm) The identity (4.23) can be used to
show that the likelihood g(j(t)) does not decrease with every iteration of the algorithm.
This property is one of the strengths of the algorithm. For example, it can be used to debug
computer implementations of the EM algorithm: if the likelihood is observed to decrease
at any iteration, then one has detected a bug in the program.
The convergence of the sequence f(t)gto a global maximum (if it exists) is highly
dependent on the initial value (0)and, in many cases, an appropriate choice of (0)may not
be clear. Typically, practitioners run the algorithm from di erent random starting points
over, to ascertain empirically that a suitable optimum is achieved.130 4.3. Expectation‚ÄìMaximization (EM) Algorithm
Example 4.2 (Censored Data) Suppose the lifetime (in years) of a certain type of
machine is modeled via a N(;2) distribution. To estimate and2, the lifetimes of
n(independent) machines are recorded up to cyears. Denote these censored lifetimes
byx1;:::; xn. Thefxigare thus realizations of iid random variables fXig, distributed as
minfY;cg, where YN(;2).
By the law of total probability (see (C.9)), the marginal pdf of each Xcan be written +428
as:
g(xj;2)= ((c )=)|          {z          }
P[Y<c]'2(x )
((c )=)1fx<cg+((c )=)|          {z          }
P[Y>c]1fx=cg;
where'2() is the pdf of the N(0;2) distribution, is the cdf of the standard normal
distribution, and :=1 . It follows that the likelihood of the data =fx1;:::; xngas a
function of the parameter :=[;2]>is:
g(j)=Y
i:xi<cexp
 (xi )2
22
p
22Y
i:xi=c((c )=):
Letncbe the total number of xisuch that xi=c. Using nclatent variables z=[z1;:::; znc]>,
we can write the joint pdf:
g(;zj)=1
(22)n=2exp 
 P
i:xi<c(xi )2
22 Pnc
i=1(zi )2
22!
1
min
izi>c
;
so thatR
g(;zj) dz=g(j). We can thus apply the EM algorithm to maximize the like-
lihood, as follows.
For the E(xpectation)-step, we have for a Ô¨Åxed :
g(zj;)=ncY
i=1g(zij;);
where g(zj;)=1fz>cg'2(z )=((c )=) is simply the pdf of the N(;2)
distribution, truncated to [ c;1).
For the M(aximization)-step, we compute the expectation of the complete log-
likelihood with respect to a Ô¨Åxed g(zj;) and use the fact that Z1;:::; Zncare iid:
Elng(;Zj)= P
i:xi<c(xi )2
22 ncE(Z )2
22 n
2ln2 n
2ln(2);
where Zhas aN(;2) distribution, truncated to [ c;1). To maximize the last expression
with respect to we set the derivative with respect to to zero, and obtain:
=ncEZ+P
i:xi<cxi
n:
Similarly, setting the derivative with respect to 2to zero gives:
2=ncE(Z )2+P
i:xi<c(xi )2
n:
In summary, the EM iterates for t=1;2;:::are as follows.Chapter 4. Unsupervised Learning 131
E-step. Given the current estimate t:=[t;2
t]>, compute the expectations t:=EZand
2
t:=E(Z t)2, where ZN(t;2
t), conditional on Z>c; that is,
t:=t+2
t'2
t(c t)
((c t)=t)
2
t:=2
t0BBBB@1+(c t)'2
t(c t)
((c t)=t)1CCCCA:
M-step. Update the estimate to t+1:=[t+1;2
t+1]>via the formulas:
t+1=nct+P
i:xi<cxi
n
2
t+1=nc2
t+P
i:xi<c(xi t+1)2
n:
4.4 Empirical Distribution and Density Estimation
In Section 1.5.2.3 we saw how the empirical cdf bFn, obtained from an iid training set + 11
=fx1;:::; xngfrom an unknown distribution on R, gives an estimate of the unknown cdf
Fof this sampling distribution. The function bFnis a genuine cdf, as it is right-continuous,
increasing, and lies between 0 and 1. The corresponding discrete probability distribution
is called the empirical distribution empirical
distributionof the data. A random variable Xdistributed according
to this empirical distribution takes the values x1;:::; xnwith equal probability 1 =n. The
concept of empirical distribution naturally generalizes to higher dimensions: a random
vector Xthat is distributed according to the empirical distribution of x1;:::; xnhas discrete
pdfP[X=xi]=1=n;i=1;:::; n. Sampling from such a distribution ‚Äî in other words
resampling the original data ‚Äî was discussed in Section 3.2.4. The preeminent usage of + 76
such sampling is the bootstrap method, discussed in Section 3.3.2. + 88
In a way, the empirical distribution is the natural answer to the unsupervised learning
question: what is the underlying probability distribution of the data? However, the empir-
ical distribution is, by deÔ¨Ånition, a discrete distribution, whereas the true sampling distri-
bution might be continuous. For continuous data it makes sense to also consider estimation
of the pdf of the data. A common approach is to estimate the density via a kernel density
estimate (KDE), the most prevalent learner to carry this out is given next.
DeÔ¨Ånition 4.1: Gaussian KDE
Letx1;:::; xn2Rdbe the outcomes of an iid sample from a continuous pdf f. A
Gaussian kernel density estimate Gaussian
kernel density
estimateoffis a mixture of normal pdfs, of the form
gn(xj)=1
nnX
i=11
(2)d=2de kx xik2
22;x2Rd; (4.25)
where>0 is called the bandwidth .132 4.4. Empirical Distribution and Density Estimation
We see that gnin (4.25) is the average of a collection of nnormal pdfs, where each
normal distribution is centered at the data point xiand has covariance matrix 2Id. A major
question is how to choose the bandwidth so as to best approximate the unknown pdf f.
Choosing very small will result in a ‚Äúspiky‚Äù estimate, whereas a large will produce
an over-smoothed estimate that may not identify important peaks that are present in the
unknown pdf. Figure 4.1 illustrates this phenomenon. In this case the data are comprised
of 20 points uniformly drawn from the unit square. The true pdf is thus 1 on [0 ;1]2and 0
elsewhere.
Figure 4.1: Two two-dimensional Gaussian KDEs, with =0:01 (left) and =0:1 (right).
Let us write the Gaussian KDE in (4.25) as
gn(xj)=1
nnX
i=11
dx xi

; (4.26)
where
(z)=1
(2)d=2e kzk2
2;z2Rd(4.27)
is the pdf of the d-dimensional standard normal distribution. By choosing a di erent prob-
ability density in (4.26), satisfying (x)=( x) for all x, we can obtain a wide variety
of kernel density estimates. A simple pdf is, for example, the uniform pdf on [  1;1]d:
(z)=8>><>>:2 d;ifz2[ 1;1]d;
0; otherwise:
Figure 4.2 shows the graph of the corresponding KDE, using the same data as in Figure 4.1
and with bandwidth =0:1. We observe qualitatively similar behavior for the Gaussian
and uniform KDEs. As a rule, the choice of the function is less important than the choice
of the bandwidth in determining the quality of the estimate.
The important issue of bandwidth selection has been extensively studied for one-
dimensional data. To explain the ideas, we use our usual setup and let =fx1;:::; xng
be the observed (one-dimensional) data from the unknown pdf f. First, we deÔ¨Åne the loss
function as
Loss( f(x);g(x))=(f(x) g(x))2
f(x): (4.28)Chapter 4. Unsupervised Learning 133
Figure 4.2: A two-dimensional uniform KDE, with bandwidth =0:1.
The risk to minimize is thus `(g) :=EfLoss( f(X);g(X))=R
(f(x) g(x))2dx:We bypass
the selection of a class of approximation functions by choosing the learner to be speciÔ¨Åed
by (4.25) for a Ô¨Åxed . The objective is now to Ô¨Ånd a that minimizes the generalization
risk`(g(j)) or the expected generalization risk E`(gT(j)). The generalization risk is
in this caseZ
(f(x) g(xj))2dx=Z
f2(x) dx 2Z
f(x)g(xj) dx+Z
g2
(xj) dx:
Minimizing this expression with respect to is equivalent to minimizing the last two terms,
which can be written as
 2Efg(Xj)+Z0BBBBB@1
nnX
i=11
x xi
1CCCCCA2
dx:
This expression in turn can be estimated by using a test sample fx0
1:::;x0
n0gfrom f, yielding
the following minimization problem:
min
 2
n0n0X
i=1g(x0
ij)+1
n2nX
i=1nX
j=1Z1
2x xi

x xj

dx;
whereR1
2x xi

x xj

dx=1p
2xi xjp
2
in the case of the Gaussian kernel (4.27) with
d=1. To estimate in this way clearly requires a test sample, or at least an application of + 37
cross-validation . Another approach is to minimize the expected generalization risk, (that
is, averaged over all training sets):
EZ
(f(x) gT(xj))2dx:
This is called the mean integrated squared error mean integrated
squared error(MISE). It can be decomposed into an
integrated squared bias and integrated variance component:
Z
(f(x) EgT(xj))2dx+Z
Var(gT(xj)) dx:134 4.4. Empirical Distribution and Density Estimation
A typical analysis now proceeds by investigating how the MISE behaves for large n, under
various assumptions on f. For example, it is shown in [114] that, for !0 and n!1 ,
the asymptotic approximation to the MISE of the Gaussian kernel density estimator (4.25)
(ford=1) is given by
1
44kf00k2+1
2np
2; (4.29)
wherekf00k2:=R
(f00(x))2dx. The asymptotically optimal value of is the minimizer
:= 1
2npkf00k2!1=5
: (4.30)
To compute the optimal in (4.30), one needs to estimate the functional kf00k2. The
Gaussian rule of thumb Gaussian rule
of thumbis to assume that fis the density of the N(x;s2) distribution, where
xands2are the sample mean and variance of the data, respectively [113]. In this case
kf00k2=s 5 1=23=8 and the Gaussian rule of thumb becomes:
rot= 4s5
3n!1=5
1:06s n 1=5:
We recommend, however, the fast and reliable theta KDE theta KDE of [14], which chooses the
bandwidth in an optimal way via a Ô¨Åxed-point procedure. Figures 4.1 and 4.2 illustrate a
common problem with traditional KDEs: for distributions on a bounded domain, such as
the uniform distribution on [0 ;1]2, the KDE assigns positive probability mass outside this
domain. An additional advantage of the theta KDE is that it largely avoids this boundary
eect. We illustrate the theta KDE with the following example.
Example 4.3 (Comparison of Gaussian and theta KDEs) The following Python pro-
gram draws an iid sample from the Exp(1) distribution and constructs a Gaussian kernel
density estimate. We see in Figure 4.3 that with an appropriate choice of the bandwidth
a good Ô¨Åt to the true pdf can be achieved, except at the boundary x=0. The theta KDE
does not exhibit this boundary e ect. Moreover, it chooses the bandwidth automatically,
to achieve a superior Ô¨Åt. The theta KDE source code is available as kde.py on the book‚Äôs
GitHub site.
0
 1
 2
 3
 4
 5
 6
0.0
0.2
0.4
0.6
0.8
1.0
Gaussian KDE
Theta KDE
True density
Figure 4.3: Kernel density estimates for Exp(1)-distributed data.Chapter 4. Unsupervised Learning 135
gausthetakde.py
import matplotlib.pyplot as plt
import numpy as np
from kde import *
sig = 0.1; sig2 = sig**2; c = 1/np.sqrt(2*np.pi)/sig #Constants
phi = lambda x,x0: np.exp(-(x-x0)**2/(2*sig2)) #Unscaled Kernel
f = lambda x: np.exp(-x)*(x >= 0) # True PDF
n = 10**4 # Sample Size
x = -np.log(np.random.uniform(size=n)) # Generate Data via IT method
xx = np.arange(-0.5,6,0.01, dtype = "d") # Plot Range
phis = np.zeros( len(xx))
for iin range (0,n):
phis = phis + phi(xx,x[i])
phis = c*phis/n
plt.plot(xx,phis , 'r')# Plot Gaussian KDE
[bandwidth ,density ,xmesh ,cdf] = kde(x,2**12,0, max(x))
idx = (xmesh <= 6)
plt.plot(xmesh[idx],density[idx]) # Plot Theta KDE
plt.plot(xx,f(xx)) # Plot True PDF
4.5 Clustering via Mixture Models
Clustering is concerned with the grouping of unlabeled feature vectors into clusters, such
that samples within a cluster are more similar to each other than samples belonging to
dierent clusters. Usually, it is assumed that the number of clusters is known in advance,
but otherwise no prior information is given about the data. Applications of clustering can
be found in the areas of communication, data compression and storage, database searching,
pattern matching, and object recognition.
A common approach to clustering analysis is to assume that the data comes from a mix-
ture of (usually Gaussian) distributions, and thus the objective is to estimate the parameters
of the mixture model by maximizing the likelihood function for the data. Direct optimiza-
tion of the likelihood function in this case is not a simple task, due to necessary constraints
on the parameters (more about this later) and the complicated nature of the likelihood func-
tion, which in general has a great number of local maxima and saddle-points. A popular
method to estimate the parameters of the mixture model is the EM algorithm, which was
discussed in a more general setting in Section 4.3. In this section we explain the basics of +128
mixture modeling and explain the workings of the EM method in this context. In addition,
we show how direct optimization methods can be used to maximize the likelihood.
4.5.1 Mixture Models
LetT:=fX1;:::; Xngbe iid random vectors taking values in some set XRd, each Xi
being distributed according to the mixture density mixture density
g(xj)=w11(x)++wKK(x);x2X; (4.31)136 4.5. Clustering via Mixture Models
where1;:::; Kare probability densities (discrete or continuous) on X, and the positive
weights w 1;:::; wKsum up to 1. This mixture pdf can be interpreted in the following way.weightsLetZbe a discrete random variable taking values 1 ;2;:::; Kwith probabilities w1;:::; wK,
and let Xbe a random vector whose conditional pdf, given Z=z, isz. By the product rule
(C.17), the joint pdf of ZandXis given by +431
Z;X(z;x)=Z(z)XjZ(xjz)=wzz(x)
and the marginal pdf of Xis found by summing the joint pdf over the values of z, which
gives (4.31). A random vector Xgcan thus be simulated in two steps:
1. First, draw Zaccording to the probabilities P[Z=z]=wz,z=1;:::; K.
2. Then draw Xaccording to the pdf Z.
AsTonly contain thefXigvariables, thefZigare viewed as latent variables. We can inter-
pretZias the hidden label of the cluster to which Xibelongs.
Typically, each kin (4.31) is assumed to be known up to some parameter vector k. It
is customary1in clustering analysis to work with Gaussian mixtures; that is, each density
kis Gaussian with some unknown expectation vector kand covariance matrix k. We
gather all unknown parameters, including the weights fwkg, into a parameter vector . As
usual,=fx1;:::; xngdenotes the outcome of T. As the components of Tare iid, their
(joint) pdf is given by
g(j) :=nY
i=1g(xij)=nY
i=1KX
k=1wkk(xijk;k): (4.32)
Following the same reasoning as for (4.5), we can estimate from an outcome by max-
imizing the log-likelihood function
l(j) :=nX
i=1lng(xij)=nX
i=1ln0BBBBB@KX
k=1wkk(xijk;k)1CCCCCA: (4.33)
However, Ô¨Ånding the maximizer of l(j) is not easy in general, since the function is typ-
ically multiextremal.
Example 4.4 (Clustering via Mixture Models) The data depicted in Figure 4.4 con-
sists of 300 data points that were independently generated from three bivariate normal
distributions, whose parameters are given in that same Ô¨Ågure. For each of these three dis-
tributions, exactly 100 points were generated. Ideally, we would like to cluster the data into
three clusters that correspond to the three cases.
To cluster the data into three groups, a possible model for the data is to assume that
the points are iid draws from an (unknown) mixture of three 2-dimensional Gaussian dis-
tributions. This is a sensible approach, although in reality the data were not simulated
in this way. It is instructive to understand the di erence between the two models. In the
mixture model, each cluster label Ztakes the valuef1;2;3gwith equal probability, and
hence, drawing the labels independently, the total number of points in each cluster would
1Other common mixture distributions include Student tandBeta distributions.Chapter 4. Unsupervised Learning 137
-8 -6 -4 -2 0 2 4-6-4-2024
cluster mean vector covariance matrix
1" 4
0# "2 1:4
1:4 1:5#
2"0:5
 1# "2 0:95
 0:95 1#
3" 1:5
 3# "2 0:1
0:1 0:1#
Figure 4.4: Cluster the 300 data points (left) into three clusters, without making any as-
sumptions about the probability distribution of the data. In fact, the data were generated
from three bivariate normal distributions, whose parameters are listed on the right.
beBin(300;1=3) distributed. However, in the actual simulation, the number of points in
each cluster is exactly 100. Nevertheless, the mixture model would be an accurate (al-
though not exact) model for these data. Figure 4.5 displays the ‚Äútarget‚Äù Gaussian mixture
density for the data in Figure 4.4; that is, the mixture with equal weights and with the exact
parameters as speciÔ¨Åed in Figure 4.4.
Figure 4.5: The target mixture density for the data in Figure 4.4.
In the next section we will carry out the clustering by using the EM algorithm.
4.5.2 EM Algorithm for Mixture Models
As we saw in Section 4.3, instead of maximizing the log-likelihood function (4.33) directly
from the data =fx1;:::; xng, the EM algorithm Ô¨Årst augments the data data
augmentationwith the vector of
latent variables ‚Äî in this case the hidden cluster labels z=fz1;:::; zng. The idea is that is138 4.5. Clustering via Mixture Models
only the observed part of the complete random data ( T;Z), which were generated via the
two-step procedure described above. That is, for each data point X, Ô¨Årst draw the cluster
label Z2f1;:::; Kgaccording to probabilities fw1;:::; wKgand then, given Z=z, draw X
fromz. The joint pdf ofTandZis
g(;zj)=nY
i=1wzizi(xi);
which is of a much simpler form than (4.32). It follows that the complete-data log-
likelihood complete -data
log-likelihoodfunction
el(j;z)=nX
i=1ln[wzizi(xi)] (4.34)
is often easier to maximize than the original log-likelihood (4.33), for any given ( ;z). But,
of course the latent variables zare not observed and so el(j;z) cannot be evaluated. In the
E-step of the EM algorithm, the complete-data log-likelihood is replaced with the expect-
ationEpel(j;Z), where the subscript pin the expectation indicates that Zis distributed
according to the conditional pdf of ZgivenT=; that is, with pdf
p(z)=g(zj;)/g(;zj): (4.35)
Note that p(z) is of the form p1(z1)pn(zn) so that, givenT=, the components of Zare
independent of each other. The EM algorithm for mixture models can now be formulated
as follows.
Algorithm 4.5.1: EM Algorithm for Mixture Models
input: Data, initial guess (0).
output: Approximation of the maximum likelihood estimate.
1t 1
2while a stopping criterion is not met do
3 Expectation Step : Find p(t)(z) :=g(zj;(t 1)) and Q(t)() :=Ep(t)el(j;Z).
4 Maximization Step : Let(t) argmaxQ(t)().
5 t t+1
6return(t)
A possible termination condition is to stop whenl((t)j) l((t 1)j)=l((t)j)< "
for some small tolerance " > 0. As was mentioned in Section 4.3, the sequence of log-
likelihood values does not decrease with each iteration. Under certain continuity con-
ditions, the sequence f(t)gis guaranteed to converge to a local maximizer of the log-
likelihood l. Convergence to a global maximizer (if it exists) depends on the appropriate
choice for the starting value. Typically, the algorithm is run from di erent random starting
points.
For the case of Gaussian mixtures, each k=(jk;k);k=1;:::; Kis the density
of a d-dimensional Gaussian distribution. Let (t 1)be the current guess for the optimal
parameter vector, consisting of the weights fw(t 1)
kg, mean vectorsf(t 1)
kg, and covariance
matricesf(t 1)
kg. We Ô¨Årst determine p(t)‚Äî the pdf of Zconditional onT=‚Äî for the
given guess(t 1). As mentioned before, the components of ZgivenT=are independent,Chapter 4. Unsupervised Learning 139
so it su ces to specify the discrete pdf, p(t)
isay, of each Zigiven the observed point Xi=xi.
The latter can be found from Bayes‚Äô formula:
p(t)
i(k)/w(t 1)
kk(xij(t 1)
k;(t 1)
k);k=1;:::; K: (4.36)
Next, in view of (4.34), the function Q(t)() can be written as
Q(t)()=Ep(t)nX
i=1
lnwZi+lnZi(xijZi;Zi)
=nX
i=1Ep(t)
ih
lnwZi+lnZi(xijZi;Zi)i
;
where thefZigare independent and Ziis distributed according to p(t)
iin (4.36). This com-
pletes the E-step . In the M-step we maximize Q(t)with respect to the parameter ; that is,
with respect to the fwkg,fkg, andfkg. In particular, we maximize
nX
i=1KX
k=1p(t)
i(k)lnwk+lnk(xijk;k);
under the conditionP
kwk=1. Using Lagrange multipliers and the fact thatPK
k=1p(t)
i(k)=1
gives the solution for the fwkg:
wk=1
nnX
i=1p(t)
i(k);k=1;:::; K: (4.37)
The solutions for kandknow follow from maximizingPn
i=1p(t)
i(k) lnk(xijk;k), lead-
ing to
k=Pn
i=1p(t)
i(k)xiPn
i=1p(t)
i(k);k=1;:::; K (4.38)
and
k=Pn
i=1p(t)
i(k) (xi k)(xi k)>
Pn
i=1p(t)
i(k);k=1;:::; K; (4.39)
which are very similar to the well-known formulas for the MLEs of the parameters of a
Gaussian distribution. After assigning the solution parameters to (t)and increasing the
iteration counter tby 1, the steps (4.36), (4.37), (4.38), and (4.39) are repeated until con-
vergence is reached. Convergence of the EM algorithm is very sensitive to the choice of
initial parameters. It is therefore recommended to try various di erent starting conditions.
For a further discussion of the theoretical and practical aspects of the EM algorithm we
refer to [85].
Example 4.5 (Clustering via EM) We return to the data in Example 4.4, depicted in
Figure 4.4, and adopt the model that the data is coming from a mixture of three bivariate
Gaussian distributions.
The Python code below implements the EM procedure described in Algorithm 4.5.1.
The initial mean vectors fkgof the bivariate Gaussian distributions are chosen (from visual
inspection) to lie roughly in the middle of each cluster, in this case [  2; 3]>;[ 4;1]>, and
[0; 1]>. The corresponding covariance matrices are initially chosen as identity matrices,
which is appropriate given the observed spread of the data in Figure 4.4. Finally, the initial
weights are 1 =3;1=3;1=3. For simplicity, the algorithm stops after 100 iterations, which in
this case is more than enough to guarantee convergence. The code and data are available
from the book‚Äôs website in the GitHub folder Chapter4.140 4.5. Clustering via Mixture Models
EMclust.py
import numpy as np
from scipy.stats import multivariate_normal
Xmat = np.genfromtxt( 'clusterdata.csv ', delimiter= ',')
K = 3
n, D = Xmat.shape
W = np.array([[1/3,1/3,1/3]])
M = np.array([[-2.0,-4,0],[-3,1,-1]], dtype=np.float32)
# Note that if above *all* entries were written as integers , M would
# be defined to be of integer type , which will give the wrong answer
C = np.zeros((3,2,2))
C[:,0,0] = 1
C[:,1,1] = 1
p = np.zeros((3,300))
for iin range (0,100):
#E-step
for kin range (0,K):
mvn = multivariate_normal( M[:,k].T, C[k,:,:] )
p[k,:] = W[0,k]*mvn.pdf(Xmat)
# M-Step
p = (p/ sum(p,0)) #normalize
W = np.mean(p,1).reshape(1,3)
for kin range (0,K):
M[:,k] = (Xmat.T @ p[k,:].T)/ sum(p[k,:])
xm = Xmat.T - M[:,k].reshape(2,1)
C[k,:,:] = xm @ (xm*p[k,:]).T/ sum(p[k,:])
The estimated parameters of the mixture distribution are given on the right-hand side
of Figure 4.6. After relabeling of the clusters, we can observe a close match with the
parameters in Figure 4.4.
The ellipses on the left-hand side of Figure 4.6 show a close match between the 95%
probability ellipses2of the original Gaussian distributions (in gray) and the estimated ones.
A natural way to cluster each point xiis to assign it to the cluster kfor which the conditional
probability pi(k) is maximal (with ties resolved arbitrarily). This gives the clustering of the
points into red, green, and blue clusters in the Ô¨Ågure.
2For each mixture component, the contour of the corresponding bivariate normal pdf is shown that en-
closes 95% of the probability mass.Chapter 4. Unsupervised Learning 141
-6 -4 -2 0 2 4-4-3-2-10123
weight mean vector covariance matrix
0:33" 1:51
 3:01# "1:75 0:03
0:03 0:095#
0:32" 4:08
 0:033# "1:37 0:92
0:92 1:03#
0:35"0:36
 0:88# "1:93 1:20
 1:20 1:44#
Figure 4.6: The results of the EM clustering algorithm applied to the data depicted in
Figure 4.4.
As an alternative to the EM algorithm, one can of course use continuous multiextremal
optimization algorithms to directly optimize the log-likelihood function l(j)=lng(j)
in (4.33) over the set of all possible . This is done for example in [15], demonstrating
superior results to EM when there are few data points. Closer investigation of the likelihood
function reveals that there is a hidden problem with any maximum likelihood approach for
clustering if is chosen as large as possible ‚Äî i.e., any mixture distribution is possible. To
demonstrate this problem, consider Figure 4.7, depicting the probability density function,
g(j) of a mixture of two Gaussian distributions, where =[w;1;2
1;2;2
2]>is the
vector of parameters for the mixture distribution. The log-likelihood function is given by
l(j)=P4
i=1lng(xij), where x1;:::; x4are the data (indicated by dots in the Ô¨Ågure).
-4 -2 0 2 4 6 800.10.20.30.40.5
Figure 4.7: Mixture of two Gaussian distributions.
It is clear that by Ô¨Åxing the mixing constant wat 0.25 (say) and centering the Ô¨Årst
cluster at x1, one can obtain an arbitrarily large likelihood value by taking the variance of
the Ô¨Årst cluster to be arbitrarily small. Similarly, for higher dimensional data, by choosing
‚Äúpoint‚Äù or ‚Äúline‚Äù clusters, or in general ‚Äúdegenerate‚Äù clusters, one can make the value of
the likelihood inÔ¨Ånite. This is a manifestation of the familiar overÔ¨Åtting problem for the142 4.6. Clustering via Vector Quantization
training loss that we already encountered in Chapter 2. Thus, the unconstrained maximiza-
tion of the log-likelihood function is an ill-posed problem, irrespective of the choice of the
optimization algorithm!
Two possible solutions to this ‚ÄúoverÔ¨Åtting‚Äù problem are:
1. Restrict the parameter set in such a way that degenerate clusters (sometimes called
spurious clusters) are not allowed.
2. Run the given algorithm and if the solution is degenerate, discard it and run the
algorithm afresh. Keep restarting the algorithm until a non-degenerate solution is
obtained.
The Ô¨Årst approach is usually applied to multiextremal optimization algorithms and the
second is used for the EM algorithm.
4.6 Clustering via Vector Quantization
In the previous section we introduced clustering via mixture models, as a form of paramet-
ric density estimation (as opposed to the nonparametric density estimation in Section 4.4).
The clusters were modeled in a natural way via the latent variables and the EM algorithm
provided a convenient way to assign the cluster members. In this section we consider a
more heuristic approach to clustering by ignoring the distributional properties of the data.
The resulting algorithms tend to scale better with the number of samples nand the dimen-
sionality d.
In mathematical terms, we consider the following clustering (also called data segment-
ation) problem. Given a collection =fx1;:::; xngof data points in some d-dimensional
spaceX, divide this data set into Kclusters (groups) such that some loss function is min-
imized. A convenient way to determine these clusters is to Ô¨Årst divide up the entire space
X, using some distance function dist( ;) on this space. A standard choice is the Euclidean
(orL2) distance:
dist(x;x0)=kx x0k=vtdX
i=1(xi x0
i)2:
Other commonly used distance measures on Rdinclude the Manhattan distance Manhattan
distance:
dX
i=1jxi x0
ij
and the maximum distance maximum
distance:
max
i=1;:::;djxi x0
ij:
On the set of strings of length d, an often-used distance measure is the Hamming distance Hamming
distance:
dX
i=11fxi,x0
ig;
that is, the number of mismatched characters. For example, the Hamming distance between
010101 and 011010 is 4.Chapter 4. Unsupervised Learning 143
We can partition the space Xinto regions as follows: First, we choose Kpoints
c1;:::; cKcalled cluster centers orsource vectors source vectors . For each k=1;:::; K, let
Rk=fx2X: dist( x;ck)6dist(x;ci) for all i,kg
be the set of points in Xthat lie closer to ckthan any other center. The regions or cells
fRkgdivide the spaceXinto what is called a Voronoi diagram or a Voronoi tessellation Voronoi
tessellation.
Figure 4.8 shows a V oronoi tessellation of the plane into ten regions, using the Euclidean
distance. Note that here the boundaries between the V oronoi cells are straight line seg-
ments. In particular, if cell RiandRjshare a border, then a point on this border must satisfy
kx cik=kx cjk; that is, it must lie on the line that passes through the point ( cj+ci)=2
(that is, the midway point of the line segment between ciandcj) and be perpendicular to
cj ci.
-2 0 2 4-202
Figure 4.8: A V oronoi tessellation of the plane into ten cells, determined by the (red) cen-
ters.
Once the centers (and thus the cells fRkg) are chosen, the points in can be clustered
according to their nearest center. Points on the boundary have to be treated separately. This
is a moot point for continuous data, as generally no data points will lie exactly on the
boundary.
The main remaining issue is how to choose the centers so as to cluster the data in some
optimal way. In terms of our (unsupervised) learning framework, we wish to approximate
a vector xvia one of c1;:::; cK, using a piecewise constant vector-valued function
g(xjC) :=KX
k=1ck1fx2R kg;
where Cis the dKmatrix [ c1;:::; cK]. Thus, g(xjC)=ckwhen xfalls in regionRk(we
ignore ties). Within this class Gof functions, parameterized by C, our aim is to minimize
the training loss. In particular, for the squared-error loss, Loss( x;x0)=kx x0k2, the training
loss is
`n(g(jC))=1
nnX
i=1kxi g(xijC)k2=1
nKX
k=1X
x2Rk\njjx ckjj2: (4.40)
Thus, the training loss minimizes the average squared distance between the centers. This
framework also combines both the encoding and decoding steps in vector quantization vector
quantization144 4.6. Clustering via Vector Quantization
[125]. Namely, we wish to ‚Äúquantize‚Äù or ‚Äúencode‚Äù the vectors in in such a way that each
vector is represented by one of Ksource vectors c1;:::; cK, such that the loss (4.40) of this
representation is minimized.
Most well-known clustering and vector quantization methods update the vector of cen-
ters, starting from some initial choice and using iterative (typically gradient-based) proced-
ures. It is important to realize that in this case (4.40) is seen as a function of the centers,
where each point xis assigned to the nearest center, thus determining the clusters. It is well
known that this type of problem ‚Äî optimization with respect to the centers ‚Äî is highly
multiextremal and, depending on the initial clusters, gradient-based procedures tend to
converge to a local minimum rather than a global minimum.
4.6.1 K-Means
One of the simplest methods for clustering is the K-means method. It is an iterative method
where, starting from an initial guess for the centers, new centers are formed by taking
sample means of the current points in each cluster. The new centers are thus the centroids centroids
of the points in each cell. Although there exist many di erent varieties of the K-means
algorithm, they are all essentially of the following form:
Algorithm 4.6.1: K-Means
input: Collection of points =fx1;:::; xng, number of clusters K, initial centers
c1;:::; cK.
output: Cluster centers and cells (regions).
1while a stopping criterion is not met do
2R1;:::;RK ; (empty sets).
3 fori=1tondo
4 d [dist( xi;c1);:::; dist(xi;cK)] // distances to centers
5 k argminjdj
6Rk R k[fxig // assign xito cluster k
7 fork=1toKdo
8 ck P
x2Rkx
jRkj// compute the new center as a centroid of points
9returnfckg,fRkg
Thus, at each iteration, for a given choice of centers, each point in is assigned to
its nearest center. After all points have been assigned, the centers are recomputed as the
centroids of all the points in the current cluster (Line 8). A typical stopping criterion is to
stop when the centers no longer change very much. As the algorithm is quite sensitive to
the choice of the initial centers, it is prudent to try multiple starting values, e.g., chosen
randomly from the bounding box of the data points.
We can see the K-means method as a deterministic (or ‚Äúhard‚Äù) version of the probab-
ilistic (or ‚Äúsoft‚Äù) EM algorithm as follows. Suppose in the EM algorithm we have Gaus-
sian mixtures with a Ô¨Åxed covariance matrix k=2Id,k=1;:::; K, where2should be
thought of as being inÔ¨Ånitesimally small. Consider iteration tof the EM algorithm. Having
obtained the expectation vectors (t 1)
kand weights w(t 1)
k;k=1;:::; K, each point xiis as-
signed a cluster label Ziaccording to the probabilities p(t)
i(k);k=1;:::; Kgiven in (4.36).Chapter 4. Unsupervised Learning 145
But for2!0 the probability distribution fp(t)
i(k)gbecomes degenerate, putting all its
probability mass on argminkkxi kk2. This corresponds to the K-means rule of assigning
xito its nearest cluster center. Moreover, in the M-step (4.38) each cluster center (t)
kis now
updated according to the average of the fxigthat have been assigned to cluster k. We thus
obtain the same deterministic updating rule as in K-means.
Example 4.6 ( K-means Clustering) We cluster the data from Figure 4.4 via K-means,
using the Python implementation below. Note that the data points are stored as a 300 2
matrix Xmat . We take the same starting centers as in the EM example: c1=[ 2; 3]>;c2=
[ 4;1]>, and c3=[0; 1]>. Note also that squared Euclidean distances are used in the
computations, as these are slightly faster to compute than Euclidean distances (as no square
root computations are required) while yielding exactly the same cluster center evaluations.
Kmeans.py
import numpy as np
Xmat = np.genfromtxt( 'clusterdata.csv ', delimiter= ',')
K = 3
n, D = Xmat.shape
c = np.array([[-2.0,-4,0],[-3,1,-1]]) #initialize centers
cold = np.zeros(c.shape)
dist2 = np.zeros((K,n))
while np.abs(c - cold). sum() > 0.001:
cold = c.copy()
for iin range (0,K): #compute the squared distances
dist2[i,:] = np. sum((Xmat - c[:,i].T)**2, 1)
label = np.argmin(dist2 ,0) #assign the points to nearest centroid
minvals = np.amin(dist2 ,0)
for iin range (0,K): # recompute the centroids
c[:,i] = np.mean(Xmat[np.where(label == i) ,:],1).reshape(1,2)
print ('Loss = {:3.3f} '.format (minvals.mean()))
Loss = 2.288
-6 -4 -2 0 2 4-5-4-3-2-10123
Figure 4.9: Results of the K-means algorithm applied to the data in Figure 4.4. The thick
black circles are the centroids and the dotted lines deÔ¨Åne the cell boundaries.146 4.6. Clustering via Vector Quantization
We found the cluster centers c1=[ 1:9286; 3:0416]>;c2=[ 3:9237;0:0131]>, and
c3=[0:5611; 1:2980]>, giving the clustering depicted in Figure 4.9. The corresponding
loss (4.40) was found to be 2 :288.
4.6.2 Clustering via Continuous Multiextremal Optimization
As already mentioned, the exact minimization of the loss function (4.40) is di cult to
accomplish via standard local search methods such as gradient descent, as the function
is highly multimodal. However, nothing is preventing us from using global optimization
methods such as the CE or SCO methods discussed in Sections 3.4.2 and 3.4.3. +100
Example 4.7 (Clustering via CE) We take the same data set as in Example 4.6 and
cluster the points via minimization of the loss (4.40) using the CE method. The Python
code below is very similar to the code in Example 3.16, except that now we are dealing +101
with a six-dimensional optimization problem. The loss function is implemented in the func-
tionScluster , which essentially reuses the squared distance computation of the K-means
code in Example 4.6. The CE program typically converges to a loss of 2 :287, correspond-
ing to the (global) minimizers c1=[ 1:9286; 3:0416]>;c2=[ 3:8681;0:0456]>, and
c3=[0:5880; 1:3526]>, which slightly di ers from the local minimizers for the K-means
algorithm.
clustCE.py
import numpy as np
np.set_printoptions(precision=4)
Xmat = np.genfromtxt( 'clusterdata.csv ', delimiter= ',')
K = 3
n, D = Xmat.shape
def Scluster(c):
n, D = Xmat.shape
dist2 = np.zeros((K,n))
cc = c.reshape(D,K)
for iin range (0,K):
dist2[i,:] = np. sum((Xmat - cc[:,i].T)**2, 1)
minvals = np.amin(dist2 ,0)
return minvals.mean()
numvar = K*D
mu = np.zeros(numvar) #initialize centers
sigma = np.ones(numvar)*2
rho = 0.1
N = 500; Nel = int(N*rho); eps = 0.001
func = Scluster
best_trj = np.array(numvar)
best_perf = np.Inf
trj = np.zeros(shape=(N,numvar))
while (np. max(sigma)>eps):
for iin range (0,numvar):Chapter 4. Unsupervised Learning 147
trj[:,i] = (np.random.randn(N,1)*sigma[i]+ mu[i]).reshape(N,)
S = np.zeros(N)
for iin range (0,N):
S[i] = func(trj[i])
sortedids = np.argsort(S) # from smallest to largest
S_sorted = S[sortedids]
best_trj = np.array(n)
best_perf = np.Inf
eliteids = sortedids[ range (0,Nel)]
eliteTrj = trj[eliteids ,:]
mu = np.mean(eliteTrj ,axis=0)
sigma = np.std(eliteTrj ,axis=0)
if(best_perf >S_sorted[0]):
best_perf = S_sorted[0]
best_trj = trj[sortedids[0]]
print (best_perf)
print (best_trj.reshape(2,3))
2.2874901831572947
[[-3.9238 -1.8477 0.5895]
[ 0.0134 -3.0292 -1.2442]]
4.7 Hierarchical Clustering
It is sometimes useful to determine data clusters in a hierarchical manner; an example
is the construction of evolutionary relationships between animal species. Establishing a
hierarchy of clusters can be done in a bottom-up or a top-down manner. In the bottom-up
approach, also called agglomerative clustering agglomerative
clustering, the data points are merged in larger and
larger clusters until all the points have been merged into a single cluster. In the top-down
ordivisive clustering divisive
clusteringapproach, the data set is divided up into smaller and smaller clusters.
The left panel of Figure 4.10 depicts a hierarchy of clusters.
In Figure 4.10, each cluster is given a cluster identiÔ¨Åer. At the lowest level are clusters
comprised of the original data points (identiÔ¨Åers 1 ;:::; 8). The union of clusters 1 and 2
form a cluster with identiÔ¨Åer 9, and the union of 3 and 4 form a cluster with identiÔ¨Åer 10.
In turn the union of clusters 9 and 10 constitutes cluster 12, and so on.
The right panel of Figure 4.10 shows a convenient way to visualize cluster hierarchies
using a dendrogram dendrogram (from the Greek dendro for tree). A dendrogram not only summarizes
how clusters are merged or split, but also shows the distance between clusters, here on the
vertical axis. The horizontal axis shows which cluster each data point (label) belongs to.
Many di erent types of hierarchical clustering can be performed, depending on how
the distance is deÔ¨Åned between two data points and between two clusters. Denote the data
set byX=fxi;i=1;:::; ng. As in Section 4.6, let dist( xi;xj) be the distance between data
points xiandxj. The default choice is the Euclidean distance dist( xi;xj)=kxi xjk.
LetIandJbe two disjoint subsets of f1;:::; ng. These sets correspond to two disjoint
subsets (that is, clusters) of X:fxi;i=Igandfxj;j=Jg. We denote the distance between148 4.7. Hierarchical Clustering
7 8 6 1 2 3 4 5
Labels10203040Distance
Figure 4.10: Left: a cluster hierarchy of 15 clusters. Right: the corresponding dendrogram.
these two clusters by d(I;J). By specifying the function d, we indicate how the clusters
are linked. For this reason it is also referred to as the linkage linkage criterion. We give a number
of examples:
¬àSingle linkage. The closest distance between the clusters.
dmin(I;J) :=min
i2I;j2Jdist(xi;xj):
¬àComplete linkage. The furthest distance between the clusters.
dmax(I;J) :=max
i2I;j2Jdist(xi;xj):
¬àGroup average. The mean distance between the clusters. Note that this depends on
the cluster sizes.
davg(I;J) :=1
jIjjJjX
i2IX
j2Jdist(xi;xj):
For these linkage criteria, Xis usually assumed to be Rdwith the Euclidean distance.
Another notable measure for the distance between clusters is Ward‚Äôs minimum vari-
ance linkage criterion. Ward‚Äôs linkage Here, the distance between clusters is expressed as the additional
amount of ‚Äúvariance‚Äù (expressed in terms of the sum of squares) that would be intro-
duced if the two clusters were merged. More precisely, for any set Kof indices (labels) let
xK=P
k2Kxk=jKjdenote its corresponding cluster mean. Then
dWard(I;J) :=X
k2I[Jkxk xI[Jk2 0BBBBBB@X
i2Ikxi xIk2+X
j2Jkxj xJk21CCCCCCA: (4.41)
It can be shown (see Exercise 8) that the Ward linkage depends only on the cluster means
and the cluster sizes for IandJ:
dWard(I;J)=jIjjJj
jIj+jJjkxI xJk2:Chapter 4. Unsupervised Learning 149
In software implementations, the Ward linkage function is often rescaled by mul-
tiplying it by a factor of 2. In this way, the distance between one-point clusters fxig
andfxjgis the squared Euclidean distance kxi xjk2.
Having chosen a distance on Xand a linkage criterion, a general agglomerative clus-
tering algorithm proceeds in the following ‚Äúgreedy‚Äù manner.
Algorithm 4.7.1: Greedy Agglomerative Clustering
input: Distance function dist, linkage function d, number of clusters K.
output: The label sets for the tree.
1Initialize the set of cluster identiÔ¨Åers: I=f1;:::; ng.
2Initialize the corresponding label sets: Li=fig,i2I.
3Initialize a distance matrix D=[di j] with di j=d(fig;fjg).
4fork=n+1to2n Kdo
5 Find iandj>iinIsuch that di jis minimal.
6 Create a new label set Lk:=Li[L j.
7 Add the new identiÔ¨Åer ktoIand remove the old identiÔ¨Åers iandjfromI.
8 Update the distance matrix Dwith respect to the identiÔ¨Åers i,j, and k.
9returnLi;i=1;:::; 2n K
Initially, the distance matrix Dcontains the (linkage) distances between the one-point
clusters containing one of the data points x1;:::; xn, and hence with identiÔ¨Åers 1 ;:::; n.
Finding the shortest distance amounts to a table lookup in D. When the closest clusters
are found, they are merged into a new cluster, and a new identiÔ¨Åer k(the smallest positive
integer that has not yet been used as an identiÔ¨Åer) is assigned to this cluster. The old iden-
tiÔ¨Åers iand jare removed from the cluster identiÔ¨Åer set I. The matrix Dis then updated
by adding a k-th column and row that contain the distances between kand any m2I. This
updating step could be computationally quite costly if the cluster sizes are large and the
linkage distance between the clusters depends on all points within the clusters. Fortunately,
for many linkage functions, the matrix Dcan be updated in an e cient manner.
Suppose that at some stage in the algorithm, clusters IandJ, with identiÔ¨Åers iandj,
respectively, are merged into a cluster K=I[J with identiÔ¨Åer k. LetM, with identiÔ¨Åer
m, be a previously assigned cluster. An update rule of the linkage distance dkmbetweenK
andMis called a Lance‚ÄìWilliams Lance‚Äì
Williamsupdate if it can be written in the form
dkm=dim+djm+di j+jdim djmj;
where;:::; depend only on simple characteristics of the clusters involved, such as the
number of elements within the clusters. Table 4.2 shows the update constants for a number
of common linkage functions. For example, for single linkage, dimis the minimal distance
betweenIandM, and djmis the minimal distance between JandM. The smallest of
these is the minimal distance between KandM. That is, dkm=minfdim;djmg=dim=2+
djm=2 jdim djmj=2.150 4.7. Hierarchical Clustering
Table 4.2: Constants for the Lance‚ÄìWilliams update rule for various linkage functions,
with ni;nj;nmdenoting the number of elements in the corresponding clusters.
Linkage    
Single 1 =2 1 =2 0  1=2
Complete 1 =2 1 =2 0 1 =2
Group avg.ni
ni+njnj
ni+nj0 0
Wardni+nm
ni+nj+nmnj+nm
ni+nj+nm nm
ni+nj+nm0
In practice, Algorithm 4.7.1 is run until a single cluster is obtained. Instead of returning
the label sets of all 2 n 1 clusters, a linkage matrix linkage matrix is returned that contains the same
information. At the end of each iteration (Line 8) the linkage matrix stores the merged
labels iandj, as well as the (minimal) distance di j. Other information such as the number
of elements in the merged cluster can also be stored. Dendrograms and cluster labels can be
directly constructed from the linkage matrix. In the following example, the linkage matrix
is returned by the method agg_cluster .
Example 4.8 (Agglomerative Hierarchical Clustering) The Python code below gives
a basic implementation of Algorithm 4.7.1 using the Ward linkage function. The methods
fcluster anddendrogram from the scipy module can be used to identify the labels in
a cluster and to draw the corresponding dendrogram.
AggCluster.py
import numpy as np
from scipy.spatial.distance import cdist
def update_distances(D,i,j, sizes): # distances for merged cluster
n = D.shape[0]
d = np.inf * np.ones(n+1)
for kin range (n): # Update distances
d[k] = ((sizes[i]+sizes[k])*D[i,k] +
(sizes[j]+sizes[k])*D[j,k] -
sizes[k]*D[i,j])/(sizes[i] + sizes[j] + sizes[k])
infs = np.inf * np.ones(n) # array of infinity
D[i,:],D[:,i],D[j,:],D[:,j] = infs ,infs ,infs ,infs # deactivate
new_D = np.inf * np.ones((n+1,n+1))
new_D[0:n,0:n] = D # copy old matrix into new_D
new_D[-1,:], new_D[:,-1] = d,d # add new row and column
return new_D
def agg_cluster(X):
n = X.shape[0]
sizes = np.ones(n)
D = cdist(X, X,metric = 'sqeuclidean ')# initialize dist. matrix
.
np.fill_diagonal(D, np.inf * np.ones(D.shape[0]))
Z = np.zeros((n-1,4)) #linkage matrix encodes hierarchy tree
for tin range (n-1):Chapter 4. Unsupervised Learning 151
i,j = np.unravel_index(D.argmin(), D.shape) # minimizer pair
sizes = np.append(sizes , sizes[i] + sizes[j])
Z[t,:]=np.array([i, j, np.sqrt(D[i,j]), sizes[-1]])
D = update_distances(D, i,j, sizes) # update distance matr.
return Z
import scipy.cluster.hierarchy as h
X = np.genfromtxt( 'clusterdata.csv ',delimiter= ',')# read the data
Z = agg_cluster(X) # form the linkage matrix
h.dendrogram(Z) # SciPy can produce a dendrogram from Z
# fcluster function assigns cluster ids to all points based on Z
cl = h.fcluster(Z, criterion = 'maxclust ', t=3)
import matplotlib.pyplot as plt
plt.figure(2), plt.clf()
cols = [ 'red','green ','blue ']
colors = [cols[i-1] for iincl]
plt.scatter(X[:,0], X[:,1],c=colors)
plt.show()
Note that the distance matrix is initialized with the squared Euclidean distance, so that
the Ward linkage is rescaled by a factor of 2. Also, note that the linkage matrix stores
the square root of the minimal cluster distances rather than the distances themselves. We
leave it as an exercise to check that by using these modiÔ¨Åcations the results agree with the
linkage method from scipy ; see Exercise 9.
In contrast to the bottom-up (agglomerative) approach to hierarchical clustering, the
divisive approach starts with one cluster, which is divided into two clusters that are as
‚Äúdissimilar‚Äù as possible, which can then be further divided, and so on. We can use the same
linkage criteria as for agglomerative clustering to divide a parent cluster into two child
clusters by maximizing the distance between the child clusters. Although it is a natural to try
to group together data by separating dissimilar ones as far as possible, the implementation
of this idea tends to scale poorly with n. The problem is related to the well-known max-cut
problem max-cut
problem: given an nnmatrix of positive costs ci j;i;j2f1;:::; ng, partition the index set
I=f1;:::; nginto two subsetsJandKsuch that the total cost across the sets, that is,
X
j2JX
k2Kdjk;
is maximal. If instead we maximize according to the average distance, we obtain the group
average linkage criterion.
Example 4.9 (Divisive Clustering via CE) The following Python code is used to di-
vide a small data set (of size 300) into two parts according to maximal group average link-
age. It uses a short cross-entropy algorithm similar to the one presented in Example 3.19.
Given a vector of probabilities fpi;i=1;:::; ng, the algorithm generates an nnmatrix +110
of Bernoulli random variables with success probability pifor column i. For each row, the
0s and 1s divide the index set into two clusters, and the corresponding average linkage152 4.7. Hierarchical Clustering
distance is computed. The matrix is then sorted row-wise according to these distances. Fi-
nally, the probabilities fpigare updated according to the mean values of the best 10% rows.
The process is repeated until the fpigdegenerate to a binary vector. This then presents the
(approximate) solution.
clustCE2.py
import numpy as np
from numpy import genfromtxt
from scipy.spatial.distance import squareform
from scipy.spatial.distance import pdist
import matplotlib.pyplot as plt
def S(x,D):
V1 = np.where(x==0)[0] # {V1,V2} is the partition
V2 = np.where(x==1)[0]
tmp = D[V1]
tmp = tmp[:,V2]
return np.mean(tmp) # the size of the cut
def maxcut(D,N,eps,rho,alpha):
n = D.shape[1]
Ne = int(rho*N)
p = 1/2*np.ones(n)
p[0] = 1.0
while (np. max(np.minimum(p,np.subtract(1,p))) > eps):
x = np.array(np.random.uniform(0,1,(N,n))<=p, dtype=np.int64)
sx = np.zeros(N)
for iin range (N):
sx[i] = S(x[i],D)
sortSX = np.flip(np.argsort(sx))
#print("gamma = ",sx[sortSX[Ne-1]], " best=",sx[sortSX[0]])
elIds = sortSX[0:Ne]
elites = x[elIds]
pnew = np.mean(elites , axis=0)
p = alpha*pnew + (1.0-alpha)*p
return np.round (p)
Xmat = genfromtxt( 'clusterdata.csv ', delimiter= ',')
n = Xmat.shape[0]
D = squareform(pdist(Xmat))
N = 1000
eps = 10**-2
rho = 0.1
alpha = 0.9
# CE
pout = maxcut(D,N,eps,rho, alpha);
cutval = S(pout ,D)Chapter 4. Unsupervised Learning 153
print ("cutvalue ",cutval)
#plot
V1 = np.where(pout==0)[0]
xblue = Xmat[V1]
V2 = np.where(pout==1)[0]
xred = Xmat[V2]
plt.scatter(xblue[:,0],xblue[:,1], c="blue")
plt.scatter(xred[:,0],xred[:,1], c="red")
cutvalue 4.625207676517948
6
 4
 2
 0
 2
 4
4
3
2
1
0
1
2
3
Figure 4.11: Division of the data in Figure 4.4 into two clusters, via the cross-entropy
method.
4.8 Principal Component Analysis (PCA)
The main idea of principal component analysis principal
component
analysis(PCA) is to reduce the dimensionality of
a data set consisting of many variables. PCA is a feature reduction (orfeature extraction )
mechanism, that helps us to handle high-dimensional data with more features than is con-
venient to interpret.
4.8.1 Motivation: Principal Axes of an Ellipsoid
Consider a d-dimensional normal distribution with mean vector 0and covariance matrix
. The corresponding pdf (see (2.33)) is + 45
f(x)=1p(2)njje 1
2x> 1x;x2Rd:
If we were to draw many iid samples from this pdf, the points would roughly have an
ellipsoid pattern, as illustrated in Figure 3.1, and correspond to the contours of f: sets of + 71154 4.8. Principal Component Analysis (PCA)
points xsuch that x> 1x=c, for some c>0. In particular, consider the ellipsoid
x> 1x=1;x2Rd: (4.42)
Let=BB>, where Bis for example the (lower) Cholesky matrix. Then, as explained +373
in Example A.5, the ellipsoid (4.42) can also be viewed as the linear transformation of +366
d-dimensional unit sphere via matrix B. Moreover, the principal axes of the ellipsoid canprincipal axesbe found via a singular value decomposition (SVD) of B(or); see Section A.6.5 andsingular value
decomposition Example A.8. In particular, suppose that an SVD of Bis
+378
B=UDV>(note that an SVD of is then UD2U>):
The columns of the matrix UDcorrespond to the principal axes of the ellipsoid, and the
relative magnitudes of the axes are given by the elements of the diagonal matrix D. If some
of these magnitudes are small compared to the others, a reduction in the dimension of the
space may be achieved by projecting each point x2Rdonto the subspace spanned by the
main (say kd) columns of U‚Äî the so-called principal components principal
components. Suppose without
loss of generality that the Ô¨Årst kprincipal components are given by the Ô¨Årst kcolumns of
U, and let Ukbe the corresponding dkmatrix.
With respect to the standard basis feig, the vector x=x1e1++xdedis represented by
thed-dimensional vector [ x1;:::; xd]>. With respect to the orthonormal basis fuigformed
by the columns of matrix U, the representation of xisU>x. Similarly, the projection of
any point xonto the subspace spanned by the Ô¨Årst kprincipal vectors is represented by the
k-dimensional vector U>
kx, with respect to the orthonormal basis formed by the columns of
Uk. So, the idea is that if a point xlies close to its projection UkU>
kx, we may represent it via
knumbers instead of d, using the combined features given by the kprincipal components.
See Section A.4 for a review of projections and orthonormal bases. +362
Example 4.10 (Principal Components) Consider the matrix
=266666666414 8 3
8 5 2
3 2 13777777775;
which can be written as =BB>, with
B=26666666641 2 3
0 1 2
0 0 13777777775:
Figure 4.12 depicts the ellipsoid x> 1x=1, which can be obtained by linearly transform-
ing the points on the unit sphere by means of the matrix B. The principal axes and sizes of
the ellipsoid are found through a singular value decomposition B=UDV>, where Uand
Dare
U=26666666640:8460 0:4828 0:2261
0:4973 0:5618 0:6611
0:1922 0:6718 0:71543777777775and D=26666666644:4027 0 0
0 0:7187 0
0 0 0 :31603777777775:Chapter 4. Unsupervised Learning 155
The columns of Ushow the directions of the principal axes of the ellipsoid, and the di-
agonal elements of Dindicate the relative magnitudes of the principal axes. We see that
the Ô¨Årst principal component is given by the Ô¨Årst column of U, and the second principal
component by the second column of U.
The projection of the point x=[1:052;0:6648;0:2271]>onto the 1-dimensional space
spanned by the Ô¨Årst principal component u1=[0:8460;0:4972;0:1922]>isz=u1u>
1x=
[1:0696;0:6287;0:2429]>. With respect to the basis vector u1,zis represented by the num-
beru>
1z=1:2643. That is, z=1:2643u1.
Figure 4.12: A ‚Äúsurfboard‚Äù ellipsoid where one principal axis is signiÔ¨Åcantly larger than
the other two.
4.8.2 PCA and Singular Value Decomposition (SVD)
In the setting above, we did not consider any data set drawn from a multivariate pdf f. The
whole analysis rested on linear algebra. In principal component analysis principal
component
analysis(PCA) we start
with data x1;:::; xn, where each xisd-dimensional. PCA does not require assumptions
how the data were obtained, but to make the link with the previous section, we can think
of the data as iid draws from a multivariate normal pdf.
Let us collect the data in a matrix Xin the usual way; that is, + 43
X=26666666666666664x11x12::: x1d
x21x22::: x2d
::::::::::::
xn1xn2::: xnd37777777777777775=26666666666666664x>
1
x>
2:::
x>
n37777777777777775:
The matrix Xwill be the PCA‚Äôs input. Under this setting, the data consists of points in d-
dimensional space, and our goal is to present the data using nfeature vectors of dimension
k<d.
In accordance with the previous section, we assume that underlying distribution of the
data has expectation vector 0. In practice, this means that before PCA is applied, the data
needs to be centered by subtracting the column mean in every column:
x0
i j=xi j xj;156 4.8. Principal Component Analysis (PCA)
where xj=1
nPn
i=1xi j.
We assume from now on that the data comes from a general d-dimensional distribution
with mean vector 0and some covariance matrix . The covariance matrix is by deÔ¨Ånition
equal to the expectation of the random matrix XX>, and can be estimated from the data
x1;:::; xnvia the sample average
b=1
nnX
i=1xix>
i=1
nX>X:
Asbis a covariance matrix, we may conduct the same analysis for bas we did for in the
previous section. SpeciÔ¨Åcally, suppose b=UD2U>is an SVD of band let Ukbe the matrix
whose columns are the kprincipal components; that is, the kcolumns of Ucorresponding to
the largest diagonal elements in D2. Note that we have used D2instead of Dto be compat-
ible with the previous section. The transformation zi=UkU>
kximaps each vector xi2Rd
(thus, with dfeatures) to a vector zi2Rdlying in the subspace spanned by the columns of
Uk. With respect to this basis, the point zihas representation zi=U>
k(UkU>
kxi)=U>
kxi2Rk
(thus with kfeatures). The corresponding covariance matrix of the zi;i=1;:::; nis diag-
onal. The diagonal elements fd``gofDcan be interpreted as standard deviations of the data
in the directions of the principal components. The quantity v=P
`=1d2
``(that is, the trace of
D2) is thus a measure for the amount of variance in the data. The proportion d2
``=vindicates
how much of the variance in the data is explained by the `-th principal component.
Another way to look at PCA is by considering the question: How can we best project the
data onto a k-dimensional subspace in such a way that the total squared distance between
the projected points and the original points is minimal? From Section A.4, we know that +362
any orthogonal projection to a k-dimensional subspace Vkcan be represented by a matrix
UkU>
k, where Uk=[u1;:::; uk] and thefu`;`=1;:::; kgare orthogonal vectors of length 1
that spanVk. The above question can thus be formulated as the minimization program:
min
u1;:::;uknX
i=1kxi UkU>
kxik2: (4.43)
Now observe that
1
nnX
i=1kxi UkU>
kxik2=1
nnX
i=1(x>
i x>
iUkU>
k)(xi UkU>
kxi)
=1
nnX
i=1kxik2
|       {z       }
c 1
nnX
i=1x>
iUkU>
kxi=c 1
nnX
i=1kX
`=1tr(x>
iu`u>
`xi)
=c 1
nkX
`=1nX
i=1u>
`xix>
iu`=c kX
`=1u>
`bu`;
where we have used the cyclic property of a trace (Theorem A.1) and the fact that UkU>
k +357
can be written asPk
`=1u`u>
`. It follows that the minimization problem(4.43) is equivalent
to the maximization problem
max
u1;:::;ukkX
`=1u>
`bu`: (4.44)Chapter 4. Unsupervised Learning 157
This maximum can be at mostPk
`=1d2
``and is attained precisely when u1;:::; ukare the
Ô¨Årstkprincipal components of b.
Example 4.11 (Singular Value Decomposition) The following data set consists of in-
dependent samples from the three-dimensional Gaussian distribution with mean vector 0
and covariance matrix given in Example 4.10:
X=26666666666666666666666666666666666666666666643:1209 1:7438 0:5479
 2:6628 1:5310 0:2763
3:7284 3:0648 1:8451
0:4203 0:3553 0:4268
 0:7155 0:6871 0:1414
5:8728 4:0180 1:4541
4:8163 2:4799 0:5637
2:6948 1:2384 0:1533
 1:1376 0:4677 0:2219
 1:2452 0:9942 0:44493777777777777777777777777777777777777777777775:
After replacing Xwith its centered version, an SVD UD2U>ofb=X>X=nyields the
principal component matrix Uand diagonal matrix D:
U=2666666664 0:8277 0:4613 0:3195
 0:5300 0:4556 0:7152
 0:1843 0:7613 0:62163777777775and D=26666666643:3424 0 0
0 0:4778 0
0 0 0 :10383777777775:
We also observe that, apart from the sign of the Ô¨Årst column, the principal component
matrix Uis similar to that in Example 4.10. Likewise for the matrix D. We see that 97.90%
of the total variance is explained by the Ô¨Årst principal component. Figure 4.13 shows the
projection of the centered data onto the subspace spanned by this principal component.
x42024y
210123z
1.00.50.00.51.01.5
Figure 4.13: Data from the ‚Äúsurfboard‚Äù pdf is projected onto the subspace spanned by the
largest principal component.
The following Python code was used.158 4.8. Principal Component Analysis (PCA)
PCAdat.py
import numpy as np
X = np.genfromtxt( 'pcadat.csv ', delimiter= ',')
n = X.shape[0]
X = X - X.mean(axis=0)
G = X.T @ X
U, _ , _ = np.linalg.svd(G/n)
# projected points
Y = X @ np.outer(U[:,0],U[:,0])
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection= '3d')
ax.w_xaxis.set_pane_color((0, 0, 0, 0))
ax.plot(Y[:,0], Y[:,1], Y[:,2], c= 'k', linewidth=1)
ax.scatter(X[:,0], X[:,1], X[:,2], c= 'b')
ax.scatter(Y[:,0], Y[:,1], Y[:,2], c= 'r')
for iin range (n):
ax.plot([X[i,0], Y[i,0]], [X[i,1],Y[i,1]], [X[i,2],Y[i,2]], 'b')
ax.set_xlabel( 'x')
ax.set_ylabel( 'y')
ax.set_zlabel( 'z')
plt.show()
Next is an application of PCA to Fisher‚Äôs famous iris data set, already mentioned in
Section 1.1, and Exercise 1.5. +2
Example 4.12 (PCA for the Iris Data Set) Theiris data set contains measurements
on four features of the iris plant: sepal length and width, and petal length and width, for a
total of 150 specimens. The full data set also contains the species name, but for the purpose
of this example we ignore it.
Figure 1.9 shows that there is a signiÔ¨Åcant correlation between the di erent features. +17
Can we perhaps describe the data using fewer features by taking certain linear combin-
ations of the original features? To investigate this, let us perform a PCA, Ô¨Årst centering
the data. The following Python code implements the PCA. It is assumed that a CSV Ô¨Åle
irisX.csv has been made that contains the iris data set (without the species information).
PCAiris.py
import seaborn as sns, numpy as np
np.set_printoptions(precision=4)
X = np.genfromtxt( 'IrisX.csv ',delimiter= ',')
n = X.shape[0]Chapter 4. Unsupervised Learning 159
X = X - np.mean(X, axis=0)
[U,D2,UT]= np.linalg.svd((X.T @ X)/n)
print ('U = \n ', U); print ('\n diag(D^2) = ', D2)
z = U[:,0].T @ X.T
sns.kdeplot(z, bw=0.15)
U =
[[-0.3614 -0.6566 0.582 0.3155]
[ 0.0845 -0.7302 -0.5979 -0.3197]
[-0.8567 0.1734 -0.0762 -0.4798]
[-0.3583 0.0755 -0.5458 0.7537]]
diag(D^2) = [4.2001 0.2411 0.0777 0.0237]
The output above shows the principal component matrix (which we called U) as well as
the diagonal of matrix D2. We see that a large proportion of the variance, 4 :2001=(4:2001 +
0:2411+0:0777+0:0237) =92:46%, is explained by the Ô¨Årst principal component. Thus, it
makes sense to transform each data point x2R4tou>
1x2R. Figure 4.14 shows the kernel
density estimate of the transformed data. Interestingly, we see two modes, indicating at
least two clusters in the data.
-4 -3 -2 -1 0 1 2 3 4
PCA-combined data00.20.40.6kernel density estimate
Figure 4.14: Kernel density estimate of the PCA-combined iris data.
Further Reading
Various information-theoretic measures to quantify uncertainty, including the Shannon en-
tropy and Kullback‚ÄìLeibler divergence, may be found in [28]. The Fisher information, the
prominent information measure in statistics, is discussed in detail in [78]. Akaike‚Äôs inform-
ation criterion appeared in [2]. The EM algorithm was introduced in [31] and [85] gives an
in-depth treatment. Convergence proofs for the EM algorithm may be found in [19, 128].
A classical reference on kernel density estimation is [113], and [14] is the main reference
for the theta kernel density estimator. Theory and applications on Ô¨Ånite mixture models
may be found in [86]. For more details on clustering applications and algorithms as well
as references on data compression, vector quantization, and pattern recognition, we refer160 Exercises
to [1, 35, 107, 125]. A useful modiÔ¨Åcation of the K-means algorithm is the fuzzy K-means
algorithm; see, e.g., [9]. A popular way to choose the starting positions in K-means is given
by the K-means ++heuristic, introduced in [4].
Exercises
1. This exercise is to show that the Fisher information matrix F() in (4.8) is equal to the
matrix H() in (4.9), in the special case where f=g(j), and under the assumption that
integration and di erentiation orders can be interchanged.
(a) Let hbe a vector-valued function and ka real-valued function. Prove the following
quotient rule for di erentiation quotient rule
for
differentiation:
@[h()=k()]
@=1
k()@h()
@ 1
k2()@k()
@h()>: (4.45)
(b) Now take h()=@g(Xj)
@andk()=g(Xj) in (4.45) and take expectations with
respect toEon both sides to show that
 H()=E26666641
g(Xj)@@g(Xj)
@
@3777775
|                    {z                    }
A F():
(c) Finally show that Ais the zero matrix.
2. Plot the mixture of N(0;1),U(0;1), and Exp(1) distributions, with weights w1=w2=
w3=1=3.
3. Denote the pdfs in Exercise 2 by f1;f2;f3, respectively. Suppose that Xis simulated via
the two-step procedure: First, draw Zfromf1;2;3g, then draw Xfrom fZ. How likely is it
that the outcome x=0:5 ofXhas come from the uniform pdf f2?
4. Simulate an iid training set of size 100 from the Gamma (2:3;0:5) distribution, and
implement the Fisher scoring method in Example 4.1 to Ô¨Ånd the maximum likelihood es-
timate. Plot the true and approximate pdfs.
5. LetT=fX1;:::; Xngbe iid data from a pdf g(xj) with Fisher matrix F(). Explain
why, under the conditions where (4.7) holds,
ST() :=1
nnX
i=1S(Xij)
for large nhas approximately a multivariate normal distribution with expectation vector 0
and covariance matrix F()=n.
6. Figure 4.15 shows a Gaussian KDE with bandwidth =0:2 on the points 0:5;0,
0:2;0:9, and 1:5. Reproduce the plot in Python. Using the same bandwidth, plot also the
KDE for the same data, but now with (z)=1=2;z2[ 1;1].Chapter 4. Unsupervised Learning 161
-1 0 1 200.20.40.60.8
Figure 4.15: The Gaussian KDE (solid line) is the equally weighted mixture of normal pdfs
centered around the data and with standard deviation =0:2 (dashed).
7. For Ô¨Åxed x0, the Gaussian kernel function
f(xjt) :=1p
2te 1
2(x x0)2
t
is the solution to Fourier‚Äôs heat equation
@
@tf(xjt)=1
2@2
@x2f(xjt);x2R;t>0;
with initial condition f(xj0)=(x x0) (the Dirac function at x0). Show this. As a con-
sequence, the Gaussian KDE is the solution to the same heat equation, but now with initial
condition f(xj0)=n 1Pn
i=1(x xi). This was the motivation for the theta KDE [14],
which is a solution to the same heat equation but now on a bounded interval.
8. Show that the Ward linkage given in (4.41) is equal to
dWard(I;J)=jIjjJj
jIj+jJjkxI xJk2:
9. Carry out the agglomerative hierarchical clustering of Example 4.8 via the linkage
method from scipy.cluster.hierarchy . Show that the linkage matrices are the same.
Give a scatterplot of the data, color coded into K=3 clusters.
10. Suppose that we have the data n=fx1;:::; xnginRand decide to train the two-
component Gaussian mixture model
g(xj)=w11q
22
1exp 
 (x 1)2
22
1!
+w21q
22
2exp 
 (x 2)2
22
2!
;
where the parameter vector =[1;2;1;2;w1;w2]>belongs to the set
 =f:w1+w2=1;w12[0;1];i2R;i>0;8ig:
Suppose that the training is via the maximum likelihood in (2.28). Show that
sup
21
nnX
i=1lng(xij)=1:
In other words, Ô¨Ånd a sequence of values for 2such that the likelihood grows without
bound. How can we restrict the set to ensure that the likelihood remains bounded?162 Exercises
11. A d-dimensional normal random vector XN(;) can be deÔ¨Åned via an a ne
transformation, X=+1=2Z, of a standard normal random vector ZN(0;Id), where
1=2(1=2)>= . In a similar way, we can deÔ¨Åne a d-dimensional Student random vector
Xt(;) via a transformation
X=+1p
S1=2Z; (4.46)
where, ZN(0;Id) and SGamma (
2;
2) are independent,  > 0, and 1=2(1=2)>= .
Note that we obtain the multivariate normal distribution as a limiting case for !1 .
(a) Show that the density of the t(0;Id) distribution is given by
t(x) := ((+d)=2)
()d=2 (=2) 
1+1
kxk2! +d
2
:
By the transformation rule (C.23), it follows that the density of Xt(;) is given +433
byt;(x ), where
t;(x) :=1
j1=2jt( 1=2x):
[Hint: conditional on S=s,Xhas aN(0;Id=s) distribution.]
(b) We wish to Ô¨Åt a t(;) distribution to given data =fx1;:::; xnginRdvia the EM
method. We use the representation (4.46) and augment the data with the vector S=
[S1;:::; Sn]>of hidden variables. Show that the complete-data likelihood is given by
g(;sj)=Y
i(=2)=2s(+d)=2 1
iexp( si
2 si
2k 1=2(xi )k2)
 (=2)(2)d=2j1=2j: (4.47)
(c) Show that, as a consequence, conditional on the data and parameter , the hidden
data are mutually independent, and
(Sij;)Gamma +d
2;+k 1=2(xi )k2
2!
;i=1;:::; n:
(d) At iteration tof the EM algorithm, let g(t)(s)=g(sj;(t 1)) be the density of the
missing data, given the observed data and the current parameter guess (t 1). Verify
that the expected complete-data log-likelihood is given by:
Eg(t)lng(;Sj)=n
2ln
2 nd
2ln(2) nln 
2
 n
2lnjj
++d 2
2nX
i=1Eg(t)lnSi nX
i=1+k 1=2(xi )k2
2Eg(t)Si:
Show that
Eg(t)Si=(t 1)+d
(t 1)+k((t 1)) 1=2(xi (t 1))k2=:w(t 1)
i
Eg(t)lnSi=  (t 1)+d
2!
 ln (t 1)+d
2!
+lnw(t 1)
i;
where :=(ln )0isdigamma function.Chapter 4. Unsupervised Learning 163
(e) Finally, show that in the M-step of the EM algorithm (t)is updated from (t 1)as
follows:
(t)=Pn
i=1w(t 1)
ixiPn
i=1w(t 1)
i
(t)=1
nnX
i=1w(t 1)
i(xi (t))(xi (t))>;
and(t)is deÔ¨Åned implicitly through the solution of the nonlinear equation:
ln
2
  
2
+  (t)+d
2!
 ln (t)+d
2!
+1+Pn
i=1
ln(w(t 1)
i) w(t 1)
i
n=0:
12. A generalization of both the gamma and inverse-gamma distribution is the generalized
inverse-gamma distribution generalized
inverse -gamma
distribution, which has density
f(s)=(a=b)p=2
2Kp(p
ab)sp 1e 1
2(as+b=s);a;b;s>0;p2R; (4.48)
where Kpis the modiÔ¨Åed Bessel function of the second kind modified Bessel
function of the
second kind, which can be deÔ¨Åned as the
integral
Kp(x)=Z1
0e xcosh( t)cosh( pt) dt;x>0;p2R: (4.49)
We write SGIG(a;b;p) to denote that Shas a pdf of the form (4.48). The function Kp
has many interesting properties. Special cases include
K1=2(x)=r
x
2e x1
x
K3=2(x)=r
x
2e x 1
x+1
x2!
K5=2(x)=r
x
2e x 1
x+3
x2+3
x3!
:
More generally, KpsatisÔ¨Åes the recursion
Kp+1(x)=Kp 1(x)+2p
xKp(x): (4.50)
(a) Using the change of variables ez=spa=b, show that
Z1
0sp 1e 1
2(as+b=s)ds=2Kp(p
ab)(b=a)p=2:
(b) Let SGIG(a;b;p). Show that
ES=p
b K p+1(p
ab)
pa K p(p
ab)(4.51)
and
ES 1=pa K p+1(p
ab)p
b K p(p
ab) 2p
b: (4.52)164 Exercises
13. In Exercise 11 we viewed the multivariate Student tdistribution as a scale-mixture scale -mixture
of the N(0;Id) distribution. In this exercise, we consider a similar transformation, but now
1=2ZN(0;) is not divided but is multiplied byp
S, with SGamma (=2;=2):
X=+p
S1=2Z; (4.53)
where SandZare independent and >0.
(a) Show, using Exercise 12, that for 1=2=Idand=0, the random vector Xhas a
d-dimensional Bessel distribution Bessel
distribution, with density:
(x) :=21 (+d)=2(+d)=4kxk( d)=2
d=2 (=2)K( d)=2
kxkp
;x2Rd;
where Kpis the modiÔ¨Åed Bessel function of the second kind given in (4.49). We write
XBessel(0;Id). A random vector Xis said to have a Bessel(;) distribution if
it can be written in the form (4.53). By the transformation rule (C.23), its density is
given by1pjj( 1=2(x )). Special instances of the Bessel pdf include:
2(x)=exp( p
2jxj)p
2
4(x)=1+2jxj
2exp( 2jxj)
4(x1;x2;x3)=1
exp
 2q
x2
1+x2
2+x2
3
d+1(x)=((d+1)=2)d=2p
(2)d=2 ((d+1)=2)exp
 p
d+1kxk
;x2Rd:
Note that k2is the (scaled) pdf of the double-exponential or Laplace distribution.
(b) Given the data =fx1;:::; xnginRd, we wish to Ô¨Åt a Bessel pdf to the data by
employing the EM algorithm, augmenting the data with the vector S=[S1;:::; Sn]>
of missing data. We assume that is known and  > d. Show that conditional on
(and given), the missing data vector Shas independent components, with Si
GIG(;bi;( d)=2), with bi:=k 1=2(xi )k2,i=1;:::; n.
(c) At iteration tof the EM algorithm, let g(t)(s)=g(sj;(t 1)) be the density of the
missing data, given the observed data and the current parameter guess (t 1). Show
that the expected complete-data log-likelihood is given by:
Q(t)() :=Eg(t)lng(;Sj)= 1
2nX
i=1bi()w(t 1)
i+constant; (4.54)
where bi()=k 1=2(xi )k2and
w(t 1)
i:=pK( d+2)=2p
bi((t 1))
p
bi((t 1))K( d)=2p
bi((t 1))  d
bi((t 1));i=1;:::; n:
(d) From (4.54) derive the M-step of the EM algorithm. That is, show how (t)is updated
from(t 1).Chapter 4. Unsupervised Learning 165
14. Consider the ellipsoid E=fx2Rd:x 1x=1gin (4.42). Let UD2U>be an SVD of
. Show that the linear transformation x7!U>D 1xmaps the points on Eonto the unit
spherefz2Rd:kzk=1g.
15. Figure 4.13 shows how the centered ‚Äúsurfboard‚Äù data are projected onto the Ô¨Årst
column of the principal component matrix U. Suppose we project the data instead onto
the plane spanned by the Ô¨Årst twocolumns of U. What are aandbin the representation
ax1+bx2=x3of this plane?
16. Figure 4.14 suggests that we can assign each feature vector xin the iris data set to
one of two clusters, based on the value of u>
1x, where u1is the Ô¨Årst principal component.
Plot the sepal lengths against petal lengths and color the points for which u>
1x<1:5 dier-
ently to points for which u>
1x>1:5. To which species of iris do these clusters correspond?166CHAPTER5
REGRESSION
Many supervised learning techniques can be gathered under the name ‚Äúregression‚Äù.
The purpose of this chapter is to explain the mathematical ideas behind regression
models and their practical aspects. We analyze the fundamental linear model in detail,
and also discuss nonlinear and generalized linear models.
5.1 Introduction
Francis Galton observed in an article in 1889 that the heights of adult o spring are, on the
whole, more ‚Äúaverage‚Äù than the heights of their parents. Galton interpreted this as a degen-
erative phenomenon, using the term ‚Äúregression‚Äù to indicate this ‚Äúreturn to mediocrity‚Äù.
Nowadays, regression regression refers to a broad class of supervised learning techniques where the
aim is to predict a quantitative response (output) variable yvia a function g(x) of an ex-
planatory (input) vector x=[x1;:::; xp]>, consisting of pfeatures, each of which can be
continuous or discrete. For instance, regression could be used to predict the birth weight of
a baby (the response variable) from the weight of the mother, her socio-economic status,
and her smoking habits (the explanatory variables).
Let us recapitulate the framework of supervised learning established in Chapter 2. The + 19
aim is to Ô¨Ånd a prediction function gthat best guesses1what the random output Ywill be
for a random input vector X. The joint pdf f(x;y) ofXandYis unknown, but a training
set=f(x1;y1);:::; (xn;yn)gis available, which is thought of as the outcome of a random
training setT=f(X1;Y1);:::; (Xn;Yn)gof iid copies of ( X;Y). Once we have selected a
loss function Loss( y;by), such as the squared-error loss squared -error
loss
Loss( y;by)=(y by)2; (5.1)
then the ‚Äúbest‚Äù prediction function gis deÔ¨Åned as the one that minimizes the risk risk `(g)=
ELoss( Y;g(X)). We saw in Section 2.2 that for the squared-error loss this optimal predic-
tion function is the conditional expectation
g(x)=E[YjX=x]:
1Recall the mnemonic use of ‚Äúg‚Äù for ‚Äúguess‚Äù
167168 5.1. Introduction
As the squared-error loss is the most widely-used loss function for regression, we will
adopt this loss function in most of this chapter.
The optimal prediction function ghas to be learned from the training set by minim-
izing the training loss
`(g)=1
nnX
i=1(yi g(xi))2(5.2)
over a suitable class of functions G. Note that in the above deÔ¨Ånition, the training set is
assumed to be Ô¨Åxed. For a random training set T, we will write the training loss as `T(g).
The function gG
that minimizes the training loss is the function we use for prediction ‚Äî
the so-called learner learner . When the function class Gis clear from the context, we drop the
superscript in the notation.
As we already saw in (2.2), conditional on X=x, the response Ycan be written as +21
Y=g(x)+"(x);
whereE"(x)=0. This motivates a standard modeling assumption in supervised learn-
ing, in which the responses Y1;:::; Yn, conditional on the explanatory variables X1=
x1;:::; Xn=xn, are assumed to be of the form
Yi=g(xi)+"i;i=1;:::; n;
where thef"igare independent with E"i=0 andVar"i=2for some function g2Gand
variance2. The above model is usually further speciÔ¨Åed by assuming that gis completely
known up to an unknown parameter vector; that is,
Yi=g(xij)+"i;i=1;:::; n: (5.3)
While the model (5.3) is described conditional on the explanatory variables, it will be
convenient to make one further model simpliÔ¨Åcation, and view (5.3) as if the fxigwere
Ô¨Åxed , while thefYigare random.
For the remainder of this chapter, we assume that the training feature vectors fxigare
Ô¨Åxed and only the responses are random; that is, T=f(x1;Y1);:::; (xn;Yn)g.
The advantage of the model (5.3) is that the problem of estimating the function g from
the training data is reduced to the (much simpler) problem of estimating the parameter
vector. An obvious disadvantage is that functions of the form g(j) may not accurately
approximate the true unknown g. The remainder of this chapter deals with the analysis
of models of the form (5.3). In the important case where the function g(j) islinear , the
analysis proceeds through the class of linear models. If, in addition, the error terms f"igare
assumed to be Gaussian , this analysis can be carried out using the rich theory of normal
linear models.Chapter 5. Regression 169
5.2 Linear Regression
The most basic regression model involves a linear relationship between the response and a
single explanatory variable. In particular, we have measurements ( x1;y1);:::; (xn;yn) that
lie approximately on a straight line, as in Figure 5.1.
-3 -2 -1 0 1 2 3-5051015
Figure 5.1: Data from a simple linear regression model.
Following the general scheme captured in (5.3), a simple model for these data is that
thefxigare Ô¨Åxed and variables fYigare random such that
Yi=0+1xi+"i;i=1;:::; n; (5.4)
for certain unknown parameters 0and1. Thef"igare assumed to be independent with
expectation 0 and unknown variance 2. The unknown line
y=0+1x|    {z    }
g(xj)(5.5)
is called the regression line regression line . Thus, we view the responses as random variables that would
lie exactly on the regression line, were it not for some ‚Äúdisturbance‚Äù or ‚Äúerror‚Äù term repres-
ented by thef"ig. The extent of the disturbance is modeled by the parameter 2. The model
in (5.4) is called simple linear regression simple linear
regression
model. This model can easily be extended to incorporate
more than one explanatory variable, as follows.
DeÔ¨Ånition 5.1: Multiple Linear Regression Model
In a multiple linear regression model multiple linear
regression
modelthe response Ydepends on a d-dimensional
explanatory vector x=[x1;:::; xd]>, via the linear relationship
Y=0+1x1++dxd+"; (5.6)
whereE"=0 andVar"=2.170 5.2. Linear Regression
Thus, the data lie approximately on a d-dimensional a ne hyperplane
y=0+1x1++dxd|                      {z                      }
g(xj);
where we deÔ¨Åne =[0;1;:::; d]>. The function g(xj) is linear in , but not linear in
the feature vector x, due to the constant 0. However, augmenting the feature space with
the constant 1, the mapping [1 ;x>]>7!g(xj) :=[1;x>]becomes linear in the feature
space and so (5.6) becomes a linear model (see Section 2.1). Most software packages for +43
regression include 1 as a feature by default.
Note that in (5.6) we only speciÔ¨Åed the model for a single pair ( x;Y). The model for the
training setT=f(x1;Y1);:::; (xn;Yn)gis simply that each YisatisÔ¨Åes (5.6) (with x=xi)
and that thefYigare independent. Setting Y=[Y1;:::; Yn]>, we can write the multiple
linear regression model for the training data compactly as
Y=X+"; (5.7)
where"=["1;:::;" n]>is a vector of iid copies of "andXis the model matrix model matrix given by
X=266666666666666641x11x12 x1d
1x21x22 x2d
:::::::::::::::
1xn1xn2 xnd37777777777777775=266666666666666641x>
1
1x>
2::::::
1x>
n37777777777777775:
Example 5.1 (Multiple Linear Regression Model) Figure 5.2 depicts a realization of
the multiple linear regression model
Yi=xi1+xi2+"i;i=1;:::; 100;
where"1;:::;" 100iidN(0;1=16). The Ô¨Åxed feature vectors (vectors of explanatory vari-
ables) xi=[xi1;xi2]>;i=1;:::; 100 lie in the unit square.
10
012
10
Figure 5.2: Data from a multiple linear regression model.Chapter 5. Regression 171
5.3 Analysis via Linear Models
Analysis of data from a linear regression model is greatly simpliÔ¨Åed through the linear
model representation (5.7). In this section we present the main ideas for parameter estima-
tion and model selection for a general linear model of the form
Y=X+"; (5.8)
where Xis an npmatrix,=[1;:::; p]>a vector of pparameters, and "=["1;:::;" n]>
ann-dimensional vector of independent error terms, with E"i=0 andVar"i=2,i=
1;:::; n. Note that the model matrix Xis assumed to be Ô¨Åxed, and Yand"are random. A
speciÔ¨Åc outcome of Yis denoted by y(in accordance with the notation in Section 2.8). + 46
Note that the multiple linear regression model in (5.7) was deÔ¨Åned using a di erent
parameterization; in particular, there we used =[0;1;:::; d]>. So, when apply-
ing the results in the present section to such models, be aware that p=d+1. Also,
in this section a feature vector xincludes the constant 1, so that X>=[x1;:::; xn].
5.3.1 Parameter Estimation
The linear model Y=X+"contains two unknown parameters, and2, which have
to be estimated from the training data . To estimate , we can repeat exactly the same
reasoning used in our recurring polynomial regression Example 2.1 as follows. For a linear + 26
prediction function g(x)=x>, the (squared-error) training loss can be written as
`(g)=1
nky Xk2;
and the optimal learner gminimizes this quantity, leading to the least-squares estimate b,
which satisÔ¨Åes the normal equations
X>X=X>y: (5.9)
The corresponding training loss can be taken as an estimate of 2; that is,
c2=1
nky Xbk2: (5.10)
To justify the latter, note that 2is the second moment of the model errors "i;i=1;:::; n,
in (5.8) and could be estimated via the method of moments (see Section C.12.1) using the +455
sample average n 1P
i"2
i=k"k2=n=kY Xk2=n, ifwere known. By replacing with
its estimator, we arrive at (5.10). Note that no distributional properties of the f"igwere used
other thanE"i=0 andVar"i=2;i=1;:::; n. The vector e:=y Xbis called the
vector of residuals residuals and approximates the (unknown) vector of model errors ". The quantity
kek2=Pn
i=1e2
iis called the residual sum of squares (RSS). Dividing the RSS by n pgivesresidual sum of
squares an unbiased estimate of 2, which we call the estimated residual squared error (RSE); see
residual
squared errorExercise 12.172 5.3. Analysis via Linear Models
In terms of the notation given in the summary Table 2.1 for supervised learning, we
thus have: +25
1. The (observed) training data is =fX;yg.
2. The function class Gis the class of linear functions of x; that isG=fg(j) :x7!
x>;2Rpg.
3. The (squared-error) training loss is `(g(j))=ky Xk2=n:
4. The learner gis given by g(x)=x>b, where b=argmin2Rpky Xk2.
5. The minimal training loss is `(g)=ky Xbk2=n=c2.
5.3.2 Model Selection and Prediction
Even if we restrict the learner to be a linear function, there is still the issue of which explan-
atory variables (features) to include. While including too few features may result in large
approximation error (underÔ¨Åtting), including too many may result in large statistical error
(overÔ¨Åtting). As discussed in Section 2.4, we need to select the features which provide the +31
best tradeo between the approximation and statistical errors, so that the (expected) gener-
alization risk of the learner is minimized. Depending on how the (expected) generalization
risk is estimated, there are a number of strategies for feature selection:
1. Use test data0=(X0;y0) that are obtained independently from the training data ,
to estimate the generalization risk EkY g(X)k2via the test loss (2.7). Then choose +24
the collection of features that minimizes the test loss. When there is an abundance of
data, part of the data can be reserved as test data, while the remaining data is used as
training data.
2. When there is a limited amount of data, we can use cross-validation to estimate the
expected generalization risk EkY gT(X)k2(whereTis a random training set), as
explained in Section 2.5.2. This is then minimized over the set of possible choices +37
for the explanatory variables.
3. When one has to choose between many potential explanatory variables, techniques
such as regularized least-squares and lasso regression become important. Such
methods o er another approach to model selection, via the regularization (or ho-
motopy) paths. This will be the topic of Section 6.2 in the next chapter. +216
4. Rather than using computer-intensive techniques, such as the ones above, one can
usetheoretical estimates of the expected generalization risk, such as the in-sample
risk, AIC, and BIC, as in Section 2.5, and minimize this to determine a good set of +35
explanatory variables.
5. All of the above approaches do not assume any distributional properties of the error
termsf"igin the linear model, other than that they are independent with expectation
0 and variance 2. If, however, they are assumed to have a normal (Gaussian) distri-
bution, (that is,f"igiidN(0;2)), then the inclusion and exclusion of variables canChapter 5. Regression 173
be decided by means of hypotheses tests . This is the classical approach to model
selection, and will be discussed in Section 5.4. As a consequence of the central limit
theorem, one can use the same approach when the error terms are not necessarily
normal, provided their variance is Ô¨Ånite and the sample size nis large.
6. Finally, when using a Bayesian approach, comparison of two models can be achieved
by computing their so-called Bayes factor (see Section 2.9).
All of the above strategies can be thought of as speciÔ¨Åcations of a simple rule formu-
lated by William of Occam, which can be interpreted as:
When presented with competing models, choose the simplest one that explains
the data.
This age-old principle, known as Occam‚Äôs razor Occam ‚Äôs razor , is mirrored in a famous quote of Einstein:
Everything should be made as simple as possible, but not simpler.
In linear regression, the number of parameters or predictors is usually a reasonable measure
of the simplicity of the model.
5.3.3 Cross-Validation and Predictive Residual Sum of Squares
We start by considering the n-fold cross-validation, also called leave-one-out cross-
validation leave -one-out
cross -validation, for the linear model (5.8). We partition the data into ndata sets, leaving out
precisely one observation per data set, which we then predict based on the n 1 remaining
observations; see Section 2.5.2 for the general case. Let by idenote the prediction for the + 37
i-th observation using all the data except yi. The error in the prediction, yi by i, is called a
predicted residual predicted
residual‚Äî in contrast to an ordinary residual, ei=yi byi, which is the di erence
between an observation and its Ô¨Åtted value byi=g(xi) obtained using the whole sample. In
this way, we obtain the collection of predicted residuals fyi by ign
i=1and summarize them
through the predicted residual sum of squares (PRESS PRESS ):
PRESS =nX
i=1(yi by i)2:
Dividing the PRESS by ngives an estimate of the expected generalization risk.
In general, computing the PRESS is computationally intensive as it involves training
and predicting nseparate times. For linear models, however, the predicted residuals can be +171
calculated quickly using only the ordinary residuals and the projection matrix P=XX+
onto the linear space spanned by the columns of the model matrix X(see (2.13)). The i-th + 28
diagonal element Piiof the projection matrix is called the i-thleverage leverage , and it can be shown
that 06Pii61 (see Exercise 10).174 5.3. Analysis via Linear Models
Theorem 5.1: PRESS for Linear Models
Consider the linear model (5.8), where the npmodel matrix Xis of full rank. Given
an outcome y=[y1;:::; yn]>ofY, the Ô¨Åtted values can be obtained as by=Py;where
P=XX+=X(X>X) 1X>is the projection matrix. If the leverage value pi:=Pii,1
for all i=1;:::; n, then the predicted residual sum of squares can be written as
PRESS =nX
i=1 ei
1 pi!2
;
where ei=yi byi=yi (Xb)iis the i-th residual.
Proof: It suces to show that the i-th predicted residual can be written as yi by i=
ei=(1 pi). Let X idenote the model matrix Xwith the i-th row, x>
i, removed, and deÔ¨Åne
y isimilarly. Then, the least-squares estimate for using all but the i-th observation is
b i=(X>
 iX i) 1X>
 iy i. Writing X>X=X>
 iX i+xix>
i, we have by the Sherman‚ÄìMorrison
formula +371
(X>
 iX i) 1=(X>X) 1+(X>X) 1xix>
i(X>X) 1
1 x>
i(X>X) 1xi;
where x>
i(X>X) 1xi=pi<1. Also, X>
 iy i=X>y xiyi. Combining all these identities,
we have
b i=(X>
 iX i) 1X>
 iy i
= 
(X>X) 1+(X>X) 1xix>
i(X>X) 1
1 pi!
(X>y xiyi)
=b+(X>X) 1xix>
ib
1 pi (X>X) 1xiyi (X>X) 1xipiyi
1 pi
=b+(X>X) 1xix>
ib
1 pi (X>X) 1xiyi
1 pi
=b (X>X) 1xi(yi x>
ib)
1 pi=b (X>X) 1xiei
1 pi:
It follows that the predicted value for the i-th observation is given by
by i=x>
ib i=x>
ib x>
i(X>X) 1xiei
1 pi=byi piei
1 pi:
Hence, yi by i=ei+piei=(1 pi)=ei=(1 pi). 
Example 5.2 (Polynomial Regression (cont.)) We return to Example 2.1, where we +26
estimated the generalization risk for various polynomial prediction functions using inde-
pendent validation data. Instead, let us estimate the expected generalization risk via cross-
validation (thus using only the training set) and apply Theorem 5.1 to compute the PRESS.
+174Chapter 5. Regression 175
polyregpress.py
import numpy as np
import matplotlib.pyplot as plt
def generate_data(beta , sig, n):
u = np.random.rand(n, 1)
y = u ** np.arange(0, 4) @ beta.reshape(4,1) + (
sig * np.random.randn(n, 1))
return u, y
np.random.seed(12)
beta = np.array([[10.0, -140, 400, -250]]).T;
sig=5; n = 10**2;
u,y = generate_data(beta ,sig,n)
X = np.ones((n, 1))
K = 12 #maximum number of parameters
press = np.zeros(K+1)
for kin range (1,K):
ifk > 1:
X = np.hstack((X, u**(k-1))) # add column to matrix
P = X @ np.linalg.pinv(X) # projection matrix
e = y - P @ y
press[k] = np. sum((e/(1-np.diag(P).reshape(n,1)))**2)
plt.plot(press[1:K]/n)
The PRESS values divided by n=100 for the constant, linear, quadratic, cubic, and
quartic order polynomial regression models are, respectively, 152 :487;56:249;51:606,
30:999, and 31 :634. Hence, the cubic polynomial regression model has the lowest PRESS,
indicating that it has the best predictive performance.
5.3.4 In-Sample Risk and Akaike Information Criterion
In Section 2.5.1 we introduced the in-sample risk as a measure for the accuracy of the + 35
prediction function. To recapitulate, given a Ô¨Åxed data set with associated response vector
yandnpmatrix of explanatory variables X, the in-sample risk of a prediction function
gis deÔ¨Åned as
`in(g) :=EXLoss( Y;g(X)); (5.11)
whereEXsigniÔ¨Åes that the expectation is taken under a di erent probability model, in
which Xtakes the values x1;:::; xnwith equal probability, and given X=xithe random
variable Yis drawn from the conditional pdf f(yjxi). The di erence between the in-sample
risk and the training loss is called the optimism . For the squared-error loss, Theorem 2.2 ex- + 36
presses the expected optimism of a learner gTas two times the average covariance between
the predicted values and the responses.
If the conditional variance of the error Y g(X) given X=xdoes not depend on x,
then the expected in-sample risk of a learner g, averaged over all training sets, has a simple
expression:176 5.3. Analysis via Linear Models
Theorem 5.2: Expected In-Sample Risk for Linear Models
LetXbe the model matrix for a linear model, of dimension np. IfVar[Y 
g(X)jX=x]=:v2does not depend on x, then the expected in-sample risk (with
respect to the squared-error loss) for a random learner gTis given by
EX`in(gT)=EX`T(gT)+2`p
n; (5.12)
where`is the irreducible risk.
Proof: The expected optimism is, by deÔ¨Ånition, EX[`in(gT) `T(gT)] which, for the
squared-error loss, is equal to 2 `p=n, using exactly the same reasoning as in Example 2.3.
Note that here `=v2. 
Equation (5.12) is the basis of the following model comparison heuristic: Estimate the
irreducible risk `=v2viabv2, using a model with relatively high complexity. Then choose
the linear model with the lowest value of
ky Xbk2+2bv2p: (5.13)
We can also use the Akaike information criterion (AIC) as a heuristic for model com-
parison. We discussed the AIC in the unsupervised learning setting in Section 4.2, but the +122
arguments used there can also be applied to the supervised case, under the in-sample model
for the data. In particular, let Z=(X;Y). We wish to predict the joint density
f(z)=f(x;y) :=1
nnX
i=11fx=xigf(yjxi);
using a prediction function g(zj) from a familyG:=fg(zj);2Rqg, where
g(zj)=g(x;yj) :=1
nnX
i=11fx=xiggi(yj):
Note that qis the number of parameters (typically larger than pfor a linear model with a
npdesign matrix).
Following Section 4.2, the in-sample cross-entropy risk in this case is
r() := EXlng(Zj);
and to approximate the optimal parameter we minimize the corresponding training loss
rn() := 1
nnX
j=1lng(zjj):
The optimal parameter bnfor the training loss is thus found by minimizing
 1
nnX
j=1
 lnn+lngj(yjj)
:Chapter 5. Regression 177
That is, it is the maximum likelihood estimate of :
bn=argmax
nX
i=1lngi(yij):
Under the assumption that f=g(j) for some parameter , we have from Theorem 4.1
that the estimated in-sample generalization risk can be approximated as +125
EXr(bn)rTn(bn)+q
n=lnn 1
nnX
j=1lngj(yjjbn)+q
n:
This leads to the heuristic of selecting the learner g(jbn) with the smallest value of the
AIC:
 2nX
i=1lngi(yijbn)+2q: (5.14)
Example 5.3 (Normal Linear Model) For the normal linear model YN(x>;2)
(see (2.34)), with a p-dimensional vector , we have + 46
gi(yij;2
|{z}
=)=1p
22exp 
 1
2(yi x>
i)2
2!
;i=1;:::; n;
so that the AIC is
nln(2)+nlnb2+ky Xbk2
b2+2q; (5.15)
where ( b;b2) is the maximum likelihood estimate and q=p+1 is the number of parameters
(including2). For model comparison we may remove the nln(2) term if all the models
are normal linear models.
Certain software packages report the AIC without the nlnb2term in (5.15). This
may lead to sub-optimal model selection if normal models are compared with non-
normal ones.
5.3.5 Categorical Features
Suppose that, as described in Chapter 1, the data is given in the form of a spreadsheet or
data frame with nrows and p+1 columns, where the Ô¨Årst element of row iis the response
variable yi, and the remaining pelements form the vector of explanatory variables x>
i.
When all the explanatory variables (features, predictors) are quantitative , then the model
matrix Xcan be directly read o from the data frame as the npmatrix with rows x>
i;i=
1;:::; n.
However, when some explanatory variables are qualitative (categorical), such a one-to-
one correspondence between data frame and model matrix no longer holds. The solution is
to include indicator ordummy variables.178 5.3. Analysis via Linear Models
Linear models with continuous responses and categorical explanatory variables often
arise in factorial experiments . These are controlled statistical experiments in which thefactorial
experiments aim is to assess how a response variable is a ected by one or more factors tested at several
factors levels . A typical example is an agricultural experiment where one wishes to investigate
levelshow the yield of a food crop depends on factors such as location, pesticide, and fertilizer.
Example 5.4 (Crop Yield) The data in Table 5.1 lists the yield of a food crop for four
dierent crop treatments (e.g., strengths of fertilizer) on four di erent blocks (plots).
Table 5.1: Crop yield for di erent treatments and blocks.
Treatment
Block 1 2 3 4
1 9.2988 9.4978 9.7604 10.1025
2 8.2111 8.3387 8.5018 8.1942
3 9.0688 9.1284 9.3484 9.5086
4 8.2552 7.8999 8.4859 8.9485
The corresponding data frame, given in Table 5.2, has 16 rows and 3 columns: one
column for the crop yield (the response variable), one column for the Treatment, with
levels 1, 2, 3, 4, and one column for the Block, also with levels 1, 2, 3, 4. The values 1,
2, 3, and 4 have no quantitative meaning (it does not make sense to take their average, for
example) ‚Äî they merely identify the category of the treatment or block.
Table 5.2: Crop yield data organized as a data frame in standard format.
Yield Treatment Block
9.2988 1 1
8.2111 1 2
9.0688 1 3
8.2552 1 4
9.4978 2 1
8.3387 2 2
:::::::::
9.5086 4 3
8.9485 4 4
In general, suppose there are rfactor (categorical) variables u1;:::; ur, where the j-
th factor has pjmutually exclusive levels, denoted by 1 ;:::; pj. In order to include these
categorical variables in a linear model, a common approach is to introduce an indicator
feature indicator
featurexjk=1fuj=kgfor each factor jat level k. Thus, xjk=1 if the value of factor j
iskand 0 otherwise. SinceP
k1fuj=kg=1, it su ces to consider only pj 1 of these
indicator features for each factor j(this prevents the model matrix from being rank deÔ¨Å-
cient). For a single response Y, the feature vector x>is thus a row vector of binary variablesChapter 5. Regression 179
that indicates which levels were observed for each factor. The model assumption is that Y
depends in a linear way on the indicator features, apart from an error term. That is,
Y=0+rX
j=1pjX
k=2jk1fuj=kg|     {z     }
xjk+";
where we have omitted one indicator feature (corresponding to level 1) for each factor
j. For independent responses Y1;:::; Yn, where each Yicorresponds to the factor values
ui1;:::; uir, letxi jk=1fui j=kg. Then, the linear model for the data becomes
Yi=0+rX
j=1pjX
k=2jkxi jk+"i; (5.16)
where thef"igare independent with expectation 0 and some variance 2. By gathering the
0andfjkginto a vector , and thefxi jkginto a matrix X, we have again a linear model of
the form (5.8). The model matrix Xhasnrows and 1 +Pr
j=1(pj 1) columns. Using the
above convention that the j1parameters are subsumed in the parameter 0(correspond-
ing to the ‚Äúconstant‚Äù feature), we can interpret 0as a baseline response when using the
explanatory vector x>for which xj1=1 for all factors j=1;:::; r. The other parameters
fjkgcan be viewed as incremental e ects incremental
effectsrelative to this baseline e ect. For example, 12
describes by how much the response is expected to change if level 2 is used instead of level
1 for factor 1.
Example 5.5 (Crop Yield (cont.)) In Example 5.4, the linear model (5.16) has eight
parameters: 0;12;13;14;22;23;24, and2. The model matrix Xdepends on how
the crop yields are organized in a vector yand on the ordering of the factors. Let
us order ycolumn-wise from Table 5.1, as in y=[9:2988;8:2111;9:0688;8:2552;
9:4978;:::; 8:9485]>, and let Treatment be Factor 1 and Block be Factor 2. Then we can
write (5.16) as
Y=2666666666666641 0 0 0 C
1 1 0 0 C
1 0 1 0 C
1 0 0 1 C377777777777775
|               {z               }
X26666666666666666666666666666640
12
13
14
22
23
243777777777777777777777777777775
|{z}
+";where C=2666666666666640 0 0
1 0 0
0 1 0
0 0 1377777777777775;
and with 1=[1;1;1;1]>and0=[0;0;0;0]>. Estimation of and2, model selection,
and prediction can now be carried out in the usual manner for linear models.
In the context of factorial experiments, the model matrix is often called the design
matrix design matrix , as it speciÔ¨Åes the design of the experiment; e.g., how many replications are taken
for each combination of factor levels. The model (5.16) can be extended by adding products
of indicator variables as new features. Such features are called interaction interaction terms.180 5.3. Analysis via Linear Models
5.3.6 Nested Models
LetXbe a npmodel matrix of the form X=[X1;X2], where X1andX2are model
matrices of dimension nkandn(p k), respectively. The linear models Y=X11+"
andY=X22+"are said to be nested models nested within the linear model Y=X+". This simply
means that certain features in Xare ignored in each of the Ô¨Årst two models. Note that ,1,
and2are parameter vectors of dimension p,k, and p k, respectively. In what follows,
we assume that n>pand that all model matrices are full-rank.
Suppose we wish to assess whether to use the full model matrix Xor the reduced model
matrix X1. Letbbe the estimate of under the full model (that is, obtained via (5.9)), and
letb1denote the estimate of 1for the reduced model. Let Y(2)=Xbbe the projection of Y
onto the space Span( X) spanned by the columns of X; and let Y(1)=X1b1be the projection
ofYonto the space Span( X1) spanned by the columns of X1only; see Figure 5.3. In order
to decide whether the features in X2are needed, we may compare the estimated error terms
of the two models, as calculated by (5.10); that is, by the residual sum of squares divided
by the number of observations n. If the outcome of this comparison is that there is little
dierence between the model error for the full and reduced model, then it is appropriate to
adopt the reduced model, as it has fewer parameters than the full model, while explaining
the data just as well. The comparison is thus between the squared norms kY Y(2)k2and
kY Y(1)k2. Because of the nested nature of the linear models, Span( X1) is a subspace of
Span( X) and, consequently, the orthogonal projection of Y(2)onto Span( X1) is the same
as the orthogonal projection of Yonto Span( X1); that is, Y(1). By Pythagoras‚Äô theorem, we
thus have the decomposition kY(2) Y(1)k2+kY Y(2)k2=kY Y(1)k2. This is also illustrated
in Figure 5.3.
Y
Y‚àíY(1)Y‚àíY(2)
Y(2)O
Span( X)
Span( X1)Y(2)‚àíY(1)
Y(1)
Figure 5.3: The residual sum of squares for the full model corresponds to kY Y(2)k2and for
the reduced model it is kY Y(1)k2. By Pythagoras‚Äôs theorem, the di erence iskY(2) Y(1)k2.
The above decomposition can be generalized to more than two model matrices. Sup-
pose that the model matrix can be decomposed into dsubmatrices: X=[X1;X2;:::; Xd],
where the matrix Xihaspicolumns and nrows, i=1;:::; d. Thus, the number of columns2
2As always, we assume the columns are linearly independent.Chapter 5. Regression 181
in the full model matrix is p=p1++pd. This creates an increasing sequence of ‚Äúnested‚Äù
model matrices: X1;[X1;X2];:::; [X1;X2;:::; Xd], from (say) the baseline normal model
matrix X1=1to the full model matrix X. Think of each model matrix corresponding to
speciÔ¨Åc variables in the model.
We follow a similar projection procedure as in Figure 5.3: First project Yonto Span( X)
to yield the vector Y(d), then project Y(d)onto Span([ X1;:::; Xd 1]) to obtain Y(d 1), and so
on, until Y(2)is projected onto Span( X1) to yield Y(1)=Y1(in the case that X1=1).
By applying Pythagoras‚Äô theorem, the total sum of squares can be decomposed as
kY Y(1)k2
|       {z       }
df=n p1=kY Y(d)k2
|       {z       }
df=n p+kY(d) Y(d 1)k2
|            {z            }
df=pd++kY(2) Y(1)k2
|          {z          }
df=p2: (5.17)
Software packages typically report the sums of squares as well as the corresponding de-
grees of freedom (df): n p;pd;:::; p2.degrees of
freedom
5.3.7 CoefÔ¨Åcient of Determination
To assess how a linear model Y=X+"compares to the default model Y=01+", we
can compare the variance of the original data, estimated viaP
i(Yi Y)2=n=kY Y1k2=n,
with the variance of the Ô¨Åtted data; estimated viaP
i(bYi Y)2=n=kbY Y1k2=n, where
bY=Xb. The sumP
i(Yi Y)2=n=kY Y1k2is sometimes called the total sum of squares total sum of
squares(TSS), and the quantity
R2=kbY Y1k2
kY Y1k2(5.18)
is called the coecient of determination coefficient of
determinationof the linear model. In the notation of Figure 5.3,
bY=Y(2)andY1=Y(1), so that
R2=kY(2) Y(1)k2
kY Y(1)k2=kY Y(1)k2 kY Y(2)k2
kY Y(1)k2=TSS RSS
TSS:
Note that R2lies between 0 and 1. An R2value close to 1 indicates that a large propor-
tion of the variance in the data has been explained by the model.
Many software packages also give the adjusted coe cient of determination adjusted
coefficient of
determination, or simply
the adjusted R2, deÔ¨Åned by
R2
adjusted =1 (1 R2)n 1
n p:
The regular R2is always non-decreasing in the number of parameters (see Exercise 15),
but this may not indicate better predictive power. The adjusted R2compensates for this
increase by decreasing the regular R2as the number of variables increases. This heuristic
adjustment can make it easier to compare the quality of two competing models.182 5.4. Inference for Normal Linear Models
5.4 Inference for Normal Linear Models
So far we have not assumed any distribution for the random vector of errors "=
["1;:::;" n]>in a linear model Y=X+". When the error terms f"igare assumed to be
normally distributed (that is, f"igiidN(0;2)), whole new avenues open up for inference
on linear models. In Section 2.8 we already saw that for such normal linear models , estim- +46
ation ofand2can be carried out via maximum likelihood methods, yielding the same
estimators from (5.9) and (5.10).
The following theorem lists the properties of these estimators. In particular, it shows
thatbandc2n=(n p) are independent and unbiased estimators of and2, respectively.
Theorem 5.3: Properties of the Estimators for a Normal Linear Model
Consider the linear model Y=X+", with"N(0;2In), whereis a p-
dimensional vector of parameters and 2a dispersion parameter. The following res-
ults hold.
1. The maximum likelihood estimators bandc2are independent.
2.bN(; 2(X>X)+).
3.nc2=22
n p, where p=rank( X).
Proof: Using the pseudo-inverse (DeÔ¨Ånition A.2), we can write the random vector bas +360
X+Y, which is a linear transformation of a normal random vector. Consequently, bhas a
multivariate normal distribution; see Theorem C.6. The mean vector and covariance matrix +435
follow from the same theorem:
Eb=X+EY=X+X=
and
Cov(b)=X+2In(X+)>=2(X>X)+:
To show that bandc2are independent, deÔ¨Åne Y(2)=Xb. Note that Y=has aN(;In)
distribution, with expectation vector =X=. A direct application of Theorem C.10
now shows that ( Y Y(2))=is independent of Y(2)=. Since b=X+Xb=X+Y(2)and +438
c2=kY Y(2)k2=n, it follows that c2is independent of b. Finally, by the same theorem,
the random variable kY Y(2)k2=2has a2
n pdistribution, as Y(2)has the same expectation
vector as Y. 
As a corollary, we see that each estimator biofihas a normal distribution with expect-
ationiand variance 2u>
iX+(X+)>ui=2ku>
iX+k2, where ui=[0;:::; 0;1;0;:::; 0]>is
thei-th unit vector; in other words, the variance is 2[(X>X)+]ii.
It is of interest to test whether certain regression parameters iare 0 or not, since if
i=0, the i-th explanatory variable has no direct e ect on the expected response and so
could be removed from the model. A standard procedure is to conduct a hypothesis test
(see Section C.14 for a review of hypothesis testing) to test the null hypothesis H0:i=0 +458Chapter 5. Regression 183
against the alternative H1:i,0, using the test statistic
T=bi=ku>
iX+k
p
RSE; (5.19)
where RSE is the residual squared error; that is RSE =RSS=(n p). This test statistic has
atn pdistribution under H0. To see this, write T=Z=p
V=(n p), with
Z=bi
ku>
iX+kand V=nc2=2:
Then, by Theorem 5.3, ZN(0;1) under H0,V2
n p, and ZandVare independent. The
result now follows directly from Corollary C.1. +439
5.4.1 Comparing Two Normal Linear Models
Suppose we have the following normal linear model for data Y=[Y1;:::; Yn]>:
Y=X11+X22|          {z          }
X+";"N(0;2In); (5.20)
where1and2are unknown vectors of dimension kand p k, respectively; and X1
andX2are full-rank model matrices of dimensions nkandn(p k), respectively.
Above we implicitly deÔ¨Åned X=[X1;X2] and>=[>
1;>
2]. Suppose we wish to test the
hypothesis H0:2=0against H1:2,0. Following Section 5.3.6, the idea is to compare
the residual sum of squares for both models, expressed as kY Y(2)k2andkY Y(1)k2. Using
Pythagoras‚Äô theorem we saw that kY Y(2)k2 kY Y(1)k2=kY(2) Y(1)k2, and so it makes
sense to base the decision whether to retain or reject H0on the basis of the quotient of
kY(2) Y(1)k2andkY Y(2)k2. This leads to the following test statistics.
Theorem 5.4: Test Statistic for Comparing Two Normal Linear Models
For the model (5.20), let Y(2)andY(1)be the projections of Yonto the space spanned
by the pcolumns of Xand the kcolumns of X1, respectively. Then under H0:2=0
the test statistic
T=kY(2) Y(1)k2=(p k)
kY Y(2)k2=(n p)(5.21)
has an F(p k;n p) distribution.
Proof: DeÔ¨Åne X:=Y=with expectation :=X=, and Xj:=Y(j)=with expectation
j,j=k;p. Note thatp=and, under H0,k=p. We can directly apply Theorem C.10
to Ô¨Ånd thatkY Y(2)k2=2=kX Xpk22
n pand, under H0,kY(2) Y(1)k2=2=kXp  +438
Xkk22
p k. Moreover, these random variables are independent of each other. The proof
is completed by applying Theorem C.11. 184 5.4. Inference for Normal Linear Models
Note that H0is rejected for large values of T. The testing procedure thus proceeds as
follows:
1. Compute the outcome, tsay, of the test statistic Tin (5.21).
2. Evaluate the P-value P(T>t), with TF(p k;n p).
3. Reject H0if this P-value is too small, say less than 0.05.
For nested models [ X1;X2;:::; Xi],i=1;2;:::; d, as in Section 5.3.6, the Ftest statistic
in Theorem 5.4 can now be used to test whether certain Xiare needed or not. In particular, +183
software packages will report the outcomes of
Fi=kY(i) Y(i 1)k2=pi
kY Y(d)k2=(n p); (5.22)
in the order i=2;3;:::; d. Under the null hypothesis that Y(i)andY(i 1)have the same ex-
pectation (that is, adding XitoXi 1has no additional e ect on reducing the approximation
error), the test statistic Fihas an F(pi;n p) distribution, and the corresponding P-values
quantify the strength of the decision to include an additional variable in the model or not.
This procedure is called analysis of variance (ANOV A).analysis of
variance
Note that the output of an ANOV A table depends on the order in which the variables
are considered.
Example 5.6 (Crop Yield (cont.)) We continue Examples 5.4 and 5.5. Decompose the
linear model as
Y=2666666666666641
1
1
1377777777777775
|{z}
X10|{z}
1+2666666666666640 0 0
1 0 0
0 1 0
0 0 1377777777777775
|     {z     }
X2266666666412
13
143777777775
|{z}
2+266666666666664C
C
C
C377777777777775
|{z}
X3266666666422
23
243777777775
|{z}
3+":
Is the crop yield dependent on treatment levels as well as blocks? We Ô¨Årst test whether we
can remove Block as a factor in the model against it playing a signiÔ¨Åcant role in explain-
ing the crop yields. SpeciÔ¨Åcally, we test 3=0versus3,0using Theorem 5.4. Now
the vector Y(2)is the projection of Yonto the ( p=7)-dimensional space spanned by the
columns of X=[X1;X2;X3]; and Y(1)is the projection of Yonto the ( k=4)-dimensional
space spanned by the columns of X12:=[X1;X2]. The test statistic, T12say, under H0has
anF(3;9) distribution.
The Python code below calculates the outcome of the test statistic T12and the corres-
ponding P-value. We Ô¨Ånd t12=34:9998, which gives a P-value 2 :7310 5. This shows
that the block e ects are extremely important for explaining the data.
Using the extended model (including the block e ects), we can test whether 2=0or
not; that is, whether the treatments have a signiÔ¨Åcant e ect on the crop yield in the presence
of the Block factor. This is done in the last six lines of the code below. The outcome ofChapter 5. Regression 185
the test statistic is 4 :4878, with a P-value of 0 :0346. By including the block e ects, we
eectively reduce the uncertainty in the model and are able to more accurately assess the
eects of the treatments, to conclude that the treatment seems to have an e ect on the crop
yield. A closer look at the data shows that within each block (row) the crop yield roughly
increases with the treatment level.
crop.py
import numpy as np
from scipy.stats import f
from numpy.linalg import lstsq , norm
yy = np.array([9.2988, 9.4978, 9.7604, 10.1025,
8.2111, 8.3387, 8.5018, 8.1942,
9.0688, 9.1284, 9.3484, 9.5086,
8.2552, 7.8999, 8.4859, 8.9485]).reshape(4,4).T
nrow , ncol = yy.shape[0], yy.shape[1]
n = nrow * ncol
y = yy.reshape(16,)
X_1 = np.ones((n,1))
KM = np.kron(np.eye(ncol),np.ones((nrow ,1)))
KM[:,0]
X_2 = KM[:,1:ncol]
IM = np.eye(nrow)
C = IM[:,1:nrow]
X_3 = np.vstack((C, C))
X_3 = np.vstack((X_3, C))
X_3 = np.vstack((X_3, C))
X = np.hstack((X_1,X_2))
X = np.hstack((X,X_3))
p = X.shape[1] #number of parameters in full model
betahat = lstsq(X, y,rcond=None)[0] #estimate under the full model
ym = X @ betahat
X_12 = np.hstack((X_1, X_2)) #omitting the block effect
k = X_12.shape[1] #number of parameters in reduced model
betahat_12 = lstsq(X_12 , y,rcond=None)[0]
y_12 = X_12 @ betahat_12
T_12=(n-p)/(p-k)*(norm(y-y_12)**2 - norm(y-ym)**2)/norm(y-ym)**2
pval_12 = 1 - f.cdf(T_12 ,p-k,n-p)
X_13 = np.hstack((X_1, X_3)) #omitting the treatment effect
k = X_13.shape[1] #number of parameters in reduced model
betahat_13 = lstsq(X_13 , y,rcond=None)[0]
y_13 = X_13 @ betahat_13
T_13=(n-p)/(p-k)*(norm(y-y_13)**2 - norm(y-ym)**2)/norm(y-ym)**2
pval_13 = 1 - f.cdf(T_13 ,p-k,n-p)186 5.4. Inference for Normal Linear Models
5.4.2 ConÔ¨Ådence and Prediction Intervals
As in all supervised learning settings, linear regression is most useful when we wish to
predict how a new response variable will behave on the basis of a new explanatory vector
x. For example, it may be di cult to measure the response variable, but by knowing the
estimated regression line and the value for x, we will have a reasonably good idea what Y
or the expected value of Yis going to be.
Thus, consider a new xand let YN(x>;2), withand2unknown. First we
are going to look at the expected value of Y, that isEY=x>. Sinceis unknown, we
do not know EYeither. However, we can estimate it via the estimator bY=x>b, where
bN(; 2(X>X)+), by Theorem 5.3. Being linear in the components of ,bYtherefore
has a normal distribution with expectation x>and variance 2kx>X+k2. Let ZN(0;1)
be the standardized version of bYandV=kY Xbk2=22
n p. Then the random variable
T:=(x>b x>)=kx>X+k
kY Xbk=p
(n p)=Zp
V=(n p)(5.23)
has, by Corollary C.1, a tn pdistribution. After rearranging the identity P(jTj6tn p;1 =2)= +439
1 , where tn p;1 =2is the (1 =2) quantile of the tn pdistribution, we arrive at the
stochastic conÔ¨Ådence interval confidence
interval
x>btn p;1 =2p
RSEkx>X+k; (5.24)
where we have identiÔ¨Åed kY Xbk2=(n p) with RSE. This conÔ¨Ådence interval quantiÔ¨Åes
the uncertainty in the learner (regression surface).
Aprediction interval prediction
intervalfor a new response Yis dierent from a conÔ¨Ådence interval for
EY. Here the idea is to construct an interval such that Ylies in this interval with a certain
guaranteed probability. Note that now we have twosources of variation:
1.YN(x>;2) itself is a random variable.
2. Estimating x>viabYbrings another source of variation.
We can construct a (1  ) prediction interval, by Ô¨Ånding two random bounds such that
the random variable Ylies between these bounds with probability 1  . We can reason as
follows. Firstly, note that YN(x>;2) andbYN(x>;2kx>X+k2) are independent. It
follows that Y bYhas a normal distribution with expectation 0 and variance
2(1+kx>X+k2): (5.25)
Secondly, letting ZN(0;1) be the standardized version of Y bY, and repeating the
steps used for the construction of the conÔ¨Ådence interval (5.24), we arrive at the prediction
interval
x>btn p;1 =2p
RSEp
1+kx>X+k2: (5.26)
This prediction interval captures the uncertainty from an as-yet-unobserved response as
well as the uncertainty in the parameters of the regression model itself.Chapter 5. Regression 187
Example 5.7 (ConÔ¨Ådence Limits in Simple Linear Regression) The following pro-
gram draws n=100 samples from a simple linear regression model with parameters
=[6;13]>and=2, where the x-coordinates are evenly spaced on the interval [0 ;1].
The parameters are estimated in the third block of the code. Estimates for andare
[6:03;13:09]>andb=1:60, respectively. The program then proceeds by calculating the
95% numeric conÔ¨Ådence and prediction intervals for various values of the explanatory
variable. Figure 5.4 shows the results.
confpred.py
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t
from numpy.linalg import inv, lstsq , norm
np.random.seed(123)
n = 100
x = np.linspace(0.01,1,100).reshape(n,1)
# parameters
beta = np.array([6,13])
sigma = 2
Xmat = np.hstack((np.ones((n,1)), x)) #design matrix
y = Xmat @ beta + sigma*np.random.randn(n)
# solve the normal equations
betahat = lstsq(Xmat , y,rcond=None)[0]
# estimate for sigma
sqMSE = norm(y - Xmat @ betahat)/np.sqrt(n-2)
tquant = t.ppf(0.975,n-2) # 0.975 quantile
ucl = np.zeros(n) #upper conf. limits
lcl = np.zeros(n) #lower conf. limits
upl = np.zeros(n)
lpl = np.zeros(n)
rl = np.zeros(n) # (true) regression line
u = 0
for iin range (n):
u = u + 1/n;
xvec = np.array([1,u])
sqc = np.sqrt(xvec.T @ inv(Xmat.T @ Xmat) @ xvec)
sqp = np.sqrt(1 + xvec.T @ inv(Xmat.T @ Xmat) @ xvec)
rl[i] = xvec.T @ beta;
ucl[i] = xvec.T @ betahat + tquant*sqMSE*sqc;
lcl[i] = xvec.T @ betahat - tquant*sqMSE*sqc;
upl[i] = xvec.T @ betahat + tquant*sqMSE*sqp;
lpl[i] = xvec.T @ betahat - tquant*sqMSE*sqp;
plt.plot(x,y, '.')
plt.plot(x,rl, 'b')
plt.plot(x,ucl, 'k:')
plt.plot(x,lcl, 'k:')
plt.plot(x,upl, 'r--')
plt.plot(x,lpl, 'r--')188 5.5. Nonlinear Regression Models
0.0
 0.2
 0.4
 0.6
 0.8
 1.0
5
10
15
20
Figure 5.4: The true regression line (blue, solid) and the upper and lower 95% prediction
curves (red, dashed) and conÔ¨Ådence curves (dotted).
5.5 Nonlinear Regression Models
So far we have been mostly dealing with linear regression models, in which the predic-
tion function is of the form g(xj)=x>. In this section we discuss some strategies for
handling general prediction functions g(xj), where the functional form is known up to an
unknown parameter vector . So the regression model becomes
Yi=g(xij)+"i;i=1;:::; n; (5.27)
where"1;:::;" nare independent with expectation 0 and unknown variance 2. The model
can be further speciÔ¨Åed by assuming that the error terms have a normal distribution.
Table 5.3 gives some common examples of nonlinear prediction functions for data tak-
ing values in R.
Table 5.3: Common nonlinear prediction functions for one-dimensional data.
Name g(xj)
Exponential aebxa;b
Power law a xba;b
Logistic (1 +ea+bx) 1a;b
Weibull 1 exp( xb=a)a;b
PolynomialPp 1
k=0kxkp,fkgp 1
k=0
The logistic and polynomial prediction functions in Table 5.3 can be readily gener-
alized to higher dimensions. For example, for x2R2a general second-order polynomial
prediction function is of the form
g(xj)=0+1x1+2x2+11x2
1+22x2
2+12x1x2: (5.28)Chapter 5. Regression 189
This function can be viewed as a second-order approximation to a general smooth predic-
tion function g(x1;x2); see also Exercise 4. Polynomial regression models are also called
response surface models. response
surface modelThe generalization of the above logistic prediction to Rdis
g(xj)=(1+e x>) 1: (5.29)
This function will make its appearance in Section 5.7 and later on in Chapters 7 and 9.
The Ô¨Årst strategy for performing regression with nonlinear prediction functions is to
extend the feature space to obtain a simpler (ideally linear) prediction function in the ex-
tended feature space. We already saw an application of this strategy in Example 2.1 for + 26
the polynomial regression model, where the original feature uwas extended to the feature
vector x=[1;u;u2;:::; up 1]>, yielding a linear prediction function. In a similar way, the
right-hand side of the polynomial prediction function in (5.28) can be viewed as a linear
function of the extended feature vector (x)=[1;x1;x2;x2
1;x2
2;x1x2]>. The function is
called a feature map feature map .
The second strategy is to transform the response variable yand possibly also the ex-
planatory variable xsuch that the transformed variables ey,exare related in a simpler (ideally
linear) way. For example, for the exponential prediction function y=ae bx, we have
lny=lna bx, which is a linear relation between ln yand [1;x]>.
Example 5.8 (Chlorine) Table 5.4 lists the free chlorine concentration (in mg per liter)
in a swimming pool, recorded every 8 hours for 4 days. A simple chemistry-based model
for the chlorine concentration yas a function of time tisy=ae b t, where ais the initial
concentration and b>0 is the reaction rate.
Table 5.4: Chlorine concentration (in mg /L) as a function of time (hours).
Hours Concentration
0 1.0056
8 0.8497
16 0.6682
24 0.6056
32 0.4735
40 0.4745
48 0.3563Hours Concentration
56 0.3293
64 0.2617
72 0.2460
80 0.1839
88 0.1867
96 0.1688
The exponential relationship y=ae btsuggests that a log transformation of ywill result
in alinear relationship between ln yand the feature vector [1 ;t]>. Thus, if for some given
data ( t1;y1);:::; (tn;yn), we plot ( t1;lny1);:::; (tn;lnyn), these points should approximately
lie on a straight line, and hence the simple linear regression model applies. The left panel of
Figure 5.5 illustrates that the transformed data indeed lie approximately on a straight line.
The estimated regression line is also drawn here. The intercept and slope are 0= 0:0555
and1= 0:0190 here. The original (non-transformed) data is shown in the right panel
of Figure 5.5, along with the Ô¨Åtted curve y=bae bbt, where ba=exp(b0)=0:9461 and
bb= b1=0:0190.190 5.5. Nonlinear Regression Models
0 50 100
t-2-1.5-1-0.500.5log y
0 50 100
t00.511.5y
Figure 5.5: The chlorine concentration seems to have an exponential decay.
Recall that for a general regression problem the learner g(x) for a given training set 
is obtained by minimizing the training (squared-error) loss
`(g(j))=1
nnX
i=1(yi g(xij))2: (5.30)
The third strategy for regression with nonlinear prediction functions is to directly minimize
(5.30) by any means possible, as illustrated in the next example.
Example 5.9 (Hougen Function) In [7] the reaction rate yof a certain chemical reac-
tion is posited to depend on three input variables: quantities of hydrogen x1, n-pentane x2,
and isopentane x3. The functional relationship is given by the Hougen function:
y=1x2 x3=5
1+2x1+3x2+4x3;
where1;:::; 5are the unknown parameters. The objective is to estimate the model para-
metersfigfrom the data, as given in Table 5.5.
Table 5.5: Data for the Hougen function.
x1 x2 x3 y
470 300 10 8.55
285 80 10 3.79
470 300 120 4.82
470 80 120 0.02
470 80 10 2.75
100 190 10 14.39
100 80 65 2.54x1 x2 x3 y
470 190 65 4.35
100 300 54 13.00
100 300 120 8.50
100 80 120 0.05
285 300 10 11.32
285 190 120 3.13
The estimation is carried out via the least-squares method. The objective function to
minimize is thus
`(g(j))=1
1313X
i=1 
yi 1xi2 xi3=5
1+2xi1+3xi2+4xi3!2
; (5.31)Chapter 5. Regression 191
where thefyigandfxi jgare given in Table 5.5.
This is a highly nonlinear optimization problem, for which standard nonlinear least- +414
squares methods do not work well. Instead, one can use global optimization methods such
as CE and SCO (see Sections 3.4.2 and 3.4.3). Using the CE method, we found the minimal +100
value 0.02299 for the objective function, which is attained at
b=[1:2526;0:0628;0:0400;0:1124;1:1914]>:
5.6 Linear Models in Python
In this section we describe how to deÔ¨Åne and analyze linear models using Python and the
data science module statsmodels . We encourage the reader to regularly refer back to
the theory in the preceding sections of this chapter, so as to avoid using Python merely
as a black box without understanding the underlying principles. To run the code start by
importing the following code snippet:
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
from statsmodels.formula.api import ols
5.6.1 Modeling
Although specifying a normal3linear model in Python is relatively easy, it requires some
subtlety. The main thing to realize is that Python treats quantitative and qualitative (that
is, categorical) explanatory variables di erently. In statsmodels , ordinary least-squares
linear models are speciÔ¨Åed via the function ols (short for ordinary least-squares). The
main argument of this function is a formula of the form
yx1+x2++xd; (5.32)
where yis the name of the response variable and x1, . . . , xdare the names of the explan-
atory variables. If all variables are quantitative , this describes the linear model
Yi=0+1xi1+2xi2++dxid+"i;i=1;:::; n; (5.33)
where xi jis the j-th explanatory variable for the i-th observation and the errors "iare
independent normal random variables such that E"i=0 andVar"i=2. Or, in matrix
form: Y=X+", with
Y=26666666664Y1
:::
Yn37777777775;X=266666666666666641x11 x1d
1x21 x2d
::::::::::::
1xn1 xnd37777777777777775;=266666666640
:::
d37777777775;and"=26666666664"1:::
"n37777777775:
3For the rest of this section, we assume all linear models to be normal.192 5.6. Linear Models in Python
Thus, the Ô¨Årst column is always taken as an ‚Äúintercept‚Äù parameter, unless otherwise spe-
ciÔ¨Åed. To remove the intercept term, add -1to the olsformula, as in ols('yx-1') .
For any linear model, the model matrix can be retrieved via the construction:
model_matrix = pd.DataFrame(model.exog,columns=model.exog_names)
Let us look at some examples of linear models. In the Ô¨Årst model the variables x1andx2
are both considered (by Python) to be quantitative.
myData = pd.DataFrame({ 'y': [10,9,4,2,4,9],
'x1': [7.4,1.2,3.1,4.8,2.8,6.5],
'x2': [1,1,2,2,3,3]})
mod = ols("y~x1+x2", data=myData)
mod_matrix = pd.DataFrame(mod.exog ,columns=mod.exog_names)
print (mod_matrix)
Intercept x1 x2
0 1.0 7.4 1.0
1 1.0 1.2 1.0
2 1.0 3.1 2.0
3 1.0 4.8 2.0
4 1.0 2.8 3.0
5 1.0 6.5 3.0
Suppose the second variable is actually qualitative; e.g., it represents a color, and the
levels 1, 2, and 3 stand for red, blue, and green. We can account for such a categorical
variable by using the astype method to redeÔ¨Åne the data type (see Section 1.2). +3
myData[ 'x2'] = myData[ 'x2'].astype( 'category ')
Alternatively, a categorical variable can be speciÔ¨Åed in the model formula by wrapping
it with C(). Observe how this changes the model matrix.
mod2 = ols("y~x1+C(x2)", data=myData)
mod2_matrix = pd.DataFrame(mod2.exog ,columns=mod2.exog_names)
print (mod2_matrix)
Intercept C(x2)[T.2] C(x2)[T.3] x1
0 1.0 0.0 0.0 7.4
1 1.0 0.0 0.0 1.2
2 1.0 1.0 0.0 3.1
3 1.0 1.0 0.0 4.8
4 1.0 0.0 1.0 2.8
5 1.0 0.0 1.0 6.5
Thus, if a statsmodels formula of the form (5.32) contains factor (qualitative) variables,
the model is no longer of the form (5.33), but contains indicator variables for each level of
the factor variable, except the Ô¨Årst level.
For the case above, the corresponding linear model is
Yi=0+1xi1+21fxi2=2g+31fxi2=3g+"i;i=1;:::; 6; (5.34)
where we have used parameters 2and3to correspond to the indicator features of the
qualitative variable. The parameter 2describes how much the response is expected toChapter 5. Regression 193
change if the factor x2switches from level 1 to 2. A similar interpretation holds for 3.
Such parameters can thus be viewed as incremental e ects.
It is also possible to model interaction interaction between two variables. For two continuous
variables, this simply adds the products of the original features to the model matrix. Adding
interaction terms in Python is achieved by replacing ‚Äú +‚Äù in the formula with ‚Äú *‚Äù, as the
following example illustrates.
mod3 = ols("y~x1*C(x2)", data=myData)
mod3_matrix = pd.DataFrame(mod3.exog ,columns=mod3.exog_names)
print (mod3_matrix)
Intercept C(x2)[T.2] C(x2)[T.3] x1 x1:C(x2)[T.2] x1:C(x2)[T.3]
0 1.0 0.0 0.0 7.4 0.0 0.0
1 1.0 0.0 0.0 1.2 0.0 0.0
2 1.0 1.0 0.0 3.1 3.1 0.0
3 1.0 1.0 0.0 4.8 4.8 0.0
4 1.0 0.0 1.0 2.8 0.0 2.8
5 1.0 0.0 1.0 6.5 0.0 6.5
5.6.2 Analysis
Let us consider some easy linear regression models by using the student survey data set
survey.csv from the book‚Äôs GitHub site, which contains measurements such as height,
weight, sex, etc., from a survey conducted among n=100 university students. Suppose we
wish to investigate the relation between the shoe size (explanatory variable) and the height
(response variable) of a person. First, we load the data and draw a scatterplot of the points
(height versus shoe size); see Figure 5.6 (without the Ô¨Åtted line).
survey = pd.read_csv( 'survey.csv ')
plt.scatter(survey.shoe , survey.height)
plt.xlabel("Shoe size")
plt.ylabel("Height")
We observe a slight increase in the height as the shoe size increases, although this
relationship is not very distinct. We analyze the data through the simple linear regression
model Yi=0+1xi+"i;i=1;:::; n. In statsmodels this is performed via the ols +169
method as follows:
model = ols("height~shoe", data=survey) # define the model
fit = model.fit() #fit the model defined above
b0, b1 = fit.params
print (fit.params)
Intercept 145.777570
shoe 1.004803
dtype: float64
The above output gives the least-squares estimates of 0and1. For this example, we
haveb0=145:778 and b1=1:005. Figure 5.6, which includes the regression line, was
obtained as follows:194 5.6. Linear Models in Python
15
 20
 25
 30
 35
Shoe size
150
160
170
180
190
200Height
Figure 5.6: Scatterplot of height (cm) against shoe size (cm), with the Ô¨Åtted line.
plt.plot(survey.shoe , b0 + b1*survey.shoe)
plt.scatter(survey.shoe , survey.height)
plt.xlabel("Shoe size")
plt.ylabel("Height")
Although olsperforms a complete analysis of the linear model, not all its calculations
need to be presented. A summary of the results can be obtained with the method summary .
print (fit.summary())
Dep. Variable: height R-squared: 0.178
Model: OLS Adj. R-squared: 0.170
Method: Least Squares F-statistic: 21.28
No. Observations: 100 Prob (F-statistic): 1.20e-05
Df Residuals: 98 Log-Likelihood: -363.88
Df Model: 1 AIC: 731.8
Covariance Type: nonrobust BIC: 737.0
=====================================================================
coef std err t P>|t| [0.025 0.975]
--------------------------------------------------------------------
Intercept 145.7776 5.763 25.296 0.000 134.341 157.214
shoe 1.0048 0.218 4.613 0.000 0.573 1.437
=====================================================================
Omnibus: 1.958 Durbin -Watson: 1.772
Prob(Omnibus): 0.376 Jarque -Bera (JB): 1.459
Skew: -0.072 Prob(JB): 0.482
Kurtosis: 2.426 Cond. No. 164.
The main output items are the following:
¬àcoef: Estimates of the parameters of the regression line.
¬àstd error: Standard deviations of the estimators of the regression line. These are
the square roots of the variances of the fbigobtained in (5.25). +186Chapter 5. Regression 195
¬àt:Realization of Student‚Äôs test statistics associated with the hypotheses H0:i=0
andH1:i,0,i=0;1. In particular, the outcome of Tin (5.19). +183
¬àP>|t| : P-value of Student‚Äôs test (two-sided test).
¬à[0.025 0.975] : 95% conÔ¨Ådence intervals for the parameters.
¬àR-Squared: Coecient of determination R2(percentage of variation explained by
the regression), as deÔ¨Åned in (5.18). +181
¬àAdj. R-Squared: adjusted R2(explained in Section 5.3.7).
¬àF-statistic: Realization of the Ftest statistic (5.21) associated with testing the +183
full model against the default model. The associated degrees of freedom ( Df Model
=1 and Df Residuals =n 2) are given, as is the P-value: Prob (F-statistic) .
¬àAIC: The AIC number in (5.15); that is, minus two times the log-likelihood plus two +177
times the number of model parameters (which is 3 here).
You can access all the numerical values as they are attributes of the fit object. First
check which names are available, as in:
dir(fit)
Then access the values via the dot construction. For example, the following extracts the
P-value for the slope.
fit.pvalues[1]
1.1994e-05
The results show strong evidence for a linear relationship between shoe size and height
(or, more accurately, strong evidence that the slope of the regression line is not zero), as
the P-value for the corresponding test is very small (1 :210 5). The estimate of the slope
indicates that the di erence between the average height of students whose shoe size is
dierent by one cm is 1.0048 cm.
Only 17:84% of the variability of student height is explained by the shoe size. We
therefore need to add other explanatory variables to the model (multiple linear regression)
to increase the model‚Äôs predictive power.
5.6.3 Analysis of Variance (ANOVA)
We continue the student survey example of the previous section, but now add an extra
variable, and also consider an analysis of variance of the model. Instead of ‚Äúexplaining‚Äù
the student height via their shoe size, we include weight as an explanatory variable. The
corresponding olsformula for this model is
heightshoe +weight;196 5.6. Linear Models in Python
meaning that each random height, denoted by Height , satisÔ¨Åes
Height =0+1shoe +2weight +";
where"is a normally distributed error term with mean 0 and variance 2. Thus, the model
has 4 parameters. Before analyzing the model we present a scatterplot of all pairs of vari-
ables, using scatter_matrix .
model = ols("height~shoe+weight", data=survey)
fit = model.fit()
axes = pd.plotting.scatter_matrix(
survey[[ 'height ','shoe ','weight ']])
plt.show()
150
175height
20
30shoe
150
175
height
50
100weight
20
30
shoe
50
100
weight
Figure 5.7: Scatterplot of all pairs of variables: height (cm), shoe (cm), and weight (kg).
As for the simple linear regression model in the previous section, we can analyze the
model using the summary method (below we have omitted some output):
fit.summary()
Dep. Variable: height R-squared: 0.430
Model: OLS Adj. R-squared: 0.418
Method: Least Squares F-statistic: 36.61
No. Observations: 100 Prob (F-statistic): 1.43e-12
Df Residuals: 97 Log-Likelihood: -345.58
Df Model: 2 AIC: 697.2
BIC: 705.0
======================================================================
coef std err t P>|t| [0.025 0.975]
----------------------------------------------------------------------
Intercept 132.2677 5.247 25.207 0.000 121.853 142.682Chapter 5. Regression 197
shoe 0.5304 0.196 2.703 0.008 0.141 0.920
weight 0.3744 0.057 6.546 0.000 0.261 0.488
TheF-statistic is used to test whether the full model (here with two explanatory
variables) is better at ‚Äúexplaining‚Äù the height than the default model. The corresponding
null hypothesis is H0:1=2=0. The assertion of interest is H1: at least one of the coe -
cientsj(j=1;2) is signiÔ¨Åcantly di erent from zero. Given the result of this test (P-value
=1:42910 12), we can conclude that at least one of the explanatory variables is associated
with height. The individual Student tests indicate that:
¬àshoe size is linearly associated with student height, after adjusting for weight, with
P-value 0.0081. At the same weight, an increase of one cm in shoe size corresponds
to an increase of 0.53 cm in average student height;
¬àweight is linearly associated with student height, after adjusting for shoe size (the
P-value is actually 2 :8210 09; the reported value of 0.000 should be read as ‚Äúless
than 0.001‚Äù). At the same shoe size, an increase of one kg in weight corresponds to
an increase of 0.3744 cm in average student height.
Further understanding is extracted from the model by conducting an analysis of vari-
ance. The standard statsmodels function is anova_lm . The input to this function is the
Ô¨Åt object (obtained from model.fit() ) and the output is a DataFrame object.
table = sm.stats.anova_lm(fit)
print (table)
df sum_sq mean_sq F PR(>F)
shoe 1.0 1840.467359 1840.467359 30.371310 2.938651e-07
weight 1.0 2596.275747 2596.275747 42.843626 2.816065e-09
Residual 97.0 5878.091294 60.598879 NaN NaN
The meaning of the columns is as follows.
¬àdf: The degrees of freedom of the variables, according to the sum of squares decom-
position (5.17). As both shoe andweight are quantitative variables, their degrees +181
of freedom are both 1 (each corresponding to a single column in the overall model
matrix). The degrees of freedom for the residuals is n p=100 3=97.
¬àsum sq: The sum of squares according to (5.17). The total sum of squares is the
sum of all the entries in this column. The residual error in the model that cannot be
explained by the variables is RSS 5878.
¬àmean sq: The sum of squares divided by their degrees of freedom. Note that the
residual square error RSE =RSS=(n p)=60:6 is an unbiased estimate of the
model variance 2; see Section 5.4. +182
¬àF: These are the outcomes of the test statistic (5.22). +184
¬àPR(>F) : These are the P-values corresponding to the test statistic in the preceding
column and are computed using an Fdistribution whose degrees of freedom are
given in the dfcolumn.198 5.6. Linear Models in Python
The ANOV A table indicates that the shoe variable explains a reasonable amount of the
variation in the model, as evidenced by a sum of squares contribution of 1840 out of 1840 +
2596+5878 =10314 and a very small P-value. After shoe is included in the model, it turns
out that the weight variable explains even more of the remaining variability, with an even
smaller P-value. The remaining sum of squares (5878) is 57% of the total sum of squares,
yielding a 43% reduction, in accordance with the R2value reported in the summary for the
olsmethod. As mentioned in Section 5.4.1, the order in which the ANOV A is conducted
is important. To illustrate this, consider the output of the following commands.
model = ols("height~weight+shoe", data=survey)
fit = model.fit()
table = sm.stats.anova_lm(fit)
print (table)
df sum_sq mean_sq F PR(>F)
weight 1.0 3993.860167 3993.860167 65.906502 1.503553e-12
shoe 1.0 442.882938 442.882938 7.308434 8.104688e-03
Residual 97.0 5878.091294 60.598879 NaN NaN
We see that weight as a single model variable explains much more of the variability
than shoe did. If we now also include shoe , we only obtain a small (but according to the
P-value still signiÔ¨Åcant) reduction in the model variability.
5.6.4 ConÔ¨Ådence and Prediction Intervals
Instatsmodels a method for computing conÔ¨Ådence or prediction intervals from a dic-
tionary of explanatory variables is get_prediction . It simply executes formula (5.24) or
(5.26). A simpler version is predict , which only returns the predicted value. +186
Continuing the student survey example, suppose we wish to predict the height of a
person with shoe size 30 cm and weight 75 kg. ConÔ¨Ådence and prediction intervals can
be obtained as given in the code below. The new explanatory variable is entered as a dic-
tionary. Notice that the 95% prediction interval (for the corresponding random response) is
much wider than the 95% conÔ¨Ådence interval (for the expectation of the random response).
x = { 'shoe ': [30.0], 'weight ': [75.0]} # new input (dictionary)
pred = fit.get_prediction(x)
pred.summary_frame(alpha=0.05).unstack()
mean 0 176.261722 # predicted value
mean_se 0 1.054015
mean_ci_lower 0 174.169795 # lower bound for CI
mean_ci_upper 0 178.353650 # upper bound for CI
obs_ci_lower 0 160.670610 # lower bound for PI
obs_ci_upper 0 191.852835 # upper bound for PI
dtype: float64
5.6.5 Model Validation
We can perform an analysis of residuals to examine whether the underlying assumptions
of the (normal) linear regression model are veriÔ¨Åed. Various plots of the residuals can beChapter 5. Regression 199
used to inspect whether the assumptions on the errors f"igare satisÔ¨Åed. Figure 5.8 gives two
such plots. The Ô¨Årst is a scatterplot of the residuals feigagainst the Ô¨Åtted values byi. When the
model assumptions are valid, the residuals, as approximations of the model error, should
behave approximately as iid normal random variables for each of the Ô¨Åtted values, with a
constant variance. In this case we see no strong aberrant structure in this plot. The residuals
are fairly evenly spread and symmetrical about the y=0 line (not shown). The second plot
is a quantile‚Äìquantile (or qq) plot. This is a useful way to check for normality of the error
terms, by plotting the sample quantiles of the residuals against the theoretical quantiles
of the standard normal distribution. Under the model assumptions, the points should lie
approximately on a straight line. For the current case there does not seem to be an extreme
departure from normality. Drawing a histogram or density plot of the residuals will also
help to verify the normality assumption. The following code was used.
plt.plot(fit.fittedvalues ,fit.resid , '.')
plt.xlabel("fitted values")
plt.ylabel("residuals")
sm.qqplot(fit.resid)
155
 160
 165
 170
 175
 180
 185
 190
 195
fitted values
25
20
15
10
5
0
5
10
15
20residuals
3
 2
 1
 0
 1
 2
 3
Theoretical Quantiles
25
20
15
10
5
0
5
10
15
20Sample Quantiles
Figure 5.8: Left: residuals against Ô¨Åtted values. Right: a qq plot of the residuals. Neither
shows clear evidence against the model assumptions of constant variance and normality.
5.6.6 Variable Selection
Among the large number of possible explanatory variables, we wish to select those which
best explain the observed responses. By eliminating redundant explanatory variables, we
reduce the statistical error without increasing the approximation error, and thus reduce the
(expected) generalization risk of the learner.
In this section, we brieÔ¨Çy present two methods for variable selection. They are illus-
trated on a few variables from the data set birthwt discussed in Section 1.5.3.2. The data + 13
set contains information on the birth weights (masses) of babies, as well as various char-
acteristics of the mother, such as whether she smokes, her age, etc. We wish to explain
the child‚Äôs weight at birth using various characteristics of the mother, her family history,
and her behavior during pregnancy. The response variable is weight at birth (quantitative
variable bwt, expressed in grams); the explanatory variables are given below.200 5.6. Linear Models in Python
The data can be obtained as explained in Section 1.5.3.2, or from statsmodels in the
following way:
bwt = sm.datasets.get_rdataset("birthwt","MASS").data
Here is some information about the explanatory variables that we will investigate.
age: mother 's age in years
lwt: mother 's weight in lbs
race: mother 's race (1 = white, 2 = black, 3 = other)
smoke: smoking status during pregnancy (0 = no, 1 = yes)
ptl: no. of previous premature labors
ht: history of hypertension (0 = no, 1 = yes)
ui: presence of uterine irritability (0 = no, 1 = yes)
ftv: no. of physician visits during first trimester
bwt: birth weight in grams
We can see the structure of the variables via bwt.info() . Check yourself that all
variables are deÔ¨Åned as quantitative (int64 ). However, the variables race ,smoke ,ht,
anduishould really be interpreted as qualitative (factors). To Ô¨Åx this, we could redeÔ¨Åne
them with the method astype , similar to what we did in Chapter 1. Alternatively, we could
use the C()construction in a statsmodels formula to let the program know that certain
variables are factors. We will use the latter approach.
Forbinary features it does not matter whether the variables are interpreted as
factorial or numerical as the numerical and summary results are identical.
We consider the explanatory variables lwt,age,ui,smoke ,ht, and two recoded binary
variables ftv1 andptl1 . We deÔ¨Åne ftv1 =1 if there was at least one visit to a physician,
andftv1 =0 otherwise. Similarly, we deÔ¨Åne ptl1 =1 if there is at least one preterm birth
in the family history, and ptl1 =0 otherwise.
ftv1 = (bwt[ 'ftv']>=1).astype( int)
ptl1 = (bwt[ 'ptl']>=1).astype( int)
5.6.6.1 Forward Selection and Backward Elimination
The forward selection forward
selectionmethod is an iterative method for variable selection. In the Ô¨Årst
iteration we consider which feature f1is the most signiÔ¨Åcant in terms of its P-value in the
models bwtf1, with f12flwt;age;:::g. This feature is then selected into the model. In
the second iteration, the feature f2that has the smallest P-value in the models bwtf1+f2
is selected, where f2,f1, and so on. Usually only features are selected that have a P-
value of at most 0.05. The following Python program automates this procedure. Instead of
selecting on the P-value one could select on the AIC or BIC value.Chapter 5. Regression 201
forwardselection.py
import statsmodels.api as sm
from statsmodels.formula.api import ols
bwt = sm.datasets.get_rdataset("birthwt","MASS").data
ftv1 = (bwt[ 'ftv']>=1).astype( int)
ptl1 = (bwt[ 'ptl']>=1).astype( int)
remaining_features = { 'lwt','age','C(ui) ','smoke ',
'C(ht) ','ftv1 ','ptl1 '}
selected_features = []
while remaining_features:
PF = [] #list of (P value , feature)
for finremaining_features:
temp = selected_features + [f] #temporary list of features
formula = 'bwt~ '+'+'.join(temp)
fit = ols(formula ,data=bwt).fit()
pval= fit.pvalues[-1]
ifpval < 0.05:
PF.append((pval ,f))
ifPF: #if not empty
PF.sort(reverse=True)
(best_pval , best_f) = PF.pop()
remaining_features.remove(best_f)
print ('feature {} with P-value = {:.2E} '.
format (best_f , best_pval))
selected_features.append(best_f)
else :
break
feature C(ui) with P-value = 7.52E-05
feature C(ht) with P-value = 1.08E-02
feature lwt with P-value = 6.01E-03
feature smoke with P-value = 7.27E-03
Inbackward elimination backward
eliminationwe start with the complete model (all features included) and
at each step, we remove the variable with the highest P-value, as long as it is not signiÔ¨Åcant
(greater than 0.05). We leave it as an exercise to verify that the order in which the fea-
tures are removed is: age,ftv1 , and ptl1 . In this case, forward selection and backward
elimination result in the same model, but this need not be the case in general.
This way of model selection has the advantage of being easy to use and of treating the
question of variable selection in a systematic manner. The main drawback is that variables
are included or deleted based on purely statistical criteria, without taking into account the
aim of the study. This usually leads to a model which may be satisfactory from a statistical
point of view, but in which the variables are not necessarily the most relevant when it comes
to understanding and interpreting the data in the study.
Of course, we can choose to investigate any combination of features, not just the ones
suggested by the above variable selection methods. For example, let us see if the mother‚Äôs
weight, her age, her race, and whether she smokes explain the baby‚Äôs birthweight.202 5.6. Linear Models in Python
formula = 'bwt~lwt+age+C(race)+ smoke '
bwt_model = ols(formula , data=bwt).fit()
print (bwt_model.summary())
OLS Regression Results
======================================================================
Dep. Variable: bwt R-squared: 0.148
Model: OLS Adj. R-squared: 0.125
Method: Least Squares F-statistic: 6.373
No. Observations: 189 Prob (F-statistic): 1.76e-05
Df Residuals: 183 Log-Likelihood: -1498.4
Df Model: 5 AIC: 3009.
BIC: 3028.
=====================================================================
coef std err t P>|t| [0.025 0.975]
----------------------------------------------------------------------
Intercept 2839.4334 321.435 8.834 0.000 2205.239 3473.628
C(race)[T.2] -510.5015 157.077 -3.250 0.001 -820.416 -200.587
C(race)[T.3] -398.6439 119.579 -3.334 0.001 -634.575 -162.713
smoke -401.7205 109.241 -3.677 0.000 -617.254 -186.187
lwt 3.9999 1.738 2.301 0.022 0.571 7.429
age -1.9478 9.820 -0.198 0.843 -21.323 17.427
======================================================================
Omnibus: 3.916 Durbin -Watson: 0.458
Prob(Omnibus): 0.141 Jarque -Bera (JB): 3.718
Skew: -0.343 Prob(JB): 0.156
Kurtosis: 3.038 Cond. No. 899.
Given the result of Fisher‚Äôs global test given by Prob (F-Statistic) in the summary
(P-value =1:7610 5), we can conclude that at least one of the explanatory variables is
associated with child weight at birth, after adjusting for the other variables. The individual
Student tests indicate that:
¬àthe mother‚Äôs weight is linearly associated with child weight, after adjusting for age,
race, and smoking status (P-value =0.022). At the same age, race, and smoking
status, an increase of one pound in the mother‚Äôs weight corresponds to an increase
of 4 g in the average child weight at birth;
¬àthe age of the mother is not signiÔ¨Åcantly linearly associated with child weight at
birth, when mother weight, race, and smoking status are already taken into account
(P-value =0.843);
¬àweight at birth is signiÔ¨Åcantly lower for a child born to a mother who smokes, com-
pared to children born to non-smoking mothers of the same age, race, and weight,
with a P-value of 0.00031 (to see this, inspect bwt_model.pvalues ). At the same
age, race, and mother weight, the child‚Äôs weight at birth is 401.720 g less for a
smoking mother than for a non-smoking mother;
¬àregarding the interpretation of the variable race , we note that the Ô¨Årst level of this
categorical variable corresponds to white mothers. The estimate of  510:501 g for
C(race)[T.2] represents the di erence in the child‚Äôs birth weight between black
mothers and white mothers (reference group), and this result is signiÔ¨Åcantly di erentChapter 5. Regression 203
from zero (P-value =0.001) in a model adjusted for the mother‚Äôs weight, age, and
smoking status.
5.6.6.2 Interaction
We can also include interaction terms in the model. Let us see whether there is any inter-
action e ect between smoke andagevia the model
Bwt=0+1age+2smoke +3agesmoke +":
InPython this can be done as follows (below we have removed some output):
formula = 'bwt~age*smoke '
bwt_model = ols(formula , data=bwt).fit()
print (bwt_model.summary())
OLS Regression Results
======================================================================
Dep. Variable: bwt R-squared: 0.069
Model: OLS Adj. R-squared: 0.054
Method: Least Squares F-statistic: 4.577
No. Observations: 189 Prob (F-statistic): 0.00407
Df Residuals: 183 Log-Likelihood: -1506.8
Df Model: 5 AIC: 3009.
BIC: 3028.
======================================================================
coef std err t P>|t| [0.025 0.975]
----------------------------------------------------------------------
Intercept 2406.1 292.190 8.235 0.000 1829.6 2982.5
smoke 798.2 484.342 1.648 0.101 -157.4 1753.7
age 27.7 12.149 2.283 0.024 3.8 51.7
age:smoke -46.6 20.447 -2.278 0.024 -86.9 -6.2
We observe that the estimate for 3( 46:6) is signiÔ¨Åcantly di erent from zero (P-value
=0.024). We therefore conclude that the e ect of the mother‚Äôs age on the child‚Äôs weight
depends on the smoking status of the mother. The results on association between mother
age and child weight must therefore be presented separately for the smoking and the non-
smoking group. For non-smoking mothers ( smoke = 0 ), the mean child weight at birth
increases on average by 27 :7 grams for each year of the mother‚Äôs age. This is statistically
signiÔ¨Åcant, as can be seen from the 95% conÔ¨Ådence intervals for the parameters (which
does not contain zero):
bwt_model.conf_int()
0 1
Intercept 1829.605754 2982.510194
age 3.762780 51.699977
smoke -157.368023 1753.717779
age:smoke -86.911405 -6.232425
Similarly, for smoking mothers, there seems to be a decrease in birthweight, b1+b3=
27:7 46:6= 18:9, but this is not statistically signiÔ¨Åcant; see Exercise 6.204 5.7. Generalized Linear Models
5.7 Generalized Linear Models
The normal linear model in Section 2.8 deals with continuous response variables ‚Äî such
as height and crop yield ‚Äî and continuous or discrete explanatory variables. Given the
feature vectorsfxig, the responsesfYigare independent of each other, and each has a normal
distribution with mean x>
i, where x>
iis the i-th row of the model matrix X. Generalized
linear models allow for arbitrary response distributions, including discrete ones.
DeÔ¨Ånition 5.2: Generalized Linear Model
In ageneralized linear model generalized
linear model(GLM) the expected response for a given feature vec-
torx=[x1;:::; xp]>is of the form
E[YjX=x]=h(x>) (5.35)
for some function h, which is called the activation function activation
function. The distribution of
Y(for a given x) may depend on additional dispersion parameters that model the
randomness in the data that is not explained by x.
Theinverse of function his called the link function link function . As for the linear model, (5.35) is
a model for a single pair ( x;Y). Using the model simpliÔ¨Åcation introduced at the end of
Section 5.1, the corresponding model for a whole training set T=f(xi;Yi)gis that thefxig
are Ô¨Åxed and that the fYigare independent; each Yisatisfying (5.35) with x=xi. Writing
Y=[Y1;:::; Yn]>and deÔ¨Åning has the multivalued function with components h, we have
EXY=h(X);
where Xis the (model) matrix with rows x>
1;:::; x>
n. A common assumption is that
Y1;:::; Yncome from the same family of distributions, e.g., normal, Bernoulli, or Pois-
son. The central focus is the parameter vector , which summarizes how the matrix of
explanatory variables Xaects the response vector Y. The class of generalized linear mod-
els can encompass a wide variety of models. Obviously the normal linear model (2.34) is
a generalized linear model, with E[YjX=x]=x>, so that his the identity function. In
this case, YN(x>;2);i=1;:::; n, where2is a dispersion parameter.
Example 5.10 (Logistic Regression) In a logistic regression logistic
regressionorlogit model , we as-
sume that the response variables Y1;:::; Ynare independent and distributed according to
YiBer(h(x>
i));where hhere is deÔ¨Åned as the cdf of the logistic distribution logistic
distribution:
h(x)=1
1+e x:
Large values of x>
ithus lead to a high probability that Yi=1, and small (negative) values
ofx>
icause Yito be 0 with high probability. Estimation of the parameter vector from
the observed data is not as straightforward as for the ordinary linear model, but can be
accomplished via the minimization of a suitable training loss, as explained below.
As thefYigare independent, the pdf of Y=[Y1;:::; Yn]>is
g(yj;X)=nY
i=1[h(x>
i)]yi[1 h(x>
i)]1 yi:Chapter 5. Regression 205
Maximizing the log-likelihood ln g(yj;X) with respect to gives the maximum likeli-
hood estimator of . In a supervised learning framework, this is equivalent to minimizing:
 1
nlng(yj;X)= 1
nnX
i=1lng(yij;xi)
= 1
nnX
i=1yilnh(x>
i)+(1 yi) ln(1 h(x>
i)):(5.36)
By comparing (5.36) with (4.4), we see that we can interpret (5.36) as the cross-entropy +123
training loss associated with comparing a true conditional pdf f(yjx) with an approxima-
tion pdf g(yj;x) via the loss function
Loss( f(yjx);g(yj;x)) := lng(yj;x)= ylnh(x>) (1 y) ln(1 h(x>)):
Minimizing (5.36) in terms of actually constitutes a convex optimization problem. Since
lnh(x>)= ln(1+e x>) and ln(1 h(x>))= x> ln(1+e x>), the cross-entropy
training loss (5.36) can be rewritten as
r() :=1
nnX
i=1h
(1 yi)x>
i+ln
1+e x>
ii
:
We leave it as Exercise 7 to show that the gradient rr() and Hessian H() ofr() are
given by
rr()=1
nnX
i=1(i yi)xi (5.37)
and
H()=1
nnX
i=1i(1 i)xix>
i; (5.38)
respectively, where i:=h(x>
i).
Notice that H() is a positive semideÔ¨Ånite matrix for all values of , implying the +403
convexity of r(). Consequently, we can Ô¨Ånd an optimal eciently; e.g., via Newton‚Äôs
method. SpeciÔ¨Åcally, given an initial value 0, for t=1;2;:::; iteratively compute +409
t=t 1 H 1(t 1)rr(t 1); (5.39)
until the sequence 0;1;2;:::is deemed to have converged, using some pre-Ô¨Åxed con-
vergence criterion.
Figure 5.9 shows the outcomes of 100 independent Bernoulli random variables, where
each success probability, (1 +exp( (0+1x))) 1, depends on xand0= 3; 1=10. The
true logistic curve is also shown (dashed line). The minimum training loss curve (red line)
is obtained via the Newton scheme (5.39), giving estimates b0= 2:66 and b1=10:08.
The Python code is given below.206 5.7. Generalized Linear Models
-1 -0.5 0 0.5 100.20.40.60.81
Figure 5.9: Logistic regression data (blue dots), Ô¨Åtted curve (red), and true curve (black
dashed).
logreg1d.py
import numpy as np
import matplotlib.pyplot as plt
from numpy.linalg import lstsq
n = 100 # sample size
x = (2*np.random.rand(n)-1).reshape(n,1) # explanatory variables
beta = np.array([-3, 10])
Xmat = np.hstack((np.ones((n,1)), x))
p = 1/(1 + np.exp(-Xmat @ beta))
y = np.random.binomial(1,p,n) # response variables
# initial guess
betat = lstsq((Xmat.T @ Xmat),Xmat.T @ y, rcond=None)[0]
grad = np.array([2,1]) # gradient
while (np. sum(np. abs(grad)) > 1e-5) : # stopping criteria
mu = 1/(1+np.exp(-Xmat @ betat))
# gradient
delta = (mu - y).reshape(n,1)
grad = np. sum(np.multiply( np.hstack((delta ,delta)),Xmat), axis
=0).T
# Hessian
H = Xmat.T @ np.diag(np.multiply(mu,(1-mu))) @ Xmat
betat = betat - lstsq(H,grad ,rcond=None)[0]
print (betat)
plt.plot(x,y, '.')# plot data
xx = np.linspace(-1,1,40).reshape(40,1)
XXmat = np.hstack( (np.ones(( len(xx),1)), xx))
yy = 1/(1 + np.exp(-XXmat @ beta))
plt.plot(xx,yy, 'r-') #true logistic curve
yy = 1/(1 + np.exp(-XXmat @ betat));
plt.plot(xx,yy, 'k--')Chapter 5. Regression 207
Further Reading
An excellent overview of regression is provided in [33] and an accessible mathematical
treatment of linear regression models can be found in [108]. For extensions to nonlinear
regression we refer the reader to [7]. A practical introduction to multilevel /hierarchical
models is given in [47]. For further discussion on regression with discrete responses (clas-
siÔ¨Åcation) we refer to Chapter 7 and the further reading therein. On the important question +251
of how to handle missing data, the classic reference is [80] (see also [85]) and a modern
applied reference is [120].
Exercises
1. Following his mentor Francis Galton, the mathematician /statistician Karl Pearson con-
ducted comprehensive studies comparing hereditary traits between members of the same
family. Figure 5.10 depicts the measurements of the heights of 1078 fathers and their
adult sons (one son per father). The data is available from the book‚Äôs GitHub site as
pearson.csv .
58 60 62 64 66 68 70 72 74 76
Height Father (in)60657075Height Son (in)
Figure 5.10: A scatterplot of heights from Pearson‚Äôs data.
(a) Show that sons are on average 1 inch taller than the fathers.
(b) We could try to ‚Äúexplain‚Äù the height of the son by taking the height of his father and
adding 1 inch. The prediction line y=x+1 (red dashed) is given Figure 5.10. The
black solid line is the Ô¨Åtted regression line. This line has a slope less than 1, and
demonstrates Galton‚Äôs ‚Äúregression‚Äù to the average. Find the intercept and slope of the
Ô¨Åtted regression line.
2. For the simple linear regression model, show that the values for b1andb0that solve the208 Exercises
equations (5.9) are:
b1=Pn
i=1(xi x)(yi y)Pn
i=1(xi x)2(5.40)
b0=y b1x; (5.41)
provided that not all xiare the same.
3. Edwin Hubble discovered that the universe is expanding. If vis a galaxy‚Äôs recession ve-
locity (relative to any other galaxy) and dis its distance (from that same galaxy), Hubble‚Äôs
law states that
v=Hd;
where His known as Hubble‚Äôs constant. The following are distance (in millions of light-
years) and velocity (thousands of miles per second) measurements made on Ô¨Åve galactic
clusters.
distance 68 137 315 405 700
velocity 2.4 4.7 12.0 14.4 26.0
State the regression model and estimate H.
4. The multiple linear regression model (5.6) can be viewed as a Ô¨Årst-order approximation
of the general model
Y=g(x)+"; (5.42)
whereE"=0,Var"=2, and g(x) is some known or unknown function of a d-
dimensional vector xof explanatory variables. To see this, replace g(x) with its Ô¨Årst-order
Taylor approximation around some point x0and write this as 0+x>. Express0and
in terms of gandx0.
5. Table 5.6 shows data from an agricultural experiment where crop yield was measured
for two levels of pesticide and three levels of fertilizer. There are three responses for each
combination.
Table 5.6: Crop yields for pesticide and fertilizer combinations.
Fertilizer
Pesticide Low Medium High
No 3.23, 3.20, 3.16 2.99, 2.85, 2.77 5.72, 5.77, 5.62
Yes 6.78, 6.73, 6.79 9.07, 9.09, 8.86 8.12, 8.04, 8.31
(a) Organize the data in standard form, where each row corresponds to a single meas-
urement and the columns correspond to the response variable and the two factor vari-
ables.Chapter 5. Regression 209
(b) Let Yi jkbe the response for the k-th replication at level ifor factor 1 and level j
for factor 2. To assess which factors best explain the response variable, we use the
ANOV A model
Yi jk=+i+j+i j+"i jk; (5.43)
whereP
ii=P
jj=P
ii j=P
ji j=0. DeÔ¨Åne=[; 1;2;1;2; 3;11;12;
13;21;22;23]>. Give the corresponding 18 12 model matrix.
(c) Note that the parameters are linearly dependent in this case. For example, 2= 1
and13= (11+12). To retain only 6 linearly independent variables consider the
6-dimensional parameter vector e=[; 1;1;2;11;12]>. Find the matrix Msuch
thatMe=.
(d) Give the model matrix corresponding to e.
6. Show that for the birthweight data in Section 5.6.6.2 there is no signiÔ¨Åcant decrease
in birthweight for smoking mothers. [Hint: create a new variable nonsmoke =1 smoke ,
which reverses the encoding for the smoking and non-smoking mothers. Then, the para-
meter1+3in the original model is the same as the parameter 1in the model
Bwt=0+1age+2nonsmoke +3agenonsmoke +":
Now Ô¨Ånd a 95% for 3and see if it contains zero.]
7. Prove (5.37) and (5.38).
8. In the Tobit regression Tobit
regressionmodel with normally distributed errors, the response is modeled
as:
Yi=8>><>>:Zi;ifui<Zi
ui;ifZi6ui; ZN(X;2In);
where the model matrix Xand the thresholds u1;:::; unare given. Typically, ui=0;i=
1;:::; n. Suppose we wish to estimate :=(;2) via the Expectation‚ÄìMaximization
method, similar to the censored data Example 4.2. Let y=[y1;:::; yn]>be the vector +130
of observed data.
(a) Show that the likelihood of yis:
g(yj)=Y
i:yi>ui'2(yi x>
i)Y
i:yi=ui((ui x>
i)=);
where is the cdf of the N(0;1) distribution and '2the pdf of the N(0;2) distribu-
tion.
(b) Let yandybe vectors that collect all yi>uiandyi=ui, respectively. Denote the
corresponding matrix of predictors by XandX, respectively. For each observation
yi=uiintroduce a latent variable ziand collect these into a vector z. For the same
indices icollect the corresponding uiinto a vector c. Show that the complete-data
likelihood is given by
g(y;zj)=1
(22)n=2exp0BBBB@ ky Xk2
22 kz Xk2
221CCCCA1fz6cg:210 Exercises
(c) For the E-step, show that, for a Ô¨Åxed ,
g(zjy;)=Y
ig(zijy;);
where each g(zijy;) is the pdf of the N((X)i;2) distribution, truncated to the in-
terval ( 1;ci].
(d) For the M-step, compute the expectation of the complete log-likelihood
 n
2ln2 n
2ln(2) ky Xk2
22 EkZ Xk2
22:
Then, derive the formulas for and2that maximize the expectation of the complete
log-likelihood.
9. Dowload data set WomenWage.csv from the book‚Äôs website. This data set is a tidied-up
version of the women‚Äôs wages data set from [91]. The Ô¨Årst column of the data ( hours ) is
the response variable Y. It shows the hours spent in the labor force by married women in
the 1970s. We want to understand what factors determine the participation rate of women
in the labor force. The predictor variables are:
Table 5.7: Features for the women‚Äôs wage data set.
Feature Description
kidslt6 Number of children younger than 6 years.
kidsge6 Number of children older than 6 years.
age Age of the married woman.
educ Number of years of formal education.
exper Number of years of ‚Äúwork experience‚Äù.
nwifeinc Non-wife income, that is, the income of the husband.
expersq The square of exper , to capture any nonlinear relationships.
We observe that some of the responses are Y=0, that is, some women did not particip-
ate in the labor force. For this reason, we model the data using the Tobit regression model,
in which the response Yis given as:
Yi=8>><>>:Zi;ifZi>0
0;ifZi60; ZN(X;2In):
With=(;2), the likelihood of the data y=[y1;:::; yn]>is:
g(yj)=Q
i:yi>0'2(yi x>
i)Q
i:yi=0((ui x>
i)=);
where is the standard normal cdf. In Exercise 8, we derived the EM algorithm for max-
imizing the log-likelihood.
(a) Write down the EM algorithm in pseudo code as it applies to this Tobit regression.Chapter 5. Regression 211
(b) Implement the EM algorithm pseudo code in Python. Comment on which factor you
think is important in determining the labor participation rate of women living in the
USA in the 1970s.
10. Let Pbe a projection matrix. Show that the diagonal elements of Pall lie in the interval
[0;1]. In particular, for P=XX+in Theorem 5.1, the leverage value pi:=PiisatisÔ¨Åes
06pi61 for all i.
11. Consider the linear model Y=X+"in (5.8), with Xbeing the npmodel matrix
and"having expectation vector 0and covariance matrix 2In. Suppose that b iis the
least-squares estimate obtained by omitting the i-th observation, Yi; that is,
b i=argmin
X
j,i(Yj x>
j)2;
where x>
jis the j-th row of X. LetbY i=x>
ib ibe the corresponding Ô¨Åtted value at xi. Also,
deÔ¨Åne Bias the least-squares estimator of based on the response data
Y(i):=[Y1;:::; Yi 1;bY i;Yi+1;:::; Yn]>:
(a) Prove that b i=Bi; that is, the linear model obtained from Ô¨Åtting all responses except
thei-th is the same as the one obtained from Ô¨Åtting the data Y(i).
(b) Use the previous result to verify that
Yi bY i=(Yi bYi)=(1 Pii);
where P=XX+is the projection matrix onto the columns of X. Hence, deduce the
PRESS formula in Theorem 5.1. +174
12. Take the linear model Y=X+",where Xis an npmodel matrix, "=0, and
Cov(")=2In. Let P=XX+be the projection matrix onto the columns of X.
(a) Using the properties of the pseudo-inverse (see DeÔ¨Ånition A.2), show that PP>=P. +360
(b) Let E=Y bYbe the (random) vector of residuals, where bY=PY. Show that the i-th
residual has a normal distribution with expectation 0 and variance 2(1 Pii) (that is,
2times 1 minus the i-th leverage).
(c) Show that 2can be unbiasedly estimated via
S2:=1
n pkY bYk2=1
n pkY Xbk2: (5.44)
[Hint: use the cyclic property of the trace as in Example 2.3.]
13. Consider a normal linear model Y=X+", where Xis an npmodel matrix and
"N(0;2In). Exercise 12 shows that for any such model the i-th standardized residual
Ei=(p1 Pii) has a standard normal distribution. This motivates the use of the leverage
Piito assess whether the i-th observation is an outlier depending on the size of the i-th
residual relative top1 Pii. A more robust approach is to include an estimate for using212 Exercises
all data except the i-th observation. This gives rise to the studentized residual studentized
residualTi, deÔ¨Åned
as
Ti:=Ei
S ip1 Pii;
where S iis an estimate of obtained by Ô¨Åtting all the observations except the i-th and
Ei=Yi bYiis the i-th (random) residual. Exercise 12 shows that we can take, for example,
S2
 i=1
n 1 pkY i X ib ik2; (5.45)
where X iis the model matrix Xwith the i-th row removed, is an unbiased estimator of
2. We wish to compute S2
 ieciently, using S2in (5.44), as the latter will typically be
available once we have Ô¨Åtted the linear model. To this end, deÔ¨Åne uias the i-th unit vector
[0;:::; 0;1;0;:::; 0]>, and let
Y(i):=Y (Yi bY i)ui=Y Ei
1 Piiui;
where we have used the fact that Yi bY i=Ei=(1 Pii), as derived in the proof of The-
orem 5.1. Now apply Exercise 11 to prove that
S2
 i=(n p)S2 E2
i=(1 Pii)
n p 1:
14. Using the notation from Exercises 11‚Äì13, Cook‚Äôs distance Cook‚Äôs distance for observation iis deÔ¨Åned
as
Di:=kbY bY(i)k2
p S2:
It measures the change in the Ô¨Åtted values when the i-th observation is removed, relative to
the residual variance of the model (estimated via S2).
By using similar arguments as those in Exercise 13, show that
Di=PiiE2
i
(1 Pii)2p S2:
It follows that there is no need to ‚Äúomit and reÔ¨Åt‚Äù the linear model in order to compute
Cook‚Äôs distance for the i-th response.
15. Prove that if we add an additional feature to the general linear model, then R2, the
coecient of determination, is necessarily non-decreasing in value and hence cannot be
used to compare models with di erent numbers of predictors.
16. Let X:=[X1;:::; Xn]>and:=[1;:::; n]>. In the fundamental Theorem C.9, we
use the fact that if XiN(i;1),i=1;:::; nare independent, then kXk2has (per deÔ¨Ånition)
a noncentral 2
ndistribution. Show that kXk2has moment generating function
etkk2=(1 2t)
(1 2t)n=2;t<1=2;
and so the distribution of kXk2depends ononly through the norm kk.Chapter 5. Regression 213
17. Carry out a logistic regression analysis on a (partial) wine data set classiÔ¨Åcation prob-
lem. The data can be loaded using the following code.
from sklearn import datasets
import numpy as np
data = datasets.load_wine()
X = data.data[:, [9,10]]
y = np.array(data.target==1,dtype=np.uint)
X = np.append(np.ones( len(X)).reshape(-1,1),X,axis=1)
The model matrix has three features, including the constant feature. Instead of using
Newton‚Äôs method (5.39) to estimate , implement a simple gradient descent procedure
t=t 1 rr(t 1);
with learning rate =0:0001, and run it for 106steps. Your procedure should deliver three
coecients; one for the intercept and the rest for the explanatory variables. Solve the same
problem using the Logit method of statsmodels.api and compare the results.
18. Consider again Example 5.10, where we train the learner via the Newton iteration
(5.39). If X>:=[x1;:::; xn] deÔ¨Ånes the matrix of predictors and t:=h(Xt), then the +205
gradient (5.37) and Hessian (5.38) for Newton‚Äôs method can be written as:
rr(t)=1
nX>(t y) and H(t)=1
nX>DtX;
where Dt:=diag(t(1 t)) is a diagonal matrix. Show that the Newton iteration (5.39)
can be written as the iterative reweighted least-squares iterative
reweighted
least squaresmethod:
t=argmin
(eyt 1 X)>Dt 1(eyt 1 X);
whereeyt 1:=Xt 1+D 1
t 1(y t 1) is the so-called adjusted response . [Hint: use the fact
that ( M>M) 1M>zis the minimizer of kM zk2.]
19. In multi-output linear regression multi -output
linear
regression, the response variable is a real-valued vector of di-
mension, say, m. Similar to (5.8), the model can be written in matrix notation:
Y=XB+26666666664">
1:::
">
n37777777775;
where:
¬àYis an nmmatrix of nindependent responses (stored as row vectors of length m);
¬àXis the usual npmodel matrix;
¬àBis an pmmatrix of model parameters;
¬à"1;:::;"n2Rmare independent error terms with E"=0andE"">=.214 Exercises
We wish to learn the matrix parameters Bandfrom the training set fY;Xg. To this end,
consider minimizing the training loss:
1
ntr
(Y XB) 1(Y XB)>
;
where tr() is the trace of a matrix. +357
(a) Show that the minimizer of the training loss, denoted bB, satisÔ¨Åes the normal equa-
tions:
X>XbB=X>Y:
(b) Noting that
(Y XB)>(Y XB)=nX
i=1"i">
i;
explain why
b:=(Y XbB)>(Y XbB)
n
is a method-of-moments estimator of , just like the one given in (5.10).CHAPTER6
REGULARIZATION AND KERNEL
METHODS
The purpose of this chapter is to familiarize the reader with two central concepts
in modern data science and machine learning: regularization and kernel methods. Reg-
ularization provides a natural way to guard against overÔ¨Åtting and kernel methods of-
fer a broad generalization of linear models. Here, we discuss regularized regression
(ridge, lasso) as a bridge to the fundamentals of kernel methods. We introduce repro-
ducing kernel Hilbert spaces and show that selecting the best prediction function in
such spaces is in fact a Ô¨Ånite-dimensional optimization problem. Applications to spline
Ô¨Åtting, Gaussian process regression, and kernel PCA are given.
6.1 Introduction
In this chapter we return to the supervised learning setting of Chapter 5 (regression) and ex-
pand its scope. Given training data =f(x1;y1);:::; (xn;yn)g, we wish to Ô¨Ånd a prediction
function (the learner) gthat minimizes the (squared-error) training loss
`(g)=1
nnX
i=1(yi g(xi))2
within a class of functions G. As noted in Chapter 2, if Gis the set of all possible functions
then choosing anyfunction gwith the property that g(xi)=yifor all iwill give zero training
loss, but will likely have poor generalization performance (that is, su er from overÔ¨Åtting).
Recall from Theorem 2.1 that the best possible prediction function (over all g) for + 21
the squared-error riskE(Y g(X))2is given by g(x)=E[YjX=x]. The classGshould
be simple enough to permit theoretical understanding and analysis but, at the same time,
rich enough to contain the optimal function g(or a function close to g). This ideal can
be realized by taking Gto be a Hilbert space Hilbert space (i.e., a complete inner product space) of
functions; see Appendix A.7. +384
Many of the classes of functions that we have encountered so far are in fact Hilbert
spaces. In particular, the set Goflinear functions on Rpis a Hilbert space. To see this,
215216 6.2. Regularization
identify with each element 2Rpthe linear function g:x7!x>and deÔ¨Åne the inner
product onGashg;gi:=>. In this way,Gbehaves in exactly the same way as (is
isomorphic to) the space Rpequipped with the Euclidean inner product (dot product). The +360
latter is a Hilbert space, because it is complete complete
vector spacewith respect to the Euclidean norm. See
Exercise 12 for a further discussion.
Let us now turn to our ‚Äúrunning‚Äù polynomial regression Example 2.1, where the feature +26
vector x=[1;u;u2;:::; up 1]>=:(u) is itself a vector-valued function of another feature
u. Then, the space of functions h:u7!(u)>is a Hilbert space, through the identiÔ¨Åca-
tionh. In fact, this is true for anyfeature mapping :u7![1(u);:::; p(u)]>.
This can be further generalized by considering feature maps u7!u, where each ufeature mapsis a real-valued function v7!u(v) on the feature space. As we shall soon see (in Sec-
tion 6.3), functions of the form u7!P1
i=1ivi(u) live in a Hilbert space of functions called
areproducing kernel Hilbert space (RKHS). RKHS In Section 6.3 we introduce the notion of a
RKHS formally, give speciÔ¨Åc examples, including the linear and Gaussian kernels, and de-
rive various useful properties, the most important of which is the representer Theorem 6.6.
Applications of such spaces include the smoothing splines (Section 6.6), Gaussian pro- +235
cess regression (Section 6.7), kernel PCA (Section 6.8), and support vector machines for
classiÔ¨Åcation (Section 7.7). +269
The RKHS formalism also makes it easier to treat the important topic of regularization .regularizationThe aim of regularization is to improve the predictive performance of the best learner in
some class of functions Gby adding a penalty term to the training loss that penalizes
learners that tend to overÔ¨Åt the data. In the next section we introduce the main ideas behind
regularization, which then segues into a discussion of kernel methods in the subsequent
sections.
6.2 Regularization
LetGbe the Hilbert space of functions over which we search for the minimizer, g, of the
training loss `(g). Often, the Hilbert space Gis rich enough so that we can Ô¨Ånd a learner
gwithinGsuch that the training loss is zero or close to zero. Consequently, if the space of
functionsGis suciently rich, we run the risk of overÔ¨Åtting. One way to avoid overÔ¨Åtting
is to restrict attention to a subset of the space Gby introducing a non-negative functional
J:G!R+which penalizes complex models (functions). In particular, we want to Ô¨Ånd
functions g2Gsuch that J(g)<cfor some ‚Äúregularization‚Äù constant c>0. Thus we can
formulate the quintessential supervised learning problem as:
minf`(g) :g2G;J(g)<cg; (6.1)
the solution (argmin) of which is our learner. When this optimization problem is convex, it
can be solved by Ô¨Årst obtaining the Lagrangian dual function
L() :=min
g2Gf`(g)+(J(g) c)g;
and then maximizing L() with respect to >0; see Section B.2.3. +407
In order to introduce the overall ideas of kernel methods and regularization, we will
proceed by exploring (6.1) in the special case of ridge regression ridge
regression, with the following run-
ning example.Chapter 6. Regularization and Kernel Methods 217
Example 6.1 (Ridge Regression) Ridge regression is simply linear regression with a
squared-norm penalty functional (also called a regularization function, or regularizer regularizer ).
Suppose we have a training set =f(xi;yi);i=1;:::; ng, with each xi2Rpand we use a
squared-norm penalty with regularization parameter regularization
parameter>0. Then, the problem is to solve
min
g2G1
nnX
i=1(yi g(xi))2+kgk2; (6.2)
whereGis the Hilbert space of linear functions on Rp. As explained in Section 6.1, we
can identify each g2Gwith a vector 2Rpand, consequently, kgk2=h;i=kk2. The
above functional optimization problem is thus equivalent to the parametric optimization
problem
min
2Rp1
nnX
i=1 yi x>
i2+kk2; (6.3)
which, in the notation of Chapter 5, further simpliÔ¨Åes to
min
2Rp1
nky Xk2+kk2: (6.4)
In other words, the solution to (6.2) is of the form x7!x>, wheresolves (6.3) (or
equivalently (6.4)). Observe that as !1 , the regularization term becomes dominant and
consequently the optimal gbecomes identically zero.
The optimization problem in (6.4) is convex, and by multiplying by the constant n=2
and setting the gradient equal to zero, we obtain
X>(X y)+n=0: (6.5)
If=0 these are simply the normal equations , albeit written in a slightly di erent form. + 28
If the matrix X>X+nIpis invertible (which is the case for any  >0; see Exercise 13),
then the solution to these modiÔ¨Åed normal equations is
b=(X>X+nIp) 1X>y:
When using regularization with respect to some Hilbert space G, it is sometimes useful
to decomposeGinto two orthogonal subspaces, HandCsay, such that every g2G can
be uniquely written as g=h+c, with h2H,c2C, andhh;ci=0. Such aGis said to be
thedirect sum direct sum ofCandH, and we writeG=HC . Decompositions of this form become
useful when functions in Hare penalized but functions in Care not. We illustrate this
decomposition with the ridge regression example where one of the features is a constant
term, which we do not wish to penalize.
Example 6.2 (Ridge Regression (cont.)) Suppose one of the features in Example 6.1
is the constant 1, which we do not wish to penalize. The reason for this is to ensure that
when!1 , the optimal gbecomes the ‚Äúconstant‚Äù model, g(x)=0, rather than the
‚Äúzero‚Äù model, g(x)=0. Let us alter the notation slightly by considering the feature vectors
to be of the form ex=[1;x>]>, where x=[x1;:::; xp]>. We thus have p+1 features, rather218 6.2. Regularization
than p. LetGbe the space of linear functions of ex. Each linear function gofexcan be
written as g:ex7!0+x>, which is the sum of the constant function c:ex7!0and
h:ex7!x>. Moreover, the two functions are orthogonal with respect to the inner product
onG:hc;hi=[0;0>][0;>]>=0, where 0is a column vector of zeros.
As subspaces ofG, bothCandHare again Hilbert spaces, and their inner products and
norms follow directly from the inner product on G. For example, each function h:ex7!
x>inHhas normkhkH=kk, and the constant function c:ex7!0inChas normj0j.
The modiÔ¨Åcation of the regularized optimization problem (6.2) where the constant term
is not penalized can now be written as
min
g2HC1
nnX
i=1(yi g(exi))2+kgk2
H; (6.6)
which further simpliÔ¨Åes to
min
0;1
nky 01 Xk2+kk2; (6.7)
where 1is the n1 vector of 1s. Observe that, in this case, as !1 the optimal gtends to
the sample mean yof thefyig; that is, we obtain the ‚Äúdefault‚Äù regression model, without ex-
planatory variables. Again, this is a convex optimization problem, and the solution follows
from
X>(01+X y)+n=0; (6.8)
with
n0=1>(y X): (6.9)
This results in solving for from
(X>X n 1X>11>X+nIp)=(X> n 1X>11>)y; (6.10)
and determining 0from (6.9).
As a precursor to the kernel methods in the following sections, let us assume that n>p
and that Xhas full (column) rank p. Then any vector 2Rpcan be written as a linear
combination of the feature vectors fxig; that is, as linear combinations of the columns of
the matrix X>. In particular, let =X>, where=[1;:::; n]>2Rn. In this case (6.10)
reduces to
(XX> n 111>XX>+nIn)=(In n 111>)y:
Assuming invertibility of ( XX> n 111>XX>+nIn), we have the solution
b=(XX> n 111>XX>+nIn) 1(In n 111>)y;
which depends on the training feature vectors fxigonly through the nnmatrix of inner
products: XX>=[hxi;xji]. This matrix is called the Gram matrix Gram matrix of thefxig. From (6.9),
the solution for the constant term is b0=n 11>(y XX>b). It follows that the learner is a
linear combination of inner products fhxi;xigplus a constant:
g(ex)=b0+x>X>b=b0+nX
i=1bihxi;xi;Chapter 6. Regularization and Kernel Methods 219
where the coe cientsb0andbionly depend on the inner products fhxi;xjig. We will see
shortly that the representer Theorem 6.6 generalizes this result to a broad class of regular- +231
ized optimization problems.
We illustrate in Figure 6.1 how the solutions of the ridge regression problems appearing
in Examples 6.1 and 6.2 are qualitatively a ected by the regularization parameter for a
simple linear regression model. The data was generated from the model yi= 1:5+0:5xi+
"i,i=1;:::; 100, where each xiis drawn independently and uniformly from the interval
[0;10] and each "iis drawn independently from the standard normal distribution.
.= 0 :1
-2-1012-1.= 1 .= 10
-2 0 2
-0-2-1012-1
-2 0 2
-0-2 0 2
-0
Figure 6.1: Ridge regression solutions for a simple linear regression problem. Each panel
shows contours of the loss function (log scale) and the e ect of the regularization parameter
2f0:1;1;10g, appearing in (6.4) and (6.7). Top row: both terms are penalized. Bottom
row: only the non-constant term is penalized. Penalized (plus) and unpenalized (diamond)
solutions are shown in each case.
The contours are those of the squared-error loss (actually the logarithm thereof), which
is minimized with respect to the model parameters 0and1. The diamonds all repres-
ent the same minimizer of this loss. The plusses show each minimizer [ 
0;
1]>of the
regularized minimization problems (6.4) and (6.7) for three choices of the regularization
parameter. For the top three panels the regularization involves both 0and1, through
the squared norm 2
0+2
1. The circles show the points that have the same squared norm as220 6.2. Regularization
the optimal solution. For the bottom three panels only 1is regularized; there, horizontal
lines indicate vectors [ 0;1]>for whichj1j=j
1j.
The problem of ridge regression discussed in Example 6.2 boils down to solving a
problem of the form in (6.7), involving a squared 2-norm penalty kk2. A natural ques-
tion to ask is whether we can replace the squared 2-norm penalty by a di erent penalty
term. Replacing it with a 1-norm gives the lasso (least absolute shrinkage and selection +408
lasso operator). The lasso equivalent of the ridge regression problem (6.7) is thus:
min
0;1
nky 01 Xk2+kk1; (6.11)
wherekk1=Pp
i=1jij.
This is again a convex optimization problem. Unlike ridge regression, the lasso gener-
ally does not have an explicit solution, and so numerical methods must be used to solve it.
Note that the problem (6.11) is of the form
min
x;zf(x)+g(z)
subject to Ax+Bz=c;(6.12)
with x:=[0;>]>,z:=,A:=[0p;Ip],B:= Ip, and c:=0p(vector of zeros), and
convex functions f(x) :=1
nky [1n;X]xk2and g(z) :=kzk1. There exist e cient al-
gorithms for solving such problems, including the alternating direction method of mul-
tipliers (ADMM) [17]. We refer to Example B.11 for details on this algorithm. +416
We repeat the examples from Figure 6.1, but now using lasso regression and taking
the square roots of the previous regularization parameters. The results are displayed in
Figure 6.2.
.=p
0:1
-2-1012-1.= 1 .=p
10
-2 0 2
-0-2-1012-1
-2 0 2
-0-2 0 2
-0
Figure 6.2: Lasso regression solutions. Compare with Figure 6.1.Chapter 6. Regularization and Kernel Methods 221
One advantage of using the lasso regularization is that the resulting optimal parameter
vector often has several components that are exactly 0. For example, in the top middle
and right panels of Figure 6.2, the optimal solution lies exactly at a corner point of the
squaref[0;1]>:j0j+j1j=j
0j+j
1jg; in this case 
0=0. For statistical models with
many parameters, the lasso can provide a methodology for model selection. Namely, as the
regularization parameter increases (or, equivalently, as the L1norm of the optimal solution
decreases), the solution vector will have fewer and fewer non-zero parameters. By plotting
the values of the parameters for each orL1one obtains the so-called regularization paths regularization
paths(also called homotopy paths orcoecient proÔ¨Åles ) for the variables. Inspection of such
paths may help assess which of the model parameters are relevant to explain the variability
in the observed responses fyig.
Example 6.3 (Regularization Paths) Figure 6.3 shows the regularization paths for p=
60 coe cients from a multiple linear regression model +169
Yi=60X
j=1jxi j+"i;i=1;:::; 150;
wherej=1 for j=1;:::; 10 andj=0 for j=11;:::; 60. The error terms f"igare inde-
pendent and standard normal. The explanatory variables fxi jgwere independently generated
from a standard normal distribution. As it is clear from the Ô¨Ågure, the estimates of the 10
non-zero coe cients are Ô¨Årst selected, as the L1norm of the solutions increases. By the
time the L1norm reaches around 4, all 10 variables for which j=1 have been correctly
identiÔ¨Åed and the remaining 50 parameters are estimated as exactly 0. Only after the L1
norm reaches around 8, will these ‚Äúspurious‚Äù parameters be estimated to be non-zero. For
this example, the regularization parameter varied from 10 4to 10.
0 5 10 15
L1norm-0.500.511.5b-
Figure 6.3: Regularization paths for lasso regression solutions as a function of the L1norm
of the solutions.222 6.3. Reproducing Kernel Hilbert Spaces
6.3 Reproducing Kernel Hilbert Spaces
In this section, we formalize the idea outlined at the end of Section 6.1 of extending Ô¨Ånite
dimensional feature maps to those that are functions by introducing a special type of Hil-
bert space of functions known as a reproducing kernel Hilbert space (RKHS). Although
the theory extends naturally to Hilbert spaces of complex-valued functions, we restrict
attention to Hilbert spaces of real-valued functions here.
To evaluate the loss of a learner gin some class of functions G, we do not need to expli-
citly construct g‚Äî rather, it is only required that we can evaluate gat all the feature vectors
x1;:::; xnof the training set. A deÔ¨Åning property of an RKHS is that function evaluation
at a point xcan be performed by simply taking the inner product of gwith some feature
functionxassociated with x. We will see that this property becomes particularly useful
in light of the representer theorem (see Section 6.5), which states that the learner gitself +230
can be represented as a linear combination of the set of feature functions fxi;i=1;:::; ng.
Consequently, we can evaluate a learner gat the feature vectors fxigby taking linear com-
binations of terms of the form (xi;xj)=hxi;xjiG. Collecting these inner products into
a matrix K=[(xi;xj);i;j=1;:::; n] (the Gram matrix of the fxig), we will see that the
feature vectorsfxigonly enter the loss minimization problem through K.
DeÔ¨Ånition 6.1: Reproducing Kernel Hilbert Space
For a non-empty set X, a Hilbert spaceGof functions g:X!Rwith inner product
h;iGis called a reproducing kernel Hilbert space reproducing
kernel Hilbert
space(RKHS) with reproducing kernel
:XX!Rif:
1. for every x2X,x:=(x;) is inG,
2.(x;x)<1for all x2X,
3. for every x2Xandg2G,g(x)=hg;xiG.
The reproducing kernel of a Hilbert space of functions, if it exists, is unique; see Exer-
cise 2. The main (third) condition in DeÔ¨Ånition 6.1 is known as the reproducing property reproducing
property.
This property allows us to evaluate any function g2Gat a point x2Xby taking the inner
product of gandx; as such,xis called the representer of evaluation . Further, by taking
g=x0and applying the reproducing property, we have hx0;xiG=(x0;x), and so by sym-
metry of the inner product it follows that (x;x0)=(x0;x). As a consequence, reproducing
kernels are necessarily symmetric functions. Moreover, a reproducing kernel is apositive
semideÔ¨Ånite positive
semidefinitefunction, meaning that for every n>1 and every choice of 1;:::; n2Rand
x1;:::; xn2X, it holds that
nX
i=1nX
j=1i(xi;xj)j>0: (6.13)
In other words, every Gram matrix Kassociated with is a positive semideÔ¨Ånite matrix;
that is>K>0 for all. The proof is addressed in Exercise 1.
The following theorem gives an alternative characterization of an RKHS. The proof
uses the Riesz representation Theorem A.17. Also note that in the theorem below we could +390Chapter 6. Regularization and Kernel Methods 223
have replaced the word ‚Äúbounded‚Äù with ‚Äúcontinuous‚Äù, as the two are equivalent for linear
functionals; see Theorem A.16.
Theorem 6.1: Continuous Evaluation Functionals Characterize a RKHS
An RKHSGon a setXis a Hilbert space in which every evaluation functional evaluation
functionalx:g7!g(x) is bounded. Conversely, a Hilbert space Gof functionsX!Rfor
which every evaluation functional is bounded is an RKHS.
Proof: Note that, since evaluation functionals xare linear operators, showing bounded-
ness is equivalent to showing continuity. Given an RKHS with reproducing kernel , sup-
pose that we have a sequence gn2Gconverging to g2G, that iskgn gkG!0. We apply
the Cauchy‚ÄìSchwarz inequality (Theorem A.15) and the reproducing property of to Ô¨Ånd +389
that for every x2Xand any n:
jxgn xgj=jgn(x) g(x)j=jhgn g;xiGj6kgn gkGkxkG=kgn gkGp
hx;xiG
=kgn gkGp
(x;x):
Noting thatp(x;x)<1by deÔ¨Ånition for every x2X, and thatkgn gkG!0 asn!1 ,
we have shown continuity of x, that isjxgn xgj!0 asn!1 for every x2X.
Conversely, suppose that evaluation functionals are bounded. Then from the Riesz
representation Theorem A.17, there exists some gx2Gsuch thatxg=hg;gxiGfor all
g2G‚Äî the representer of evaluation. If we deÔ¨Åne (x;x0)=gx(x0) for all x;x02X, then
x:=(x;)=gxis an element ofGfor every x2Xandhg;xiG=xg=g(x), so that the
reproducing property in DeÔ¨Ånition 6.1 is veriÔ¨Åed. 
The fact that an RKHS has continuous evaluation functionals means that if two func-
tions g;h2Gare ‚Äúclose‚Äù with respect to kkG, then their evaluations g(x);h(x) are close
for every x2X. Formally, convergence in kkGnorm implies pointwise convergence for
allx2X.
The following theorem shows that any Ô¨Ånite function :XX!Rcan serve as a
reproducing kernel as long as it is Ô¨Ånite, symmetric, and positive semideÔ¨Ånite. The cor-
responding (unique!) RKHS Gis the completion of the set of all functions of the formPn
i=1ixiwherei2Rfor all i=1;:::; n.
Theorem 6.2: Moore‚ÄìAronszajn
Given a non-empty set Xand any Ô¨Ånite symmetric positive semideÔ¨Ånite function
:XX!R, there exists an RKHS Gof functions g:X!Rwith reproducing
kernel. Moreover,Gis unique.
Proof: (Sketch) As the proof of uniqueness is treated in Exercise 2, the objective is to
prove existence. The idea is to construct a pre-RKHS G0from the given function that has
the essential structure and then to extend G0to an RKHSG.
In particular, deÔ¨Åne G0as the set of Ô¨Ånite linear combinations of functions x,x2X:
G0:=
g=nX
i=1ixix1;:::; xn2X; i2R;n2N
:224 6.4. Construction of Reproducing Kernels
DeÔ¨Åne onG0the following inner product:
hf;giG0:=*nX
i=1ixi;mX
j=1jx0
j+
G0:=nX
i=1mX
j=1ij(xi;x0
j):
ThenG0is an inner product space. In fact, G0has the essential structure we require, namely
that (i) evaluation functionals are bounded /continuous (Exercise 4) and (ii) Cauchy se-
quences inG0that converge pointwise also converge in norm (see Exercise 5).
We then enlargeG0to the setGof all functions g:X!Rfor which there exists a
Cauchy sequence in G0converging pointwise to gand deÔ¨Åne an inner product on Gas the
limit
hf;giG:=lim
n!1hfn;gniG0; (6.14)
where fn!fandgn!g. To show thatGis an RKHS it remains to be shown that (1) this
inner product is well deÔ¨Åned; (2) evaluation functionals remain bounded; and (3) the space
Gis complete. A detailed proof is established in Exercises 6 and 7. 
6.4 Construction of Reproducing Kernels
In this section we describe various ways to construct a reproducing kernel :XX!
Rfor some feature space X. Recall that needs to be a Ô¨Ånite, symmetric, and positive
semideÔ¨Ånite function (that is, it satisÔ¨Åes (6.13)). In view of Theorem 6.2, specifying the
spaceXand a reproducing kernel :XX!Rcorresponds to uniquely specifying an
RKHS.
6.4.1 Reproducing Kernels via Feature Mapping
Perhaps the most fundamental way to construct a reproducing kernel is via a feature
map:X!Rp. We deÔ¨Åne (x;x0) :=h(x);(x0)i, whereh;idenotes the Euclidean
inner product. The function is clearly Ô¨Ånite and symmetric. To verify that is positive
semideÔ¨Ånite, let be the matrix with rows (x1)>;:::;(xn)>and let=[1;:::; n]>2
Rn. Then,
nX
i=1nX
j=1i(xi;xj)j=nX
i=1nX
j=1i>(xi)(xj)j=>>=k>k2>0:
Example 6.4 (Linear Kernel) Taking the identity feature map (x)=xonX=Rp,
gives the linear kernel linear kernel
(x;x0)=hx;x0i=x>x0:
As can be seen from the proof of Theorem 6.2, the RKHS of functions corresponding to
the linear kernel is the space of linear functions on Rp. This space is isomorphic to Rp
itself, as discussed in the introduction (see also Exercise 12).
It is natural to wonder whether a given kernel function corresponds uniquely to a feature
map. The answer is no, as we shall see by way of example.Chapter 6. Regularization and Kernel Methods 225
Example 6.5 (Feature Maps and Kernel Functions) LetX=Rand consider feature
maps1:X!Rand2:X!R2, with1(x) :=xand2(x) :=[x;x]>=p
2. Then
1(x;x0)=h1(x);1(x0)i=xx0;
but also
2(x;x0)=h2(x);2(x0)i=xx0:
Thus, we arrive at the same kernel function deÔ¨Åned for the same underlying set Xvia two
dierent feature maps.
6.4.2 Kernels from Characteristic Functions
Another way to construct reproducing kernels on X=Rpmakes use of the properties of
characteristic functions . In particular, we have the following result. We leave its proof as +441
Exercise 10.
Theorem 6.3: Reproducing Kernel from a Characteristic Function
LetXbe anRp-valued random vector that is symmetric about the origin (that
is,Xand Xare identically distributed), and let  be its characteristic function:
 (t)=Eeit>X=R
eit>x(dx) for t2Rp. Then(x;x0) := (x x0) is a valid repro-
ducing kernel on Rp.
Example 6.6 (Gaussian Kernel) The multivariate normal distribution with mean vec-
tor0and covariance matrix b2Ipis clearly symmetric around the origin. Its characteristic
function is
 (t)=exp 
 1
2b2ktk2!
;t2Rp:
Taking b2=1=2, this gives the popular Gaussian kernel Gaussian
kernelonRp:
(x;x0)=exp 
 1
2kx x0k2
2!
: (6.15)
The parameter is sometimes called the bandwidth bandwidth . Note that in the machine learning
literature, the Gaussian kernel is sometimes referred to as ‚Äúthe‚Äù radial basis function (rbf)
kernel radial basis
function (rbf)
kernel.1
From the proof of Theorem 6.2, we see that the RKHS Gdetermined by the Gaussian
kernelis the space of pointwise limits of functions of the form
g(x)=nX
i=1iexp 
 1
2kx xik2
2!
:
We can think of each point xihaving a feature xithat is a scaled multivariate Gaussian pdf
centered at xi.
1The term radial basis function is sometimes used more generally to mean kernels of the form (x;x0)=
f(kx x0k) for some function f:R!R.226 6.4. Construction of Reproducing Kernels
Example 6.7 (Sinc Kernel) The characteristic function of a Uniform [ 1;1] random
variable (which is symmetric around 0) is  (t)=sinc( t) :=sin(t)=t, so(x;x0)=sinc( x x0)
is a valid kernel.
Inspired by kernel density estimation (Section 4.4), we may be tempted to use the pdf +131
of a random variable that is symmetric about the origin to construct a reproducing kernel.
However, doing so will not work in general, as the next example illustrates.
Example 6.8 (Uniform pdf Does not Construct a Valid Reproducing Kernel) Take
the function  (t)=1
21fjtj61g, which is the pdf of XUniform [ 1;1]. Unfortunately, the
function(x;x0)= (x x0) is not positive semideÔ¨Ånite, as can be seen for example by
constructing the matrix A=[(ti;tj);i;j=1;2;3] for the points t1=0,t2=0:75, and
t3=1:5 as follows:
A=0BBBBBBBB@ (0) ( 0:75) ( 1:5)
 (0:75) (0) ( 0:75)
 (1:5) (0:75) (0)1CCCCCCCCA=0BBBBBBBB@0:5 0:5 0
0:5 0:5 0:5
0 0:5 0:51CCCCCCCCA:
The eigenvalues of Aaref1=2 p1=2;1=2;1=2+p1=2gf  0:2071;0:5;1:2071gand so
by Theorem A.9, Ais not a positive semideÔ¨Ånite matrix, since it has a negative eigenvalue. +367
Consequently, is not a valid reproducing kernel.
One of the reasons why the Gaussian kernel (6.15) is popular is that it enjoys the uni-
versal approximation property universal
approximation
property[88]: the space of functions spanned by the Gaussian kernel
is dense in the space of continuous functions with support ZRp. Naturally, this is a
desirable property especially if there is little prior knowledge about the properties of g.
However, note that every function gin the RKHSGassociated with a Gaussian kernel is
inÔ¨Ånitely di erentiable. Moreover, a Gaussian RKHS does not contain non-zero constant
functions. Indeed, if AZ is non-empty and open, then the only function of the form
g(x)=c1fx2Agcontained inGis the zero function ( c=0).
Consequently, if it is known that gis dierentiable only to a certain order, one may
prefer the Mat√©rn kernel Mat¬¥ern kernel with parameters ;> 0:
(x;x0)=21 
 ()p
2kx x0k=Kp
2kx x0k=
; (6.16)
which gives functions that are (weakly) di erentiable to order bc(but not necessarily to
orderde). Here, Kdenotes the modiÔ¨Åed Bessel function of the second kind; see (4.49).
The particular form of the Mat√©rn kernel appearing in (6.16) ensures that lim !1(x;x0)= +163
(x;x0), whereis the Gaussian kernel appearing in (6.15).
We remark that Sobolev spaces are closely related to the Mat√©rn kernel. Up to constants
(which scale the unit ball in the space), in dimension pand for a parameter s>p=2, these
spaces can be identiÔ¨Åed with  (t)=21 s
 (s)ktks p=2Kp=2 s(ktk), which in turn can be viewed as
the characteristic function corresponding to the (radially symmetric) multivariate Student‚Äôs
tdistribution with sdegrees of freedom: that is, with pdf f(x)/(1+kxk2) s. +162Chapter 6. Regularization and Kernel Methods 227
6.4.3 Reproducing Kernels Using Orthonormal Features
We have seen in Sections 6.4.1 and 6.4.2 how to construct reproducing kernels from feature
maps and characteristic functions. Another way to construct kernels on a space Xis to work
directly from the function class L2(X;); that is, the set of square-integrable2functions
onXwith respect to ; see also DeÔ¨Ånition A.4. For simplicity, in what follows, we will +385
considerto be the Lebesgue measure, and will simply write L2(X) rather than L2(X;).
We will also assume that XRp.
Letf1;2;:::gbe an orthonormal basis of L2(X) and let c1;c2;:::be a sequence of
positive numbers. As discussed in Section 6.4.1, the kernel corresponding to a feature map
:X!Rpis(x;x0)=(x)>(x0)=Pp
i=1i(x)i(x0). Now consider a (possibly inÔ¨Ånite)
sequence of feature functions i=cii;i=1;2;:::and deÔ¨Åne
(x;x0) :=X
i>1i(x)i(x0)=X
i>1ii(x)i(x0); (6.17)
wherei=c2
i;i=1;2;:::. This is well-deÔ¨Åned as long asP
i>1i<1, which we assume
from now on. Let Hbe the linear space of functions of the form f=P
i>1ii, whereP
i>12
i=i<1. As every function f2L2(X) can be represented as f=P
i>1hf;iii, we
see thatHis a linear subspace of L2(X). OnHdeÔ¨Åne the inner product
hf;giH:=X
i>1hf;iihg;ii
i:
With this inner product, the squared norm of f=P
i>1iiiskfk2
H=P
i>12
i=i<1.
We show thatHis actually an RKHS with kernel by verifying the conditions of DeÔ¨Åni-
tion 6.1. First,
x=X
i>1ii(x)i2H;
asP
ii<1by assumption, and so is Ô¨Ånite. Second, the reproducing property holds.
Namely, let f=P
i>1ii. Then,
hx;fiH=X
i>1hx;iihf;ii
i=X
i>1ii(x)i
i=X
i>1ii(x)=f(x):
The discussion above demonstrates that kernels can be constructed via (6.17). In fact,
(under mild conditions) any given reproducing kernel can be written in the form (6.17),
where this series representation enjoys desirable convergence properties. This result is
known as Mercer‚Äôs theorem, and is given below. We leave the full proof including the
precise conditions to, e.g., [40], but the main idea is that a reproducing kernel can be
thought of as a generalization of a positive semideÔ¨Ånite matrix K, and can also be writ-
ten in spectral form (see also Section A.6.5). In particular, by Theorem A.9, we can write +367
K=VDV>, where Vis a matrix of orthonormal eigenvectors [ v`] and Dthe diagonal
matrix of the (positive) eigenvalues [ `]; that is,
K(i;j)=X
`>1`v`(i)v`(j):
2A function f:X!Ris said to be square-integrable ifR
f2(x)(dx)<1, whereis a measure onX.228 6.4. Construction of Reproducing Kernels
In (6.18) below, x;x0play the role of i;j, and`plays the role of v`.
Theorem 6.4: Mercer
Let:XX !Rbe a reproducing kernel for a compact set X Rp. Then
(under mild conditions) there exists a countable sequence of non-negative numbers
f`gdecreasing to zero and functions f`gorthonormal in L2(X) such that
(x;x0)=X
`>1``(x)`(x0); for all x;x02X; (6.18)
where (6.18) converges absolutely and uniformly on XX .
Further, if`>0, then (`;`) is an (eigenvalue, eigenfunction) pair for the integral
operator K:L2(X)!L2(X) deÔ¨Åned by [ K f](x) :=R
X(x;y)f(y) dyforx2X.
Theorem 6.4 holds if (i) the kernel is continuous onXX , (ii) the function e(x) :=
(x;x) deÔ¨Åned for x2Xis integrable. Extensions of Theorem 6.4 to more general spaces
Xand measures hold; see, e.g., [115] or [40].
The key importance of Theorem 6.4 lies in the fact that the series representation (6.18)
converges absolutely and uniformly on XX . The uniform convergence is a much stronger
condition than pointwise convergence, and means for instance that properties of the se-
quence of partial sums, such as continuity and integrability, are transferred to the limit.
Example 6.9 (Mercer) SupposeX=[ 1;1] and the kernel is (x;x0)=1+xx0which
corresponds to the RKHS Gof ane functions from X !R. To Ô¨Ånd the (eigenvalue,
eigenfunction) pairs for the integral operator appearing in Theorem 6.4, we need to Ô¨Ånd
numbersf`gand orthonormal functions f`(x)gthat solve
Z1
 1(1+xx0)`(x0) dx0=``(x);for all x2[ 1;1]:
Consider Ô¨Årst a constant function 1(x)=c. Then, for all x2[ 1;1], we have that 2 c=1c,
and the normalization condition requires thatR1
 1c2dx=1. Together, these give 1=2 and
c=1=p
2. Next, consider an a ne function 2(x)=a+bx. Orthogonality requires that
Z1
 1c(a+bx) dx=0;
which implies a=0 (since c,0). Moreover, the normalization condition then requires
Z1
 1b2x2dx=1;
or, equivalently, 2 b2=3=1, implying b=p3=2. Finally, the integral equation reads
Z1
 1(1+xx0)bx0dx0=2bx()2bx
3=2bx;Chapter 6. Regularization and Kernel Methods 229
implying that 2=2=3. We take the positive solutions (i.e., c>0 and b>0), and note that
11(x)1(x0)+22(x)2(x0)=21p
21p
2+2
3p
3p
2xp
3p
2x0=1+xx0=(x;x0);
and so we have found the decomposition appearing in (6.18). As an aside, observe that 1
and2are orthonormal versions of the Ô¨Årst two Legendre polynomials. The corresponding +387
feature map can be explicitly identiÔ¨Åed as 1(x)=p
11(x)=1 and2(x)=p22(x)=
x.
6.4.4 Kernels from Kernels
The following theorem lists some useful properties for constructing reproducing kernels
from existing reproducing kernels.
Theorem 6.5: Rules for Constructing Kernels from Other Kernels
1. If:RpRp!Ris a reproducing kernel and :X!Rpis a function, then
((x);(x0)) is a reproducing kernel from XX!R.
2. If:XX!Ris a reproducing kernel and f:X!R+is a function, then
f(x)(x;x0)f(x0) is also a reproducing kernel from XX!R.
3. If1and2are reproducing kernels from XX!R, then so is their sum 1+2.
4. If1and2are reproducing kernels from XX!R, then so is their product
12.
5. If1and2are reproducing kernels from XX !RandYY ! Rre-
spectively, then +((x;y);(x0;y0)) :=1(x;x0)+2(y;y0) and((x;y);(x0;y0)) :=
1(x;x0)2(y;y0) are reproducing kernels from ( XY )(XY )!R.
Proof: For Rules 1, 2, and 3 it is easy to verify that the resulting function is Ô¨Ånite, sym-
metric, and positive semideÔ¨Ånite, and so is a valid reproducing kernel by Theorem 6.2.
For example, for Rule 1 we havePn
i=1Pn
j=1i(yi;yj)j>0 for every choice of fign
i=1
andfyign
i=12Rp, sinceis a reproducing kernel. In particular, it holds true for yi=(xi),
i=1;:::; n. Rule 4 is easy to show for kernels 1;2that admit a representation of the form
(6.17), since
1(x;x0)2(x;x0)=0BBBBB@X
i>1(1)
i(x)(1)
i(x0)1CCCCCA0BBBBBB@X
j>1(2)
j(x)(2)
j(x0)1CCCCCCA
=X
i;j>1(1)
i(x)(2)
j(x)(1)
i(x0)(2)
j(x0)
=X
k>1k(x)k(x0)=:(x;x0);
showing that =12also admits a representation of the form (6.17), where the new (pos-
sibly inÔ¨Ånite) sequence of features ( k) is identiÔ¨Åed in a one-to-one way with the sequence
((1)
i(2)
j). We leave the proof of rule 5 as an exercise (Exercise 8). 230 6.5. Representer Theorem
Example 6.10 (Polynomial Kernel) Consider x;x02R2with
(x;x0)=(1+hx;x0i)2;
wherehx;x0i=x>x0. This is an example of a polynomial kernel polynomial
kernel. Combining the fact that
sums and products of kernels are again kernels (rules 3 and 4 of Theorem 6.5), we Ô¨Ånd that,
sincehx;x0iand the constant function 1 are kernels, so are 1 +hx;x0iand (1 +hx;x0i)2. By
writing
(x;x0)=(1+x1x0
1+x2x0
2)2
=1+2x1x0
1+2x2x0
2+2x1x2x0
1x0
2+(x1x0
1)2+(x2x0
2)2;
we see that(x;x0) can be written as the inner product in R6of the two feature vectors (x)
and(x0), where the feature map :R2!R6can be explicitly identiÔ¨Åed as
(x)=[1;p
2x1;p
2x2;p
2x1x2;x2
1;x2
2]>:
Thus, the RKHS determined by can be explicitly identiÔ¨Åed with the space of functions
x7!(x)>for some2R6.
In the above example we could explicitly identify the feature map. However, in general
a feature map need not be explicitly available. Using a particular reproducing kernel cor-
responds to using an implicit (possibly inÔ¨Ånite dimensional!) feature map that never needs
to be explicitly computed.
6.5 Representer Theorem
Recall the setting discussed at the beginning of this chapter: we are given training data
=f(xi;yi)gn
i=1and a loss function that measures the Ô¨Åt to the data, and we wish to Ô¨Ånd
a function gthat minimizes the training loss, with the addition of a regularization term,
as described in Section 6.2. To do this, we assume Ô¨Årst that the class Gof prediction
functions can be decomposed as the direct sum of an RKHS H, deÔ¨Åned by a kernel function
:XX!R, and another linear space of real-valued functions H0onX; that is,
G=HH 0;
meaning that any element g2Gcan be written as g=h+h0, with h2H andh02H 0.
In minimizing the training loss we wish to penalize the hterm of gbut not the h0term.
SpeciÔ¨Åcally, the aim is to solve the functional optimization problem
min
g2HH 01
nnX
i=1Loss( yi;g(xi))+kgk2
H: (6.19)
Here, we use a slight abuse of notation: kgkHmeanskhkHifg=h+h0, as above. In this
way, we can view H0as the null space of the functional g7!kgkH. This null space may be
empty, but typically has a small dimension m; for example it could be the one-dimensional
space of constant functions, as in Example 6.2. +217Chapter 6. Regularization and Kernel Methods 231
Example 6.11 (Null Space) Consider again the setting of Example 6.2, for which we
have feature vectors ex=[1;x>]>andGconsists of functions of the form g:ex7!0+x>.
Each function gcan be decomposed as g=h+h0, where h:ex7!x>, and h0:ex7!0.
Given g2G, we havekgkH=kk, and so the null space H0of the functional g7!kgkH
(that is, the set of all functions g2Gfor whichkgkH=0) is the set of constant functions
here, which has dimension m=1.
Regularization favors elements in H0and penalizes large elements in H. As the reg-
ularization parameter varies between zero and inÔ¨Ånity, solutions to (6.19) vary from
‚Äúcomplex‚Äù ( g2HH 0) to ‚Äúsimple‚Äù ( g2H 0).
A key reason why RKHSs are so useful is the following. By choosing Hto be an
RKHS in (6.19) this functional optimization problem e ectively becomes a parametric
optimization problem. The reason is that any solution to (6.19) can be represented as a
Ô¨Ånite-dimensional linear combination of kernel functions, evaluated at the training sample.
This is known as the kernel trick kernel trick .
Theorem 6.6: Representer Theorem
The solution to the penalized optimization problem (6.19) is of the form
g(x)=nX
i=1i(xi;x)+mX
j=1jqj(x); (6.20)
wherefq1;:::; qmgis a basis ofH0.
Proof: LetF=Spanxi;i=1;:::; n	. Clearly,FH . Then, the Hilbert space Hcan
be represented as H=FF?, whereF?is the orthogonal complement of F. In other
words,F?is the class of functions
ff?2H :hf?;fiH=0;f2Fgf f?:hf?;xiiH=0;8ig:
It follows, by the reproducing kernel property, that for all f?2F?:
f?(xi)=hf?;xiiH=0;i=1;:::; n:
Now, take any g2HH 0, and write it as g=f+f?+h0, with f2F;f?2F?, and
h02H 0. By the deÔ¨Ånition of the null space H0, we havekgk2
H=kf+f?k2
H. Moreover, by
Pythagoras‚Äô theorem, the latter is equal to kfk2
H+kf?k2
H. It follows that
1
nnX
i=1Loss( yi;g(xi))+kgk2
H=1
nnX
i=1Loss( yi;f(xi)+h0(xi))+
kfk2
H+kf?k2
H
>1
nnX
i=1Loss( yi;f(xi)+h0(xi))+kfk2
H:
Since we can obtain equality by taking f?=0, this implies that the minimizer of the pen-
alized optimization problem (6.19) lies in the subspace FH 0ofG=HH 0, and hence
is of the form (6.20). 232 6.5. Representer Theorem
Substituting the representation (6.20) of ginto (6.19) gives the Ô¨Ånite-dimensional op-
timization problem:
min
2Rn;2Rm1
nnX
i=1Loss( yi;(K+Q)i)+>K; (6.21)
where
¬àKis the nn(Gram) matrix with entries [ (xi;xj);i=1;:::; n;j=1;:::; n].
¬àQis the nmmatrix with entries [ qj(xi);i=1;:::; n;j=1;:::; m].
In particular, for the squared-error loss we have
min
2Rn;2Rm1
ny (K+Q)2+>K: (6.22)
This is a convex optimization problem, and its solution is found by di erentiating (6.22)
with respect to andand equating to zero, leading to the following system of ( n+m)
linear equations:"KK>+nK KQ
Q>K>Q>Q#"
#
="K>
Q>#
y: (6.23)
As long as Qis of full column rank, the minimizing function is unique.
Example 6.12 (Ridge Regression (cont.)) We return to Example 6.2 and identify that
His the RKHS with linear kernel function (x;x0)=x>x0andC=H0is the linear space of
constant functions. In this case, H0is spanned by the function q11. Moreover, K=XX>
andQ=1.
If we appeal to the representer theorem directly, then the problem in (6.6) becomes, as
a result of (6.21):
min
;01
ny 01 XX>2+kX>k2:
This is a convex optimization problem, and so the solution follows by taking derivatives
and setting them to zero. This gives the equations
XX> (XX>+nIn)+01 y=0;
and
n0=1>(y XX>):
Note that these are equivalent to (6.8) and (6.9) (once again assuming that n>pandXhas
full rank p). Equivalently, the solution is found by solving (6.23):
"XX>XX>+nXX>XX>1
1>XX>n#"
0#
="XX>
1>#
y:
This is a system of ( n+1) linear equations, and is typically of much larger dimension than
the ( p+1) linear equations given by (6.8) and (6.9). As such, one may question the prac-
ticality of reformulating the problem in this way. However, the beneÔ¨Åt of this formulation
is that the problem can be expressed entirely through the Gram matrix K, without having
to explicitly compute the feature vectors ‚Äî in turn permitting the (implicit) use of inÔ¨Ånite
dimensional feature spaces.Chapter 6. Regularization and Kernel Methods 233
Example 6.13 (Estimating the Peaks Function) Figure 6.4 shows the surface plot of
thepeaks function:
f(x1;x2)=3(1 x1)2e x2
1 (x2+1)2 10x1
5 x3
1 x5
2
e x2
1 x2
2 1
3e (x1+1)2 x2
2: (6.24)
The goal is to learn the function y=f(x) based on a small set of training data (pairs of
(x;y) values). The red dots in the Ô¨Ågure represent data =f(xi;yi)g20
i=1, where yi=f(xi) and
thefxighave been chosen in a quasi-random quasi -random way, using Hammersley points (with bases 2
and 3) on the square [  3;3]2. Quasi-random point sets have better space-Ô¨Ålling properties
than either a regular grid of points or a set of pseudo-random points. We refer to [71] for
details. Note that there is no observation noise in this particular problem.
-5
2 -20
0 05
-2 2
Figure 6.4: Peaks function sampled at 20 Hammersley points.
The purpose of this example is to illustrate how, using the small data set of size n=20,
the entire peaks function can be approximated well using kernel methods. In particular, we
use the Gaussian kernel (6.15) on R2, and denote byHthe unique RKHS corresponding
to this kernel. We omit the regularization term in (6.19), and thus our objective is to Ô¨Ånd
the solution to
min
g2H1
nnX
i=1(yi g(xi))2:
By the representer theorem, the optimal function is of the form
g(x)=nX
i=1iexp 
 1
2kx xik2
2!
;
where:=[1;:::; n]>is, by (6.23), the solution to the set of linear equations KK>=
Ky.
Note that we are performing regression over the class of functions Hwith an implicit
feature space. Due to the representer theorem, the solution to this problem coincides with
the solution to the linear regression problem for which the i-th feature (for i=1;:::; n) is
chosen to be the vector [ (x1;xi);:::; (xn;xi)]>.
The following code performs these calculations and gives the contour plots of gand
thepeaks functions, shown in Figure 6.5. We see that the two are quite close. Code for the
generation of Hammersley points is available from the book‚Äôs GitHub site as genham.py .234 6.5. Representer Theorem
peakskernel.py
from genham import hammersley
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
from numpy.linalg import norm
import numpy as np
def peaks(x,y):
z = (3*(1-x)**2 * np.exp(-(x**2) - (y+1)**2)
- 10*(x/5 - x**3 - y**5) * np.exp(-x**2 - y**2)
- 1/3 * np.exp(-(x+1)**2 - y**2))
return (z)
n = 20
x = -3 + 6*hammersley([2,3],n)
z = peaks(x[:,0],x[:,1])
xx, yy = np.mgrid[-3:3:150j,-3:3:150j]
zz = peaks(xx,yy)
plt.contour(xx,yy,zz,levels=50)
fig=plt.figure()
ax = fig.add_subplot(111,projection= '3d')
ax.plot_surface(xx,yy,zz,rstride=1,cstride=1,color= 'c',alpha=0.3,
linewidth=0)
ax.scatter(x[:,0],x[:,1],z,color= 'k',s=20)
plt.show()
sig2 = 0.3 # kernel parameter
def k(x,u):
return (np.exp(-0.5*norm(x- u)**2/sig2))
K = np.zeros((n,n))
for iin range (n):
for jin range (n):
K[i,j] = k(x[i,:],x[j])
alpha = np.linalg.solve(K@K.T, K@z)
N, = xx.flatten().shape
Kx = np.zeros((n,N))
for iin range (n):
for jin range (N):
Kx[i,j] = k(x[i,:],np.array([xx.flatten()[j],yy.flatten()[j
]]))
g = Kx.T @ alpha
dim = np.sqrt(N).astype( int)
yhat = g.reshape(dim,dim)
plt.contour(xx,yy,yhat ,levels=50)Chapter 6. Regularization and Kernel Methods 235
-2 0 2-3-2-10123
-2 0 2
Figure 6.5: Contour plots for the prediction function g(left) and the peaks function given
in (6.24) (right).
6.6 Smoothing Cubic Splines
A striking application of kernel methods is to Ô¨Åtting ‚Äúwell-behaved‚Äù functions to data.
Key examples of ‚Äúwell-behaved‚Äù functions are those that do not have large second-
order derivatives. Consider functions g: [0;1]!Rthat are twice di erentiable and deÔ¨Åne
kg00k2:=R1
0(g00(x))2dxas a measure of the size of the second derivative.
Example 6.14 (Behavior of kg00k2)Intuitively, the larger kg00k2is, the more ‚Äúwiggly‚Äù
the function gwill be. As an explicit example, consider g(x)=sin(!x) for x2[0;1], where
!is a free parameter. We can explicitly compute g00(x)= !2sin(!x), and consequently
kg00k2=Z1
0!4sin2(!x) dx=!4
2(1 sinc(2!)):
Asj!j!1 , the frequency of gincreases and we have kg00k2!1 .
Now, in the context of data Ô¨Åtting, consider the following penalized least-squares op-
timization problem on [0 ;1]:
min
g2G1
nnX
i=1(yi g(xi))2+kg00k2; (6.25)
where we will specify Gin what follows. In order to apply the kernel machinery, we want
to write this in the form (6.19), for some RKHS Hand null spaceH0. Clearly, the norm on
Hshould be of the form kgkH=kg00kand should be well-deÔ¨Åned (i.e., Ô¨Ånite and ensuring
gandg0are absolutely continuous). This suggests that we take
H=fg2L2[0;1] :kg00k<1;g;g0absolutely continuous ;g(0)=g0(0)=0g;
with inner product
hf;giH:=Z1
0f00(x)g00(x) dx:236 6.6. Smoothing Cubic Splines
One rationale for imposing the boundary conditions g(0)=g0(0)=0 is as follows: when
expanding gabout the point x=0, Taylor‚Äôs theorem (with integral remainder term) states
that
g(x)=g(0)+g0(0)x+Zx
0g00(s) (x s) ds:
Imposing the condition that g(0)=g0(0)=0 for functions in Hwill ensure thatG=
HH 0where the null space H0contains only linear functions, as we will see.
To see that thisHis in fact an RKHS, we derive its reproducing kernel. Using integra-
tion by parts (or directly from the Taylor expansion above), write
g(x)=Zx
0g0(s) ds=Zx
0g00(s) (x s) ds=Z1
0g00(s) (x s)+ds:
Ifis a kernel, then by the reproducing property it must hold that
g(x)=hg;xiH=Z1
0g00(s)00
x(s) ds;
so thatmust satisfy@2
@s2(x;s)=(x s)+, where y+:=maxfy;0g. Therefore, noting that
(x;u)=hx;uiH, we have (see Exercise 15)
(x;u)=Z1
0@2(x;s)
@s2@2(u;s)
@s2ds=maxfx;ugminfx;ug2
2 minfx;ug3
6:
The last expression is a cubic function with quadratic and cubic terms that misses the
constant and linear monomials. This is not surprising considering the Taylor‚Äôs theorem
interpretation of a function g2H . If we now takeH0as the space of functions of the
following form (having zero second derivative):
h0=1+2x;x2[0;1];
then (6.25) is exactly of the form (6.19).
As a consequence of the representer Theorem 6.6, the optimal solution to (6.25) is a
linear combination of piecewise cubic functions:
g(x)=1+2x+nX
i=1i(xi;x): (6.26)
Such a function is called a cubic spline cubic spline with n knots (with one knot at each data point xi)
‚Äî so called, because the piecewise cubic function between knots is required to be ‚Äútied
together‚Äù at the knots. The parameters ;are determined from (6.21) for instance by
solving (6.23) with matrices K=[(xi;xj)]n
i;j=1andQwith i-th row of the form [1 ;xi] for
i=1;:::; n.
Example 6.15 (Smoothing Spline) Figure 6.6 shows various cubic smoothing splines
for the data (0 :05;0:4);(0:2;0:2);(0:5;0:6);(0:75;0:7);(1;1). In the Ô¨Ågure, we use the re-
parameterization r=1=(1+n) for the smoothing parameter. Thus r2[0;1], where r=0
means an inÔ¨Ånite penalty for curvature (leading to the ordinary linear regression solution)Chapter 6. Regularization and Kernel Methods 237
andr=1 does not penalize curvature at all and leads to a perfect Ô¨Åt via the so-called nat-
ural spline . Of course the latter will generally lead to overÔ¨Åtting. For rfrom 0 up to 0.8 the
solutions will be close to the simple linear regression line, while only for rvery close to 1,
the shape of the curve changes signiÔ¨Åcantly.
0 0.2 0.4 0.6 0.8 100.20.40.60.81
Figure 6.6: Various cubic smoothing splines for smoothing parameter r=1=(1+n)2
f0:8;0:99;0:999;0:999999g. For r=1, the natural spline through the data points is ob-
tained; for r=0, the simple linear regression line is found.
The following code Ô¨Årst computes the matrices KandQ, and then solves the linear
system (6.23). Finally, the smoothing curve is determined via (6.26), for selected points,
and then plotted. Note that the code plots only a single curve corresponding to the speciÔ¨Åed
value of p.
smoothspline.py
import matplotlib.pyplot as plt
import numpy as np
x = np.array([[0.05, 0.2, 0.5, 0.75, 1.]]).T
y = np.array([[0.4, 0.2, 0.6, 0.7, 1.]]).T
n = x.shape[0]
r = 0.999
ngamma = (1-r)/r
k = lambda x1, x2 : (1/2)* np. max((x1,x2)) * np. min((x1,x2)) ** 2 \
- ((1/6)* np. min((x1,x2))**3)
K = np.zeros((n,n))
for iin range (n):
for jin range (n):
K[i,j] = k(x[i], x[j])
Q = np.hstack((np.ones((n,1)), x))
m1 = np.hstack((K @ K.T + (ngamma * K), K @ Q))238 6.7. Gaussian Process Regression
m2 = np.hstack((Q.T @ K.T, Q.T @ Q))
M = np.vstack((m1,m2))
c = np.vstack((K, Q.T)) @ y
ad = np.linalg.solve(M,c)
# plot the curve
xx = np.arange(0,1+0.01,0.01).reshape(-1,1)
g = np.zeros_like(xx)
Qx = np.hstack((np.ones_like(xx), xx))
g = np.zeros_like(xx)
N = np.shape(xx)[0]
Kx = np.zeros((n,N))
for iin range (n):
for jin range (N):
Kx[i,j] = k(x[i], xx[j])
g = g + np.hstack((Kx.T, Qx)) @ ad
plt.ylim((0,1.15))
plt.plot(xx, g, label = 'r = {} '.format (r), linewidth = 2)
plt.plot(x,y, 'b.', markersize=15)
plt.xlabel( '$x$')
plt.ylabel( '$y$')
plt.legend()
6.7 Gaussian Process Regression
Another application of the kernel machinery is to Gaussian process regression. A Gaussian
process Gaussian
process(GP) on a spaceXis a stochastic process fZx;x2Xg where, for any choice of
indices x1;:::; xn, the vector [ Zx1;:::Zxn]>has a multivariate Gaussian distribution. As
such, the distribution of a GP is completely speciÔ¨Åed by its mean and covariance functions
:X!Rand:XX!R, respectively. The covariance function is a Ô¨Ånite positive
semideÔ¨Ånite function, and hence, in view of Theorem 6.2, can be viewed as a reproducing
kernel onX.
As for ordinary regression, the objective of GP regression is to learn a regression func- +168
tiongthat predicts a response y=g(x) for each feature vector x. This is done in a Bayesian
fashion, by establishing (1) a prior pdf for gand (2) the likelihood of the data, for a given
g. From these two we then derive, via Bayes‚Äô formula, the posterior distribution of ggiven
the data. We refer to Section 2.9 for the general Bayesian framework. +47
A simple Bayesian model for GP regression is as follows. First, the prior distribution of
gis taken to be the distribution of a GP with some known mean function and covariance
function (that is, kernel) . Most often is taken to be a constant, and for simplicity of
exposition, we take it to be 0. The Gaussian kernel (6.15) is often used for the covariance
function. For radial basis function kernels (including the Gaussian kernel), points that areChapter 6. Regularization and Kernel Methods 239
closer will be more highly correlated or ‚Äúsimilar‚Äù [97], independent of translations in space.
Second, similar to standard regression, we view the observed feature vectors x1;:::; xn
as Ô¨Åxed and the responses y1;:::; ynas outcomes of random variables Y1;:::; Yn. SpeciÔ¨Åc-
ally, given g, we model thefYigas
Yi=g(xi)+"i;i=1;:::; n; (6.27)
wheref"igiidN(0;2). To simplify the analysis, let us assume that 2is known, so no prior
needs to be speciÔ¨Åed for 2. Let g=[g(x1);:::; g(xn)]>be the (unknown) vector of re-
gression values. Placing a GP prior on the function gis equivalent to placing a multivariate
Gaussian prior on the vector g:
gN(0;K); (6.28)
where the covariance matrix Kofgis a Gram matrix (implicitly associated with a feature
map through the kernel ), given by:
K=26666666666666664(x1;x1)(x1;x2):::  (x1;xn)
(x2;x1)(x2;x2):::  (x2;xn)
::::::::::::
(xn;x1)(xn;x2):::  (xn;xn)37777777777777775: (6.29)
The likelihood of our data given g, denoted p(yjg), is obtained directly from the model
(6.27):
(Yjg)N(g;2In): (6.30)
Solving this Bayesian problem involves deriving the posterior distribution of ( gjY). To
do so, we Ô¨Årst note that since Yhas covariance matrix K+2In(which can be seen from
(6.27)), the joint distribution of Yandgis again normal, with mean 0and covariance
matrix:
Ky;g="K+2InK
K K#
: (6.31)
The posterior can then be found by conditioning on Y=y, via Theorem C.8, giving +436
(gjy)N
K>(K+2In) 1y;K K>(K+2In) 1K
:
This only gives information about gat the observed points x1;:::; xn. It is more interesting
to consider the posterior predictive distribution of eg:=g(ex) for a new input ex. We can Ô¨Ånd
the corresponding posterior predictive pdf p(egjy) by integrating out the joint posterior pdf
p(eg;gjy), which is equivalent to taking the expectation of p(egjg) when gis distributed
according to the posterior pdf p(gjy); that is,
p(egjy)=Z
p(egjg)p(gjy) dg:
To do so more easily than direct evaluation via the above integral representation of p(egjy),
we can begin with the joint distribution of [ y>;eg]>, which is multivariate normal with mean
0and covariance matrix
eK="K+2In
>(ex;ex)#
; (6.32)240 6.7. Gaussian Process Regression
where=[(ex;x1);:::; (ex;xn)]>. It now follows, again by using Theorem C.8, that ( egjy)
has a normal distribution with mean and variance given respectively by
(ex)=>(K+2In) 1y (6.33)
and
2(ex)=(ex;ex) >(K+2In) 1: (6.34)
These are sometimes called the predictive predictive mean and variance. It is important to note that
we are predicting the expected responseEeY=g(ex) here, and not the actual response eY.
Example 6.16 (GP Regression) Suppose the regression function is
g(x)=2 sin(2x);x2[0;1]:
We use GP regression to estimate g, using a Gaussian kernel of the form (6.15) with band-
width parameter 0 :2. The explanatory variables x1;:::; x30were drawn uniformly on the
interval [0;1], and the responses were obtained from (6.27), with noise level =0:5. Fig-
ure 6.7 shows 10 samples from the prior distribution for gas well as the data points and
the true sinusoidal regression function g.
0 0.2 0.4 0.6 0.8 1
x-3-2-10123y
0 0.2 0.4 0.6 0.8 1
x-3-2-10123y
Figure 6.7: Left: samples drawn from the GP prior distribution. Right: the true regression
function with the data points.
Again assuming that the variance 2, is known, the predictive distribution as determ-
ined by (6.33) and (6.34) is shown in Figure 6.8 for bandwidth 0 :2 (left) and 0 :02 (right).
Clearly, decreasing the bandwidth leads to the covariance between points xandx0decreas-
ing at a faster rate with respect to the squared distance kx x0k2, leading to a predictive
mean that is less smooth.
In the above exposition, we have taken the mean function for the prior distribution
ofgto be identically zero. If instead we have a general mean function mand write
m=[m(x1);:::; m(xn)]>then the predictive variance (6.34) remains unchanged, and the
predictive mean (6.33) is modiÔ¨Åed to read
(ex)=m(ex)+>(K+2In) 1(y m): (6.35)Chapter 6. Regularization and Kernel Methods 241
0 0.2 0.4 0.6 0.8 1
x-3-2-10123yg(x)
Predictive Mean
0 0.2 0.4 0.6 0.8 1
x-3-2-10123yg(x)
Predictive Mean
Figure 6.8: GP regression of synthetic data set with bandwidth 0 :2 (left) and 0.02 (right).
The black dots represent the data and the blue curve is the latent function g(x)=2 sin(2x).
The red curve is the mean of the GP predictive distribution given by (6.33), and the shaded
region is the 95% conÔ¨Ådence band, corresponding to the predictive variance given in (6.34).
Typically, the variance 2appearing in (6.27) is not known, and the kernel itself
depends on several parameters ‚Äî for instance a Gaussian kernel (6.15) with an unknown
bandwidth parameter. In the Bayesian framework, one typically speciÔ¨Åes a hierarchical
model by introducing a prior p() for the vector of such hyperparameters hyperparamet -
ers. Now, the
GP prior ( gj) (equivalently, specifying p(gj)) and the model for the likelihood of the
data given Yjg;, namely p(yjg;), are both dependent on . The posterior distribution of
(gjy;) is as before.
One approach to setting the hyperparameter is to determine its posterior p(jy) and
obtain a point estimate, for instance via its maximum a posteriori estimate. However, this
can be a computationally demanding exercise. What is frequently done in practice is to
consider instead the marginal likelihood p (yj) and maximize this with respect to . This
procedure is called empirical Bayes empirical Bayes .
Considering again the mean function mto be identically zero, from (6.31), we have
that ( Yj) is multivariate normal with mean 0and covariance matrix Ky=K+2In,
immediately giving an expression for the marginal log-likelihood:
lnp(yj)= n
2ln(2) 1
2lnjdet(Ky)j 1
2y>K 1
yy: (6.36)
We notice that only the second and third terms in (6.36) depend on . Considering a partial
derivative of (6.36) with respect to a single element of the hyperparameter vector yields
@
@lnp(yj)= 1
2tr 
K 1
y"@
@Ky#!
+1
2y>K 1
y"@
@Ky#
K 1
yy; (6.37)
whereh
@
@Kyi
is the element-wise derivative of matrix Kywith respect to . If these partial
derivatives can be computed for each hyperparameter , gradient information could be used
when maximizing (6.36).242 6.8. Kernel PCA
Example 6.17 (GP Regression (cont.)) Continuing Example 6.16, we plot in Fig-
ure 6.9 the marginal log-likelihood as a function of the noise level and bandwidth para-
meter.
10-210-110010-1100
Figure 6.9: Contours of the marginal log-likelihood for the GP regression example. The
maximum is denoted by a cross.
The maximum is attained for a bandwidth parameter around 0 :20 and0:44, which
is very close to the left panel of Figure 6.8 for the case where was assumed to be known
(and equal to 0 :5). We note here that the marginal log-likelihood is extremely Ô¨Çat, perhaps
owing to the small number of points.
6.8 Kernel PCA
In its basic form, kernel PCA (principal component analysis) can be thought of as PCA in
feature space. The main motivation for PCA introduced in Section 4.8 was as a dimension- +153
ality reduction technique. There, the analysis rested on an SVD of the matrix b=1
nX>X,
where the data in Xwas Ô¨Årst centered via x0
i;j=xi;j xjwhere xi=1
nPn
i=1xi;j.
What we shall do is to Ô¨Årst re-cast the problem in terms of the Gram matrix K=XX>=
[hxi;xji] (note the di erent order of XandX>), and subsequently replace the inner product
hx;x0iwith(x;x0) for a general reproducing kernel . To make the link, let us start with
an SVD of X>:
X>=UDV>: (6.38)
The dimensions of X>,U,D, and Varedn,dd,dn, and nn, respectively. Then an
SVD of X>Xis
X>X=(UDV>)(UDV>)>=U(DD>)U>
and an SVD of Kis
K=(UDV>)>(UDV>)=V(D>D)V>:
Let1>>r>0 denote the non-zero eigenvalues of X>X(or, equivalently, of K) and
denote the corresponding rrdiagonal matrix by . Without loss of generality we canChapter 6. Regularization and Kernel Methods 243
assume that the eigenvector of X>Xcorresponding to kis the k-th column of Uand that
thek-th column of Vis an eigenvector of K. Similar to Section 4.8, let UkandVkcontain +153
the Ô¨Årst kcolumns of UandV, respectively, and let kbe the corresponding kksubmatrix
of,k=1;:::; r.
By the SVD (6.38), we have X>Vk=UDV>Vk=Uk1=2
k. Next, consider the projection
of a point xonto the k-dimensional linear space spanned by the columns of Uk‚Äî the Ô¨Årst
kprincipal components. We saw in Section 4.8 that this projection simply is the linear
mapping x7!U>
kx. Using the fact that Uk=X>Vk 1=2, we Ô¨Ånd that xis projected to a
point zgiven by
z= 1=2
kV>
kXx= 1=2
kV>
kx;
where we have (suggestively) deÔ¨Åned x:=[hx1;xi;:::;hxn;xi]>. The important point
is that zis completely determined by the vector of inner products xand the kprincipal
eigenvalues and (right) eigenvectors of the Gram matrix K. Note that each component zm
ofzis of the form
zm=nX
i=1m;i(xi;x);m=1;:::; k: (6.39)
The preceding discussion assumed centering of the columns of X. Consider now an
uncentered data matrix eX. Then the centered data can be written as X=eX 1
nEneX, where
Enis the nnmatrix of ones. Consequently,
XX>=eXeX> 1
nEneXeX> 1
neXeX>En+1
n2EneXeX>En;
or, more compactly, XX>=HeXeX>H, where H=In 1
n1n1>
n,Inis the nnidentity matrix,
and1nis the n1 vector of ones.
To generalize to the kernel setting, we replace eXeX>byK=[(xi;xj);i;j=1;:::; n]
and setx=[(x1;x);:::; (xn;x)]>, so that kis the diagonal matrix of the klargest eigen-
values of HKH andVkis the corresponding matrix of eigenvectors. Note that the ‚Äúusual‚Äù
PCA is recovered when we use the linear kernel (x;y)=x>y. However, instead of having
only kernels that are explicitly inner products of feature vectors, we are now permitted to
implicitly use inÔ¨Ånite feature maps (functions) by using kernels.
Example 6.18 (Kernel PCA) We simulated 200 points, x1;:::; x200, from the uniform
distribution on the set B1[(B4\Bc
3), where Br:=f(x;y)2R2:x2+y26r2g(disk with
radius r). We apply kernel PCA with Gaussian kernel (x;x0)=exp
 kx x0k2
and
compute the functions zm(x);m=1;:::; 9 in (6.39). Their density plots are shown in Fig-
ure 6.10. The data points are superimposed in each plot. From this we see that the principal
components identify the radial structure present in the data. Finally, Figure 6.11 shows
the projections [ z1(xi);z2(xi)]>;i=1;:::; 200 of the original data points onto the Ô¨Årst two
principal components. We see that the projected points can be separated by a straight line,
whereas this is not possible for the original data; see also, Example 7.6 for a related prob- +272
lem.244 6.8. Kernel PCA
Figure 6.10: First nine eigenfunctions using a Gaussian kernel for the two-dimensional
data set formed by the red and cyan points.
-0.4 -0.2 0 0.2 0.4 0.6 0.8-0.8-0.6-0.4-0.200.20.40.6
Figure 6.11: Projection of the data onto the Ô¨Årst two principal components. Observe that
already the projections of the inner and outer points are well separated.Chapter 6. Regularization and Kernel Methods 245
Further Reading
For a good overview of the ridge regression and the lasso, we refer the reader to [36, 56].
For overviews of the theory of RKHS we refer to [3, 115, 126], and for in-depth background
on splines and their connection to RKHSs we refer to [123]. For further details on GP
regression we refer to [97] and for kernel PCA in particular we refer to [12, 92]. Finally,
many facts about kernels and their corresponding RKHSs can be found in [115].
Exercises
1. LetGbe an RKHS with reproducing kernel . Show that is a positive semideÔ¨Ånite
function.
2. Show that a reproducing kernel, if it exists, is unique.
3. LetGbe a Hilbert space of functions g:X!R. Recall that the evaluation func-
tional is the mapx:g7!g(x) for a given x2X. Show that evaluation functionals
are linear operators.
4. LetG0be the pre-RKHS G0constructed in the proof of Theorem 6.2. Thus, g2G 0
is of the form g=Pn
i=1ixiand
hg;xiG0=nX
i=1ihxi;xiG0=nX
i=1i(xi;x)=g(x):
Therefore, we may write the evaluation functional of g2G 0atxasxg:=hg;xiG0.
Show thatxis bounded onG0for every x; that is,jxfj<kfkG0, for some<1.
5. Continuing Exercise 4, let ( fn) be a Cauchy sequence in G0such thatjfn(x)j!0 for
allx. Show thatkfnkG0!0.
6. Continuing Exercises 5 and 4, to show that the inner product (6.14) is well deÔ¨Åned,
a number of facts have to be checked.
(a) Verify that the limit converges.
(b) Verify that the limit is independent of the Cauchy sequences used.
(c) Verify that the properties of an inner product are satisÔ¨Åed. The only non-trivial
property to verify is that hf;fiG=0 if and only if f=0.
7. Exercises 4‚Äì6 show that GdeÔ¨Åned in the proof of Theorem 6.2 is an inner product
space. It remains to prove that Gis an RKHS. This requires us to prove that the inner
product spaceGis complete (and thus Hilbert), and that its evaluation functionals
are bounded and hence continuous (see Theorem A.16). This is done in a number of +389
steps.
(a) Show thatG0is dense inGin the sense that every f2Gis a limit point (with
respect to the norm on G) of a Cauchy sequence ( fn) inG0.246 Exercises
(b) Show that every evaluation functional xonGis continuous at the 0 function.
That is,
8">0 :9>0 :8f2G:kfkG<)jf(x)j<": (6.40)
Continuity of xat all functions g2Gthen follows automatically from linearity.
(c) Show thatGis complete; that is, every Cauchy sequence ( fn)2Gconverges in
the normjjjjG.
8. If1and2are kernels onXandY, then+((x;y);(x0;y0)) :=1(x;x0)+2(y;y0)
and((x;y);(x0;y0) :=1(x;x0)2(y;y0) are kernels on the Cartesian product XY .
Prove this.
9. An RKHS enjoys the following desirable smoothness property: if ( gn) is a sequence
belonging to RKHS GonX, andkgn gkG!0, then g(x)=lim ngn(x) for all x2X.
Prove this, using Cauchy‚ÄìSchwarz.
10. Let Xbe anRd-valued random variable that is symmetric about the origin (that is,
Xand ( X) are identically distributed). Denote by is its distribution and  (t)=
Eeit>X=R
eit>x(dx) for t2Rdis its characteristic function. Verify that (x;x0)=
 (x x0) is a real-valued positive semideÔ¨Ånite function.
11. Suppose an RKHS Gof functions from X!R(with kernel ) is invariant under a
groupTof transformations T:X!X ; that is, for all f;g2GandT2T, we have
(i)fT2Gand (ii)hfT;gTiG=hf;giG. Show that (Tx;Tx0)=(x;x0) for
allx;x02XandT2T.
12. Given two Hilbert spaces HandG, we call a mapping A:H!G aHilbert space
isomorphism Hilbert space
isomorphismif it is
(i) a linear map; that is, A(a f+bg)=aA(f)+bA(g) for any f;g2H anda;b2R.
(ii) a surjective map; and
(iii) an isometry; that is, for all f;g2H, it holds thathf;giH=hA f;AgiG.
LetH=Rp(equipped with the usual Euclidean inner product) and construct its
(continuous) dual spaceG, consisting of all continuous linear functions from Rpto
R, as follows: (a) For each 2Rp, deÔ¨Åne g:Rp!Rviag(x)=h;xi=>x, for
allx2Rp. (b) EquipGwith the inner product hg;giG:=>.
Show that A:H!G deÔ¨Åned by A()=gfor2Rpis a Hilbert space isomorph-
ism.
13. Let Xbe an npmodel matrix. Show that X>X+nIpfor>0 is invertible.
14. As Example 6.8 clearly illustrates, the pdf of a random variable that is symmetric
about the origin is not in general a valid reproducing kernel. Take two such iid ran-
dom variables XandX0with common pdf f, and deÔ¨Åne Z=X+X0. Denote by  Z
andfZthe characteristic function and pdf of Z, respectively.
Show that if  Zis in L1(R),fZis a positive semideÔ¨Ånite function. Use this to show
that(x;x0)=fZ(x x0)=1fjx x0j62g(1 jx x0j=2) is a valid reproducing kernel.Chapter 6. Regularization and Kernel Methods 247
15. For the smoothing cubic spline of Section 6.6, show that (x;u)=maxfx;ugminfx;ug2
2 
minfx;ug3
6.
16. Let Xbe an npmodel matrix and let u2Rpbe the unit-length vector with k-th
entry equal to one ( uk=kuk=1). Suppose that the k-th column of Xisvand that it
is replaced with a new predictor w, so that we obtain the new model matrix:
eX=X+(w v)u>:
(a) Denoting
:=X>(w v)+kw vk2
2u;
show that
eX>eX=X>X+u>+u>=X>X+(u+)(u+)>
2 (u )(u )>
2:
In other words, eX>eXdiers from X>Xby a symmetric matrix of rank two.
(b) Suppose that B:=(X>X+nIp) 1is already computed. Explain how the
Sherman‚ÄìMorrison formulas in Theorem A.10 can be applied twice to com- +371
pute the inverse and log-determinant of the matrix eX>eX+nIpinO((n+p)p)
computing time, rather than the usual O((n+p2)p) computing time.3
(c) Write a Python program for updating a matrix B=(X>X+nIp) 1when we
change the k-th column of X, as shown in the following pseudo-code.
Algorithm 6.8.1: Updating via Sherman‚ÄìMorrison Formula
input: Matrices XandB, index k, and replacement wfor the k-th column of X.
output: Updated matrices XandB.
1Setv2Rnto be the k-th column of X.
2Setu2Rpto be the unit-length vector such that uk=kuk=1.
3B B Bu>B
1+>Bu
4B B Bu>B
1+u>B
5Update the k-th column of Xwithw.
6return X;B
17. Use Algorithm 6.8.1 from Exercise 16 to write Python code that computes the ridge
regression coe cientin (6.5) and use it to replicate the results on Figure 6.1. The +217
following pseudo-code (with running cost of O((n+p)p2)) may help with the writing
of the Python code.
3This Sherman‚ÄìMorrison updating is not always numerically stable. A more numerically stable method
will perform two consecutive rank-one updates of the Cholesky decomposition of X>X+nIp.248 Exercises
Algorithm 6.8.2: Ridge Regression Coe cients via Sherman‚ÄìMorrison Formula
input: Training setfX;ygand regularization parameter >0.
output: Solution b=(nIp+X>X) 1X>y.
1SetAto be an npmatrix of zeros and B (nIp) 1.
2forj=1;:::; pdo
3 Setwto be the j-th column of X.
4 UpdatefA;Bgvia Algorithm 6.8.1 with inputs fA;B;j;wg.
5b B(X>y)
6return b
18. Consider Example 2.10 with D=diag(1;:::; p) for some nonnegative vector 2 +55
Rp, so that twice the negative logarithm of the model evidence can be written as
 2 lng(y)=l() :=nln[y>(I XX>)y]+lnjDj lnjj+c;
where cis a constant that depends only on n.
(a) Use the Woodbury identities (A.15) and (A.16) to show that +371
I XX>=(I+XDX>) 1
lnjDj lnjj=lnjI+XDX>j:
Deduce that l()=nln[y>Cy] lnjCj+c, where C:=(I+XDX>) 1.
(b) Let [ v1;:::; vp] :=Xdenote the pcolumns /predictors of X. Show that
C 1=I+pX
k=1kvkv>
k:
Explain why setting k=0 has the e ect of excluding the k-th predictor from
the regression model. How can this observation be used for model selection?
(c) Prove the following formulas for the gradient and Hessian elements of l():
@l
@i=v>
iCvi n(v>
iCy)2
y>Cy
@2l
@i@j=(n 1)(v>
iCvj)2 n266664v>
iCvj (v>
iCy)(v>
jCy)
y>Cy3777752
:(6.41)
(d) One method to determine which predictors in Xare important is to compute
:=argmin
>0l()
using, for example, the interior-point minimization Algorithm B.4.1 with gradi- +419
ent and Hessian computed from (6.41). Write Python code to compute and
use it to select the best polynomial model in Example 2.10.Chapter 6. Regularization and Kernel Methods 249
19. (Exercise 18 continued.) Consider again Example 2.10 with D=diag(1;:::; p) for + 55
some nonnegative model-selection parameter 2Rp. A Bayesian choice for is the
maximizer of the marginal likelihood g(yj); that is,
=argmax
>0"
g(;2;yj) dd2;
where
lng(;2;yj)= ky Xk2+>D 1
22 1
2lnjDj n+p
2ln(22) ln2:
To maximize g(yj), one can use the EM algorithm withand2acting as latent +128
variables in the complete-data log-likelihood lng(;2;yj). DeÔ¨Åne
:=(D 1+X>X) 1
:=X>y
b2:=
kyk2 y>Xn:(6.42)
(a) Show that the conditional density of the latent variables and2is such that

 2;y
Gamman
2;n
2b2

;2;y
N
; 2
:
(b) Use Theorem C.2 to show that the expected complete-data log-likelihood is +430
 >D 1
2b2 tr(D 1)+lnjDj
2+c1;
where c1is a constant that does not depend on .
(c) Use Theorem A.2 to simplify the expected complete-data log-likelihood and to +359
show that it is maximized at i=ii+(i=b)2fori=1;:::; p:Hence, deduce
the following E and M steps in the EM algorithm:
E-step. Given, update ( ;;b2) via the formulas (6.42).
M-step. Given ( ;;b2), updateviai=ii+(i=b)2;i=1;:::; p:
(d) Write Python code to compute via the EM algorithm, and use it to select
the best polynomial model in Example 2.10. A possible stopping criterion is to
terminate the EM iterations when
lng(yjt+1) lng(yjt)<"
for some small ">0, where the marginal log-likelihood is
lng(yj)= n
2ln(nb2) 1
2lnjDj+1
2lnjj+ln (n=2):250 Exercises
20. In this exercise we explore how the early stopping of the gradient descent iterations
(see Example B.10), +412
xt+1=xt rf(xt);t=0;1;:::;
is (approximately) equivalent to the global minimization of f(x)+1
2kxk2for certain
values of the ridge regularization parameter >0 (see Example 6.1). We illustrate
theearly stopping early stopping idea on the quadratic function f(x)=1
2(x )>H(x ), where
H2Rnnis a symmetric positive-deÔ¨Ånite (Hessian) matrix with eigenvalues fkgn
k=1.
(a) Verify that for a symmetric matrix A2Rnsuch that I Ais invertible, we have
I+A++At 1=(I At)(I A) 1:
(b) Let H=QQ>be the diagonalization of Has per Theorem A.8. If x0=0, +366
show that the formula for xtis
xt= Q(I )tQ>:
Hence, deduce that a necessary condition for xtto converge is <2=max kk.
(c) Show that the minimizer of f(x)+1
2kxk2can be written as
x= Q(I+ 1) 1Q>:
(d) For a Ô¨Åxed value of t, let the learning rate #0. Using part (b) and (c), show
that if'1=(t) as#0, then xt'x. In other words, xtis approximately
equal to xfor small, provided that is inversely proportional to t.CHAPTER7
CLASSIFICATION
The purpose of this chapter is to explain the mathematical ideas behind well-known
classiÔ¨Åcation techniques such as the na√Øve Bayes method, linear and quadratic discrim-
inant analysis, logistic /softmax classiÔ¨Åcation, the K-nearest neighbors method, and
support vector machines.
7.1 Introduction
ClassiÔ¨Åcation methods are supervised learning methods in which a categorical response
variable Ytakes one of cpossible values (for example whether a person is sick or healthy),
which is to be predicted from a vector Xofexplanatory variables (for example, the blood
pressure, age, and smoking status of the person), using a prediction function g . In this
sense, gclassiÔ¨Åes the input Xinto one of the classes, say in the set f0;:::; c 1g. For this
reason, we will call gaclassiÔ¨Åcation function or simply classiÔ¨Åer classifier . As with any supervised
learning technique (see Section 2.3), the goal is to minimize the expected loss or risk
`(g)=ELoss( Y;g(X)) (7.1)
for some loss function, Loss( y;by), that quantiÔ¨Åes the impact of classifying a response yvia
by=g(x). The natural loss function is the zero‚Äìone (also written 0‚Äì1) or indicator loss indicator loss :
Loss( y;by) :=1fy,byg; that is, there is no loss for a correct classiÔ¨Åcation ( y=by) and a
unit loss for a misclassiÔ¨Åcation ( y,by). In this case the optimal classiÔ¨Åer gis given in the
following theorem.
Theorem 7.1: Optimal classiÔ¨Åer
For the loss function Loss( y;by)=1fy,byg, an optimal classiÔ¨Åcation function is
g(x)=argmax
y2f0;:::;c 1gP[Y=yjX=x]: (7.2)
Proof: The goal is to minimize `(g)=E1fY,g(X)gover all functions gtaking values in
f0;:::; c 1g. Conditioning on Xgives, by the tower property, `(g)=E(P[Y,g(X)jX] ), +431
and so minimizing `(g) with respect to gcan be accomplished by maximizing P[Y=
251252 7.1. Introduction
g(x)jX=x] with respect to g(x), for every Ô¨Åxed x. In other words, take g(x) to be equal
to the class label yfor whichP[Y=yjX=x] is maximal. 
The formulation (7.2) allows for ‚Äúties‚Äù, when there is an equal probability between
optimal classes for a feature vector x. Assigning one of these tied classes arbitrarily (or
randomly) to xdoes not a ect the loss function and so we assume for simplicity that g(x)
is always a scalar value.
Note that, as was the case for the regression (see, e.g., Theorem 2.1), the optimal pre- +21
diction function depends on the conditional pdf f(yjx)=P[Y=yjX=x]. However, since
we assign xto class yiff(yjx)>f(zjx) for all z, we do not need to learn the entire sur-
face of the function f(yjx); we only need to estimate it well enough near the decision
boundaryfx:f(yjx)=f(zjx)gfor any choice of classes yandz. This is because the as-
signment (7.2) divides the feature space into cregions,Ry=fx:f(yjx)=max zf(zjx)g,
y=0;:::; c 1.
Recall that for any supervised learning problem the smallest possible expected loss
(that is, the irreducible risk) is given by `=`(g). For the indicator loss, the irreducible
risk is equal to P[Y,g(X)]. This smallest possible probability of misclassiÔ¨Åcation is
often called the Bayes error rate Bayes error
rate.
For a given training set , a classiÔ¨Åer is often derived from a pre-classiÔ¨Åer g , which
is a prediction function (learner) that can take any real value, rather than only values
in the set of class labels. A typical situation is the case of binary classiÔ¨Åcation with
labels 1 and 1, where the prediction function gis a function taking values in the
interval [ 1;1] and the actual classiÔ¨Åer is given by sign( g). It will be clear from
the context whether a prediction function gshould be interpreted as a classiÔ¨Åer or
pre-classiÔ¨Åer.
The indicator loss function may not always be the most appropriate choice of loss
function for a given classiÔ¨Åcation problem. For example, when diagnosing an illness, the
mistake in misclassifying a person as being sick when in fact the person is healthy may
be less serious than classifying the person as healthy when in fact the person is sick. In
Section 7.2 we consider various classiÔ¨Åcation metrics.
There are many ways to Ô¨Åt a classiÔ¨Åer to a training set =f(x1;y1);:::; (xn;yn)g. The
approach taken in Section 7.3 is to use a Bayesian framework for classiÔ¨Åcation. Here the
conditional pdf f(yjx) is viewed as a posterior pdf f(yjx)/f(xjy)f(y) for a given class
prior f(y) and likelihood f(xjy). Section 7.4 discusses linear and quadratic discriminant
analysis for classiÔ¨Åcation, which assumes that the class of approximating functions for the
conditional pdf f(xjy) is a parametric class Gof Gaussian densities. As a result of this
choice ofG, the marginal f(x) is approximated via a Gaussian mixture density.
In contrast, in the logistic or soft-max classiÔ¨Åcation in Section 7.5, the conditional
pdff(yjx) is approximated using a more Ô¨Çexible class of approximating functions. As a
result of this, the approximation to the marginal density f(x) does not belong to a simple
parametric class (such as a Gaussian mixture). As in unsupervised learning, the cross-
entropy loss is the most common choice for training the learner.
TheK-nearest neighbors method, discussed in Section 7.6, is yet another approach to
classiÔ¨Åcation that makes minimal assumptions on the class G. Here the aim is to directlyChapter 7. ClassiÔ¨Åcation 253
estimate the conditional pdf f(yjx) from the training data, using only feature vectors in
the neighborhood of x. In Section 7.7 we explain the support vector methodology for clas-
siÔ¨Åcation; this is based on the same Reproducing Kernel Hilbert Space ideas that proved
successful for regression analysis in Section 6.3. Finally, a versatile way to do both clas- +222
siÔ¨Åcation and regression is to use classiÔ¨Åcation and regression trees. This is the topic of
Chapter 8. Neural networks (Chapter 9) provide yet another way to perform classiÔ¨Åcation. +287
+323
7.2 ClassiÔ¨Åcation Metrics
The e ectiveness of a classiÔ¨Åer gis, theoretically, measured in terms of the risk (7.1), which
depends on the loss function used. Fitting a classiÔ¨Åer to iid training data =f(xi;yi)gn
i=1is
established by minimizing the training loss
`(g)=1
nnX
i=1Loss( yi;g(xi)) (7.3)
over some class of functions G. As the training loss is often a poor estimator of the risk,
the risk is usually estimated as in (7.3), using instead a test set 0=f(x0
i;y0
i)gn0
i=1gthat is
independent of the training set, as explained in Section 2.3. To measure the performance + 23
of a classiÔ¨Åer on a training or test set, it is convenient to introduce the notion of a loss
matrix loss matrix . Consider a classiÔ¨Åcation problem with classiÔ¨Åer g, loss function Loss, and classes
0;:::; c 1. If an input feature vector xis classiÔ¨Åed as by=g(x) when the observed class
isy, the loss incurred is, by deÔ¨Ånition, Loss( y;by). Consequently, we may identify the loss
function with a matrix L=[Loss( j;k);j;k2f0;:::; c 1g]. For the indicator loss function,
the matrix Lhas 0s on the diagonal and 1s everywhere else. Another useful matrix is the
confusion matrix confusion
matrix, denoted by M, where the ( j;k)-th element of Mcounts the number of
times that, for the training or test data, the actual (observed) class is jwhereas the predicted
class is k. Table 7.1 shows the confusion matrix of some Dog /Cat/Possum classiÔ¨Åer.
Table 7.1: Confusion matrix for three classes.
Predicted
Actual Dog Cat Possum
Dog 30 2 6
Cat 8 22 15
Possum 7 4 41
We can now express the classiÔ¨Åer performance (7.3) in terms of LandMas
1
nX
j;k[LM]jk; (7.4)
where LMis the elementwise product of LandM. Note that for the indicator loss, (7.4)
is simply 1 tr(M)=n, and is called the misclassiÔ¨Åcation error . The expression (7.4) makesmisclassification
error it clear that both the counts and the loss are important in determining the performance of a
classiÔ¨Åer.254 7.2. ClassiÔ¨Åcation Metrics
In the spirit of Table C.4 for hypothesis testing, it is sometimes useful to divide the +459
elements of a confusion matrix into four groups. The diagonal elements are the true positivetrue positivecounts; that is, the numbers of correct classiÔ¨Åcations for each class. The true positive counts
for the Dog, Cat, and Possum classes in Table 7.1 are 30 ;22, and 41, respectively. Similarly,
thetrue negative true negative count for a class is the sum of all matrix elements that do not belong to the
row or the column of this particular class. For the Dog class it is 22 +15+4+41=82. The
false positive false positive count for a class is the sum of the corresponding column elements without
the diagonal element. For the Dog class it is 8 +7=15. Finally, the false negative false negative count
for a speciÔ¨Åc class, can be calculated by summing over the corresponding row elements
(again, without counting the diagonal element). For the Dog class it is 2 +6=8.
In terms of the elements of the confusion matrix, we have the following counts for class
j=0;:::; c 1:
True positive tpj=Mj j;
False positive fpj=X
k,jMk j; (column sum)
False negative fn j=X
k,jMjk; (row sum)
True negative tn j=n fnj fpj tpj:
Note that in the binary classiÔ¨Åcation case ( c=2), and using the indicator loss function,
the misclassiÔ¨Åcation error (7.4) can be written as
error j=fpj+fnj
n: (7.5)
This does not depend on which of the two classes is considered, as fp0+fn0=fp1+fn1.
Similarly, the accuracy accuracy measures the fraction of correctly classiÔ¨Åed objects:
accuracyj=1 error j=tpj+tnj
n: (7.6)
In some cases, classiÔ¨Åcation error (or accuracy) alone is not su cient to adequately
describe the e ectiveness of a classiÔ¨Åer. As an example, consider the following two classi-
Ô¨Åcation problems based on a Ô¨Ångerprint detection system:
1. IdentiÔ¨Åcation of authorized personnel in a top-secret military facility.
2. IdentiÔ¨Åcation to get an online discount for some retail chain.
Both problems are binary classiÔ¨Åcation problems. However, a false positive in the Ô¨Årst
problem is extremely dangerous, while a false positive in the second problem will make
a customer happy. Let us examine a classiÔ¨Åer in the top-secret facility. The corresponding
confusion matrix is given in Table 7.2.
Table 7.2: Confusion matrix for authorized personnel classiÔ¨Åcation.
Predicted
Actual authorized non-authorized
authorized 100 400
non-authorized 50 100,000Chapter 7. ClassiÔ¨Åcation 255
From (7.6), we conclude that the accuracy of classiÔ¨Åcation is equal to
accuracy =tp+tn
tp+tn+fp+fn=100+100;000
100+100;000+50+40099:55%:
However, we can see that in this particular case, accuracy is a problematic metric, since the
algorithm allowed 50 non-authorized personnel to enter the facility. One way to deal with
this issue is to modify the loss function to give a much higher loss to non-authorized access.
Thus, instead of an (indicator) loss matrix, we could for example take the loss matrix
L= 0 1
1000 0!
:
An alternative approach is to keep the indicator loss function and consider additional clas-
siÔ¨Åcation metrics. Below we give a list of commonly used metrics. For simplicity we call
an object whose actual class is ja ‚Äúj-object‚Äù.
¬àThe precision precision (also called positive predictive value ) is the fraction of all objects
classiÔ¨Åed as jthat are actually j-objects. SpeciÔ¨Åcally,
precisionj=tpj
tpj+fpj:
¬àThe recall recall (also called sensitivity ) is the fraction of all j-objects that are correctly
classiÔ¨Åed as such. That is,
recall j=tpj
tpj+fnj:
¬àThespeciÔ¨Åcity specificity measures the fraction of all non- j-objects that are correctly classiÔ¨Åed
as such. SpeciÔ¨Åcally,
speciÔ¨Åcityj=tnj
fpj+tnj:
¬àTheFscore Fscore is a combination of the precision and the recall and is used as a single
measurement for a classiÔ¨Åer‚Äôs performance. The Fscore is given by
F;j=(2+1) tpj
(2+1) tpj+2fnj+fpj:
For=0 we obtain the precision and for !1 we obtain the recall.
The particular choice of metric is clearly application dependent. For example, in the
classiÔ¨Åcation of authorized personnel in a top-secret military facility, suppose we have
two classiÔ¨Åers. The Ô¨Årst (ClassiÔ¨Åer 1) has a confusion matrix given in Table 7.2, and the
second (ClassiÔ¨Åer 2) has a confusion matrix given in Table 7.3. Various metrics for these
two classiÔ¨Åers are show in Table 7.4. In this case we prefer ClassiÔ¨Åer 1, which has a much
higher precision.256 7.2. ClassiÔ¨Åcation Metrics
Table 7.3: Confusion matrix for authorized personnel classiÔ¨Åcation, using a di erent clas-
siÔ¨Åer (ClassiÔ¨Åer 2).
Predicted
Actual Authorized Non-Authorized
authorized 50 10
non-authorized 450 100,040
Table 7.4: Comparing the metrics for the confusion matrices in Tables 7.2 and 7.3.
Metric ClassiÔ¨Åer 1 ClassiÔ¨Åer 2
accuracy 9 :95510 19:95410 1
precision 6 :66710 11:00010 1
recall 2 :00010 18:33310 1
speciÔ¨Åcity 9 :99510 19:95510 1
F1 3:07710 11:78610 1
Remark 7.1 (Multilabel and Hierarchical ClassiÔ¨Åcation) In standard classiÔ¨Åcation
the classes are assumed to be mutually exclusive. For example a satellite image could
be classiÔ¨Åed as ‚Äúcloudy‚Äù, ‚Äúclear‚Äù, or ‚Äúfoggy‚Äù. In multilabel classiÔ¨Åcation multilabel
classificationthe classes (often
called labels) do not have to be mutually exclusive. In this case the response is a subset
Yof some collection of labels f0;:::; c 1g. Equivalently, the response can be viewed as
a binary vector of length c, where the y-th element is 1 if the response belongs to label y
and 0 otherwise. Again, consider the satellite image example and add two labels, such as
‚Äúroad‚Äù and ‚Äúriver‚Äù to the previous three labels. Clearly, an image can contain both a road
and a river. In addition, the image can be clear, cloudy, or foggy.
Inhierarchical classiÔ¨Åcation hierarchical
classificationa hierarchical relation between classes /labels is taken into
account during the classiÔ¨Åcation process. Usually, the relations are modeled via a tree or a
directed acyclic graph. A visual comparison between the hierarchical and non-hierarchical
(Ô¨Çat) classiÔ¨Åcation tasks for satellite image data is presented in Figure 7.1.
root
rural
farm barnurban
skyscraper
root
rural barn farm urban skyscraper
Figure 7.1: Hierarchical (left) and non-hierarchical (right) classiÔ¨Åcation schemes. Barns
and farms are common in rural areas, while skyscrapers are generally located in cities.
While this relation can be clearly observed in the hierarchical model scheme, the connec-
tion is missing in the non-hierarchical design.
In multilabel classiÔ¨Åcation, both the prediction bY:=g(x) and the true response Yare
subsets of the label setf0;:::; c 1g. A reasonable metric is the so-called exact match ratio exact match
ratio,Chapter 7. ClassiÔ¨Åcation 257
deÔ¨Åned as
exact match ratio =Pn
i=11fbYi=Yig
n:
The exact match ratio is rather stringent, as it requires a full match. In order to consider
partial correctness, the following metrics could be used instead.
¬àTheaccuracy is deÔ¨Åned as the ratio of correctly predicted labels and the total number
of predicted and actual labels. The formula is given by
accuracy =Pn
i=1jYi\bYij
Pn
i=1jYi[bYij:
¬àTheprecision is deÔ¨Åned as the ratio of correctly predicted labels and the total number
of predicted labels. SpeciÔ¨Åcally,
precision =Pn
i=1jYi\bYij
Pn
i=1jbYij: (7.7)
¬àTherecall is deÔ¨Åned as the ratio of correctly predicted labels and the total number of
actual labels. SpeciÔ¨Åcally,
recall =Pn
i=1jYi\bYijPn
i=1jYij: (7.8)
¬àTheHamming loss counts the average number of incorrect predictions for all classes,
calculated as
Hamming =1
n cnX
i=1c 1X
y=01fy2bYig1fy<Yig+1fy<bYig1fy2Y ig:
7.3 ClassiÔ¨Åcation via Bayes‚Äô Rule
We saw from Theorem 7.1 that the optimal classiÔ¨Åer for classes 0 ;:::; c 1 divides the
feature space into cregions, depending on f(yjx): the conditional pdf of the response Y
given the feature vector X=x. In particular, if f(yjx)>f(zjx) for all z,y, the feature
vector xis classiÔ¨Åed as y. Classifying feature vectors on the basis of their conditional class
probabilities is a natural thing to do, especially in a Bayesian learning context; see Sec-
tion 2.9 for an overview of Bayesian terminology and usage. SpeciÔ¨Åcally, the conditional + 47
probability f(yjx) is interpreted as a posterior probability, of the form
f(yjx)/f(xjy)f(y); (7.9)
where f(xjy) is the likelihood of obtaining feature vector xfrom class yand f(y) is the
prior probability1of class y. By making various modeling assumptions about the prior
1Here we have used the Bayesian notation convention of ‚Äúoverloading‚Äù the notation f.258 7.3. ClassiÔ¨Åcation via Bayes‚Äô Rule
(e.g., all classes are a priori equally likely) and the likelihood function, one obtains the
posterior pdf via Bayes‚Äô formula (7.9). A class byis then assigned to a feature vector x
according to the highest posterior probability; that is, we classify according to the Bayes
optimal decision rule Bayes optimal
decision rule:
by=argmax
yf(yjx); (7.10)
which is exactly (7.2). Since the discrete density f(yjx),y=0;:::; c 1 is usually not
known, the aim is to approximate it well with a function g(yjx) from some class of func-
tionsG. Note that in this context, g(jx) refers to a discrete density (a probability mass
function) for a given x.
Suppose a feature vector x=[x1;:::; xp]>ofpfeatures has to be classiÔ¨Åed into one of
the classes 0 ;:::; c 1. For example, the classes could be di erent people and the features
could be various facial measurements, such as the width of the eyes divided by the distance
between the eyes, or the ratio of the nose height and mouth width. In the na√Øve Bayes na¬®iveBayes
method, the class of approximating functions Gis chosen such that g(xjy)=g(x1jy)
g(xpjy), that is, conditional on the label, all features are independent. Assuming a uniform
prior for y, the posterior pdf can thus be written as
g(yjx)/pY
j=1g(xjjy);
where the marginal pdfs g(xjjy);j=1;:::; pbelong to a given class of approximating
functionsG. To classify x, simply take the ythat maximizes the unnormalized posterior
pdf.
For instance, suppose that the approximating class Gis such that ( Xjjy)N(y j;2),
y=0;:::; c 1,j=1;:::; p. The corresponding posterior pdf is then
g(yj;x)/exp0BBBBBB@ 1
2pX
j=1(xj y j)2
21CCCCCCA=exp0BBBB@ 1
2kx yk2
21CCCCA;
wherey:=[y1;:::; yp]>and:=f0;:::;c 1;2gcollects all model parameters. The
probability g(yj;x) is maximal when kx ykis minimal. Thus by=argminykx ykis
the classiÔ¨Åer that maximizes the posterior probability. That is, classify xasywhenyis
closest to xin Euclidean distance. Of course, the parameters (here, the fygand2) are
unknown and have to be estimated from the training data.
We can extend the above idea to the case where also the variance 2depends on the
class yand feature j, as in the next example.
Example 7.1 (Na√Øve Bayes ClassiÔ¨Åcation) Table 7.5 lists the means and standard de-
viationsofp=3 normally distributed features, for c=4 dierent classes. How should
a feature vector x=[1:67;2:00;4:23]>be classiÔ¨Åed? The posterior pdf is
g(yj;x)/(y1y2y3) 1exp0BBBBBB@ 1
23X
j=1(xj y j)2
2
y j1CCCCCCA;
where:=fj;jgc 1
j=0again collects all model parameters. The (unscaled) values for
g(yj;x),y=0;1;2;3 are 53:5, 0:24, 8:37, and 3:510 6, respectively. Hence, the feature
vector should be classiÔ¨Åed as 0. The code follows.Chapter 7. ClassiÔ¨Åcation 259
Table 7.5: Feature parameters.
Feature 1 Feature 2 Feature 3
Class      
0 1.6 0.1 2.4 0.5 4.3 0.2
1 1.5 0.2 2.9 0.6 6.1 0.9
2 1.8 0.3 2.5 0.3 4.2 0.3
3 1.1 0.2 3.1 0.7 5.6 0.3
naiveBayes.py
import numpy as np
x = np.array([1.67,2,4.23]).reshape(1,3)
mu = np.array([1.6, 2.4, 4.3,
1.5, 2.9, 6.1,
1.8, 2.5, 4.2,
1.1, 3.1, 5.6]).reshape(4,3)
sig = np.array([0.1, 0.5, 0.2,
0.2, 0.6, 0.9,
0.3, 0.3, 0.3,
0.2, 0.7, 0.3]).reshape(4,3)
g = lambda y: 1/np.prod(sig[y,:]) * np.exp(
-0.5*np. sum((x-mu[y,:])**2/sig[y,:]**2));
for yin range (0,4):
print ('{:3.2e} '.format (g(y)))
5.35e+01
2.42e-01
8.37e+00
3.53e-06
7.4 Linear and Quadratic Discriminant Analysis
The Bayesian viewpoint for classiÔ¨Åcation of the previous section (not limited to na√Øve
Bayes) leads in a natural way to the well-established technique of discriminant analysis discriminant
analysis.
We discuss the binary classiÔ¨Åcation case Ô¨Årst, with classes 0 and 1.
We consider a class of approximating functions Gsuch that, conditional on the class
y2f0;1g, the feature vector X=[X1;:::; Xp]>has aN(y;y) distribution (see (2.33)): + 45
g(xj;y)=1p(2)pjyje 1
2(x y)> 1
y(x y);x2Rp;y2f0;1g; (7.11)
where=fj;j;jgc 1
j=0collects all model parameters, including the probability vector 
(that is,P
ii=1 andi>0) which helps deÔ¨Åne the prior density: g(yj)=y;y2f0;1g:
Then, the posterior density is
g(yj;x)/yg(xj;y);260 7.4. Linear and Quadratic Discriminant Analysis
and, according to the Bayes optimal decision rule (7.10), we classify xto come from class
0 if0g(xj;0)> 1g(xj;1) or, equivalently (by taking logarithms) if,
ln0 1
2lnj0j 1
2(x 0)> 1
0(x 0)>ln1 1
2lnj1j 1
2(x 1)> 1
1(x 1):
The function
y(x)=lny 1
2lnjyj 1
2(x y)> 1
y(x y);x2Rp(7.12)
is called the quadratic discriminant function quadratic
discriminant
functionfor class y=0;1. A point xis classiÔ¨Åed to
class yfor whichy(x) is largest. The function is quadratic in xand so the decision bound-
aryfx2Rp:0(x)=1(x)gis quadratic as well. An important simpliÔ¨Åcation arises for the
case where the assumption is made that 0=1=. Now, the decision boundary is the
set of xfor which
ln0 1
2(x 0)> 1(x 0)=ln1 1
2(x 1)> 1(x 1):
Expanding the above expression shows that the quadratic term in xis eliminated, giving a
linear decision boundary in x:
ln0 1
2>
0 10+x> 10=ln1 1
2>
1 11+x> 11:
The corresponding linear discriminant function linear
discriminant
functionfor class yis
y(x)=lny 1
2>
y 1y+x> 1y;x2Rp: (7.13)
Example 7.2 (Linear Discriminant Analysis) Consider the case where 0=1=1=2
and
="2 0:7
0:7 2#
;0="0
0#
;1="2
4#
:
The distribution of Xis a mixture of two bivariate normal distributions. Its pdf, +135
1
2g(xj;y=0)+1
2g(xj;y=1);
is depicted in Figure 7.2.
Figure 7.2: A Gaussian mixture density where the two mixture components have the same
covariance matrix.Chapter 7. ClassiÔ¨Åcation 261
We used the following Python code to make this Ô¨Ågure.
LDAmixture.py
import numpy as np, matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import LightSource
mu0, mu1 = np.array([0,0]), np.array([2,4])
Sigma = np.array([[2,0.7],[0.7, 2]])
x, y = np.mgrid[-4:6:150j,-5:8:150j]
mvn0 = multivariate_normal( mu0, Sigma )
mvn1 = multivariate_normal( mu1, Sigma )
xy = np.hstack((x.reshape(-1,1),y.reshape(-1,1)))
z = 0.5*mvn0.pdf(xy).reshape(x.shape) + 0.5*mvn1.pdf(xy).reshape(x.
shape)
fig = plt.figure()
ax = fig.gca(projection= '3d')
ls = LightSource(azdeg=180, altdeg=65)
cols = ls.shade(z, plt.cm.winter)
surf = ax.plot_surface(x, y, z, rstride=1, cstride=1, linewidth=0,
antialiased=False , facecolors=cols)
plt.show()
The following Python code, which imports the previous code, draws a contour plot of
the mixture density, simulates 1000 data points from the mixture density, and draws the
decision boundary. To compute and display the linear decision boundary, let [ a1;a2]>=
2 1(1 0) and b=>
0 10 >
1 11. Then, the decision boundary can be written
asa1x1+a2x2+b=0 or, equivalently, x2= (a1x1+b)=a2. We see in Figure 7.3 that the
decision boundary nicely separates the two modes of the mixture density.
LDA.py
from LDAmixture import *
from numpy.random import rand
from numpy.linalg import inv
fig = plt.figure()
plt.contourf(x, y,z, cmap=plt.cm.Blues , alpha= 0.9,extend= 'both ')
plt.ylim(-5.0,8.0)
plt.xlim(-4.0,6.0)
M = 1000
r = (rand(M,1) < 0.5)
for iin range (0,M):
ifr[i]:
u = np.random.multivariate_normal(mu0,Sigma ,1)
plt.plot(u[0][0],u[0][1], '.r',alpha = 0.4)
else :
u = np.random.multivariate_normal(mu1,Sigma ,1)
plt.plot(u[0][0],u[0][1], '+k',alpha = 0.6)262 7.4. Linear and Quadratic Discriminant Analysis
a = 2*inv(Sigma) @ (mu1-mu0);
b = ( mu0.reshape(1,2) @ inv(Sigma) @ mu0.reshape(2,1)
- mu1.reshape(1,2) @ inv(Sigma) @mu1.reshape(2,1) )
xx = np.linspace(-4,6,100)
yy = (-(a[0]*xx +b)/a[1])[0]
plt.plot(xx,yy, 'm')
plt.show()
4
 2
 0
 2
 4
 6
4
2
0
2
4
6
8
Figure 7.3: The linear discriminant boundary lies between the two modes of the mixture
density and is linear.
To illustrate the di erence between the linear and quadratic case, we specify di erent
covariance matrices for the mixture components in the next example.
Example 7.3 (Quadratic Discriminant Analysis) As in Example 7.2 we consider a
mixture of two Gaussians, but now with di erent covariance matrices. Figure 7.4 shows
the quadratic decision boundary. The Python code follows.
2
 1
 0
 1
 2
 3
 4
3
2
1
0
1
2
3
4
5
Figure 7.4: A quadratic decision boundary.Chapter 7. ClassiÔ¨Åcation 263
QDA.py
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
mu1 = np.array([0,0])
mu2 = np.array([2,2])
Sigma1 = np.array([[1,0.3],[0.3, 1]])
Sigma2 = np.array([[0.3,0.3],[0.3, 1]])
x, y = np.mgrid[-2:4:150j,-3:5:150j]
mvn1 = multivariate_normal( mu1, Sigma1 )
mvn2 = multivariate_normal( mu2, Sigma2 )
xy = np.hstack((x.reshape(-1,1),y.reshape(-1,1)))
z = ( 0.5*mvn1.pdf(xy).reshape(x.shape) +
0.5*mvn2.pdf(xy).reshape(x.shape) )
plt.contour(x,y,z)
z1 = ( 0.5*mvn1.pdf(xy).reshape(x.shape) -
0.5*mvn2.pdf(xy).reshape(x.shape))
plt.contour(x,y,z1, levels=[0],linestyles = 'dashed ',
linewidths = 2, colors = 'm')
plt.show()
Of course, in practice the true parameter =fj;j;jgc
j=1is not known and must be
estimated from the training data ‚Äî for example, by minimizing the cross-entropy training
loss(4.4) with respect to : +123
1
nnX
i=1Loss( f(xi;yi);g(xi;yij))= 1
nnX
i=1lng(xi;yij);
where
lng(x;yj)=lny 1
2lnjyj 1
2(x y)> 1
y(x y) p
2ln(2):
The corresponding estimates of the model parameters (see Exercise 2) are:
by=ny
n
by=1
nyX
i:yi=yxi
by=1
nyX
i:yi=y(xi by)(xi by)>(7.14)
fory=0;:::; c 1, where ny:=Pn
i=11fyi=yg. For the case where y=for all y, we
haveb=P
ybyby.
When c>2 classes are involved, the classiÔ¨Åcation procedure carries through in exactly
the same way, leading to quadratic and linear discriminant functions (7.12) and (7.13) for
each class. The space Rpnow is partitioned into cregions, determined by the linear or
quadratic boundaries determined by each pair of Gaussians.264 7.4. Linear and Quadratic Discriminant Analysis
For the linear discriminant case (that is, when y=for all y), it is convenient to Ô¨Årst
‚Äúwhiten‚Äù or sphere the data sphere the data as follows. Let Bbe an invertible matrix such that =BB>,
obtained, for example, via the Cholesky method. We linearly transform each data point x +373
tox0:=B 1xand each mean yto0
y:=B 1y,y=0;:::; c 1. Let the random vector X
be distributed according to the mixture pdf
gX(xj) :=X
yy1p(2)pjyje 1
2(x y)> 1
y(x y):
Then, by the transformation Theorem C.4, the vector X0=B 1Xhas density +433
gX0(x0j)=gX(xj)
jB 1j=c 1X
y=0yp(2)pe 1
2(x y)>(BB>) 1(x y)
=c 1X
y=0yp(2)pe 1
2(x0 0
y)>(x0 0
y)=c 1X
y=0yp(2)pe 1
2kx0 0
yk2:
This is the pdf of a mixture of standard p-dimensional normal distributions. The name
‚Äúsphering‚Äù derives from the fact that the contours of each mixture component are perfect
spheres. ClassiÔ¨Åcation of the transformed data is now particularly easy: classify xasby:=
argminyfkx0 0
yk2 2 lnyg. Note that this rule only depends on the prior probabilities and
the distance from x0to the transformed means f0
yg. This procedure can lead to a signiÔ¨Åcant
dimensionality reduction of the data. Namely, the data can be projected onto the space
spanned by the di erences between the mean vectors f0
yg. When there are cclasses, this
is a ( c 1)-dimensional space, as opposed to the p-dimensional space of the original data.
We explain the precise ideas via an example.
Example 7.4 (ClassiÔ¨Åcation after Data Reduction) Consider an equal mixture of
three 3-dimensional Gaussian distributions with identical covariance matrices. After spher-
ing the data, the covariance matrices are all equal to the identity matrix. Suppose the mean
vectors of the sphered data are 1=[2;1; 3]>,2=[1; 4;0]>, and3=[2;4;6]>. The
left panel of Figure 7.5 shows the 3-dimensional (sphered) data from each of the three
classes.
4202464202454321012
6
 4
 2
 0
 2
 4
 6
4
2
0
2
4
6
Figure 7.5: Left: original data. Right: projected data.
The data are stored in three 1000 3 matrices X1,X2, and X3. Here is how the data was
generated and plotted.Chapter 7. ClassiÔ¨Åcation 265
datared.py
import numpy as np
from numpy.random import randn
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
n=1000
mu1 = np.array([2,1,-3])
mu2 = np.array([1,-4,0])
mu3 = np.array([2,4,0])
X1 = randn(n,3) + mu1
X2 = randn(n,3) + mu2
X3 = randn(n,3) + mu3
fig = plt.figure()
ax = fig.gca(projection= '3d',)
ax.plot(X1[:,0],X1[:,1],X1[:,2], 'r.',alpha=0.5,markersize=2)
ax.plot(X2[:,0],X2[:,1],X2[:,2], 'b.',alpha=0.5,markersize=2)
ax.plot(X3[:,0],X3[:,1],X3[:,2], 'g.',alpha=0.5,markersize=2)
ax.set_xlim3d(-4,6)
ax.set_ylim3d(-5,5)
ax.set_zlim3d(-5,2)
plt.show()
Since we have equal mixtures, we classify each data point xaccording to the closest
distance to1,2, or3. We can achieve a reduction in the dimensionality of the data by
projecting the data onto the two-dimensional a ne space spanned by the fig; that is, all
vectors are of the form
1+1(2 1)+2(3 1);  1;22R:
In fact, one may just as well project the data onto the subspace spanned by the vectors
21=2 1and31=3 1. Let W=[21;31] be the 32 matrix whose columns
are21and31. The orthogonal projection matrix onto the subspace Wspanned by the
columns of Wis (see Theorem A.4): +362
P=WW+=W(W>W) 1W>:
LetUDV>be the singular value decomposition of W. Then Pcan also be written as
P=UD(D>D) 1D>U>:
Note that Dhas dimension 32, so is not square. The Ô¨Årst two columns of U, say u1
andu2, form an orthonormal basis of the subspace W. What we want to do is rotate this
subspace to the x yplane, mapping u1andu2to [1;0;0]>and [0;1;0]>, respectively. This
is achieved via the rotation matrix U 1=U>, giving the skewed projection matrix
R=U>P=D(D>D) 1D>U>;
whose 3rd row only contains zeros. Applying Rto all the data points, and ignoring the
3rd component of the projected points (which is 0), gives the right panel of Figure 7.5.
We see that the projected points are much better separated than the original ones. We have
achieved dimensionality reduction of the data while retaining all the necessary information
required for classiÔ¨Åcation. Here is the rest of the Python code.266 7.5. Logistic Regression and Softmax ClassiÔ¨Åcation
dataproj.py
from datared import *
from numpy.linalg import svd, pinv
mu21 = (mu2 - mu1).reshape(3,1)
mu31 = (mu3 - mu1).reshape(3,1)
W = np.hstack((mu21 , mu31))
U,_,_ = svd(W) # we only need U
P = W @ pinv(W)
R = U.T @ P
RX1 = (R @ X1.T).T
RX2 = (R @ X2.T).T
RX3 = (R @ X3.T).T
plt.plot(RX1[:,0],RX1[:,1], 'b.',alpha=0.5,markersize=2)
plt.plot(RX2[:,0],RX2[:,1], 'g.',alpha=0.5,markersize=2)
plt.plot(RX3[:,0],RX3[:,1], 'r.',alpha=0.5,markersize=2)
plt.show()
7.5 Logistic Regression and Softmax ClassiÔ¨Åcation
In Example 5.10 we introduced the logistic (logit) regression model as a generalized linear +204
model where, conditional on a p-dimensonal feature vector x, the random response Yhas
aBer(h(x>)) distribution with h(u)=1=(1+e u). The parameter was then learned from
the training data by maximizing the likelihood of the training responses or, equivalently,
by minimizing the supervised version of the cross-entropy training loss (4.4): +123
 1
nnX
i=1lng(yij;xi);
where g(y=1j;x)=1=(1+e x>) and g(y=0j;x)=e x>=(1+e x>). In particular,
we have
lng(y=1j;x)
g(y=0j;x)=x>: (7.15)
In other words, the log-odds ratio log-odds ratio is a linear function of the feature vector. As a con-
sequence, the decision boundary fx:g(y=0j;x)=g(y=1j;x)gis the hyperplane
x>=0. Note that xtypically includes the constant feature. If the constant feature is con-
sidered separately, that is x=[1;ex>]>, then the boundary is an a ne hyperplane in ex.
Suppose that training on =f(xi;yi)gyields the estimate bwith the corresponding
learner g(y=1jx)=1=(1+e x>b). The learner can be used as a pre-classiÔ¨Åer from which
we obtain the classiÔ¨Åer 1fg(y=1jx)>1=2gor, equivalently,
by:=argmax
j2f0;1gg(y=jjx);
in accordance with the fundamental classiÔ¨Åcation rule (7.2).
The above classiÔ¨Åcation methodology for the logit model can be generalized to the
multi-logit multi -logit model where the response takes values in the set f0;:::; c 1g. The key idea isChapter 7. ClassiÔ¨Åcation 267
to replace (7.15) with
lng(y=jjW;b;x)
g(y=0jW;b;x)=x>j;j=1;:::; c 1; (7.16)
where the matrix W2R(c 1)(p 1)and vector b2Rc 1reparameterize all j2Rpsuch that
(recall x=[1;ex>]>):
Wex+b=[1;:::;c 1]>x:
Observe that the random response Yis assumed to have a conditional probability distri-
bution for which the log-odds ratio with respect to class jand a ‚Äúreference‚Äù class (in this
case 0) is linear . The separating boundaries between two pairs of classes are again a ne
hyperplanes.
The model (7.16) completely speciÔ¨Åes the distribution of Y, namely:
g(yjW;b;x)=exp(zy+1)Pc
k=1exp(zk);y=0;:::; c 1;
where z1is an arbitrary constant, say 0, corresponding to the ‚Äúreference‚Äù class y=0, and
[z2;:::; zc]>:=Wex+b:
Note that g(yjW;b;x) is the ( y+1)-st component of a=softmax( z), where
softmax : z7!exp(z)P
kexp(zk)
is the softmax softmax function and z=[z1;:::; zc]>. Finally, we can write the classiÔ¨Åer as
by=argmax
j2f0;:::;c 1gaj+1:
In summary, we have the sequence of mappings transforming the input xinto the output by:
x!Wex+b!softmax( z)!argmax
j2f0;:::;c 1gaj+1!by:
In Example 9.4 we will revisit the multi-logit model and reinterpret this sequence of map- +333
pings as a neural network . In the context of neural networks, Wis called a weight matrix
andbis called a bias vector.
The parameters Wandbhave to be learned from the training data, which involves
minimization of the supervised version of the cross-entropy training loss (4.4): +123
1
nnX
i=1Loss( f(yijxi);g(yijW;b;xi))= 1
nnX
i=1lng(yijW;b;xi):
Using the softmax function, the cross-entropy loss can be simpliÔ¨Åed to:
Loss( f(yjx);g(yjW;b;x))= zy+1+lncX
k=1exp(zk): (7.17)
The discussion on training is postponed until Chapter 9, where we reinterpret the multi-
logit model as a neural net, which can be trained using the limited-memory BFGS method
(Exercise 11). Note that in the binary case ( c=2), where there is only one vector to +352
be estimated, Example 5.10 already established that minimization of the cross-entropy
training loss is equivalent to likelihood maximization.268 7.6. K-Nearest Neighbors ClassiÔ¨Åcation
7.6 K-Nearest Neighbors ClassiÔ¨Åcation
Let=f(xi;yi)gn
i=1be the training set, with yi2f0;:::; c 1g, and let xbe a new feature
vector. DeÔ¨Åne x(1);x(2);:::; x(n)as the feature vectors ordered by closeness to xin some dis-
tance dist( x;xi), e.g., the Euclidean distance kx x0k. Let(x) :=f(x(1);y(1)):::;(x(K);y(K))g
be the subset of that contains Kfeature vectors xithat are closest to x. Then the K-nearest
neighbors K-nearest
neighborsclassiÔ¨Åcation rule classiÔ¨Åes xaccording to the most frequently occurring class
labels in(x). If two or more labels receive the same number of votes, the feature vector
is classiÔ¨Åed by selecting one of these labels randomly with equal probability. For the case
K=1 the set(x) contains only one element, say ( x0;y0), and xis classiÔ¨Åed as y0. This
divides the space into nregions
Ri=fx: dist( x;xi)6dist(x;xj);j,ig;i=1;:::; n:
For a feature space Rpwith the Euclidean distance, this gives a V oronoi tessellation of the
feature space, similar to what was done for vector quantization in Section 4.6. +142
Example 7.5 (Nearest Neighbor ClassiÔ¨Åcation) The Python program below simulates
80 random points above and below the line x2=x1. Points above the line x2=x1have
label 0 and points below this line have label 1. Figure 7.6 shows the V oronoi tessellation
obtained from the 1-nearest neighbor classiÔ¨Åcation.
2
 1
 0
 1
 2
 3
4
3
2
1
0
1
2
3
4
Figure 7.6: The 1-nearest neighbor algorithm divides up the space into V oronoi cells.
nearestnb.py
import numpy as np
from numpy.random import rand ,randn
import matplotlib.pyplot as plt
from scipy.spatial import Voronoi , voronoi_plot_2dChapter 7. ClassiÔ¨Åcation 269
np.random.seed(12345)
M = 80
x = randn(M,2)
y = np.zeros(M) # pre-allocate list
for iin range (M):
ifrand() <0.5:
x[i,1], y[i] = x[i,0] + np. abs(randn()), 0
else :
x[i,1], y[i] = x[i,0] - np. abs(randn()), 1
vor = Voronoi(x)
plt_options = { 'show_vertices ':False , 'show_points ':False ,
'line_alpha ':0.5}
fig = voronoi_plot_2d(vor, **plt_options)
plt.plot(x[y==0,0], x[y==0,1], 'bo',
x[y==1,0], x[y==1,1], 'rs', markersize=3)
7.7 Support Vector Machine
Suppose we are given the training set =f(xi;yi)gn
i=1, where each response2yitakes either
the value 1 or 1, and we wish to construct a classiÔ¨Åer taking values in f 1;1g. As this
merely involves a relabeling of the 0‚Äì1 classiÔ¨Åcation problem in Section 7.1, the optimal
classiÔ¨Åcation function for the indicator loss, 1fy,byg, is, by Theorem 7.1, equal to
g(x)=8>><>>:1 ifP[Y=1jX=x]>1=2;
 1 ifP[Y=1jX=x]<1=2:
It is not di cult to show, see Exercise 5, that the function gcan be viewed as the minimizer
of the risk for the hinge loss hinge loss function, Loss( y;by)=(1 yby)+:=maxf0;1 ybyg, over all
prediction functions g(not necessarily taking values only in the set f 1;1g). That is,
g=argmin
gE(1 Y g(X))+: (7.18)
Given the training set , we can approximate the risk `(g)=E(1 Y g(X))+with the train-
ing loss
`(g)=1
nnX
i=1(1 yig(xi))+;
and minimize this over a (smaller) class of functions to obtain the optimal prediction func-
tiong. Finally, as the prediction function ggenerally is not a classiÔ¨Åer by itself (it usually
does not only take values  1 or 1), we take the classiÔ¨Åer
sign g(x):
2The reason why we use responses  1 and 1 here, instead of 0 and 1, is that the notation becomes easier.270 7.7. Support Vector Machine
Therefore, a feature vector xis classiÔ¨Åed according to 1 or  1 depending on whether
g(x)>0 or<0, respectively. The optimal decision boundary optimal decision
boundaryis given by the set of xfor
which g(x)=0.
Similar to the cubic smoothing spline or RKHS setting in (6.19), we can consider Ô¨Ånd-
ing the best classiÔ¨Åer, given the training data, via the penalized goodness-of-Ô¨Åt optimiza-
tion:
min
g2HH 01
nnX
i=1[1 yig(xi)]++ekgk2
H;
for some regularization parameter e. It will be convenient to deÔ¨Åne :=2neand to solve
the equivalent problem
min
g2HH 0nX
i=1[1 yig(xi)]++
2kgk2
H:
We know from the Representer Theorem 6.6 that if is the reproducing kernel cor- +231
responding toH, then the solution is of the form (assuming that the null space H0has a
constant term only):
g(x)=0+nX
i=1i(xi;x): (7.19)
Substituting into the minimization expression yields the analogue of (6.21): +232
min
;0nX
i=1[1 yi(0+fKgi)]++
2>K; (7.20)
where Kis the Gram matrix. This is a convex optimization problem, as it is the sum of a
convex quadratic and piecewise linear term in . DeÔ¨Åningi:=i=yi,i=1;:::; nand
:=[1;:::; n]>, we show in Exercise 10 that the optimal and0in (7.20) can be
obtained by solving the ‚Äúdual‚Äù convex optimization problem
max
nX
i=1i 1
2nX
i=1nX
j=1ijyiyj(xi;xj)
subject to: >y=0;0661;(7.21)
and0=yj P
i=1i(xi;xj) for any jfor whichj2(0;1). In view of (7.19), the optimal
prediction function (pre-classiÔ¨Åer) gis then given by
g(x)=0+nX
i=1i(xi;x)=0+1
nX
i=1yii(xi;x): (7.22)
To mitigate possible numerical problems in the calculation of 0it is customary to take
an overall average:
0=1
jJjX
j2J8>><>>:yj nX
i=1i(xi;xj)9>>=>>;;
whereJ:=fj:j2(0;1)g.Chapter 7. ClassiÔ¨Åcation 271
Note that, from (7.22), the optimal pre-classiÔ¨Åer g(x) and the classiÔ¨Åer sign g(x) only
depend on vectors xifor whichi,0. These vectors are called the support vectors support vectors of the
support vector machine. It is also important to note that the quadratic function in (7.21)
depends on the regularization parameter . By deÔ¨Åning i:=i=,i=1;:::; n, we can
rewrite (7.21) as
min
1
2X
i;jijyiyj(xi;xj) nX
i=1i
subject to:nX
i=1iyi=0;06i61==:C;i=1;:::; n:(7.23)
For perfectly separable data, that is, data for which an a ne plane can be drawn to perfectly
separate the two classes, we may take C=1, as explained below. Otherwise, Cneeds to
be chosen via cross-validation or a test data set, for example.
Geometric interpretation
For the linear kernel function (x;x0)=x>x0, we have
g(x)=0+>x;
with0=0and= 1Pn
i=1iyixi=Pn
i=1ixi, and so the decision boundary is an a ne
plane. The situation is illustrated in Figure 7.7. The decision boundary is formed by the
points xsuch that g(x)=0. The two setsfx:g(x)= 1gandfx:g(x)=1gare called
themargins . The distance from the points on a margin to the decision boundary is 1 =kk.
123
Figure 7.7: Classifying two classes (red and blue) using SVM.
Based on the ‚Äúmultipliers‚Äù fig, we can divide the training samples f(xi;yi)ginto three
categories (see Exercise 11):
¬àPoints for which i2(0;1). These are the support vectors on the margins (green
encircled in the Ô¨Ågure) and are correctly classiÔ¨Åed.272 7.7. Support Vector Machine
¬àPoints for which i=1. These points, which are also support vectors, lie strictly
inside the margins (points 1, 2, and 3 in the Ô¨Ågure). Such points may or may not be
correctly classiÔ¨Åed.
¬àPoints for which i=0. These are the non-support vectors, which all lie outside the
margins. Every such point is correctly classiÔ¨Åed.
If the classes of points fxi:yi=1gandfxi:yi= 1gare perfectly separable by some
ane plane, then there will be no points strictly inside the margins, so all support vectors
will lie exactly on the margins. In this case (7.20) reduces to
min
;0kk2
subject to: yi(0+x>
i)>1;i=1;:::; n;(7.24)
using the fact that 0=0andK=XX>=X. We may replace min kk2in (7.24) with
max 1=kk, as this gives the same optimal solution. As 1 =kkis equal to half the margin
width, the latter optimization problem has a simple interpretation: separate the points via
an ane hyperplane such that the margin width is maximized.
Example 7.6 (Support Vector Machine) The data in Figure 7.8 was uniformly gener-
ated on the unit disc. Class-1 points (blue dots) have a radius less than 1 /2 (y-values 1) and
class-2 points (red crosses) have a radius greater than 1 /2 (y-values 1).
-1 -0.5 0 0.5 1-0.8-0.6-0.4-0.200.20.40.60.8
Figure 7.8: Separate the two classes.
Of course it is not possible to separate the two groups of points via a straight line in
R2. However, it is possible to separate them in R3by considering three-dimensional feature
vectors z=[z1;z2;z3]>=[x1;x2;x2
1+x2
2]>. For any x2R2, the corresponding feature vec-
torzlies on a quadratic surface. In this space it is possible to separate the fzigpoints into
two groups by means of a planar surface, as illustrated in Figure 7.9.Chapter 7. ClassiÔ¨Åcation 273
Figure 7.9: In feature space R3the points can be separated by a plane.
We wish to Ô¨Ånd a separating plane in R3using the transformed features. The following
Python code uses the SVCfunction of the sklearn module to solve the quadratic optimiz-
ation problem (7.23) (with C=1). The results are summarized in Table 7.6. The data is
available from the book‚Äôs GitHub site as svmcirc.csv .
svmquad.py
import numpy as np
from numpy import genfromtxt
from sklearn.svm import SVC
data = genfromtxt( 'svmcirc.csv ', delimiter= ',')
x = data[:,[0,1]] #vectors are rows
y = data[:,[2]].reshape( len(x),) #labels
tmp = np. sum(np.power(x,2),axis=1).reshape( len(x),1)
z = np.hstack((x,tmp))
clf = SVC(C = np.inf, kernel= 'linear ')
clf.fit(z,y)
print ("Support Vectors \n", clf.support_vectors_)
print ("Support Vector Labels ",y[clf.support_])
print ("Nu",clf.dual_coef_)
print ("Bias",clf.intercept_)
Support Vectors
[[ 0.038758 0.53796 0.29090314]
[-0.49116 -0.20563 0.28352184]
[-0.45068 -0.04797 0.20541358]
[-0.061107 -0.41651 0.17721465]]
Support Vector Labels [-1. -1. 1. 1.]
Nu [[ -46.49249413 -249.01807328 265.31805855 30.19250886]]
Bias [5.617891]274 7.7. Support Vector Machine
Table 7.6: Optimal support vector machine parameters for the R3data.
z>y=y
0.0388 0.5380 0.2909  1 46.4925
 0.4912 0.2056 0.2835  1 249.0181
 0.4507 0.0480 0.2054 1 265.3181
 0.0611 0.4165 0.1772 1 30.1925
It follows that the normal vector of the plane is
=X
i2Sizi=[ 0:9128;0:8917; 24:2764]>;
whereSis the set of indices of the support vectors. We see that the plane is almost per-
pendicular to the z1;z2plane. The bias term 0can also be found from the table above. In
particular, for any x>andyin Table 7.6, we have y >z=0=5:6179.
To draw the separating boundary in R2we need to project the intersection of the sep-
arating plane with the quadratic surface onto the z1;z2plane. That is, we need to Ô¨Ånd all
points ( z1;z2) such that
5:6179 0:9128 z1+0:8917 z2=24:2764 ( z2
1+z2
2): (7.25)
This is the equation of a circle with (approximate) center (0 :019; 0:018) and radius 0 :48,
which is very close to the true circular boundary between the two groups, with center (0 ;0)
and radius 0.5. This circle is drawn in Figure 7.10.
-1 0 1-101
Figure 7.10: The circular decision boundary can be viewed equivalently as (a) the pro-
jection onto the x1;x2plane of the intersection of the separating plane with the quadratic
surface (both in R3), or (b) the set of points x=(x1;x2) for which g(x)=0+>(x)=0.
An equivalent way to derive this circular separating boundary is to consider the feature
map(x)=[x1;x2;x2
1+x2
2]>onR2, which deÔ¨Ånes a reproducing kernel
(x;x0)=(x)>(x0);Chapter 7. ClassiÔ¨Åcation 275
onR2, which in turn gives rise to a (unique) RKHS H. The optimal prediction function
(7.19) is now of the form
g(x)=0+1
nX
i=1yii(xi)>(x)=0+>(x); (7.26)
where0=0and
=1
nX
i=1yii(xi):
The decision boundary, fx:g(x)=0g, is again a circle in R2. The following code de-
termines the Ô¨Åtted model parameters and the decision boundary. Figure 7.10 shows the
optimal decision boundary, which is identical to (7.25). The function mykernel speciÔ¨Åes
the custom kernel above.
svmkern.py
import numpy as np, matplotlib.pyplot as plt
from numpy import genfromtxt
from sklearn.svm import SVC
def mykernel(U,V):
tmpU = np. sum(np.power(U,2),axis=1).reshape( len(U),1)
U = np.hstack((U,tmpU))
tmpV = np. sum(np.power(V,2),axis=1).reshape( len(V),1)
V = np.hstack((V,tmpV))
K = U @ V.T
print (K.shape)
return K
# read in the data
inp = genfromtxt( 'svmcirc.csv ', delimiter= ',')
data = inp[:,[0,1]] #vectors are rows
y = inp[:,[2]].reshape( len(data),) #labels
clf = SVC(C = np.inf, kernel=mykernel , gamma= 'auto ')# custom kernel
# clf = SVC(C = np.inf, kernel="rbf", gamma= 'scale ') # inbuilt
clf.fit(data ,y)
print ("Support Vectors \n", clf.support_vectors_)
print ("Support Vector Labels ",y[clf.support_])
print ("Nu ",clf.dual_coef_)
print ("Bias ",clf.intercept_)
# plot
d = 0.001
x_min , x_max = -1,1
y_min , y_max = -1,1
xx, yy = np.meshgrid(np.arange(x_min , x_max , d), np.arange(y_min ,
y_max , d))
plt.plot(data[clf.support_ ,0],data[clf.support_ ,1], 'go')
plt.plot(data[y==1,0],data[y==1,1], 'b.')
plt.plot(data[y==-1,0],data[y==-1,1], 'rx')276 7.7. Support Vector Machine
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contour(xx, yy, Z,colors ="k")
plt.show()
Finally, we illustrate the use of the Gaussian kernel
(x;x0)=e ckx x0k2; (7.27)
where c>0 is some tuning constant. This is an example of a radial basis function kernel ,
which are reproducing kernels of the form (x;x0)=f(kx x0k), for some positive real-
valued function f. Each feature vector xis now transformed to a functionx=(x;). We
can think of it as the (unnormalized) pdf of a Gaussian distribution centered around x, and
gis a (signed) mixture of these pdfs, plus a constant; that is,
g(x)=0+nX
i=1ie ckxi xk2:
Replacing in Line 2 of the previous code mykernel with 'rbf' produces the SVM
parameters given in Table 7.7. Figure 7.11 shows the decision boundary, which is not ex-
actly circular, but is close to the true (circular) boundary fx:kxk=1=2g. There are now
seven support vectors, rather than the four in Figure 7.10.
Table 7.7: Optimal support vector machine parameters for the Gaussian kernel case.
x>y(109)
0.0388 0.5380  1 0.0635
 0.4912 0.2056 1 9.4793
0.5086 0.1576  1 0.5240
 0.4507 0.0480 1 5.5405x>y(109)
 0.4374 0.3854  1 1.4399
0.3402 0.5740 1 0.1000
 0.4098 0.1763 1 6.0662
-1 0 1-101
Figure 7.11: Left: The decision boundary fx:g(x)=0gis roughly circular, and separates
the two classes well. There are seven support vectors, indicated by green circles. Right:
The graph of gis a scaled mixture of Gaussian pdfs plus a constant.Chapter 7. ClassiÔ¨Åcation 277
Remark 7.2 (Scaling and Penalty Parameters) When using a radial basis function in
SVCinsklearn , the scaling c(7.27) can be set via the parameter gamma . Note that large
values of gamma lead to highly peaked predicted functions, and small values lead to highly
smoothed predicted functions. The parameter CinSVCrefers C=1=in (7.23).
7.8 ClassiÔ¨Åcation with Scikit-Learn
In this section we apply several classiÔ¨Åcation methods to a real-world data set, using the
Python module sklearn (the package name is Scikit-Learn). SpeciÔ¨Åcally, the data is ob-
tained from UCI‚Äôs Breast Cancer Wisconsin data set. This data set, Ô¨Årst published and
analyzed in [118], contains the measurements related to 569 images of 357 benign and
212 malignant breast masses. The goal is to classify a breast mass as benign or malig-
nant based on 10 features: Radius, Texture, Perimeter, Area, Smoothness, Compactness,
Concavity, Concave Points, Symmetry, and Fractal Dimension of each mass. The mean,
standard error, and ‚Äúworst‚Äù of these attributes were computed for each image, resulting in
30 features. For instance, feature 1 is Mean Radius, feature 11 is Radius SE, feature 21 is
Worst Radius.
The following Python code reads the data, extracts the response vector and model (fea-
ture) matrix and divides the data into a training and test set.
skclass1.py
from numpy import genfromtxt
from sklearn.model_selection import train_test_split
url1 = "http://mlr.cs.umass.edu/ml/machine -learning -databases/"
url2 = "breast -cancer -wisconsin/"
name = "wdbc.data"
data = genfromtxt(url1 + url2 + name , delimiter= ',', dtype= str)
y = data[:,1] #responses
X = data[:,2:].astype( 'float ')#features as an ndarray matrix
X_train , X_test , y_train , y_test = train_test_split(
X, y, test_size = 0.4, random_state = 1234)
To visualize the data we create a 3D scatterplot for the features mean radius , mean
texture , and mean concavity , which correspond to the columns 0, 1, and 6 of the model
matrix X. Figure 7.12 suggests that the malignant and benign breast masses could be well
separated using these three features.
skclass2.py
from skclass1 import X, y
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
Bidx = np.where(y == 'B')
Midx= np.where(y == 'M')
# plot features Radius (column 0), Texture (1), Concavity (6)278 7.8. ClassiÔ¨Åcation with Scikit-Learn
fig = plt.figure()
ax = fig.gca(projection = '3d')
ax.scatter(X[Bidx ,0], X[Bidx ,1], X[Bidx ,6],
c='r', marker= '^', label= 'Benign ')
ax.scatter(X[Midx ,0], X[Midx ,1], X[Midx ,6],
c='b', marker= 'o', label= 'Malignant ')
ax.legend()
ax.set_xlabel( 'Mean Radius ')
ax.set_ylabel( 'Mean Texture ')
ax.set_zlabel( 'Mean Concavity ')
plt.show()
Mean Radius10
15
20
25Mean Texture10152025303540Mean Concavity
0.00.10.20.30.4
Benign
Malignant
Figure 7.12: Scatterplot of three features of the benign and malignant breast masses.
The following code uses various classiÔ¨Åers to predict the category of breast masses
(benign or malignant). In this case the training set has 341 elements and the test set has 228
elements. For each classiÔ¨Åer the percentage of correct predictions (that is, the accuracy) in
the test set is reported. We see that in this case quadratic discriminant analysis gives the
highest accuracy (0 :956). Exercise 18 explores the question whether this metric is the most
appropriate for these data.
skclass3.py
from skclass1 import X_train , y_train , X_test , y_test
from sklearn.metrics import accuracy_score
import sklearn.discriminant_analysis as DA
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
names = ["Logit","NBayes", "LDA", "QDA", "KNN", "SVM"]Chapter 7. ClassiÔ¨Åcation 279
classifiers = [LogisticRegression(C=1e5),
GaussianNB(),
DA.LinearDiscriminantAnalysis(),
DA.QuadraticDiscriminantAnalysis(),
KNeighborsClassifier(n_neighbors=5),
SVC(kernel= 'rbf', gamma = 1e-4)]
print ('Name Accuracy\n '+14* '-')
for name , clf in zip (names , classifiers):
clf.fit(X_train , y_train)
y_pred = clf.predict(X_test)
print ('{:6} {:3.3f} '.format (name , accuracy_score(y_test ,y_pred)))
Name Accuracy
--------------
Logit 0.943
NBayes 0.908
LDA 0.943
QDA 0.956
KNN 0.925
SVM 0.939
Further Reading
An excellent source for understanding various pattern recognition techniques is the book
[35] by Duda et al. Theoretical foundations of classiÔ¨Åcation, including the Vapnik‚Äì
Chernovenkis dimension and the fundamental theorem of learning, are discussed in
[109, 121, 122]. A popular measure for characterizing the performance of a binary classi-
Ô¨Åer is the receiver operating characteristic (ROC) curve [38]. The na√Øve Bayes classiÔ¨Åc-
ation paradigm can be extended to handle explanatory variable dependency via graphical
models such as Bayesian networks and Markov random Ô¨Åelds [46, 66, 69]. For a detailed
discussion on Bayesian decision theory, see [8].
Exercises
1. Let 06w61. Show that the solution to the convex optimization problem
min
p1;:::;pnnX
i=1p2
i
subject to:n 1X
i 1pi=wandnX
i=1pi=1;(7.28)
is given by pi=w=(n 1);i=1;:::; n 1 and pn=1 w.
2. Derive the formulas (7.14) by minimizing the cross-entropy training loss:
 1
nnX
i=1lng(xi;yij);280 Exercises
where g(x;yj) is such that:
lng(x;yj)=lny 1
2lnjyj 1
2(x y)> 1
y(x y) p
2ln(2):
3. Adapt the code in Example 7.2 to plot the estimated decision boundary instead of the
true one in Figure 7.3. Compare the true and estimated decision boundaries.
4. Recall from equation (7.16) that the decision boundaries of the multi-logit classiÔ¨Åer are
linear, and that the pre-classiÔ¨Åer can be written as a conditional pdf of the form:
g(yjW;b;x)=exp(zy+1)Pc
i=1exp(zi);y2f0;:::; c 1g;
where x>=[1;ex>] and z=Wex+b.
(a) Show that the linear discriminant pre-classiÔ¨Åer in Section 7.4 can also be written as a
conditional pdf of the form ( =fy;y;ygc 1
y=0):
g(yj;x)=exp(zy+1)Pc
i=1exp(zi);y2f0;:::; c 1g;
where x>=[1;ex>] and z=Wex+b. Find formulas for the corresponding bandW
in terms of the linear discriminant parameters fy;y;ygc 1
y=0, where y=for all y.
(b) Explain which pre-classiÔ¨Åer has smaller approximation error: the linear discriminant
or multi-logit one? Justify your answer by proving an inequality between the two
approximation errors.
5. Consider a binary classiÔ¨Åcation problem where the response Ytakes values inf 1;1g.
Show that optimal prediction function for the hinge loss Loss( y;by)=(1 yby)+:=maxf0;1 
ybygis the same as the optimal prediction function gfor the indicator loss:
g(x)=8>><>>:1 ifP[Y=1jX=x]>1=2;
 1 ifP[Y=1jX=x]<1=2:
That is, show that
E(1 Y h(X))+>E(1 Y g(X))+ (7.29)
for all functions h.
6. In Example 4.12, we applied a principal component analysis (PCA) to the iris data, +158
but refrained from classifying the Ô¨Çowers based on their feature vectors x. Implement a
1-nearest neighbor algorithm, using a training set of 50 randomly chosen data pairs ( x;y)
from the iris data set. How many of the remaining 100 Ô¨Çowers are correctly classiÔ¨Åed?
Now classify these entries with an o -the-shelf multi-logit classiÔ¨Åer, e.g., such as can be
found in the sklearn andstatsmodels packages.
7. Figure 7.13 displays two groups of data points, given in Table 7.8. The convex hulls
have also been plotted. It is possible to separate the two classes of points via a straight line.Chapter 7. ClassiÔ¨Åcation 281
In fact, many such lines are possible. SVM gives the best separation, in the sense that the
gap (margin) between the points is maximal.
-3 -2 -1 0 1 2 3 4-2-10123456
Figure 7.13: Separate the points by a straight line so that the separation between the two
groups is maximal.
Table 7.8: Data for Figure 7.13.
x1 x2 y
2:4524 5 :5673 1
1:2743 0 :8265 1
0:8773 0:5478 1
1:4837 3 :0464 1
0:0628 4 :0415 1
 2:4151 0:9309 1
1:8152 3 :9202 1
1:8557 2 :7262 1
 0:4239 1 :8349 1
1:9630 0 :6942 1x1 x2 y
0:5819 1:0156 1
1:2065 3 :2984 1
2:6830 0 :4216 1
 0:0734 1 :3457 1
0:0787 0 :6363 1
0:3816 5 :2976 1
0:3386 0 :2882 1
 0:1493 0:7095 1
1:5554 4 :9880 1
3:2031 4 :4614 1
(a) Identify from the Ô¨Ågure the three support vectors.
(b) For a separating boundary (line) given by 0+>x=0, show that the margin width
is 2=kk.
(c) Show that the parameters 0andthat solve the convex optimization problem (7.24)
provide the maximal width between the margins.
(d) Solve (7.24) using a penalty approach; see Section B.4. In particular, minimize the +415282 Exercises
penalty function
S(;0)=kk2 CnX
i=1minn
(0+>xi)yi 1;0o
for some positive penalty constant C.
(e) Find the solution the dual optimization problem (7.21) by using sklearn ‚ÄôsSCV
method. Note that, as the two point sets are separable, the constraint 61 may
be removed, and the value of can be set to 1.
8. In Example 7.6 we used the feature map (x)=[x1;x2;x2
1+x2
2]>to classify the points.
An easier way is to map the points into R1via the feature map (x)=kxkor any monotone
function thereof. Translated back into R2this yields a circular separating boundary. Find
the radius and center of this circle, using the fact that here the sorted norms for the two
groups are:::;0:4889;0:5528;:::.
9. Let Y2f0;1gbe a response variable and let h(x) be the regression function
h(x) :=E[YjX=x]=P[Y=1jX=x]:
Recall that the Bayes classiÔ¨Åer is g(x)=1fh(x)>1=2g:Letg:R! f0;1gbe any
other classiÔ¨Åer function. Below, we denote all probabilities and expectations conditional
onX=xasPx[] andEx[].
(a) Show that
Px[g(x),Y]=irreducible errorz           }|           {
Px[g(x),Y]+j2h(x) 1j1fg(x),g(x)g:
Hence, deduce that for a learner gTconstructed from a training set T, we have
E[Px[gT(x),YjT]]=Px[g(x),Y]+j2h(x) 1jP[gT(x),g(x)];
where the Ô¨Årst expectation and last probability operations are with respect to T.
(b) Using the previous result, deduce that for the unconditional error (that is, we no longer
condition on X=x), we have
P[g(X),Y]6P[gT(X),Y]:
(c) Show that, if gT:=1fhT(x)>1=2gis a classiÔ¨Åer function such that as n!1
hT(x)d !ZN((x);2(x))
for some mean and variance functions (x) and2(x), respectively, then
Px[gT(x),g(x)] ! sign(1 2h(x))(2(x) 1)
2(x)!
;
where is the cdf of a standard normal random variable.Chapter 7. ClassiÔ¨Åcation 283
10. The purpose of this exercise is to derive the dual program (7.21) from the primal
program (7.20). The starting point is to introduce a vector of auxiliary variables :=
[1;:::; n]>and write the primal program as
min
;0;nX
i=1i+
2>K
subject to: >0;
yi(0+fKgi)>1 i;i=1;:::; n:(7.30)
(a) Apply the Lagrangian optimization theory from Section B.2.2 to obtain the Lag- +406
rangian functionL(f0;;g;f;g), whereandare the Lagrange multipliers cor-
responding to the Ô¨Årst and second inequality constraints, respectively.
(b) Show that the Karush‚ÄìKuhn‚ÄìTucker (see Theorem B.2) conditions for optimizing L +407
are:
>y=0
=y=
0661
(1 )=0;  i(yig(xi) 1+i)=0;i=1;:::; n
>0;yig(xi) 1+i>0;i=1;:::; n:(7.31)
Herestands for componentwise multiplication; e.g., y=[y11;:::; ynn]>, and
we have abbreviated 0+fKgitog(xi), in view of (7.19). [Hint: one of the KKT
conditions is =1 ; thus we can eliminate .]
(c) Using the KKT conditions (7.31), reduce the Lagrange dual function L() :=
min0;;L(f0;;g;f;1 g) to
L()=nX
i=1i 1
2nX
i=1nX
j=1ijyiyj(xi;xj): (7.32)
(d) As a consequence of (7.19) and (a)‚Äì(c), show that the optimal prediction function g
is given by
g(x)=0+1
nX
i=1yii(xi;x); (7.33)
whereis the solution to
max
L()
subject to: >y=0;0661;(7.34)
and0=yj 1
Pn
i=1yii(xi;xj) for any jsuch thatj2(0;1).
11. Consider SVM classiÔ¨Åcation as illustrated in Figure 7.7. The goal of this exercise is to
classify the training points f(xi;yi)gbased on the value of the multipliers figin Exercise 10.
Letibe the auxiliary variable in Exercise 10, i=1;:::; n.284 Exercises
(a) Fori2(0;1) show that ( xi;yi) lies exactly on the decision border.
(b) Fori=1, show that ( xi;yi) lies strictly inside the margins.
(c) Show that for i=0 the point ( xi;yi) lies outside the margins and is correctly classi-
Ô¨Åed.
12. A well-known data set is the MNIST handwritten digit database, containing many
thousands of digitalized numbers (from 0 to 9), each described by a 28 28 matrix of gray
scales. A similar but much smaller data set is described in [63]. Here, each handwritten
digit is summarized by a 8 8 matrix with integer entries from 0 (white) to 15 (black).
Figure 7.14 shows the Ô¨Årst 50 digitized images. The data set can be accessed with Python
using the sklearn package as follows.
from sklearn import datasets
digits = datasets.load_digits()
x_digits = digits.data # explanatory variables
y_digits = digits.target # responses
Figure 7.14: Classify the digitized images.
(a) Divide the data into a 75% training set and 25% test set.
(b) Compare the e ectiveness of the K-nearest neighbors and na√Øve Bayes method to
classify the data.
(c) Assess which Kto use in the K-nearest neighbors classiÔ¨Åcation.
13. Download the winequality-red.csv data set from UCI‚Äôs wine-quality website.
The response here is the wine quality (from 0 to 10) as speciÔ¨Åed by a wine ‚Äúexpert‚Äù
and the explanatory variables are various characteristics such as acidity and sugar con-
tent. Use the SVC classiÔ¨Åer of sklearn.svm with a linear kernel and penalty para-
meter C=1 (see Remark 7.2) to Ô¨Åt the data. Use the method cross_val_score fromChapter 7. ClassiÔ¨Åcation 285
sklearn.model_selection to obtain a Ô¨Åve-fold cross-validation score as an estimate of
the probability that the predicted class matches the expert‚Äôs class.
14. Consider the credit approval data set crx.data from UCI‚Äôs credit approval website.
The data set is concerned with credit card applications. The last column in the data set
indicates whether the application is approved ( +) or not ( ). With the view of preserving
data privacy, all 15 explanatory variables were anonymized. Note that some explanatory
variables are continuous and some are categorical.
(a) Load and prepare the data for analysis with sklearn . First, eliminate data
rows with missing values. Next, encode categorical explanatory variables using a
OneHotEncoder object from sklearn.preprocessing to create a model matrix X
with indicator variables for the categorical variables, as described in Section 5.3.5. +177
(b) The model matrix should contain 653 rows and 46 columns. The response variable
should be a 0 /1 variable (reject /approve). We will consider several classiÔ¨Åcation al-
gorithms and test their performance (using a zero-one loss) via ten-fold cross valida-
tion.
i. Write a function which takes 3 parameters: X;y, and a model, and returns the
ten-fold cross-validation estimate of the expected generalization risk.
ii. Consider the following sklearn classiÔ¨Åers: KNeighborsClassifier (k=5),
LogisticRegression , and MPLClassifier (multilayer perceptron). Use the
function from (i) to identify the best performing classiÔ¨Åer.
15. Consider a synthetic data set that was generated in the following fashion. The explan-
atory variable follows a standard normal distribution. The response label is 0 if the explan-
atory variable is between the 0.95 and 0.05 quantiles of the standard normal distribution,
and 1, otherwise. The data set was generated using the following code.
import numpy as np
import scipy.stats
# generate data
np.random.seed(12345)
N = 100
X = np.random.randn(N)
q = scipy.stats.norm.ppf(0.95)
y = np.zeros(N)
y[X>=q] = 1
y[X<=-q] = 1
X = X.reshape(-1,1)
Compare the K-nearest neighbors classiÔ¨Åer with K=5 and logistic regression classi-
Ô¨Åer. Without computation, which classiÔ¨Åer is likely to be better for these data? Verify your
answer by coding both classiÔ¨Åers and printing the corresponding training 0‚Äì1 loss.
16. Consider the digits data set from Exercise 12. In this exercise, we would like to train
a binary classiÔ¨Åer for the identiÔ¨Åcation of digit 8.
(a) Divide the data such that the Ô¨Årst 1000 rows are used as the training set and the rest
are used as the test set.286 Exercises
(b) Train the LogisticRegression classiÔ¨Åer from the sklearn.linear_model pack-
age.
(c) ‚ÄúTrain‚Äù a na√Øve classiÔ¨Åer that always returns 0. That is, the na√Øve classiÔ¨Åer identiÔ¨Åes
each instance as being not 8.
(d) Compare the zero-one test losses of the logistic regression and the na√Øve classiÔ¨Åers.
(e) Find the confusion matrix, the precision, and the recall of the logistic regression clas-
siÔ¨Åer.
(f) Find the fraction of eights that are correctly detected by the logistic regression clas-
siÔ¨Åer.
17. Repeat Exercise 16 with the original MNIST data set. Use the Ô¨Årst 60,000 rows as the
train set and the remaining 10,000 rows as the test set. The original data set can be obtained
using the following code.
from sklearn.datasets import fetch_openml
X, y = fetch_openml( 'mnist_784 ', version=1, return_X_y=True)
18. For the breast cancer data in Section 7.8, investigate and discuss whether accuracy is +277
the relevant metric to use or if other metrics discussed in Section 7.2 are more appropriate. +253CHAPTER8
DECISION TREES AND ENSEMBLE
METHODS
Statistical learning methods based on decision trees have gained tremendous pop-
ularity due to their simplicity, intuitive representation, and predictive accuracy. This
chapter gives an introduction to the construction and use of such trees. We also dis-
cuss two key ensemble methods, namely bootstrap aggregation and boosting, which
can further improve the e ciency of decision trees and other learning methods.
8.1 Introduction
Tree-based methods provide a simple, intuitive, and powerful mechanism for both regres-
sion and classiÔ¨Åcation. The main idea is to divide a (potentially complicated) feature space
Xinto smaller regions and Ô¨Åt a simple prediction function to each region. For example,
in a regression setting, one could take the mean of the training responses associated with
the training features that fall in that speciÔ¨Åc region. In the classiÔ¨Åcation setting, a com-
monly used prediction function takes the majority vote among the corresponding response
variables. We start with a simple classiÔ¨Åcation example.
Example 8.1 (Decision Tree for ClassiÔ¨Åcation) The left panel of Figure 8.1 shows a
training set of 15 two-dimensional points (features) falling into two classes (red and blue).
How should the new feature vector (black point) be classiÔ¨Åed?
20 10 0 10 202010010203040
20 10 0 10 202010010203040
Figure 8.1: Left: training data and a new feature. Right: a partition of the feature space.
287288 8.1. Introduction
It is not possible to linearly separate the training set, but we can partition the feature
spaceX=R2into rectangular regions and assign a class (color) to each region, as shown
in the right panel of Figure 8.1. Points in these regions are classiÔ¨Åed accordingly as blue
or red. The partition thus deÔ¨Ånes a classiÔ¨Åer (prediction function) gthat assigns to each
feature vector xa class ‚Äúred‚Äù or ‚Äúblue‚Äù. For example, for x=[ 15;0]>(solid black point),
g(x)=‚Äúblue‚Äù, since it belongs to a blue region of the feature space.
Both the classiÔ¨Åcation procedure and the partitioning of the feature space can be con-
veniently represented by a binary decision tree decision tree . This is a tree where each node vcorres-
ponds to a region (subset) Rvof the feature space X‚Äî the root node corresponding to the
feature space itself.
x2‚â§12.0
x1‚â§ ‚àí20.5
x1‚â§20.0
x1‚â§2.5
x1‚â§ ‚àí5.0True
TrueFalse
True
True
True FalseFalseFalseFalse
Figure 8.2: The decision-
tree that corresponds to the
partition in Figure 8.1.Each internal node vcontains a logical condition that di-
videsRvinto two disjoint subregions. The leaf nodes (the ter-
minal nodes of the tree) are not subdivided, and their corres-
ponding regions form a partition of X, as they are disjoint and
their union isX. Associated with each leaf node wis also a
regional prediction function gwonRw.
The partitioning of Figure 8.1 was obtained from
the decision tree shown in Figure 8.2. As an illustra-
tion of the decision procedure, consider again the input
x=[x1;x2]>=[ 15;0]>. The classiÔ¨Åcation process starts
from the tree root, which contains the condition x2612:0. As
the second component of xis 0, the root condition is satisÔ¨Åed
and we proceed to the left child, which contains the condition
x16 20:5. The next step is similar. As 0 > 20:5, the condi-
tion is not satisÔ¨Åed and we proceed to the right child. Such an
evaluation of logical conditions along the tree path will even-
tually bring us to a leaf node and its associated region. In this
case the process terminates in a leaf that corresponds to the
left blue region in the right-hand panel of Figure 8.1.
More generally, a binary tree Twill partition the feature space Xinto as many regions
as there are leaf nodes. Denote the set of leaf nodes by W. The overall prediction function
gthat corresponds to the tree can then be written as
g(x)=X
w2Wgw(x)1fx2R wg; (8.1)
where 1denotes the indicator function. The representation (8.1) is very general and de-
pends on (1) how the regions fRwgare constructed via the logical conditions in the decision
tree, as well as (2) how the regional prediction functions regional
prediction
functionsof the leaf nodes are deÔ¨Åned.
Simple logical conditions of the form xj6split a Euclidean feature space into rect-
angles aligned with the axes. For example, Figure 8.2 partitions the feature space into six
rectangles: two blue and four red rectangles.
In a classiÔ¨Åcation setting, the regional prediction function gwcorresponding to a leaf
node wtakes values in the set of possible class labels. In most cases, as in Example 8.1, it
is taken to be constant on the corresponding region Rw. In a regression setting, gwis real-
valued and also usually takes only one value. That is, every feature vector in Rwleads toChapter 8. Decision Trees and Ensemble Methods 289
the same predicted value. Of course, di erent regions will usually have di erent predicted
values.
Constructing a tree with a training set =f(xi;yi)ggn
i=1amounts to minimizing the
training loss
`(g)=1
nnX
i=1Loss( yi;g(xi)) (8.2)
for some loss function; see Chapter 2. With gof the form (8.1), we can write + 19
`(g)=1
nnX
i=1Loss( yi;g(xi))=1
nnX
i=1X
w2W1fxi2R wgLoss( yi;g(xi)) (8.3)
=X
w2W1
nnX
i=11fxi2R wgLoss( yi;gw(xi))
|                                    {z                                    }
(); (8.4)
where () is the contribution by the regional prediction function gwto the overall training
loss. In the case where all fxigare di erent, Ô¨Ånding a decision tree Tthat gives a zero
squared-error or zero‚Äìone training loss is easy, see Exercise 1, but such an ‚ÄúoverÔ¨Åtted‚Äù tree
will have poor predictive behavior, expressed in terms of the generalization risk. Instead
we consider a restricted class of decision trees and aim to minimize the training loss within
that class. It is common to use a top-down greedy approach, which can only achieve an
approximate minimization of the training loss.
8.2 Top-Down Construction of Decision Trees
Let=f(xi;yi)gn
i=1be the training set. The key to constructing a binary decision tree T
is to specify a splitting rule splitting rule for each node v, which can be deÔ¨Åned as a logical function
s:X!f False;Truegor, equivalently, a binary function s:X!f 0;1g. For example,
in the decision tree of Figure 8.2 the root node has splitting rule x7!1fx2612:0g, in
correspondence with the logical condition fx2612:0g. During the construction of the tree,
each node vis associated with a speciÔ¨Åc region RvX and therefore also the training
subsetf(x;y)2:x2R vg. Using a splitting rule s, we can divide any subset of the
training set into two sets:
T:=f(x;y)2:s(x)=TruegandF:=f(x;y)2:s(x)=Falseg: (8.5)
Starting from an empty tree and the initial data set , a generic decision tree con-
struction takes the form of the recursive Algorithm 8.2.1. Here we use the notation
Tvfor a subtree of Tstarting from node v. The Ô¨Ånal tree Tis thus obtained via T=
Construct_Subtree (v0;), where v0is the root of the tree.290 8.2. Top-Down Construction of Decision Trees
Algorithm 8.2.1: Construct_Subtree
Input: A node vand a subset of the training data: .
Output: A (sub) decision tree Tv.
1iftermination criterion is met then //vis a leaf node
2 Train a regional prediction function gvusing the training data .
3else // split the node
4 Find the best splitting rule svfor node v.
5 Create successors vTandvFofv.
6T f(x;y)2:sv(x)=Trueg
7F f(x;y)2:sv(x)=Falseg
8TvT Construct_Subtree (vT;T) // left branch
9TvF Construct_Subtree (vF;F) // right branch
10returnTv
The splitting rule svdivides the region Rvinto two disjoint parts, say RvTandRvF. The
corresponding prediction functions, gTandgF, satisfy
gv(x)=gT(x)1fx2R vTg+gF(x)1fx2R vFg;x2R v:
In order to implement the procedure described in Algorithm 8.2.1, we need to address
the construction of the regional prediction functions gvat the leaves (Line 2), the speciÔ¨Åc-
ation of the splitting rule (Line 4), and the termination criterion (Line 1). These important
aspects are detailed in the following Sections 8.2.1, 8.2.2, and 8.2.3, respectively.
8.2.1 Regional Prediction Functions
In general, there is no restriction on how to choose the prediction function gwfor a leaf
node v=win Line 2 of Algorithm 8.2.1. In principle we can train any model from the
data; e.g., via linear regression. However, in practice very simple prediction functions are
used. Below, we detail a popular choice for classiÔ¨Åcation, as well as one for regression.
1. In the classiÔ¨Åcation setting with class labels 0 ;:::; c 1, the regional prediction
function gwfor leaf node wis usually chosen to be constant and equal to the most
common class label of the training data in the associated region Rw(ties can be
broken randomly). More precisely, let nwbe the number of feature vectors in region
Rwand let
pw
z=1
nwX
f(x;y)2:x2Rwg1fy=zg;
be the proportion of feature vectors in Rwthat have class label z=0;:::; c 1. The
regional prediction function for node wis chosen to be the constant
gw(x)=argmax
z2f0;:::;c 1gpw
z: (8.6)
2. In the regression setting, gwis usually chosen as the mean response in the region;
that is,
gw(x)=yRw:=1
nwX
f(x;y)2:x2Rwgy; (8.7)Chapter 8. Decision Trees and Ensemble Methods 291
where nwis again the number of feature vectors in Rw. It is not di cult to show that
gw(x)=yRwminimizes the squared-error loss with respect to all constant functions,
in the regionRw; see Exercise 2.
8.2.2 Splitting Rules
In Line 4 in Algorithm 8.2.1, we divide region Rvinto two sets, using a splitting rule
(function) sv. Consequently, the data set associated with node v(that is, the subset of the
original data set whose feature vectors lie in Rv), is also split ‚Äî into TandF. What is
the beneÔ¨Åt of such a split in terms of a reduction in the training loss? If vwere set to a leaf
node, its contribution to the training loss would be (see (8.4)):
1
nnX
i=11f(x;y)2gLoss( yi;gv(xi)): (8.8)
Ifvwere to be split instead, its contribution to the overall training loss would be:
1
nnX
i=11f(x;y)2TgLoss( yi;gT(xi))+1
nnX
i=11f(x;y)2FgLoss( yi;gF(xi)); (8.9)
where gTandgFare the prediction functions belonging to the child nodes vTandvF. A
greedy heuristic is to pretend that the tree construction algorithm immediately terminates
after the split, in which case vTandvFare leaf nodes, and gTandgFare readily evaluated
‚Äî e.g., as in Section 8.2.1. Note that for any splitting rule the contribution (8.8) is always
greater than or equal to (8.9). It therefore makes sense to choose the splitting rule such that
(8.9) is minimized. Moreover, the termination criterion may involve comparing (8.9) with
(8.8). If their di erence is too small it may not be worth further splitting the feature space.
As an example, suppose the feature space is X=Rpand we consider splitting rules of
the form
s(x)=1fxj6g; (8.10)
for some 16j6pand2R, where we identify 0 with False and 1 with True . Due to the
computational and interpretative simplicity, such binary splitting rules are implemented in
many software packages and are considered to be the de facto standard. As we have seen,
these rules divide up the feature space into rectangles, as in Figure 8.1. It is natural to ask
how jandshould be chosen so as to minimize (8.9). For a regression problem, using a
squared-error loss and a constant regional prediction function as in (8.7), the sum (8.9) is
given by
1
nX
(x;y)2:xj6 y yT2+1
nX
(x;y)2:xj> y yF2; (8.11)
where yTandyFare the average responses for the TandFdata, respectively. Let fxj;kgm
k=1
denote the possible values of xj;j=1;:::; pwithin the training subset (with m6n
elements). Note that, for a Ô¨Åxed j, (8.11) is a piecewise constant function of , and that its
minimal value is attained at some value xj;k. As a consequence, to minimize (8.11) over
alljand, it su ces to evaluate (8.11) for each of the mpvalues xj;kand then take the
minimizing pair ( j;xj;k).292 8.2. Top-Down Construction of Decision Trees
For a classiÔ¨Åcation problem, using the indicator loss and a constant regional prediction
function as in (8.6), the aim is to choose a splitting rule that minimizes
1
nX
(x;y)2T1fy,y
Tg+1
nX
(x;y)2F1fy,y
Fg; (8.12)
where y
T=gT(x) is the most prevalent class (majority vote) in the data set Tandy
Fis the
most prevalent class in F. If the feature space is X=Rpand the splitting rules are of the
form (8.10), then the optimal splitting rule can be obtained in the same way as described
above for the regression case; the only di erence is that (8.11) is replaced with (8.12).
We can view the minimization of (8.12) as minimizing a weighted average of ‚Äúimpur-
ities‚Äù of nodes TandF. Namely, for an arbitrary training subset , ifyis the most
prevalent label, then
1
jjX
(x;y)21fy,yg=1 1
jjX
(x;y)21fy=yg=1 py=1 max
z2f0;:::;c 1gpz;
where pzis the proportion of data points in that have class label z,z=0;:::; c 1. The
quantity
1 max
z2f0;:::;c 1gpz
measures the diversity of the labels in and is called the misclassiÔ¨Åcation impurity . Con-misclassification
impurity sequently, (8.12) is the weighted sum of the misclassiÔ¨Åcation impurities of TandF, with
weights byjTj=nandjFj=n, respectively. Note that the misclassiÔ¨Åcation impurity only
depends on the label proportions rather than on the individual responses. Instead of using
the misclassiÔ¨Åcation impurity to decide if and how to split a data set , we can use other
impurity measures that only depend on the label proportions. Two popular choices are the
entropy impurity entropy
impurity:
 c 1X
z=0pzlog2(pz)
and the Gini impurity Gini impurity :
1
20BBBBBB@1 c 1X
z=0p2
z1CCCCCCA:
All of these impurities are maximal when the label proportions are equal to 1 =c. Typical
shapes of the above impurity measures are illustrated in Figure 8.3 for the two-label case,
with class probabilities pand 1 p. We see here the similarity of the di erent impurity
measures. Note that impurities can be arbitrarily scaled, and so using ln( pz)=log2(pz) ln(2)
instead of log2(pz) above gives an equivalent entropy impurity.
8.2.3 Termination Criterion
When building a tree, one can deÔ¨Åne various types of termination conditions. For example,
we might stop when the number of data points in the tree node (the size of the input set
in Algorithm 8.2.1) is less than or equal to some predeÔ¨Åned number. Or we might choose
the maximal depth of the tree in advance. Another possibility is to stop when there is noChapter 8. Decision Trees and Ensemble Methods 293
0 0 .2 0 .4 0 .6 0 .8 100.20.40.6
pimpuritycross-entropy
Gini index
misclassiÔ¨Åcation
Figure 8.3: Entropy, Gini, and misclassiÔ¨Åcation impurities for binary classiÔ¨Åcation, with
class frequencies p1=pandp2=1 p. The entropy impurity was normalized (divided by
2), to ensure that all impurity measures attain the same maximum value of 1 /2 atp=1=2.
signiÔ¨Åcant advantage, in terms of training loss, to split regions. Ultimately, the quality of a
tree is determined by its predictive performance (generalization risk) and the termination
condition should aim to strike a balance between minimizing the approximation error and
minimizing the statistical error, as discussed in Section 2.4. + 31
Example 8.2 (Fixed Tree Depth) To illustrate how the tree depth impacts on the gener-
alization risk, consider Figure 8.4, which shows the typical behavior of the cross-validation
loss as a function of the tree depth. Recall that the cross-validation loss is an estimate of the
expected generalization risk. Complicated (deep) trees tend to overÔ¨Åt the training data by
producing many divisions of the feature space. As we have seen, this overÔ¨Åtting problem is
typical of all learning methods; see Chapter 2 and in particular Example 2.1. To conclude, + 26
increasing the maximal depth does not necessarily result in better performance.
0 5 10 15 20 25 300.30.350.40.45
tree depthloss
Figure 8.4: The ten-fold cross-validation loss as a function of the maximal tree depth for a
classiÔ¨Åcation problem. The optimal maximal tree depth is here 6.294 8.2. Top-Down Construction of Decision Trees
To create Figure 8.4 we used1the Python method make_blobs from the sklearn
module to produce a training set of size n=5000 with ten-dimensional feature vectors +490
(thus, p=10 andX=R10), each of which is classiÔ¨Åed into one of c=3 classes. The full
code is given below.
TreeDepthCV.py
import numpy as np
from sklearn.datasets import make_blobs
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import zero_one_loss
import matplotlib.pyplot as plt
def ZeroOneScore(clf, X, y):
y_pred = clf.predict(X)
return zero_one_loss(y, y_pred)
# Construct the training set
X, y = make_blobs(n_samples=5000, n_features=10, centers=3,
random_state=10, cluster_std=10)
# construct a decision tree classifier
clf = DecisionTreeClassifier(random_state=0)
# Cross -validation loss as a function of tree depth (1 to 30)
xdepthlist = []
cvlist = []
tree_depth = range (1,30)
for dintree_depth:
xdepthlist.append(d)
clf.max_depth=d
cv = np.mean(cross_val_score(clf, X, y, cv=10, scoring=
ZeroOneScore))
cvlist.append(cv)
plt.xlabel( 'tree depth ', fontsize=18, color= 'black ')
plt.ylabel( 'loss ', fontsize=18, color= 'black ')
plt.plot(xdepthlist , cvlist , '-*', linewidth=0.5)
The code above relies heavily on sklearn and hides the implementation details. To
show how decision trees are actually constructed using the previous theory, we proceed
with a very basic implementation.
8.2.4 Basic Implementation
In this section we implement a regression tree, step by step. To run the program, amalgam-
ate the code snippets below into one Ô¨Åle, in the order presented. First, we import various
packages and deÔ¨Åne a function to generate the training and test data.
1The data used for Figure 8.1 was produced in a similar way.Chapter 8. Decision Trees and Ensemble Methods 295
BasicTree.py
import numpy as np
from sklearn.datasets import make_friedman1
from sklearn.model_selection import train_test_split
def makedata():
n_points = 500 # number of samples
X, y = make_friedman1(n_samples=n_points , n_features=5,
noise=1.0, random_state =100)
return train_test_split(X, y, test_size=0.5, random_state=3)
The ‚Äúmain‚Äù method calls the makedata method, uses the training data to build a regres-
sion tree, and then predicts the responses of the test set and reports the mean squared-error
loss.
def main():
X_train , X_test , y_train , y_test = makedata()
maxdepth = 10 # maximum tree depth
# Create tree root at depth 0
treeRoot = TNode(0, X_train ,y_train)
# Build the regression tree with maximal depth equal to max_depth
Construct_Subtree(treeRoot , maxdepth)
# Predict
y_hat = np.zeros( len(X_test))
for iin range (len(X_test)):
y_hat[i] = Predict(X_test[i],treeRoot)
MSE = np.mean(np.power(y_hat - y_test ,2))
print ("Basic tree: tree loss = ", MSE)
The next step is to specify a tree node as a Python class. Each node has a number of
attributes, including the features and the response data ( Xandy) and the depth at which
the node is placed in the tree. The root node has depth 0. Each node wcan calculate its
contribution to the squared-error training lossPn
i=11fxi2Rwg(yi gw(xi))2. Note that we
have omitted the constant 1 =nterm when training the tree, which simply scales the loss
(8.2).
class TNode:
def __init__(self , depth , X, y):
self.depth = depth
self.X = X # matrix of features
self.y = y # vector of response variables
# initialize optimal split parameters
self.j = None
self.xi = None
# initialize children to be None
self.left = None
self.right = None
# initialize the regional predictor296 8.2. Top-Down Construction of Decision Trees
self.g = None
def CalculateLoss(self):
if(len(self.y)==0):
return 0
return np.sum(np.power(self.y - self.y.mean() ,2))
The function below implements the training (tree-building) Algorithm 8.2.1.
def Construct_Subtree(node , max_depth):
if(node.depth == max_depth or len (node.y) == 1):
node.g = node.y.mean()
else :
j, xi = CalculateOptimalSplit(node)
node.j = j
node.xi = xi
Xt, yt, Xf, yf = DataSplit(node.X, node.y, j, xi)
if(len(yt)>0):
node.left = TNode(node.depth+1,Xt,yt)
Construct_Subtree(node.left , max_depth)
if(len(yf)>0):
node.right = TNode(node.depth+1, Xf,yf)
Construct_Subtree(node.right , max_depth)
return node
This requires an implementation of the CalculateOptimalSplit function. To start,
we implement a function DataSplit that splits the data according to s(x)=1fxj6g.
def DataSplit(X,y,j,xi):
ids = X[:,j]<=xi
Xt = X[ids == True ,:]
Xf = X[ids == False ,:]
yt = y[ids == True]
yf = y[ids == False]
return Xt, yt, Xf, yf
TheCalculateOptimalSplit method runs through the possible splitting thresholds
from the setfxj;kgand Ô¨Ånds the optimal split.
def CalculateOptimalSplit(node):
X = node.X
y = node.y
best_var = 0
best_xi = X[0,best_var]
best_split_val = node.CalculateLoss()
m, n = X.shape
for jin range (0,n):Chapter 8. Decision Trees and Ensemble Methods 297
for iin range (0,m):
xi = X[i,j]
Xt, yt, Xf, yf = DataSplit(X,y,j,xi)
tmpt = TNode(0, Xt, yt)
tmpf = TNode(0, Xf, yf)
loss_t = tmpt.CalculateLoss()
loss_f = tmpf.CalculateLoss()
curr_val = loss_t + loss_f
if(curr_val < best_split_val):
best_split_val = curr_val
best_var = j
best_xi = xi
return best_var , best_xi
Finally, we implement the recursive method for prediction.
def Predict(X,node):
if(node.right == None and node.left != None):
return Predict(X,node.left)
if(node.right != None and node.left == None):
return Predict(X,node.right)
if(node.right == None and node.left == None):
return node.g
else :
if(X[node.j] <= node.xi):
return Predict(X,node.left)
else :
return Predict(X,node.right)
Running the main function deÔ¨Åned above gives a similar2result to what one would
achieve with the sklearn package, using the DecisionTreeRegressor method.
main() # run the main program
# compare with sklearn
from sklearn.tree import DecisionTreeRegressor
X_train , X_test , y_train , y_test = makedata() # use the same data
regTree = DecisionTreeRegressor(max_depth = 10, random_state=0)
regTree.fit(X_train ,y_train)
y_hat = regTree.predict(X_test)
MSE2 = np.mean(np.power(y_hat - y_test ,2))
print ("DecisionTreeRegressor: tree loss = ", MSE2)
Basic tree: tree loss = 9.067077996170276
DecisionTreeRegressor: tree loss = 10.197991295531748
2After establishing a best split =xj;k,sklearn assigns the corresponding feature vector randomly to
one of the two child nodes, rather than to the True child.298 8.3. Additional Considerations
8.3 Additional Considerations
8.3.1 Binary Versus Non-Binary Trees
While it is possible to split a tree node into more than two groups (multiway splits), it
generally produces inferior results compared to the simple binary split. The major reason
is that multiway splits can lead to too many nodes near the tree root that have only a
few data points, thus leaving insu cient data for later splits. As multiway splits can be
represented as several binary splits, the latter is preferred [55].
8.3.2 Data Preprocessing
Sometimes, it can be beneÔ¨Åcial to preprocess the data prior to the tree construction. For
example, PCA can be used with a view to identify the most important dimensions, which +153
in turn will lead to simpler and possibly more informative splitting rules in the internal
nodes.
8.3.3 Alternative Splitting Rules
We restricted our attention to splitting rules of the type s(x)=1fxj6g, where j2
f1;:::; pgand2R. These types of rules may not always result in a simple partition
of the feature space, as illustrated by the binary data in Figure 8.5. In this case, the feature
space could have been partitioned into just two regions, separated by a straight line.
Figure 8.5: The two groups of points can here be separated by a straight line. Instead, the
classiÔ¨Åcation tree divides up the space into many rectangles, leading to an unnecessarily
complicated classiÔ¨Åcation procedure.
In this case many classiÔ¨Åcation methods discussed in Chapter 7, such as linear discrim-
inant analysis (Section 7.4), will work very well, whereas the classiÔ¨Åcation tree is rather +259
elaborate, dividing the feature set into too many regions. An obvious remedy is to use
splitting rules of the form
s(x)=1fa>x6g:Chapter 8. Decision Trees and Ensemble Methods 299
In some cases, such as the one just discussed, it may be useful to use a splitting rule
that involves several variables, as opposed to a single one. The decision regarding the split
type clearly depends on the problem domain. For example, for logical (binary) variables
our domain knowledge may indicate that a di erent behavior is expected when both xiand
xj(i,j) are True . In this case, we will naturally introduce a decision rule of the form:
s(x)=1fxi=True andxj=Trueg:
8.3.4 Categorical Variables
When an explanatory variable is categorical with labels (levels) say f1;:::; kg, the split-
ting rule is generally deÔ¨Åned via a partition of the label set f1;:::; kginto two subsets.
SpeciÔ¨Åcally, let LandRbe a partition off1;:::; kg. Then, the splitting rule is deÔ¨Åned via
s(x)=1fxj2Lg:
For the general supervised learning case, Ô¨Ånding the optimal partition in the sense of min-
imal loss requires one to consider 2ksubsets off1;:::; kg. Consequently, Ô¨Ånding a good
splitting rule for categorical variables can be challenging when the number of labels pis
large.
8.3.5 Missing Values
Missing data is present in many real-life problems. Generally, when working with incom-
plete feature vectors, where one or more values are missing, it is typical to either com-
pletely delete the feature vector from the data (which may distort the data) or to impute
(guess) its missing values from the available data; see e.g., [120]. Tree methods, however,
allow an elegant approach for handling missing data. SpeciÔ¨Åcally, in the general case, the
missing data problem can be handled via surrogate splitting rules [20].
When dealing with categorical (factor) features, we can introduce an additional cat-
egory ‚Äúmissing‚Äù for the absent data.
The main idea of surrogate rules is as follows. First, we construct a decision (regression
or a classiÔ¨Åcation) tree via Algorithm 8.2.1. During this construction process, the solution
of the optimization problem (8.9) is calculated only over the observations that are not
missing a particular variable. Suppose that a tree node vhas a splitting rule s(x)=1fxj6
gfor some 16j6pand threshold .
For the node vwe can introduce a set of alternative splitting rules that resemble the
original splitting rule, sometimes called the primary splitting rule, using di erent variables
and thresholds. Namely, we look for a binary splitting rule s(xjj;),j,jsuch that the
data split introduced by swill be similar to the original data split from s. The similarity is
generally measured via a binary misclassiÔ¨Åcation loss, where the true classes of observa-
tions are determined by the primary splitting rule and the surrogate splitting rules serve as
classiÔ¨Åers. Consider, for example, the data in Table 8.1 and suppose that the primary split-
ting rule at node vis1fAge625g. That is, the Ô¨Åve data points are split such that the left
and the right child of vcontains two and three data points, respectively. Next, the following
surrogate splitting rules can be considered:300 8.4. Controlling the Tree Shape
1.1fSalary61500g, and
2.1fHeight6173g.
Table 8.1: Example data with three variables (Age, Height, and Salary).
Id Age Height Salary
1 20 173 1000
2 25 168 1500
3 38 191 1700
4 49 170 1900
5 62 182 2000
The1fSalary61500gsurrogate rule completely mimics the primary rule, in the sense
that the data splits induced by these rules are identical. Namely, both rules partition the
data into two sets (by Id) f1;2gandf3;4;5g. On the other hand, the 1fHeight6173grule
is less similar to the primary rule, since it causes the di erent partitionf1;2;4gandf3;5g.
It is up to the user to deÔ¨Åne the number of surrogate rules for each tree node. As soon as
these surrogate rules are available, we can use them to handle a new data point, even if the
main rule cannot be applied due to a missing value of the primary variable xj. SpeciÔ¨Åcally,
if the observation is missing the primary split variable, we apply the Ô¨Årst (best) surrogate
rule. If the Ô¨Årst surrogate variable is also missing, we apply the second best surrogate rule,
and so on.
8.4 Controlling the Tree Shape
Eventually, we are interested in getting the right-size tree. Namely, a tree that shows good
generalization properties. It was already discussed in Section 8.2.3 (Figure 8.4) that shal-
low trees tend to underÔ¨Åt and deep trees tend to overÔ¨Åt the data. Basically, a shallow tree
does not produce a su cient number of splits and a deep tree will produce many partitions
and thus many leaf nodes. If we grow the tree to a su cient depth, each training sample
will occupy a separate leaf and we will observe a zero loss with respect to the training data.
The above phenomenon is illustrated in Figure 8.6, which presents the cross-validation loss
and the training loss as a function of the tree depth.
In order to overcome the under- and the overÔ¨Åtting problem, Breiman et al. [20] ex-
amined the possibility of stopping the tree from growing as soon as the decrease in loss
due to a split of node v, as expressed in the di erence of (8.8) and (8.9), is smaller than
some predeÔ¨Åned parameter 2R. Under this setting, the tree construction process will
terminate when no leaf node can be split such that the contribution to the training loss after
this split is greater than .
The authors found that this approach was unsatisfactory. SpeciÔ¨Åcally, it was noted that a
very smallleads to an excessive amount of splitting and thus causes overÔ¨Åtting. Increasing
did not work either. The problem is that the nature of the proposed rule is one-step-look-
ahead . To see this, consider a tree node for which the best possible decrease in loss isChapter 8. Decision Trees and Ensemble Methods 301
0 5 10 15 2000.10.20.30.4
tree depthlosstrain
CV
Figure 8.6: The cross-validation and the training loss as a function of the tree depth for a
binary classiÔ¨Åcation problem.
smaller than . According to the proposed procedure, this node will not be split further. This
may, however, be sub-optimal, because it could happen that one of the node‚Äôs descendants ,
if split, could lead to a major decrease in loss.
To address these issues, a so-called pruning tree pruning routine can be employed. The idea is as
follows. We Ô¨Årst grow a very deep tree and then prune (remove nodes) it upwards until we
reach the root node. Consequently, the pruning process causes the number of tree nodes
to decrease. While the tree is being pruned, the generalization risk gradually decreases up
to the point where it starts increasing again, at which point the pruning is stopped. This
decreasing /increasing behavior is due to the bias‚Äìvariance tradeo (2.22).
We next describe the details. To start with, let vandv0be tree nodes. We say that v0is
adescendant ofvif there is a path down the tree, which leads from vtov0. If such a path
exists, we also say that vis an ancestor ofv0. Consider the tree in Figure 8.7.
To formally deÔ¨Åne pruning, we will require the following DeÔ¨Ånition 8.1. An example
of pruning is demonstrated in Figure 8.8.
DeÔ¨Ånition 8.1: Branches and Pruning
1. A tree branch tree branch Tvof the treeTis a sub-tree of Trooted at node v2T.
2. The pruning of branch Tvfrom a treeTis performed via deletion of the entire
branchTvfromTexcept the branch‚Äôs root node v. The resulting pruned tree is
denoted byT Tv.
3. A sub-tree T Tvis called a pruned sub-tree of T. We indicate this with the
notationT TvTorTT Tv.
A basic decision tree pruning procedure is summarized in Algorithm 8.4.1.302 8.4. Controlling the Tree Shape
v1
v2
v4
v7v5
v8 v9v3
v6
v10 v11
Figure 8.7: The node v9is a descendant of v2, and v2is an ancestor offv4;v5;v7;v8;v9g, but
v6is not a descendant of v2.
v1
v2
v4
v7v5
v8 v9v3
v6
v10 v11
(a)T
v2
v4
v7v5
v8 v9 (b)Tv2
v1
v2 v3
v6
v10 v11 (c)T Tv2
Figure 8.8: The pruned tree T Tv2in (c) is the result of pruning the Tv2branch in (b) from
the original tree Tin (a).
Algorithm 8.4.1: Decision Tree Pruning
Input: Training set .
Output: Sequence of decision trees T0T1
1Build a large decision tree T0via Algorithm 8.2.1. [A possible termination
criterion for that algorithm is to have some small predetermined number of data
points at each terminal node of T0.]
2T0 T0
3k 0
4whileT0has more than one node do
5 k k+1
6 Choose v2T0.
7 Prune the branch rooted at vfromT0.
8Tk T0 TvandT0 Tk.
9returnT0;T1;:::;TkChapter 8. Decision Trees and Ensemble Methods 303
LetT0be the initial (deep) tree and let Tkbe the tree obtained after the k-th pruning
operation, for k=1;:::; K. As soon as the sequence of trees T0T1TKis avail-
able, one can choose the best tree of fTkgK
k=1according to the smallest generalization risk.
SpeciÔ¨Åcally, we can split the data into training and validation sets. In this case, Algorithm
8.4.1 is executed using the training set and the generalization risks of fTkgK
k=1are estimated
via the validation set.
While Algorithm 8.4.1 and the corresponding best tree selection process look appeal-
ing, there is still an important question to consider; namely, how to choose the node vand
the corresponding branch Tvin Line 6 of the algorithm. In order to overcome this problem,
Breiman proposed a method called cost complexity pruning , which we discuss next.
8.4.1 Cost-Complexity Pruning
LetTT0be a tree obtained via pruning of a tree T0. Denote the set of leaf (terminal)
nodes ofTbyW. The number of leaves jWj is a measure for the complexity of the tree;
recall thatjWj is the number of regions fRwgin the partition of X. Corresponding to each
treeTis a prediction function g, as in (8.1). In cost-complexity pruning cost-complexity
pruningthe objective is to
Ô¨Ånd a prediction function g(or, equivalently, tree T) that minimizes the training loss `(g)
while taking into account the complexity of the tree. The idea is to regularize the training
loss, similar to what was done in Chapter 6, by adding a penalty term for the complexity
of the tree. This leads to the following deÔ¨Ånition.
DeÔ¨Ånition 8.2: Cost-Complexity Measure
Let=f(xi;yi)gn
i=1be a data set and >0 be a real number. For a given tree T, the
cost-complexity measure cost-complexity
measureC(;T) is deÔ¨Åned as:
C(;T) :=1
nX
w2W0BBBBB@nX
i=11fxi2R wgLoss( yi;gw(xi))1CCCCCA+jWj (8.13)
=`(g)+jWj;
where`(g)is the training loss (8.2).
Small values of result in a small penalty for the tree complexity jWj, and thus large
trees (that Ô¨Åt the entire training data well) will minimize the measure C(;T). In particular,
for=0,T=T0will be the minimizer of C(;T). On the other hand, large values of 
will prefer smaller trees or, more precisely, trees with fewer leaves. For su ciently large
, the solution Twill collapse to a single (root) node.
It can be shown that, for every value of , there exists a smallest minimizing sub-
tree with respect to the cost-complexity measure. In practice, a suitable is selected via
observing the performance of the learner on the validation set or by cross-validation.
These advantages and the corresponding limitations are detailed next.304 8.4. Controlling the Tree Shape
8.4.2 Advantages and Limitations of Decision Trees
We list a number of advantages and disadvantages of decision trees, as compared with
other supervised learning methods such as were discussed in Chapters 5, 6, and 7.
Advantages
1. The tree structure can handle both categorical and numerical features in a natural
and straightforward way. SpeciÔ¨Åcally, there is no need to pre-process categorical
features, say via the introduction of dummy variables.
2. The Ô¨Ånal tree obtained after the training phase can be compactly stored for the pur-
pose of making predictions for new feature vectors. The prediction process only
involves a single tree traversal from the tree root to a leaf.
3. The hierarchical nature of decision trees allows for an e cient encoding of the fea-
ture‚Äôs conditional information. SpeciÔ¨Åcally, after an internal split of a feature xjvia
the standard splitting rule (8.10), Algorithm 8.2.1 will only consider such subsets of
data that were constructed based on this split, thus implicitly exploiting the corres-
ponding conditional information from the initial split of xj.
4. The tree structure can be easily understood and interpreted by domain experts with
little statistical knowledge, since it is essentially a logical decision Ô¨Çow diagram.
5. The sequential decision tree growth procedure in Algorithm 8.2.1, and in particular
the fact that the tree has been split using the most important features, provides an
implicit step-wise variable elimination procedure. In addition, the partition of the
variable space into smaller regions results in simpler prediction problems in these
regions.
6. Decision trees are invariant under monotone transformations of the data. To see this,
consider the (optimal) splitting rule s(x)=1fx362g, where x3is a positive feature.
Suppose that x3is transformed to x0
3=x2
3. Now, the optimal splitting rule will take
the form s(x)=1fx0
364g.
7. In the classiÔ¨Åcation setting, it is common to report not only the predicted value of a
feature vector, e.g., as in (8.6), but also the respective class probabilities. Decision
trees handle this task without any additional e ort. SpeciÔ¨Åcally, consider a new fea-
ture vector. During the estimation process, we will perform a tree traversal and the
point will end up in a certain leaf w. The probability of this feature vector lying in
class zcan be estimated as the proportion of training points in wthat are in class z.
8. As each training point is treated equally in the construction of a tree, their structure
of the tree will be relatively robust to outliers. In a way, trees exhibit a similar kind
of robustness as the sample median does for real-valued data.Chapter 8. Decision Trees and Ensemble Methods 305
Limitations
Despite the fact that the decision trees are extremely interpretable, the predictive accuracy
is generally inferior to other established statistical learning methods. In addition, decision
trees, and in particular very deep trees that were not subject to pruning, are heavily reliant
on their training set. A small change in the training set can result in a dramatic change of the
resulting decision tree. Their inferior predictive accuracy, however, is a direct consequence
of the bias‚Äìvariance tradeo . SpeciÔ¨Åcally, a decision tree model generally exhibits a high
variance. To overcome the above limitations, several promising approaches such as bag-
ging,random forest , and boosting are introduced below.
The bagging approach was initially introduced in the context of an ensemble of
decision trees. However, both the bagging and the boosting methods can be applied
to improve the accuracy of general prediction functions.
8.5 Bootstrap Aggregation
The major idea of the bootstrap aggregation orbagging bagging method is to combine prediction
functions learned from multiple data sets, with a view to improving overall prediction
accuracy. Bagging is especially beneÔ¨Åcial when dealing with predictors that tend to overÔ¨Åt
the data, such as in decision trees, where the (unpruned) tree structure is very sensitive to
small changes in the training set [37, 55].
To start with, consider an idealized setting for a regression tree, where we have access
toBiid copies3T1;:::;TBof a training setT. Then, we can train Bseparate regression
models ( Bdierent decision trees) using these sets, giving learners gT1;:::; gTB, and take
their average:
gavg(x)=1
BBX
b=1gTb(x): (8.14)
By the law of large numbers, as B! 1 , the average prediction function converges to +445
the expected prediction function gy:=EgT. The following result shows that using gyas
a prediction function (if it were known) would result in an expected squared-error gen-
eralization risk that is less than or equal to the expected generalization risk for a general + 24
prediction function gT. It thus suggests that taking an average of prediction functions may
lead to a better expected squared-error generalization risk.
Theorem 8.1: Expected Squared-Error Generalization Risk
LetTbe a random training set and let X;Ybe a random feature vector and response
that are independent of T. Then,
E
Y gT(X)2
>E
Y gy(X)2:
3In this sectionTkmeans the k-th training set, not a training set of size k.306 8.5. Bootstrap Aggregation
Proof: We have
E"
Y gT(X)2X;Y#
>
E[YjX;Y] E[gT(X)jX;Y]2
=
Y gy(X)2
;
where the inequality follows from EU2>(EU)2for any (conditional) expectation. Con-
sequently, by the tower property, +431
E
Y gT(X)2
=Eh
Eh Y gT(X)2jX;Yii
>E
Y gy(X)2
:

Unfortunately, multiple independent data sets are rarely available. But we can substi-
tute them by bootstrapped ones. SpeciÔ¨Åcally, instead of the T1;:::;TBsets, we can obtain
random training sets T
1;:::;T
Bby resampling them from a single (Ô¨Åxed) training set , +76
similar to Algorithm 3.2.6, and use them to train Bseparate models. By model averaging
as in (8.14) we obtain the bootstrapped aggregated estimator or bagged estimator bagged
estimatorof the
form:
gbag(x)=1
BBX
b=1gT
b(x): (8.15)
Algorithm 8.5.1: Bootstrap Aggregation Sampling
Input: Training set =f(xi;yi)gn
i=1and resample size B.
Output: Bootstrapped data sets.
1forb=1toBdo
2T
b ;
3 fori=1tondo
4 Draw UU(0;1)
5 I dnUe // select random index
6T
b T
b[f(xI;yI)g.
7returnT
b;b=1;:::; B.
Remark 8.1 (Bootstrap Aggregation for ClassiÔ¨Åcation Problems) Note that (8.15)
is suitable for handling regression problems. However, the bagging idea can be readily
extended to handle classiÔ¨Åcation settings as well. For example, gbagcan take the majority
vote amongfgT
bg;b=1;:::; B; that is, to accept the most frequent class among Bpredict-
ors.
While bagging can be applied for any statistical model (such as decision trees, neural
networks, linear regression, K-nearest neighbors, and so on), it is most e ective for pre-
dictors that are sensitive to small changes in the training set. The reason becomes clear
when we decompose the expected generalization risk as
E`(gT)=`+E(E[gT(X)jX] g(X))2
|                           {z                           }
expected squared bias+E[Var[gT(X)jX]]|                 {z                 }
expected variance; (8.16)Chapter 8. Decision Trees and Ensemble Methods 307
similar to (2.22). Compare this with the same decomposition for the average prediction + 35
function gbagin (8.14). As Egbag(x)=EgT(x), we see that any possible improvement in
the generalization risk must be due to the expected variance term. Averaging and bagging
are thus only useful for predictors with a large expected variance, relative to the other two
terms. Examples of such ‚Äúunstable‚Äù predictors include decision trees, neural networks, and
subset selection in linear regression [22]. On the other hand, ‚Äústable‚Äù predictors are in-
sensitive to small data changes, an example being the K-nearest neighbors method. Note
that for independent training sets T1;:::;TBa reduction of the variance by a factor Bis
achieved:Vargbag(x)=B 1VargT(x). Again, it depends on the squared bias and irredu-
cible loss how signiÔ¨Åcant this reduction is for the generalization risk.
Remark 8.2 (Limitations of Bagging) It is important to remember that gbagis not ex-
actly equal to gavg, which in turn is not exactly gy. SpeciÔ¨Åcally, gbagis constructed from the
bootstrap approximation of the sampling pdf f. As a consequence, for stable predictors,
it can happen that gbagwill perform worse than gT. In addition to the deterioration of the
bagging performance for stable procedures, it can also happen that gThas already achieved
a near optimal predictive accuracy given the available training data. In this case, bagging
will not introduce a signiÔ¨Åcant improvement.
The bagging process provides an opportunity to estimate the generalization risk of
the bagged model without an additional test set. SpeciÔ¨Åcally, recall that we obtain the
T
1;:::;T
Bsets from a single training set by sampling via Algorithm 8.5.1, and use them
to train Bseparate models. It can be shown (see Exercise 8) that, for large sample sizes, on
average about a third (more precisely, a fraction e 10:37) of the original sample points
are not included in bootstrapped set T
bfor 16b6B. Therefore, these samples can be
used for the loss estimation. These samples are called out-of-bag out-of-bag (OOB) observations.
SpeciÔ¨Åcally, for each sample from the original data set, we calculate the OOB loss using
predictors that were trained without this particular sample. The estimation procedure is
summarized in Algorithm 8.5.2. Hastie et al. [55] observe that, under certain conditions, the
OOB loss is almost identical to the n-fold cross-validation loss. In addition, the OOB loss
can be used to determine the number of trees required. SpeciÔ¨Åcally, we can train predictors
until the OOB loss stops changing. Namely, decision trees are added until the OOB loss
stabilizes.
Algorithm 8.5.2: Out-of-Bag Loss Estimation
Input: The original data set =f(x1;y1);:::; (xn;yn)g, the bootstrapped data sets
fT
1;:::;T
Bg, and the trained predictorsn
gT
1;:::; gT
Bo
.
Output: Out-of-bag loss for the averaged model.
1fori=1tondo
2Ci ; // Indices of predictors not depending on (xi;yi)
3 forb=1toBdo
4 if(xi;yi)<T
bthenCi C i[fbg
5 Y0
i jC ij 1P
b2CigT
b(xi)
6 Li Loss
yi;Y0
i
7LOOB 1
nPn
i=1Li
8return LOOB.308 8.5. Bootstrap Aggregation
Example 8.3 (Bagging for a Regression Tree) We next proceed with a basic bagging
example for a regression tree, in which we compare the decision tree estimator with the
corresponding bagged estimator. We use the R2metric (coe cient of determination) for
comparison.
BaggingExample.py
import numpy as np
from sklearn.datasets import make_friedman1
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
np.random.seed(100)
# create regression problem
n_points = 1000 # points
x, y = make_friedman1(n_samples=n_points , n_features=15,
noise=1.0, random_state =100)
# split to train/test set
x_train , x_test , y_train , y_test = \
train_test_split(x, y, test_size=0.33, random_state =100)
# training
regTree = DecisionTreeRegressor(random_state =100)
regTree.fit(x_train ,y_train)
# test
yhat = regTree.predict(x_test)
# Bagging construction
n_estimators=500
bag = np.empty((n_estimators), dtype= object )
bootstrap_ds_arr = np.empty((n_estimators), dtype= object )
for iin range (n_estimators):
# sample bootstrapped data set
ids = np.random.choice( range (0,len(x_train)),size= len(x_train),
replace=True)
x_boot = x_train[ids]
y_boot = y_train[ids]
bootstrap_ds_arr[i] = np.unique(ids)
bag[i] = DecisionTreeRegressor()
bag[i].fit(x_boot ,y_boot)
# bagging prediction
yhatbag = np.zeros( len(y_test))
for iin range (n_estimators):
yhatbag = yhatbag + bag[i].predict(x_test)
yhatbag = yhatbag/n_estimators
# out of bag loss estimationChapter 8. Decision Trees and Ensemble Methods 309
oob_pred_arr = np.zeros( len(x_train))
for iin range (len(x_train)):
x = x_train[i].reshape(1, -1)
C = []
for bin range (n_estimators):
if(np.isin(i, bootstrap_ds_arr[b])==False):
C.append(b)
for pred in bag[C]:
oob_pred_arr[i] = oob_pred_arr[i] + (pred.predict(x)/ len(C))
L_oob = r2_score(y_train , oob_pred_arr)
print ("DecisionTreeRegressor R^2 score = ",r2_score(y_test , yhat),
"\nBagging R^2 score = ", r2_score(y_test , yhatbag),
"\nBagging OOB R^2 score = ",L_oob)
DecisionTreeRegressor R^2 score = 0.575438224929718
Bagging R^2 score = 0.7612121189201985
Bagging OOB R^2 score = 0.7758253149069059
The decision tree bagging improves the test-set R2score by about 32% (from 0 :575
to 0:761). Moreover, the OOB score (0 :776) is very close to the true generalization risk
(0:761) of the bagged estimator.
The bagging procedure can be further enhanced by introducing random forests, which
is discussed next.
8.6 Random Forests
In Section 8.5, we discussed the intuition behind the prediction averaging procedure. Spe-
ciÔ¨Åcally, for some feature vector xletZb=gTb(x);b=1;2;:::; Bbe iid prediction val-
ues, obtained from independent training sets T1;:::;TB. Suppose that VarZb=2for all
b=1;:::; B. Then the variance of the average prediction value ZBis equal to2=B. How-
ever, if bootstrapped data sets fT
bgare used instead, the corresponding random variables
fZbgwill be correlated . In particular, Zb=gT
b(x) for b=1;:::; Bare identically distrib-
uted (but not independent) with some positive pairwise correlation %. It then holds that (see
Exercise 9)
VarZB=%2+2(1 %)
B: (8.17)
While the second term of (8.17) goes to zero as the number of observation Bincreases, the
Ô¨Årst term remains constant.
This issue is particularly relevant for bagging with decision trees. For example, con-
sider a situation in which there exists a feature that provides a very good split of the data.
Such a feature will be selected and split for every fgT
bgB
b=1at the root level and we will
consequently end up with highly correlated predictions. In such a situation, prediction
averaging will not introduce the desired improvement in the performance of the bagged
predictor.310 8.6. Random Forests
The major idea of random forests is to perform bagging in combination with a ‚Äúdecor-
relation‚Äù of the trees by including only a subset of features during the tree construction. For
each bootstrapped training set T
bwe build a decision tree using a randomly selected subset
ofm6pfeatures for the splitting rules. This simple but powerful idea will decorrelate the
trees, since strong predictors will have a smaller chance to be considered at the root levels.
Consequentially, we can expect to improve the predictive performance of the bagged
estimator. The resulting predictor (random forest) construction is summarized in Algorithm
8.6.1.
Algorithm 8.6.1: Random Forest Construction
Input: Training set =f(xi;yi)gn
i=1, the number of trees in the forest B, and the
number m6pof features to be included, where pis the total number of
features in x.
Output: Ensemble of trees.
1Generate bootstrapped training sets fT
1;:::;T
Bgvia Algorithm 8.5.1.
2forb=1toBdo
3 Train a decision tree gT
bvia Algorithm 8.2.1, where each split is performed
using mrandomly selected features out of p.
4returnfgT
bgB
b=1.
For regression problems, the output of Algorithm 8.6.1 is combined to yield the random
forest prediction function:
gRF(x)=1
BBX
b=1gT
b(x):
In the classiÔ¨Åcation setting, similar to Remark 8.1, we take instead the majority vote from
thefgT
bg.
Example 8.4 (Random Forest for a Regression Tree) We continue with the basic
bagging Example 8.3 for a regression tree, in which we compared the decision tree es-
timator with the corresponding bagged estimator. Here, however, we use the random forest
with B=500 trees and a subset size m=8. It can be seen that the random forest‚Äôs R2score
is outperforming that of the bagged estimator.
BaggingExampleRF.py
from sklearn.datasets import make_friedman1
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
# create regression problem
n_points = 1000 # points
x, y = make_friedman1(n_samples=n_points , n_features=15,
noise=1.0, random_state =100)
# split to train/test set
x_train , x_test , y_train , y_test = \
train_test_split(x, y, test_size=0.33, random_state =100)
rf = RandomForestRegressor(n_estimators=500, oob_score = True ,Chapter 8. Decision Trees and Ensemble Methods 311
max_features=8,random_state=100)
rf.fit(x_train ,y_train)
yhatrf = rf.predict(x_test)
print ("RF R^2 score = ", r2_score(y_test , yhatrf),
"\nRF OOB R^2 score = ", rf.oob_score_)
RF R^2 score = 0.8106589580845707
RF OOB R^2 score = 0.8260541058404149
Remark 8.3 (The Optimal Number of Subset Features m)The default values for m
arebp=3candjppk
for regression and classiÔ¨Åcation setting, respectively. However, the
standard practice is to treat mas a hyperparameter that requires tuning, depending on the
speciÔ¨Åc problem at hand [55].
Note that the procedure of bagging decision trees is a special case of a random forest
construction (see Exercise 11). Consequently, the OOB loss is readily available for random
forests.
While the advantage of bagging in the sense of enhanced accuracy is clear, we should
also consider its negative aspects and, in particular, the loss of interpretability. SpeciÔ¨Åcally
a random forest consists of many trees, thus making the prediction process both hard to
visualize and interpret. For example, given a random forest, it is not easy to determine a
subset of features that are essential for accurate prediction.
The feature importance measure intends to address this issue. The idea is as follows.
Each internal node of a decision tree induces a certain decrease in the training loss; see
(8.9). Let us denote this decrease in the training loss by Loss(v), where vis not a leaf node
ofT. In addition, recall that for splitting rules of the type 1fxj6g(16j6p), each node
vis associated with a feature xjthat determines the split. Using the above deÔ¨Ånitions, we
can deÔ¨Åne the feature importance feature
importanceofxjas
IT(xj)=X
vinternal2TLoss(v)1fxjis associated with vg;16j6p: (8.18)
While (8.18) is deÔ¨Åned for a single tree, it can be readily extended to random forests.
SpeciÔ¨Åcally, the feature importance in that case will be averaged over all trees of the forest;
that is, for a forest consisting of BtreesfT1;:::;TBg, the feature importance measure is:
IRF(xj)=1
BBX
b=1ITb(xj);16j6p: (8.19)
Example 8.5 (Feature Importance) We consider a classiÔ¨Åcation problem with 15 fea-
tures. The data is speciÔ¨Åcally designed to contain only 5 informative features out of 15.
In the code below, we apply the random forest procedure and calculate the corresponding
feature importance measures, which are summarized in Figure 8.9.312 8.6. Random Forests
VarImportance.py
import numpy as np
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt, pylab
n_points = 1000 # create regression data with 1000 data points
x, y = make_classification(n_samples=n_points , n_features=15,
n_informative=5, n_redundant=0, n_repeated=0, random_state=100,
shuffle=False)
rf = RandomForestClassifier(n_estimators=200, max_features="log2")
rf.fit(x,y)
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]
for fin range (15):
print ("Feature %d (%f)" % (indices[f]+1, importances[indices[f
]]))
std = np.std([rf.feature_importances_ for tree inrf.estimators_],
axis=0)
f = plt.figure()
plt.bar( range (x.shape[1]), importances[indices],
color="b", yerr=std[indices], align="center")
plt.xticks( range (x.shape[1]), indices+1)
plt.xlim([-1, x.shape[1]])
pylab.xlabel("feature index")
pylab.ylabel("importance")
plt.show()
5 1 2 4 3 7 11 13 6 9 15 8 14 10 1200.10.2
feature indeximportance
Figure 8.9: Importance measure for the 15-feature data set with only 5 informative features
x1;x2;x3;x4, and x5.Chapter 8. Decision Trees and Ensemble Methods 313
Clearly, it is hard to visualize and understand the prediction process based on 200 trees.
However, Figure 8.9 shows that the features x1;x2;x3;x4;andx5were correctly identiÔ¨Åed
as being important.
8.7 Boosting
Boosting is a powerful idea that aims to improve the accuracy of any learning algorithm,
especially when involving weak learners weak learners ‚Äî simple prediction functions that exhibit per-
formance slightly better than random guessing. Shallow decision trees typically yield weak
learners.
Originally, boosting was developed for binary classiÔ¨Åcation tasks, but it can be readily
extended to handle general classiÔ¨Åcation and regression problems. The boosting approach
has some similarity with the bagging method in the sense that boosting uses an ensemble of
prediction functions. Despite this similarity, there exists a fundamental di erence between
these methods. SpeciÔ¨Åcally, while bagging involves the Ô¨Åtting of prediction functions to
bootstrapped data, the predicting functions in boosting are learned sequentially . That is,
each learner uses information from previous learners.
The idea is to start with a simple model (weak learner) g0for the data =f(xi;yi)gn
i=1
and then to improve or ‚Äúboost‚Äù this learner to a learner g1:=g0+h1. Here, the function h1
is found by minimizing the training loss for g0+h1over all functions hin some class of
functionsH. For example,Hcould be the set of prediction functions that can be obtained
via a decision tree of maximal depth 2. Given a loss function Loss, the function h1is thus
obtained as the solution to the optimization problem
h1=argmin
h2H1
nnX
i=1Loss (yi;g0(xi)+h(xi)): (8.20)
This process can be repeated for g1to obtain g2=g1+h2, and so on, yielding the boosted
prediction function
gB(x)=g0(x)+BX
b=1hb(x): (8.21)
Instead of using the updating step gb=gb 1+hb, one prefers to use the smooth updating
step gb=gb 1+hb, for some suitably chosen step-size parameter . As we shall see
shortly, this helps reduce overÔ¨Åtting.
Boosting can be used for regression and classiÔ¨Åcation problems. We start with a simple
regression setting, using the squared-error loss; thus, Loss( y;by)=(y by)2. In this case, it
is common to start with g0(x)=n 1Pn
i=1yi, and each hbforb=1;:::; Bis chosen as a
learner for the data set bof residuals corresponding to gb 1. That is,b:=n
xi;e(b)
ion
i=1,
with
e(b)
i:=yi gb 1(xi): (8.22)
This leads to the following boosting procedure for regression with squared-error loss.314 8.7. Boosting
Algorithm 8.7.1: Regression Boosting with Squared-Error Loss
Input: Training set =f(xi;yi)gn
i=1, the number of boosting rounds B, and a
shrinkage step-size parameter .
Output: Boosted prediction function.
1Setg0(x) n 1Pn
i=1yi.
2forb=1toBdo
3 Sete(b)
i yi gb 1(xi) for i=1;:::; n, and letb n
xi;e(b)
ion
i=1.
4 Fit a prediction function hbon the training data b.
5 Setgb(x) gb 1(x)+hb(x).
6return gB.
Thestep-size parameter  step-size
parameterintroduced in Algorithm 8.7.1 controls the speed of the
Ô¨Åtting process. SpeciÔ¨Åcally, for small values of , boosting takes smaller steps to-
wards the training loss minimization. The step-size is of great practical import-
ance, since it helps the boosting algorithm to avoid overÔ¨Åtting. This phenomenon is
demonstrated in Figure 8.10.
‚àí2‚àí1 0 1 2‚àí50050train data
g1000 , Œ≥= 1
‚àí2‚àí1 0 1 2‚àí50050train data
g1000 , Œ≥= 0 .005
Figure 8.10: The left and the right panels show the Ô¨Åtted boosting regression model g1000
with=1:0 and=0:005, respectively. Note the overÔ¨Åtting on the left.
A very basic implementation of Algorithm 8.7.1 which reproduces Figure 8.10 is
provided below.
RegressionBoosting.py
import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression
import matplotlib.pyplot as plt
def TrainBoost(alpha ,BoostingRounds ,x,y):
g_0 = np.mean(y)Chapter 8. Decision Trees and Ensemble Methods 315
residuals = y-alpha*g_0
# list of basic regressor
g_boost = []
for iin range (BoostingRounds):
h_i = DecisionTreeRegressor(max_depth=1)
h_i.fit(x,residuals)
residuals = residuals - alpha*h_i.predict(x)
g_boost.append(h_i)
return g_0, g_boost
def Predict(g_0, g_boost ,alpha , x):
yhat = alpha*g_0*np.ones( len(x))
for jin range (len(g_boost)):
yhat = yhat+alpha*g_boost[j].predict(x)
return yhat
np.random.seed(1)
sz = 30
# create data set
x,y = make_regression(n_samples=sz, n_features=1, n_informative=1,
noise=10.0)
# boosting algorithm
BoostingRounds = 1000
alphas = [1, 0.005]
for alpha inalphas:
g_0, g_boost = TrainBoost(alpha ,BoostingRounds ,x,y)
yhat = Predict(g_0, g_boost , alpha , x)
# plot
tmpX = np.reshape(np.linspace(-2.5,2,1000) ,(1000,1))
yhatX = Predict(g_0, g_boost , alpha , tmpX)
f = plt.figure()
plt.plot(x,y, '*')
plt.plot(tmpX ,yhatX)
plt.show()
The parameter can be viewed as a step size made in the direction of the negative
gradient of the squared-error training loss. To see this, note that the negative gradient
 @Loss (yi;z)
@zz=gb 1(xi)= @(yi z)2
@zz=gb 1(xi)=2(yi gb 1(xi))
is two times the residual e(b)
igiven in (8.22) that is used in Algorithm 8.7.1 to Ô¨Åt the pre-
diction function hb.
In fact, one of the major advances in the theory of boosting was the recognition that
one can use a similar gradient descent method for any di erentiable loss function. The316 8.7. Boosting
resulting algorithm is called gradient boosting gradient
boosting. The general gradient boosting algorithm is
summarized in Algorithm 8.7.2. The main idea is to mimic a gradient descent algorithm
in the following sense. At each stage of the boosting procedure, we calculate a negative +412
gradient on ntraining points x1;:::; xn(Lines 3‚Äì4). Then, we Ô¨Åt a simple model (such as
a shallow decision tree) to approximate the gradient (Line 5) for any feature x. Finally,
similar to the gradient descent method, we make a -sized step in the direction of the
negative gradient (Line 6).
Algorithm 8.7.2: Gradient Boosting
Input: Training set =f(xi;yi)gn
i=1, the number of boosting rounds B, a
dierentiable loss function Loss( y;by), and a gradient step-size parameter .
Output: Gradient boosted prediction function.
1Setg0(x) 0.
2forb=1toBdo
3 fori=1tondo
4 Evaluate the negative gradient of the loss at ( xi;yi) via
r(b)
i  @Loss (yi;z)
@zz=gb 1(xi)i=1;:::; n:
5 Approximate the negative gradient by solving
hb=argmin
h2H1
nnX
i=0
r(b)
i h(xi)2: (8.23)
6 Setgb(x) gb 1(x)+hb(x).
7return gB
Example 8.6 (Gradient Boosting for a Regression Tree) Let us continue with the ba-
sic bagging and random forest examples for a regression tree (Examples 8.3 and 8.4), where
we compared the standard decision tree estimator with the corresponding bagging and ran-
dom forest estimators. Now, we use the gradient boosting estimator from Algorithm 8.7.2,
as implemented in sklearn . We use=0:1 and perform B=100 boosting rounds. As
a prediction function hbforb=1;:::; Bwe use small regression trees of depth at most
3. Note that such individual trees do not usually give good performance; that is, they are
weak prediction functions. We can see that the resulting boosting prediction function gives
theR2score equal to 0 :899, which is better than R2scores of simple decision tree (0 :5754),
the bagged tree (0 :761), and the random forest (0 :8106).
GradientBoostingRegression.py
import numpy as np
from sklearn.datasets import make_friedman1
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_scoreChapter 8. Decision Trees and Ensemble Methods 317
# create regression problem
n_points = 1000 # points
x, y = make_friedman1(n_samples=n_points , n_features=15,
noise=1.0, random_state =100)
# split to train/test set
x_train , x_test , y_train , y_test = \
train_test_split(x, y, test_size=0.33, random_state =100)
# boosting sklearn
from sklearn.ensemble import GradientBoostingRegressor
breg = GradientBoostingRegressor(learning_rate=0.1,
n_estimators=100, max_depth =3, random_state=100)
breg.fit(x_train ,y_train)
yhat = breg.predict(x_test)
print ("Gradient Boosting R^2 score = ",r2_score(y_test , yhat))
Gradient Boosting R^2 score = 0.8993055635639531
We proceed with the classiÔ¨Åcation setting and consider the original boosting algorithm:
AdaBoost AdaBoost . The inventors of the AdaBoost method considered a binary classiÔ¨Åcation prob-
lem, where the response variable belongs to the f 1;1gset. The idea of AdaBoost is similar
to the one presented in the regression setting, that is, AdaBoost Ô¨Åts a sequence of prediction
functions g0;g1=g0+h1;g2=g0+h1+h2;:::with Ô¨Ånal prediction function
gB(x)=g0(x)+BX
b=1hb(x); (8.24)
where each function hbis of the form hb(x)=bcb(x), withb2R+and where cbis a
proper (but weak) classiÔ¨Åer in some class C. Thus, cb(x)2f 1;1g. Exactly as in (8.20), we
solve at each boosting iteration the optimization problem
(b;cb)=argmin
>0;c2C1
nnX
i=1Loss (yi;gb 1(xi)+c(xi)): (8.25)
However, in this case the loss function is deÔ¨Åned as Loss( y;by)=e yby. The algorithm starts
with a simple model g0:=0 and for each successive iteration b=1;:::; Bsolves (8.25).
Thus,
(b;cb)=argmin
>0;c2CnX
i=1e yigb 1(xi)|    {z    }
w(b)
ie yic(xi)=argmin
>0;c2CnX
i=1w(b)
ie yic(xi);
where w(b)
i:=expf yigb 1(xi)gdoes not depend on orc. It follows that
(b;cb)=argmin
>0;c2Ce nX
i=1w(b)
i1fc(xi)=yig+enX
i=1w(b)
i1fc(xi),yig
=argmin
>0;c2C(e e )`(b)
(c)+e ; (8.26)318 8.7. Boosting
where
`(b)
(c) :=Pn
i=1w(b)
i1fc(xi),yig
Pn
i=1w(b)
i
can be interpreted as the weighted zero‚Äìone training loss at iteration b.
For any>0, the program (8.26) is minimized by a classiÔ¨Åer c2Cthat minimizes
this weighted training loss; that is,
cb(x)=argmin
c2C`(b)
: (8.27)
Substituting (8.27) into (8.26) and solving for the optimal gives
b=1
2ln 1 `(b)
(cb)
`(b)
(cb)!
: (8.28)
This gives the AdaBoost algorithm, summarized below.
Algorithm 8.7.3: AdaBoost
Input: Training set =f(xi;yi)gn
i=1, and the number of boosting rounds B.
Output: AdaBoost prediction function.
1Setg0(x) 0.
2fori=1tondo
3 w(1)
i 1=n
4forb=1toBdo
5 Fit a classiÔ¨Åer cbon the training set by solving
cb=argmin
c2C`(b)
(c)=argmin
c2CPn
i=1w(b)
i1fc(xi),yig
Pn
i=1w(b)
i:
6 Setb 1
2ln 1 `(b)
(cb)
`(b)
(cb)!
. // Update weights
7 fori=1tondo
8 w(b+1)
i w(b)
iexpf yibcb(xi)g.
9return gB(x) :=PB
b=1bcb(x).
Algorithm 8.7.3 is quite intuitive. At the Ô¨Årst step ( b=1), AdaBoost assigns an equal
weight w(1)
i=1=nto each training sample ( xi;yi) in the set =f(xi;yi)gn
i=1. Note that, in
this case, the weighted zero‚Äìone training loss is equal to the regular zero‚Äìone training loss.
At each successive step b>1, the weights of observations that were incorrectly classiÔ¨Åed
by the previous boosting prediction function gbare increased, and the weights of correctly
classiÔ¨Åed observations are decreased. Due to the use of the weighted zero‚Äìone loss, the set
of incorrectly classiÔ¨Åed training samples will receive an extra weight and thus have a better
chance of being classiÔ¨Åed correctly by the next classiÔ¨Åer cb+1. As soon as the AdaBoost
algorithm Ô¨Ånds the prediction function gB, the Ô¨Ånal classiÔ¨Åcation is delivered via
sign0BBBBB@BX
b=1bcb(x)1CCCCCA:Chapter 8. Decision Trees and Ensemble Methods 319
The step-size parameter bfound by the AdaBoost algorithm in Line 6 can be
viewed as an optimal step-size in the sense of training loss minimization. How-
ever, similar to the regression setting, one can slow down the AdaBoost algorithm
by settingbto be a Ô¨Åxed (small) value b=. As usual, when the latter is done in
practice, it is tackling the problem of overÔ¨Åtting.
We consider an implementation of Algorithm 8.7.3 for a binary classiÔ¨Åcation problem.
SpeciÔ¨Åcally, during all boosting rounds, we use simple decision trees of depth 1 (also called
decision tree stumps stumps ) as weak learners. The exponential and zero‚Äìone training losses as a
function of the number of boosting rounds are presented in Figure 8.11.
AdaBoost.py
from sklearn.datasets import make_blobs
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import zero_one_loss
import numpy as np
def ExponentialLoss(y,yhat):
n = len(y)
loss = 0
for iin range (n):
loss = loss+np.exp(-y[i]*yhat[i])
loss = loss/n
return loss
# create binary classification problem
np.random.seed(100)
n_points = 100 # points
x, y = make_blobs(n_samples=n_points , n_features=5, centers=2,
cluster_std=20.0, random_state =100)
y[y==0]=-1
# AdaBoost implementation
BoostingRounds = 1000
n = len(x)
W = 1/n*np.ones(n)
Learner = []
alpha_b_arr = []
for iin range (BoostingRounds):
clf = DecisionTreeClassifier(max_depth=1)
clf.fit(x,y, sample_weight=W)
Learner.append(clf)
train_pred = clf.predict(x)
err_b = 0320 8.7. Boosting
for iin range (n):
if(train_pred[i]!=y[i]):
err_b = err_b+W[i]
err_b = err_b/np. sum(W)
alpha_b = 0.5*np.log((1-err_b)/err_b)
alpha_b_arr.append(alpha_b)
for iin range (n):
W[i] = W[i]*np.exp(-y[i]*alpha_b*train_pred[i])
yhat_boost = np.zeros( len(y))
for jin range (BoostingRounds):
yhat_boost = yhat_boost+alpha_b_arr[j]*Learner[j].predict(x)
yhat = np.zeros(n)
yhat[yhat_boost >=0] = 1
yhat[yhat_boost <0] = -1
print ("AdaBoost Classifier exponential loss = ", ExponentialLoss(y,
yhat_boost))
print ("AdaBoost Classifier zero --one loss = ",zero_one_loss(y,yhat))
AdaBoost Classifier exponential loss = 0.004224013663777142
AdaBoost Classifier zero --one loss = 0.0
200 400 600 800 1 ,00000.20.40.60.81
Blossexponential loss
zero-one loss
Figure 8.11: Exponential and zero‚Äìone training loss as a function of the number of boosting
rounds Bfor a binary classiÔ¨Åcation problem.Chapter 8. Decision Trees and Ensemble Methods 321
Further Reading
Breiman‚Äôs book on decision trees, [20], serves as a great starting point. Some additional
advances can be found in [62, 96]. From the computational point of view, there exists
an ecient recursive procedure for tree pruning; see Chapters 3 and 10 in [20]. Several
advantages and disadvantages of using decision trees are debated in [37, 55]. A detailed
discussion on bagging and random forests can be found in [21] and [23], respectively.
Freund and Schapire [44] provide the Ô¨Årst boosting algorithm, the AdaBoost. While Ad-
aBoost was developed in the context of the computational complexity of learning, it was
later discovered by Friedman [45] that AdaBoost is a special case of an additive model.
In addition, it was shown that for any di erentiable loss function, there exists an e cient
boosting procedure which mimics the gradient descent algorithm. The foundation of the
resulting gradient boosting method is detailed in [45]. Python packages that implement
gradient boosting include XGBoost andLightGBM .
Exercises
1. Show that any training set =f(x;yi);i=1;:::; ngcan be Ô¨Åtted via a tree with zero
training loss.
2. Suppose during the construction of a decision tree we wish to specify a constant re-
gional prediction function gwon the regionRw, based on the training data in Rw, say
f(x1;y1);:::; (xk;yk)g. Show that gw(x) :=k 1Pk
i=1yiminimizes the squared-error loss.
3. Using the program from Section 8.2.4, write a basic implementation of a decision tree
for a binary classiÔ¨Åcation problem. Implement the misclassiÔ¨Åcation, Gini index, and en-
tropy impurity criteria to split nodes. Compare the results.
4. Suppose in the decision tree of Example 8.1, there are 3 blue and 2 red data points in
a certain tree region. Calculate the misclassiÔ¨Åcation impurity, the Gini impurity, and the
entropy impurity. Repeat these calculations for 2 blue and 3 red data points.
5. Consider the procedure of Ô¨Ånding the best splitting rule for a categorical variable with
klabels from Section 8.3.4. Show that one needs to consider 2ksubsets off1;:::; kgto Ô¨Ånd
the optimal partition of labels.
6. Reproduce Figure 8.6 using the following classiÔ¨Åcation data.
from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=5000, n_features=10, centers=3,
random_state=10, cluster_std=10)
7. Prove (8.13); that is, show that
X
w2W0BBBBB@nX
i=11fxi2R wgLoss( yi;gw(xi))1CCCCCA=n`(g):322 Exercises
8. Suppose is a training set with nelements and , also of size n, is obtained from 
by bootstrapping; that is, resampling with replacement. Show that for large n,does not
contain a fraction of about e 10:37 of the points from .
9. Prove Equation (8.17).
10. Consider the following training /test split of the data. Construct a random forest re-
gressor and identify the optimal subset size min the sense of R2score (see Remark 8.3).
import numpy as np
from sklearn.datasets import make_friedman1
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
# create regression problem
n_points = 1000 # points
x, y = make_friedman1(n_samples=n_points , n_features=15,
noise=1.0, random_state =100)
# split to train/test set
x_train , x_test , y_train , y_test = \
train_test_split(x, y, test_size=0.33, random_state =100)
11. Explain why bagging decision trees are a special case of random forests.
12. Show that (8.28) holds.
13. Consider the following classiÔ¨Åcation data and module imports:
from sklearn.datasets import make_blobs
from sklearn.metrics import zero_one_loss
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingClassifier
X_train , y_train = make_blobs(n_samples=5000, n_features=10,
centers=3, random_state=10, cluster_std=5)
Using the gradient boosting algorithm with B=100 rounds, plot the training loss as a
function of , for=0:1;0:3;0:5;0:7;1. What is your conclusion regarding the relation
between Band?CHAPTER9
DEEPLEARNING
In this chapter, we show how one can construct a rich class of approximating func-
tions called neural networks. The learners belonging to the neural-network class of
functions have attractive properties that have made them ubiquitous in modern machine
learning applications ‚Äî their training is computationally feasible and their complexity
is easy to control and Ô¨Åne-tune.
9.1 Introduction
In Chapter 2 we described the basic supervised learning task; namely, we wish to predict a
random output Yfrom a random input X, using a prediction function g:x7!ythat belongs
to a suitably chosen class of approximating functions G. More generally, we may wish to
predict a vector-valued output yusing a prediction function g:x7!yfrom classG.
In this chapter ydenotes the vector-valued output for a given input x. This di ers
from our previous use (e.g., in Table 2.1), where ydenotes a vector of scalar outputs.
In the machine learning context, the class Gis sometimes referred to as the hypothesis
space or the universe of possible models , and the representational capacity of a hypothesisrepresentational
capacity spaceGis simply its complexity.
Suppose that we have a class of functions GL, indexed by a parameter Lthat controls
the complexity of the class, so that GL G L+1 G L+2  . In selecting a suitable
class of functions, we have to be mindful of the approximation‚Äìestimation tradeo . On the + 31
one hand, the class GLmust be complex (rich) enough to accurately represent the optimal
unknown prediction function g, which may require a very large L. On the other hand, the
learners in the class GLmust be simple enough to train with small estimation error and
with minimal demands on computer memory, which may necessitate a small L.
In balancing these competing objectives, it helps if the more complex class GL+1is
easily constructed from an already existing and simpler GL. The simpler class of functions
GLmay itself be constructed by modifying an even simpler class GL 1, and so on.
A class of functions that permits such a natural hierarchical construction is the class of
neural networks . Conceptually, a neural network with Llayers is a nonlinear parametricneural
networks regression model whose representational capacity can easily be controlled by L.
+188323324 9.1. Introduction
Alternatively, in (9.3) we will deÔ¨Åne the output of a neural network as the repeated
composition of linear and (componentwise) nonlinear functions. As we shall see, this rep-
resentation of the output will provide a Ô¨Çexible class of nonlinear functions that can be
easily di erentiated. As a result, the training of learners via gradient optimization methods
involves mostly standard matrix operations that can be performed very e ciently. +412
Historically, neural networks were originally intended to mimic the workings of the
human brain, with the network nodes modeling neurons and the network links modeling
the axons connecting neurons. For this reason, rather than using the terminology of the
regression models in Chapter 5, we prefer to use a nomenclature inspired by the apparent
resemblance of neural networks to structures in the human brain.
We note, however, that the attempts at building e cient machine learning algorithms by
mimicking the functioning of the human brain have been as unsuccessful as the attempts
at building Ô¨Çying aircraft by mimicking the Ô¨Çapping of birds‚Äô wings. Instead, many ef-
fective machine algorithms have been inspired by age-old mathematical ideas for function
approximation. One such idea is the following fundamental result (see [119] for a proof).
Theorem 9.1: Kolmogorov (1957)
Every continuous function g: [0;1]p7!Rwith p>2 can be written as
g(x)=2p+1X
j=1hj0BBBBB@pX
i=1hi j(xi)1CCCCCA;
wherefhj;hi jgis a set of univariate continuous functions that depend on g.
This result tells us that any continuous high-dimensional map can be represented as
the function composition of much simpler (one-dimensional) maps. The composition of
the maps needed to compute the output g(x) for a given input x2Rpare depicted in
Figure 9.1, showing a directed graph or neural network with three layers, denoted as l=
0;1;2.
hijxi
xpzjhj‚Üí ajz1h1‚Üí a1
zqhq‚Üí aqg‚àó(x)x1
hpqh11
Figure 9.1: Every continuous function g: [0;1]p7!Rcan be represented by a neural net-
work with one hidden layer ( l=1), an input layer ( l=0), and an output layer ( l=2).Chapter 9. Deep Learning 325
In particular, each of the pcomponents of the input xis represented as a node in the
input layer (l=0). In the hidden layer (l=1) there are q:=2p+1 nodes, each of whichhidden layeris associated with a pair of variables ( z;a) with values
zj:=pX
i=1hi j(xi) and aj:=hj(zj):
A link between nodes ( zj;aj) and xiwith weight hi jsigniÔ¨Åes that the value of zjdepends
on the value of xivia the function hi j. Finally, the output layer (l=2) represents the value
g(x)=Pq
j=1aj. Note that the arrows on the graph remind us that the sequence of the
computations is executed from left to right, or from the input layer l=0 through to the
output layer l=2.
In practice, we do not know the collection of functions fhj;hi jg, because they depend
on the unknown g. In the unlikely event that gis linear, then all of the (2 p+1)(p+1)
one-dimensional functions will be linear as well. However, in general, we should expect
that each of the functions in fhj;hi jgis nonlinear.
Unfortunately, Theorem 9.1 only asserts the existence of fhj;hi jg, and does not tell us
how to construct these nonlinear functions. One way out of this predicament is to replace
these (2 p+1)(p+1) unknown functions with a much larger number of known nonlinear
functions called activation functions .1For example, a logistic activation function isactivation
functionsS(z)=(1+exp( z)) 1:
We then hope that such a network, built from a su ciently large number of activation
functions, will have similar representational capacity as the neural network in Figure 9.1
with (2 p+1)(p+1) functions.
In general, we wish to use the simplest activation functions that will allow us to build
a learner with large representational capacity and low training cost. The logistic function
is merely one possible choice for an activation function from among inÔ¨Ånite possibilit-
ies. Figure 9.2 shows a small selection of activation functions with di erent regularity or
smoothness properties.
Heaviside or unit step rectiÔ¨Åed linear unit (ReLU) logistic
-2 0 200.51
-2 0 20123
-2 0 200.51
1fz>0g z1fz>0g (1+exp( z)) 1
Figure 9.2: Some common activation functions S(z) with their deÔ¨Åning formulas and plots.
The logistic function is an example of a sigmoid (that is, an S-shaped) function. Some
books deÔ¨Åne the logistic function as 2 S(z) 1 (in terms of our deÔ¨Ånition).
In addition to choosing the type and number of activation functions in a neural network,
we can improve its representational capacity in another important way: introduce more
hidden layers. In the next section we explore this possibility in detail.
1Activation functions derive their name from models of a neuron‚Äôs response when exposed to chemical
or electric stimuli.326 9.2. Feed-Forward Neural Networks
9.2 Feed-Forward Neural Networks
In a neural network with L+1 layers, the zero or input layer ( l=0) encodes the input feature
vector x, and the last or output layer ( l=L) encodes the (multivalued) output function g(x).
The remaining layers are called hidden layers . Each layer has a number of nodes, say pl
nodes for layer l=0;:::; L. In this notation, p0is the dimension of the input feature vector
xand, for example, pL=1 signiÔ¨Åes that g(x) is a scalar output. All nodes in the hidden
layers ( l=1;:::; L 1) are associated with a pair of variables ( z;a), which we gather
into pl-dimensional column vectors zlandal. In the so-called feed-forward feed-forward networks, the
variables in any layer lare simple functions of the variables in the preceding layer l 1. In
particular, zlandal 1are related via the linear relation zl=Wlal 1+bl, for some weight
matrix Wlandbias vector bl.weight matrix
bias vector Within any hidden layer l=1;:::; L 1, the components of the vectors zlandal
are related via al=Sl(zl), where Sl:Rpl7!Rplis a nonlinear multivalued function. All of
these multivalued functions are typically of the form
Sl(z)=[S(z1);:::; S(zdim(z))]>;l=1;:::; L 1; (9.1)
where Sis an activation function common to all hidden layers. The function SL:RpL 17!
RpLin the output layer is more general and its speciÔ¨Åcation depends, for example, on
whether the network is used for classiÔ¨Åcation or for the prediction of a continuous output
Y. A four-layer ( L=3) network is illustrated in Figure 9.3.
Input layer Hidden layers Output layer
bias
weightb1,j
z3,mS3 w1,jiz2,kS‚Üía2,kb2,kb3,m
z1,jS‚Üía1,jw2,k j w3,mkg(x) xi
Figure 9.3: A neural network with L=3: the l=0 layer is the input layer, followed by two
hidden layers, and the output layer. Hidden layers may have di erent numbers of nodes.
The output of this neural network is determined by the input vector x, (nonlinear)
functionsfSlg, as well as weight matrices Wl=[wl;i j] and bias vectors bl=[bl;j] for
l=1;2;3.Chapter 9. Deep Learning 327
Here, the ( i;j)-th element of the weight matrix Wl=[wl;i j] is the weight that con-
nects the j-th node in the ( l l)-st layer with the i-th node in the l-th layer.
The name given to L(the number of layers without the input layer) is the network depthnetwork depthand max lplis called the network width . While we mostly study networks that have an equalnetwork widthnumber of nodes in the hidden layers ( p1==pL 1), in general there can be di erent
numbers of nodes in each hidden layer.
The output g(x) of a multiple-layer neural network is obtained from the input xvia the
following sequence of computations:
x|{z}
a0!W1a0+b1|       {z       }
z1!S1(z1)|{z}
a1!W2a1+b2|       {z       }
z2!S2(z2)|{z}
a2!
!WLaL 1+bL|          {z          }
zL!SL(zL)|{z}
aL=g(x):(9.2)
Denoting the function z7!Wlz+blbyMl, the output g(x) can thus be written as the
function composition
g(x)=SLML S2M2S1M1(x): (9.3)
The algorithm for computing the output g(x) for an input xis summarized next. Note
that we leave open the possibility that the activation functions fSlghave di erent deÔ¨Ånitions
for each layer. In some cases, Slmay even depend on some or all of the already computed
z1;z2;:::anda1;a2;:::.
Algorithm 9.2.1: Feed-Forward Propagation for a Neural Network
input: Feature vector x; weightsfwl;i jg, biasesfbl;igfor each layer l=1;:::; L.
output: The value of the prediction function g(x).
1a0 x // the zero or input layer
2forl=1toLdo
3 Compute the hidden variable zl;ifor each node iin layer l:
zl Wlal 1+bl
4 Compute the activation function al;ifor each node iin layer l:
al Sl(zl)
5return g(x) aL // the output layer328 9.2. Feed-Forward Neural Networks
Example 9.1 (Nonlinear Multi-Output Regression) Given the input x2Rp0and an
activation function S:R7!R, the output g(x) :=[g1(x);:::; gp2(x)]>of anonlinear multi-
output regression model can be computed via a neural network with: +213
z1=W1x+b1;where W12Rp1p0;b12Rp1;
a1;k=S(z1;k);k=1;:::; p1;
g(x)=W2a1+b2;where W22Rp2p1;b22Rp2;
which is a neural network with one hidden layer and output function S2(z)=z. In the
special case where p1=p2=1,b2=0;W2=1, and we collect all parameters into the
vector>=[b1;W1]2Rp0+1, the neural network can be interpreted as a generalized linear
model withE[YjX=x]=h([1;x>]) for some activation function h. +204
Example 9.2 (Multi-Logit ClassiÔ¨Åcation) Suppose that, for a classiÔ¨Åcation problem,
an input xhas to be classiÔ¨Åed into one of cclasses, labeled 0 ;:::; c 1. We can perform the
classiÔ¨Åcation via a neural network with one hidden layer, with p1=cnodes. In particular,
we have
z1=W1x+b1;a1=S1(z1);
where S1is the softmax function:softmax
softmax : z7!exp(z)P
kexp(zk):
For the output, we take g(x)=[g1(x);:::; gc(x)]>=a1, which can then be used as a
pre-classiÔ¨Åer ofx. The actual classiÔ¨Åer of xinto one of the categories 0 ;1;:::; c 1 is then +252
argmax
k2f0;:::;c 1ggk+1(x):
This is equivalent to the multi-logit classiÔ¨Åer in Section 7.5. Note, however, that there we +266
used a slightly di erent notation, with exinstead of xand we have a reference class; see
Exercise 13.
In practical implementations, the softmax function can cause numerical over- and
under-Ô¨Çow errors when either one of the exp( zk) happens to be extremely large orP
kexp(zk) happens to be very small. In such cases we can exploit the invariance
property (Exercise 1):
softmax( z)=softmax( z+c1) for any constant c:
Using this property, we can compute softmax( z) with greater numerical stability via
softmax( z max kfzkg1).
When neural networks are used for classiÔ¨Åcation into cclasses and the number of out-
put nodes is c 1, then the gi(x) may be viewed as nonlinear discriminant functions . +260Chapter 9. Deep Learning 329
Example 9.3 (Density Estimation) Estimating the density fof some random feature
X2Ris the prototypical unsupervised learning task, which we tackled in Section 4.5.2 us-
ing Gaussian mixture models. We can view a Gaussian mixture model with p1components +137
and a common scale parameter  > 0 as a neural network with two hidden layers, similar
to the one on Figure 9.3. In particular, if the activation function in the Ô¨Årst hidden layer,
S1, is of the form (9.1) with S(z) :=exp( z2=(22))=p
22, then the density value g(x) is
computed via:
z1=W1x+b1;a1=S1(z1);
z2=W2a1+b2;a2=S2(z2);
g(x)=a>
1a2;
where W1=1is ap11 column vector of ones, W2=Ois ap1p1matrix of zeros, and S2
is the softmax function. We identify the column vector b1with the p1location parameters,
[1;:::; p1]>of the Gaussian mixture and b22Rp1with the p1weights of the mixture.
Note the unusual activation function of the output layer ‚Äî it requires the value of a1from
the Ô¨Årst hidden layer and a2from the second hidden layer.
There are a number of key design characteristics of a feed-forward network. First, we
need to choose the activation function(s). Second, we need to choose the loss function for
the training of the network. As we shall explain in the next section, the most common
choices are the ReLU activation function and the cross-entropy loss. Crucially, we need
to carefully construct the network architecture ‚Äî the number of connections among thenetwork
architecture nodes in di erent layers and the overall number of layers of the network.
For example, if the connections from one layer to the next are pruned (called sparse
connectivity ) and the links share the same weight values fwl;i jg(called parameter sharing )
for allf(i;j) :ji jj=0;1;:::g, then the weight matrices will be sparse and Toeplitz . +379
Intuitively, the parameter sharing and sparse connectivity can speed up the training of
the network, because there are fewer parameters to learn, and the Toeplitz structure permits
quick computation of the matrix-vector products in Algorithm 9.2.1. An important example
of such a network is the convolution neural network (CNN), in which some or all of theconvolution
neural
networknetwork layers encode the linear operation of convolution :
Wlal 1=wlal 1;
where [ xy]i:=P
kxkyi k+1. As discussed in Example A.10, a convolution matrix is a +380
special type of sparse Toeplitz matrix, and its action on a vector of learning parameters can
be evaluated quickly via the fast Fourier transform . +394
CNNs are particularly suited to image processing problems, because their convolution
layers closely mimic the neurological properties of the visual cortex. In particular, the
cortex partitions the visual Ô¨Åeld into many small regions and assigns a group of neurons to
every such region. Moreover, some of these groups of neurons respond only to the presence
of particular features (for example, edges).
This neurological property is naturally modeled via convolution layers in the neural
network. SpeciÔ¨Åcally, suppose that the input image is given by an m1m2matrix of pixels.
Now, deÔ¨Åne a kkmatrix (sometimes called a kernel , where kis generally taken to be 3
or 5). Then, the convolution layer output can be calculated using the discrete convolution330 9.3. Back-Propagation
of all possible kkinput matrix regions and the kernel matrix; (see Example A.10). In
particular, by noting that there are ( m1 k+1)(m2 k+1) possible regions in the original
image, we conclude that the convolution layer output size is ( m1 k+1)(m2 k+1).
In practice, we frequently deÔ¨Åne several kernel matrices, giving an output layer of size
(m1 k+1)(m2 k+1)(the number of kernels). Figure 9.4 shows a 5 5 input image
and a 22 kernel with a 4 4 output matrix. An example of using a CNN for image
classiÔ¨Åcation is given in Section 9.5.2.
Figure 9.4: An example 5 5 input image and a 2 2 kernel. The kernel is applied to every
22 region of the original image.
9.3 Back-Propagation
The training of neural networks is a major challenge that requires both ingenuity and much
experimentation. The algorithms for training neural networks with great depth are collect-
ively referred to as deep learning methods. One of the simplest and most e ective methodsdeep learningfor training is via steepest descent and its variations.+412Steepest descent requires computation of the gradient with respect to all bias vectors
and weight matrices. Given the potentially large number of parameters (weight and bias
terms) in a neural network, we need to Ô¨Ånd an e cient method to calculate this gradient.
To illustrate the nature of the gradient computations, let =fWl;blgbe a column vec-
tor of length dim( )=PL
l=1(pl 1pl+pl) that collects all the weight parameters (number-
ingPL
l=1pl 1pl) and bias parameters (numberingPL
l=1pl) of a multiple-layer network with
training loss:
`(g(j)) :=1
nnX
i=1Loss( yi;g(xij)):
Writing Ci() :=Loss( yi;g(xij)) for short (using Cforcost), we have
`(g(j))=1
nnX
i=1Ci(); (9.4)
so that obtaining the gradient of `requires computation of @Ci=@for every i. For ac-
tivation functions of the form (9.1), deÔ¨Åne Dlas the diagonal matrix with the vector of
derivatives
S0(z) :=[S0(zl;1);:::; S0(zl;pl)]>
down its main diagonal; that is,
Dl:=diag( S0(zl;1);:::; S0(zl;pl));l=1;:::; L 1:Chapter 9. Deep Learning 331
The following theorem provides us with the formulas needed to compute the gradient of a
typical Ci().
Theorem 9.2: Gradient of Training Loss
For a given (input, output) pair ( x;y), let g(xj) be the output of Algorithm 9.2.1,
and let C()=Loss( y;g(xj)) be an almost-everywhere di erentiable loss func-
tion. Supposefzl;algL
l=1are the vectors obtained during the feed-forward propagation
(a0=x;aL=g(xj)). Then, we have for l=1;:::; L:
@C
@Wl=la>
l 1and@C
@bl=l;
wherel:=@C=@zlis computed recursively for l=L;:::; 2:
l 1=Dl 1W>
llwithL=@SL
@zL@C
@g: (9.5)
Proof: The scalar value Cis obtained from the transitions (9.2), followed by the mapping
g(xj)7!Loss( y;g(xj)). Using the chain rule (see Appendix B.1.2), we have +400
L=@C
@zL=@g(x)
@zL@C
@g(x)=@SL
@zL@C
@g:
Recall that the vector /vector derivative of a linear mapping z7!Wzis given by W>; see
(B.5). It follows that, since zl=Wlal 1+blandal=S(zl), the chain rule gives +399
@zl
@zl 1=@al 1
@zl 1@zl
@al 1=Dl 1W>
l:
Hence, the recursive formula (9.5):
l 1=@C
@zl 1=@zl
@zl 1@C
@zl=Dl 1W>
ll;l=L;:::; 3;2:
Using theflg, we can now compute the derivatives with respect to the weight matrices
and the biases. In particular, applying the ‚Äúscalar /matrix‚Äù di erentiation rule (B.10) to
zl=Wlal 1+blgives:
@C
@Wl=@C
@zl@zl
@Wl=la>
l 1;l=1;:::; L
and
@C
@bl=@zl
@bl@C
@zl=l;l=1;:::; L:

From the theorem we can see that for each pair ( x;y) in the training set, we can compute the
gradient@C=@in a sequential manner, by computing L;:::;1. This procedure is called
back-propagation back-
propagation. Since back-propagation mostly involves simple matrix multiplication, it332 9.3. Back-Propagation
can be e ciently implemented using dedicated computing hardware such as graphical pro-
cessor units (GPUs) and other parallel computing architecture. Note also that many matrix
computations that run in quadratic time can be replaced with linear-time componentwise
multiplication. SpeciÔ¨Åcally, multiplication of a vector with a diagonal matrix is equivalent
to componentwise multiplication:
A|{z}
diag( a)b=ab:
Consequently, we can write l 1=Dl 1W>
llas:l 1=S0(zl 1)W>
ll;l=L;:::; 3;2.
We now summarize the back-propagation algorithm for the computation of a typical
@C=@. In the following algorithm, Lines 1 to 5 are the feed-forward part of the algorithm,
and Lines 7 to 10 are the back-propagation part of the algorithm.
Algorithm 9.3.1: Computing the Gradient of a Typical C()
input: Training example ( x;y), weight matrices and bias vectors fWl;blgL
l=1=:,
activation functions fSlgL
l=1.
output: The derivatives with respect to all weight matrices and bias vectors.
1a0 x
2forl=1;:::; Ldo // feed-forward
3 zl Wlal 1+bl
4 al Sl(zl)
5L @SL
@zL@C
@g
6z0 0 // arbitrary assignment needed to finish the loop
7forl=L;:::; 1do // back-propagation
8@C
@bl l
9@C
@Wl la>
l 1
10l 1 S0(zl 1)W>
ll
11return@C
@Wland@C
@blfor all l=1;:::; Land the value g(x) aL(if needed)
Note that for the gradient of C() to exist at every point, we need the activation func-
tions to be di erentiable everywhere. This is the case, for example, for the logistic activa-
tion function in Figure 9.2. It is not the case for the ReLU function, which is di erentiable
everywhere, except at z=0. However, in practice, the kink of the ReLU function at z=0
is unlikely to trip the back-propagation algorithm, because rounding errors and the Ô¨Ånite-
precision computer arithmetic make it extremely unlikely that we will need to evaluate the
ReLU at precisely z=0. This is the reason why in Theorem 9.2 we merely required that
C() is almost-everywhere di erentiable.
In spite of its kink at the origin, the ReLU has an important advantage over the logistic
function. While the derivative of the logistic function decays exponentially fast to zero as
we move away from the origin, a phenomenon referred to as saturation , the derivative ofsaturationthe ReLU function is always unity for positive z. Thus, for large positive z, the derivative of
the logistic function does not carry any useful information, but the derivative of the ReLU
can help guide a gradient optimization algorithm. The situation for the Heaviside function
in Figure 9.2 is even worse, because its derivative is completely noninformative for any
z,0. In this respect, the lack of saturation of the ReLU function for z>0 makes it a
desirable activation function for training a network via back-propagation.Chapter 9. Deep Learning 333
Finally, note that to obtain the gradient @`=@of the training loss, we simply need to
loop Algorithm 9.3.1 over all the ntraining examples, as follows.
Algorithm 9.3.2: Computing the Gradient of the Training Loss
input: Training set =f(xi;yi)gn
i=1, weight matrices and bias vectors
fWl;blgL
l=1=:, activation functions fSlgL
l=1.
output: The gradient of the training loss.
1fori=1;:::; ndo // loop over all training examples
2 Run Algorithm 9.3.1 with input ( xi;yi) to computen@Ci
@Wl;@Ci
@bloL
l=1
3return@C
@Wl=1
nPn
i=1@Ci
@Wland@C
@bl=1
nPn
i=1@Ci
@blfor all l=1;:::; L
Example 9.4 (Squared-Error and Cross-Entropy Loss) The back-propagation Al-
gorithm 9.3.1 requires a formula for Lin line 5. In particular, to execute line 5 we need to
specify both a loss function and an SLthat deÔ¨Ånes the output layer: g(xj)=aL=SL(zL).
For instance, in the multi-logit classiÔ¨Åcation of inputs xinto pLcategories labeled +266
0;1;:::; (pL 1), the output layer is deÔ¨Åned via the softmax function:
SL:zL7!exp(zL)PpL
k=1exp(zL;k):
In other words, g(xj) is a probability vector such that its ( y+1)-st component gy+1(xj)=
g(yj;x) is the estimate or prediction of the true conditional probability f(yjx). Combin-
ing the softmax output with the cross-entropy loss, as was done in (7.17), yields: +267
Loss( f(yjx);g(yj;x))= lng(yj;x)
= lngy+1(xj)
= zy+1+lnPpL
k=1exp(zk):
Hence, we obtain the vector Lwith components ( k=1;:::; pL)
L;k=@
@zk
 zy+1+lnPpL
k=1exp(zk)
=gk(xj) 1fy=k 1g:
Note that we can remove a node from the Ô¨Ånal layer of the multi-logit network, be-
cause g1(xj) (which corresponds to the y=0 class) can be eliminated, using the fact
thatg1(xj)=1 PpL
k=2gk(xj). For a numerical comparison, see Exercise 13.
As another example, in nonlinear multi-output regression (see Example 9.1), the out-
put function SLis typically of the form (9.1), so that @SL=@z=diag( S0
L(z1);:::; S0
L(zpL)).
Combining the output g(xj)=SL(zL) with the squared-error loss yields:
Loss( y;g(xj))=ky g(xj)k2=pLX
j=1(yj gj(xj))2:
Hence, line 5 in Algorithm 9.3.1 simpliÔ¨Åes to:
L=@SL
@z@C
@g=S0
L(zL)2(g(xj) y):334 9.4. Methods for Training
9.4 Methods for Training
Neural networks have been studied for a long time, yet it is only recently that there have
been su cient computational resources to train them e ectively. The training of neural
networks requires minimization of a training loss, `(g(j))=1
nPn
i=1Ci(), which is typ-
ically a di cult high-dimensional optimization problem with multiple local minima. We
next consider a number of simple training methods.
In this section, the vectors tand1tuse the notation of Section B.3.2 and should not
be confused with the derivative and the prediction function g, respectively.
9.4.1 Steepest Descent
If we can compute the gradient of `(g(j)) via back-propagation, then we can apply the
steepest descent algorithm, which reads as follows. Starting from a guess 1, we iterate the +412
following step until convergence:
t+1=t tut; t=1;2;:::; (9.6)
where ut:=@`
@(t) andtis the learning rate learning rate .
Observe that, rather than operating directly on the weights and biases, we operate in-
stead on:=fWl;blgL
l=1‚Äî a column vector of lengthPL
l=1(pl 1pl+pl) that stores all the
weight and bias parameters. The advantage of organizing the computations in this way is
that we can easily compute the learning rate t; for example, via the Barzilai‚ÄìBorwein
formula in (B.26). +413
Algorithm 9.4.1: Training via Steepest Descent
input: Training set =f(xi;yi)gn
i=1, initial weight matrices and bias vectors
fWl;blgL
l=1=:1, activation functions fSlgL
l=1.
output: The parameters of the trained learner.
1t 1; 0:11;ut 1 0;  0:1 // initialization
2while stopping condition is not met do
3 compute the gradient ut=@`
@(t) using Algorithm 9.3.2
41 ut ut 1
5 if>1>0then // check if Hessian is positive-definite
6 >1k1k2// Barzilai-Borwein
7 else
8 2 // failing positivity, do something heuristic
9  ut
10t+1 t+
11 t t+1
12returntas the minimizer of the training loss
Typically, we initialize the algorithm with small random values for 1, while being
careful to avoid saturating the activation function. For example, in the case of the ReLUChapter 9. Deep Learning 335
activation function, we will use small positive values to ensure that its derivative is not
zero. A zero derivative of the activation function prevents the propagation of information
useful for computing a good search direction.
Recall that computation of the gradient of the training loss via Algorithm 9.3.2 requires
averaging over all training examples. When the size nof the training set nis too large,
computation of the gradient @`n=@via Algorithm 9.3.2 may be too costly. In such cases,
we may employ the stochastic gradient descent stochastic
gradient
descentalgorithm. In this algorithm, we view the
training loss as an expectation that can be approximated via Monte Carlo sampling. In
particular, if Kis a random variable with distribution P[K=k]=1=nfork=1;:::; n, then
we can write
`(g(j))=1
nnX
k=1Loss( yk;g(xkj))=ELoss( yK;g(xKj)):
We can thus approximate `(g(j)) via a Monte Carlo estimator using Niid copies of K:
b`(g(j)) :=1
NNX
i=1Loss( yKi;g(xKij)):
The iid Monte Carlo sample K1;:::; KNis called a minibatch minibatch (see also Exercise 3). Typic-
ally, nNso that the probability of observing ties in a minibatch of size Nis negligible.
Finally, note that if the learning rate of the stochastic gradient descent algorithm sat-
isÔ¨Åes the conditions in (3.30), then the stochastic gradient descent algorithm is simply a +106
version of the stochastic approximation Algorithm 3.4.5.
9.4.2 Levenberg‚ÄìMarquardt Method
Since a neural network with squared-error loss is a special type of nonlinear regression
model, it is possible to train it using classical nonlinear least-squares minimization meth-
ods, such as the Levenberg‚ÄìMarquardt algorithm. +415
For simplicity of notation, suppose that the output of the net for an input xis ascalar
g(x). For a given input parameter of dimension d=dim(), the Levenberg‚ÄìMarquardt
Algorithm B.3.3 requires computation of the following vector of outputs:
g(j) :=[g(x1j);:::; g(xnj)]>;
as well as the ndmatrix of Jacobi, G, ofgat. To compute these quantities, we can
again use the back-propagation Algorithm 9.3.1, as follows.
Algorithm 9.4.2: Output for Training via Levenberg‚ÄìMarquardt
input: Training set =f(xi;yi)gn
i=1, parameter.
output: Vector g(j) and matrix of Jacobi Gfor use in Algorithm B.3.3.
1fori=1;:::; ndo // loop over all training examples
2 Run Algorithm 9.3.1 with input ( xi;yi) (using@C
@g=1 in line 5) to compute
g(xij) and@g(xij)
@.
3g(j) [g(x1j);:::; g(xnj)]>
4G h@g(x1j)
@;;@g(xnj)
@i>
5return g(j) and G336 9.4. Methods for Training
The Levenberg‚ÄìMarquardt algorithm is not suitable for networks with a large number
of parameters, because the cost of the matrix computations becomes prohibitive. For in-
stance, obtaining the Levenberg‚ÄìMarquardt search direction in (B.28) usually incurs an
O(d3) cost. In addition, the Levenberg‚ÄìMarquardt algorithm is applicable only when we
wish to train the network using the squared-error loss. Both of these shortcomings are
mitigated to an extent with the quasi-Newton or adaptive gradient methods described next.
9.4.3 Limited-Memory BFGS Method
All the methods discussed so far have been Ô¨Årst-order optimization methods, that is, meth-
ods that only use the gradient vector ut:=@`
@(t) at the current (and /or immediate past) can-
didate solution t. In trying to design a more e cient second-order optimization method,
we may be tempted to use Newton‚Äôs method with a search direction: +410
 H 1
tut;
where Htis the ddmatrix of second-order partial derivatives of `(g(j)) att.
There are two problems with this approach. First, while the computation of utvia Al-
gorithm 9.3.2 typically costs O(d), the computation of HtcostsO(d2). Second, even if we
have somehow computed Htvery fast, computing the search direction H 1
tutstill incurs an
O(d3) cost. Both of these considerations make Newton‚Äôs method impractical for large d.
Instead, a practical alternative is to use a quasi-Newton method , in which we directlyquasi -newton
method aim to approximate H 1
tvia a matrix Ctthat satisÔ¨Åes the secant condition :
+411Ct1t=t;
wheret:=t t 1and1t:=ut ut 1.
An ingenious formula that generates a suitable sequence of approximating matrices
fCtg(each satisfying the secant condition) is the BFGS updating formula (B.23), which
can be written as the recursion (see Exercise 9):
Ct=
I t1t>
t>Ct 1
I t1t>
t
+tt>
t;  t:=(1>
tt) 1: (9.7)
This formula allows us to update Ct 1toCtand then compute CtutinO(d2) time. While
this quasi-Newton approach is better than the O(d3) cost of Newton‚Äôs method, it may be
still too costly in large-scale applications.
Instead, an approximate or limited memory BFGS limited memory
bfgsupdating can be achieved in O(d)
time. The idea is to store a few of the most recent pairs ft;1tgin order to evaluate its action
on a vector utwithout explicitly constructing and storing Ctin computer memory. This is
possible, because updating C0toC1in (9.7) requires only the pair 1;11, and similarly
computing Ctfrom C0only requires the history of the updates 1;11:::;t;1t, which can
be shown as follows.
DeÔ¨Åne the matrices At;:::; A0via the backward recursion ( j=1;:::; t):
At:=I;Aj 1:=
I j1j>
j
Aj;
and observe that all matrix vector products: Aju=:qj;forj=0;:::; tcan be computed
eciently via the backward recursion starting with qt=u:
j:=>
jqj;qj 1=qj jj1j;j=t;t 1;:::; 1: (9.8)Chapter 9. Deep Learning 337
In addition tofqjg, we will make use of the vectors frjgdeÔ¨Åned via the recursion:
r0:=C0q0;rj=rj 1+j
j 1>
jrj 1
j;j=1;:::; t: (9.9)
At the Ô¨Ånal iteration t, the BFGS updating formula (9.7) can be rewritten in the form:
Ct=A>
t 1Ct 1At 1+tt>
t:
By iterating the recursion (9.7) backwards to C0, we can write:
Ct=A>
0C0A0+tX
j=1jA>
jj>
jAj;
that is, we can express Ctin terms of the initial C0and the entire history of all BFGS values
fj;1jg, as claimed. Further, with the fqj;rjgcomputed via (9.8) and (9.9), we can write:
Ctu=A>
0C0q0+tX
j=1j
>
jqj
A>
jj
=A>
0r0+11A>
11+tX
j=2jjA>
jj
=A>
1 I 111>
1r0+111+tX
j=2jjA>
jj:
Hence, from the deÔ¨Ånition of the frjgin (9.9), we obtain
Ctu=A>
1r1+tX
j=2jjA>
jj
=A>
2r2+tX
j=3jjA>
jj
==A>
trt+0=rt:
Given C0and the history of all recent BFGS values fj;1jgh
j=1, the computation of the quasi-
Newton search direction d= Chucan be accomplished via the recursions (9.8) and (9.9)
as summarized in Algorithm 9.4.3.
Note that if C0is a diagonal matrix, say the identity matrix, then C0qis cheap to
compute and the cost of running Algorithm 9.4.3 is O(h d). Thus, for a Ô¨Åxed length of the
BFGS history, the cost of the limited-memory BFGS updating grows linearly in d, making
it a viable optimization algorithm in large-scale applications.338 9.4. Methods for Training
Algorithm 9.4.3: Limited-Memory BFGS Update
input: BFGS history list fj;1jgh
j=1, initial C0, and input u.
output: d= Chu, where Ct= I tt1>
tCt 1
I t1t>
t
+tt>
t.
1q u
2fori=h;h 1;:::; 1do // backward recursion to compute A0u
3i 
>
i1i 1
4i >
iq
5 q q ii1i
6q C0q // compute C0(A0u)
7fori=1;:::; hdo // compute recursion (9.9)
8 q q+i(i 1>
iq)i
9return d  q,the value of Chu
In summary, a quasi-Newton algorithm with limited-memory BFGS updating reads as
follows.
Algorithm 9.4.4: Quasi-Newton Minimization with Limited-Memory BFGS
input: Training set =f(xi;yi)gn
i=1, initial weight matrices and bias vectors
fWl;blgL
l=1=:1, activation functions fSlgL
l=1, and history parameter h.
output: The parameters of the trained learner.
1t 1; 0:11;ut 1 0 // initialization
2while stopping condition is not met do
3 Compute`value=`(g(jt)) and ut=@`
@(t) via Algorithm 9.3.2.
41 ut ut 1
5 Add (;1) to the BFGS history as the newest BFGS pair.
6 ifthe number of pairs in the BFGS history is greater than hthen
7 remove the oldest pair from the BFGS history
8 Compute dvia Algorithm 9.4.3 using the BFGS history, C0=I, and ut.
9 1
10 while`(g(jt+d))>`value+10 4d>utdo
11 =1:5 // line-search along quasi-Newton direction
12 d
13t+1 t+
14 t t+1
15returntas the minimizer of the training loss
9.4.4 Adaptive Gradient Methods
Recall that the limited-memory BFGS method in the previous section determines a search
direction using the recent history of previously computed gradients futgand input paramet-
ersftg. This is because the BFGS pairs ft;1tgcan be easily constructed from the identities:
t=t t 1and1t=ut ut 1. In other words, using only past gradient computations and
with little extra computation, it is possible to infer some of the second-order informationChapter 9. Deep Learning 339
contained in the Hessian matrix of `(). In addition to the BFGS method, there are other
ways in which we can exploit the history of past gradient computations.
One approach is to use the normal approximation method , in which the Hessian of ` +414
attis approximated via
bHt=I+1
htX
i=t h+1uiu>
i; (9.10)
where ut h+1;:::; utare the hmost recently computed gradients and is a tuning parameter
(for example, =1=h). The search direction is then given by
 bH 1
tut;
which can be computed quickly in O(h2d) time either using the QR decomposition (Exer-
cises 5 and 6), or the Sherman‚ÄìMorrison Algorithm A.6.1. This approach requires that we +373
store the last hgradient vectors in memory.
Another approach that completely bypasses the need to invert a Hessian approximation
is the Adaptive Gradient orAdaGrad AdaGrad method, in which we only store the diagonal of bHt
and use the search direction:
 diag(bHt) 1=2ut:
We can avoid storing any of the gradient history by instead using the slightly di erent
search direction2
 ut.p
vt+1;
where the vector vtis updated recursively via
vt= 
1 1
h!
vt 1+1
hutut:
With this updating of vt, the di erence between the vector vt+1and the diagonal of
the Hessian bHtwill be negligible.
A more sophisticated version of AdaGrad is the adaptive moment estimation orAdam Adam
method, in which we not only average the vectors fvtg, but also average the gradient vectors
futg, as follows.
Algorithm 9.4.5: Updating of Search Direction at Iteration tviaAdam
input: ut,but 1,vt 1,t, and parameters ( ;hv;hu), equal to, e.g., (10 3;103;10).
output: but,vt,t+1.
1but 
1 1
hu
but 1+1
huut
2vt 
1 1
hv
vt 1+1
hvutut
3u
t but.
1 (1 h 1
u)t
4v
t vt.
1 (1 h 1
v)t
5t+1 t u
t.p
v
t+10 81
6return but,vt,t+1
2Here we divide two vectors componentwise.340 9.5. Examples in Python
Yet another computationally cheap approach is the momentum method , in which themomentum
method steepest descent iteration (9.6) is modiÔ¨Åed to
t+1=t tut+t;
wheret=t t 1andis a tuning parameter. This strategy frequently performs better
than the ‚Äúvanilla‚Äù steepest descent method, because the search direction is less likely to
change abruptly.
Numerical experience suggests that the vanilla steepest-descent Algorithm 9.4.1 and
the Levenberg‚ÄìMarquardt Algorithm B.3.3 are e ective for networks with shallow archi-
tectures, but not for networks with deep architectures. In comparison, the stochastic gradi-
ent descent method, the limited-memory BFGS Algorithm 9.4.4, or any of the adaptive
gradient methods in this section, can frequently handle networks with many hidden lay-
ers (provided that any tuning parameters and initialization values are carefully chosen via
experimentation).
9.5 Examples in Python
In this section we provide two numerical examples in Python. In the Ô¨Årst example, we
train a neural network with the stochastic gradient descent method using the polynomial
regression data from Example 2.1, and without using any specialized Python packages. +26
In the second example, we consider a realistic application of a neural network to image
recognition and classiÔ¨Åcation. Here we use the specialized open-source Python package
Pytorch .
9.5.1 Simple Polynomial Regression
Consider again the polynomial regression data set depicted in Figure 2.4. We use a network
with architecture
[p0;p1;p2;p3]=[1;20;20;1]:
In other words, we have two hidden layers with 20 neurons, resulting in a learner with a
total of dim( )=481 parameters. To implement such a neural network, we Ô¨Årst import the
numpy and the matplotlib packages, then read the regression problem data and deÔ¨Åne
the feed-forward neural network layers.
NeuralNetPurePython.py
import numpy as np
import matplotlib.pyplot as plt
#%%
# import data
data = np.genfromtxt( 'polyreg.csv ',delimiter= ',')
X = data[:,0].reshape(-1,1)
y = data[:,1].reshape(-1,1)
# Network setup
p = [X.shape[1],20,20,1] # size of layers
L = len(p)-1 # number of layersChapter 9. Deep Learning 341
Next, the initialize method generates random initial weight matrices and bias vec-
torsfWl;blgL
l=1. SpeciÔ¨Åcally, all parameters are initialized with values distributed according
to the standard normal distribution.
def initialize(p, w_sig = 1):
W, b = [[]]* len(p), [[]]* len(p)
for lin range (1,len(p)):
W[l]= w_sig * np.random.randn(p[l], p[l-1])
b[l]= w_sig * np.random.randn(p[l], 1)
return W,b
W,b = initialize(p) # initialize weight matrices and bias vectors
The following code implements the ReLU activation function from Figure 9.2 and the
squared error loss. Note that these functions return both the function values and the corres-
ponding gradients.
def RELU(z,l): # RELU activation function: value and derivative
ifl == L: return z, np.ones_like(z)
else :
val = np.maximum(0,z) # RELU function element -wise
J = np.array(z>0, dtype = float )# derivative of RELU
element -wise
return val, J
def loss_fn(y,g):
return (g - y)**2, 2 * (g - y)
S = RELU
Next, we implement the feed-forward and backward-propagation Algorithm 9.3.1.
Here, we have implemented Algorithm 9.3.2 inside the backward-propagation loop.
def feedforward(x,W,b):
a, z, gr_S = [0]*(L+1), [0]*(L+1), [0]*(L+1)
a[0] = x.reshape(-1,1)
for lin range (1,L+1):
z[l] = W[l] @ a[l-1] + b[l] # affine transformation
a[l], gr_S[l] = S(z[l],l) # activation function
return a, z, gr_S
def backward(W,b,X,y):
n =len(y)
delta = [0]*(L+1)
dC_db , dC_dW = [0]*(L+1), [0]*(L+1)
loss=0
for iin range (n): # loop over training examples
a, z, gr_S = feedforward(X[i,:].T, W, b)
cost , gr_C = loss_fn(y[i], a[L]) # cost i and gradient wrt g
loss += cost/n342 9.5. Examples in Python
delta[L] = gr_S[L] @ gr_C
for lin range (L,0,-1): # l = L,...,1
dCi_dbl = delta[l]
dCi_dWl = delta[l] @ a[l-1].T
# ---- sum up over samples ----
dC_db[l] = dC_db[l] + dCi_dbl/n
dC_dW[l] = dC_dW[l] + dCi_dWl/n
# -----------------------------
delta[l-1] = gr_S[l-1] * W[l].T @ delta[l]
return dC_dW , dC_db , loss
As explained in Section 9.4, it is sometimes more convenient to collect all the weight
matrices and bias vectors fWl;blgL
l=1into a single vector . Consequently, we code two
functions that map the weight matrices and the bias vectors into a single parameter vector,
and vice versa.
def list2vec(W,b):
# converts list of weight matrices and bias vectors into
# one column vector
b_stack = np.vstack([b[i] for iin range (1,len(b))] )
W_stack = np.vstack(W[i].flatten().reshape(-1,1) for iin range
(1,len(W)))
vec = np.vstack([b_stack , W_stack])
return vec
#%%
def vec2list(vec, p):
# converts vector to weight matrices and bias vectors
W, b = [[]]* len(p) ,[[]]* len(p)
p_count = 0
for lin range (1,len(p)): # construct bias vectors
b[l] = vec[p_count:(p_count+p[l])].reshape(-1,1)
p_count = p_count + p[l]
for lin range (1,len(p)): # construct weight matrices
W[l] = vec[p_count:(p_count + p[l]*p[l-1])].reshape(p[l], p[
l-1])
p_count = p_count + (p[l]*p[l-1])
return W, b
Finally, we run the stochastic gradient descent for 104iterations using a minibatch of
size 20 and a constant learning rate of t=0:005.
batch_size = 20
lr = 0.005
beta = list2vec(W,b)
loss_arr = []Chapter 9. Deep Learning 343
n = len(X)
num_epochs = 10000
print ("epoch | batch loss")
print ("----------------------------")
for epoch in range (1,num_epochs+1):
batch_idx = np.random.choice(n,batch_size)
batch_X = X[batch_idx].reshape(-1,1)
batch_y=y[batch_idx].reshape(-1,1)
dC_dW , dC_db , loss = backward(W,b,batch_X ,batch_y)
d_beta = list2vec(dC_dW ,dC_db)
loss_arr.append(loss.flatten()[0])
if(epoch==1 ornp.mod(epoch ,1000)==0):
print (epoch ,": ",loss.flatten()[0])
beta = beta - lr*d_beta
W,b = vec2list(beta ,p)
# calculate the loss of the entire training set
dC_dW , dC_db , loss = backward(W,b,X,y)
print ("entire training set loss = ",loss.flatten()[0])
xx = np.arange(0,1,0.01)
y_preds = np.zeros_like(xx)
for iin range (len(xx)):
a, _, _ = feedforward(xx[i],W,b)
y_preds[i], = a[L]
plt.plot(X,y, 'r.', markersize = 4,label = 'y')
plt.plot(np.array(xx), y_preds , 'b',label = 'fit')
plt.legend()
plt.xlabel( 'x')
plt.ylabel( 'y')
plt.show()
plt.plot(np.array(loss_arr), 'b')
plt.xlabel( 'iteration ')
plt.ylabel( 'Training Loss ')
plt.show()
epoch | batch loss
----------------------------
1 : 158.6779278688539
1000 : 54.52430507401445
2000 : 38.346572088604965
3000 : 31.02036319180713
4000 : 22.91114276931535
5000 : 27.75810262906341
6000 : 22.296907007032928
7000 : 17.337367420038046
8000 : 19.233689945334195
9000 : 39.54261478969857
10000 : 14.754724387604416
entire training set loss = 28.904957963612727
The left panel of Figure 9.5 shows a trained neural network with a training loss of
approximately 28 :9. As seen from the right panel of Figure 9.5, the algorithm initially
makes rapid progress until it settles down into a stationary regime after 400 iterations.344 9.5. Examples in Python
0.00
 0.25
 0.50
 0.75
 1.00
input u
0
20
40output y
Ô¨Åt
y
0
 500
 1000
 1500
 2000
iteration
0
100
200
300Batch Loss
Figure 9.5: Left panel: The Ô¨Åtted neural network with training loss of `(g)28:9. Right
panel: The evolution of the estimated loss, b`(g(j)), over the steepest-descent iterations.
9.5.2 Image ClassiÔ¨Åcation
In this section, we will use the package Pytorch , which is an open-source machine learn-
ing library for Python. Pytorch can easily exploit any graphics processing unit (GPU)
for accelerated computation. As an example, we consider the Fashion-MNIST data set
from https://www.kaggle.com/zalando-research/fashionmnist . The Fashion-
MNIST data set contains 28 28 gray-scale images of clothing. Our task is to classify
each image according to its label. SpeciÔ¨Åcally, the labels are: T-Shirt, Trouser, Pullover,
Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle Boot. Figure 9.6 depicts a typical
ankle boot in the left panel and a typical dress in the right panel. To start with, we import
the required libraries and load the Fashion-MNIST data set.
Figure 9.6: Left: an ankle boot. Right: a dress.
ImageClassificationPytorch.py
import torch
import torch.nn as nn
from torch.autograd import Variable
import pandas as pd
import numpy as npChapter 9. Deep Learning 345
import matplotlib.pyplot as plt
from torch.utils.data import Dataset , DataLoader
from PIL import Image
import torch.nn.functional as F
#################################################################
# data loader class
#################################################################
class LoadData(Dataset):
def __init__(self , fName , transform=None):
data = pd.read_csv(fName)
self.X = np.array(data.iloc[:, 1:], dtype=np.uint8).reshape
(-1, 1, 28, 28)
self.y = np.array(data.iloc[:, 0])
def __len__(self):
return len (self.X)
def __getitem__(self , idx):
img = self.X[idx]
lbl = self.y[idx]
return (img, lbl)
# load the image data
train_ds = LoadData( 'fashionmnist/fashion -mnist_train.csv ')
test_ds = LoadData( 'fashionmnist/fashion -mnist_test.csv ')
# set labels dictionary
labels = {0 : 'T-Shirt ', 1 : 'Trouser ', 2 : 'Pullover ',
3 : 'Dress ', 4 : 'Coat ', 5 : 'Sandal ', 6 : 'Shirt ',
7 : 'Sneaker ', 8 : 'Bag', 9 : 'Ankle Boot '}
Since an image input data is generally memory intensive, it is important to partition
the data set into (mini-)batches. The code below deÔ¨Ånes a batch size of 100 images and
initializes the Pytorch data loader objects. These objects will be used for e cient iteration
over the data set.
# load the data in batches
batch_size = 100
train_loader = torch.utils.data.DataLoader(dataset=train_ds ,
batch_size=batch_size ,
shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_ds ,
batch_size=batch_size ,
shuffle=True)
Next, to deÔ¨Åne the network architecture in Pytorch all we need to do is deÔ¨Åne an
instance of the torch.nn.Module class. Choosing a network architecture with good gen-
eralization properties can be a di cult task. Here, we use a network with two convolution
layers (deÔ¨Åned in the cnn_layer block), a 33 kernel, and three hidden layers (deÔ¨Åned in
theflat_layer block). Since there are ten possible output labels, the output layer has ten
nodes. More speciÔ¨Åcally, the Ô¨Årst and the second convolution layers have 16 and 32 output
channels. Combining this with the deÔ¨Ånition of the 3 3 kernel, we conclude that the size346 9.5. Examples in Python
of the Ô¨Årst Ô¨Çat hidden layer should be:
0BBBBBBBB@second convolution layerz                      }|                      {
(28 3+1)|        {z        }
Ô¨Årst convolution layer 3+11CCCCCCCCA2
32=18432;
where the multiplication by 32 follows from the fact that the second convolution layer has
32 output channels. Having said that, the flat_fts variable determines the number of
output layers of the convolution block. This number is used to deÔ¨Åne the size of the Ô¨Årst
hidden layer of the flat_layer block. The rest of the hidden layers have 100 neurons and
we use the ReLU activation function for all layers. Finally, note that the forward method
in the CNNclass implements the forward pass.
# define the network
class CNN(nn.Module):
def __init__(self):
super (CNN, self).__init__()
self.cnn_layer = nn.Sequential(
nn.Conv2d(1, 16, kernel_size=3, stride=(1,1)),
nn.ReLU(),
nn.Conv2d(16, 32, kernel_size=3, stride=(1,1)),
nn.ReLU(),
)
self.flat_fts = (((28-3+1) -3+1)**2)*32
self.flat_layer = nn.Sequential(
nn.Linear(self.flat_fts , 100),
nn.ReLU(),
nn.Linear(100, 100),
nn.ReLU(),
nn.Linear(100, 100),
nn.ReLU(),
nn.Linear(100, 10))
def forward(self , x):
out = self.cnn_layer(x)
out = out.view(-1, self.flat_fts)
out = self.flat_layer(out)
return out
Next, we specify how the network will be trained. We choose the device type, namely,
the central processing unit (CPU) or the GPU (if available), the number of training itera-
tions (epochs), and the learning rate. Then, we create an instance of the proposed convolu-
tion network and send it to the predeÔ¨Åned device (CPU or GPU). Note how easily one can
switch between the CPU or the GPU without major changes to the code.
In addition to the speciÔ¨Åcations above, we need to choose an appropriate loss function
and training algorithm. Here, we use the cross-entropy loss and the Adam adaptive gradi- +267
ent Algorithm 9.4.5. Once these parameters are set, the learning proceeds to evaluate the
gradient of the loss function via the back-propagation algorithm.Chapter 9. Deep Learning 347
# learning parameters
num_epochs = 50
learning_rate = 0.001
#device = torch.device ( 'cpu') # use this to run on CPU
device = torch.device ( 'cuda ')# use this to run on GPU
#instance of the Conv Net
cnn = CNN()
cnn.to(device=device)
#loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)
# the learning loop
losses = []
for epoch in range (1,num_epochs+1):
for i, (images , labels) in enumerate (train_loader):
images = Variable(images. float ()).to(device=device)
labels = Variable(labels).to(device=device)
optimizer.zero_grad()
outputs = cnn(images)
loss = criterion(outputs , labels)
loss.backward()
optimizer.step()
losses.append(loss.item())
if(epoch==1 orepoch % 10 == 0):
print ("Epoch : ", epoch , ", Training Loss: ", loss.item())
# evaluate on the test set
cnn. eval ()
correct = 0
total = 0
for images , labels intest_loader:
images = Variable(images. float ()).to(device=device)
outputs = cnn(images)
_, predicted = torch. max(outputs.data , 1)
total += labels.size(0)
correct += (predicted.cpu() == labels). sum()
print ("Test Accuracy of the model on the 10,000 training test images
: ", (100 * correct.item() / total),"%")
# plot
plt.rc( 'text ', usetex=True)
plt.rc( 'font ', family= 'serif ',size=20)
plt.tight_layout()
plt.plot(np.array(losses)[10: len(losses)])
plt.xlabel(r '{iteration} ',fontsize=20)
plt.ylabel(r '{Batch Loss} ',fontsize=20)
plt.subplots_adjust(top=0.8)
plt.show()348 Exercises
Epoch : 1 , Training Loss: 0.412550151348114
Epoch : 10 , Training Loss: 0.05452106520533562
Epoch : 20 , Training Loss: 0.07233225554227829
Epoch : 30 , Training Loss: 0.01696968264877796
Epoch : 40 , Training Loss: 0.0008199119474738836
Epoch : 50 , Training Loss: 0.006860652007162571
Test Accuracy of the model on the 10,000 training test images: 91.02 %
Finally, we evaluate the network performance using the test data set. A typical mini-
batch loss as a function of iteration is shown in Figure 9.7 and the proposed neural network
achieves about 91% accuracy on the test set.
0
 200
 400
 600
 800
 1000
iteration
0.2
0.4
0.6
0.8
1.0Batch Loss
Figure 9.7: The batch loss history.
Further Reading
A popular book written by some of the pioneers of deep learning is [53]. For an excellent
and gentle introduction to the intuition behind neural networks, we recommend [94]. A
summary of many e ective gradient descent methods for training of deep networks is given
in [105]. An early resource on the limited-memory BFGS method is [81], and a more recent
resource includes [13], which makes recommendations on the best choice for the length of
the BFGS history (that is, the value of the parameter h).
Exercises
1. Show that the softmax function
softmax : z7!exp(z)P
kexp(zk)
satisÔ¨Åes the invariance property:
softmax( z)=softmax( z+c1);for any constant c:Chapter 9. Deep Learning 349
2.Projection pursuit projection
pursuitis a network with one hidden layer that can be written as:
g(x)=S(!>x);
where Sis a univariate smoothing cubic spline . If we use squared-error loss with n= +235
fyi;xign
i=1, we need to minimize the training loss:
1
nnX
i=1 yi S(!>xi)2
with respect to !and all cubic smoothing splines. This training of the network is typically
tackled iteratively in a manner similar to the EM algorithm . In particular, we iterate ( t= +139
1;2;:::) the following steps until convergence.
(a) Given the missing data !t, compute the spline Stby training a cubic smoothing spline
onfyi;!>
txig. The smoothing coe cient of the spline may be determined as part of
this step.
(b) Given the spline function St, compute the next projection vector !t+1viaiterative
reweighted least squares :iterative
reweighted
least squares
!t+1=argmin
(et X)>t(et X); (9.11)
where
et;i:=!>
txi+yi St(!>
txi)
S0
t(!>
txi);i=1;:::; n
is the adjusted response, and 1=2
t=diag( S0
t(!>
tx1);:::; S0
t(!>
txn)) is a diagonal mat-
rix.
Apply Taylor‚Äôs Theorem B.1 to the function Stand derive the iterative reweighted +400
least-squares optimization program (9.11).
3. Suppose that in the stochastic gradient descent method we wish to repeatedly draw +335
minibatches of size Nfromn, where we assume that Nm=nfor some large integer m.
Instead of repeatedly resampling from n, an alternative is to reshu envia a random per-
mutation and then advance sequentially through the reshu ed training set to construct +115
mnon-overlapping minibatches. A single traversal of such a reshu ed training set is called
anepoch epoch . The following pseudo-code describes the procedure.350 Exercises
Algorithm 9.5.1: Stochastic Gradient Descent with Reshu ing
input: Training set n=f(xi;yi)gn
i=1, initial weight matrices and bias vectors
fWl;blgL
l=1!1, activation functions fSlgL
l=1, learning ratesf1;2;:::g.
output: The parameters of the trained learner.
1t 1 and epoch 0
2while stopping condition is not met do
3 Draw U1;:::; UniidU(0;1).
4 Letbe the permutation of f1;:::; ngthat satisÔ¨Åes U1<<Un:
5 (xi;yi) (xi;yi) for i=1;:::; n // reshuffle n
6 forj=1;:::; mdo
7 b` 1
NPjN
i=(j 1)N+1Loss( yi;g(xij))
8t+1 t t@b`
@(t)
9 t t+1
10 epoch epoch +1 // number of reshuffles or epochs
11returntas the minimizer of the training loss
Write Python code that implements the stochastic gradient descent with data reshu ing,
and use it to train the neural net in Section 9.5.1. +340
4. Denote the pdf of the N(0;) distribution by '(), and let
D(0;0j1;1)=Z
Rd'0(x 0) ln'0(x 0)
'1(x 1)dx
be the Kullback‚ÄìLeibler divergence between the densities of the N(0;0) andN(1;1) +42
distributions on Rd. Show that
2D(0;0j1;1)=tr( 1
10) lnj 1
10j+(1 0)> 1
1(1 0) d:
Hence, deduce the formula in (B.22).
5. Suppose that we wish to compute the inverse and log-determinant of the matrix
In+UU>;
where Uis an nhmatrix with hn. Show that
(In+UU>) 1=In QnQ>
n;
where Qncontains the Ô¨Årst nrows of the ( n+h)hmatrix Qin the QR factorization of +375
the (n+h)hmatrix:"U
Ih#
=QR:
In addition, show that ln jIn+UU>j=Ph
i=1lnr2
ii, wherefriigare the diagonal elements of
thehhmatrix R.Chapter 9. Deep Learning 351
6. Suppose that
U=[u0;u1;:::; uh 1];
where all u2Rnare column vectors and we have computed ( In+UU>) 1via the QR
factorization method in Exercise 5. If the columns of matrix Uare updated to
[u1;:::; uh 1;uh];
show that the inverse ( In+UU>) 1can be updated in O(h n) time (rather than computed
from scratch inO(h2n) time). Deduce that the computing cost of updating the Hessian
approximation (9.10) is the same as that for the limited-memory BFGS Algorithm 9.4.3.
In your solution you may use the following facts from [29]. Suppose we are given the
QandRfactors in the QR factorization of a matrix A2Rnh. If a row /column is added to
matrix A, then the QandRfactors need not be recomputed from scratch (in O(h2n) time),
but can be updated e ciently inO(h n) time. Similarly, if a row /column is removed from
matrix A, then the QandRfactors can be updated in O(h2) time.
7. Suppose that U2Rnhhas its k-th column vreplaced with w, giving the updated eU.
(a) If e2Rhdenotes the unit-length vector such that ek=kek=1 and
r:=p
2
2U>(w v)+p
2kw vk2
4ep
2
2e;
show that
eU>eU=U>U+r+r>
+ r r>
 :
[Hint: You may Ô¨Ånd Exercise 16 in Chapter 6 useful.] +247
(b) Let B:=(Ih+U>U) 1. Use the Woodbury identity (A.15) to show that +371
(In+eUeU>) 1=In eU
B 1+r+r>
+ r r>
  1eU>:
(c) Suppose that we have stored Bin computer memory. Use Algorithm 6.8.1 and parts
(a) and (b) to write pseudo-code that updates ( In+UU>) 1to (In+eUeU>) 1inO((n+h)h)
computing time.
8. Equation (9.7) gives the rank-two BFGS update of the inverse Hessian Ct 1toCt. In-
stead of using a two-rank update, we can consider a one-rank update, in which Ct 1is
updated to Ctby the general rank-one formula:
Ct=Ct 1+trtr>
t:
Find values for the scalar tand vector rt, such that CtsatisÔ¨Åes the secant condition Ct1t=
t.
9. Show that the BFGS formula (B.23) can be written as:
C 
I 1>>C
I 1>
+>;
where:=(1>) 1.352 Exercises
10. Show that the BFGS formula (B.23) is the solution to the constrained optimization
problem:
CBFGS= argmin
Asubject to A1=;A=A>D(0;Cj0;A);
whereDis the Kullback‚ÄìLeibler discrepancy deÔ¨Åned in (B.22). On the other hand, show
that the DFP formula (B.24) is the solution to the constrained optimization problem:
CDFP= argmin
Asubject to A1=;A=A>D(0;Aj0;C):
11. Consider again the logistic regression model in Exercise 5.18, which used iterative +213
reweighted least squares for training the learner. Repeat all the computations, but this
time using the limited-memory BFGS Algorithm 9.4.4. Which training algorithm converges
faster to the optimal solution?
12. Download the seeds_dataset.txt data set from the book‚Äôs GitHub site, which con-
tains 210 independent examples. The categorical output (response) here is the type of wheat
grain: Kama, Rosa, and Canadian (encoded as 1, 2, and 3), so that c=3. The seven con-
tinuous features (explanatory variables) are measurements of the geometrical properties of
the grain (area, perimeter, compactness, length, width, asymmetry coe cient, and length
of kernel groove). Thus, x2R7(which does not include the constant feature 1) and the
multi-logit pre-classiÔ¨Åer in Example 9.2 can be written as g(x)=softmax( Wx+b), where +328
W2R37andb2R3. Implement and train this pre-classiÔ¨Åer on the Ô¨Årst n=105 examples
of the seeds data set using, for example, Algorithm 9.4.1. Use the remaining n0=105
examples in the data set to estimate the generalization risk of the learner using the cross-
entropy loss. [Hint: Use the cross-entropy loss formulas from Example 9.4.]
13. In Exercise 12 above, we train the multi-logit classiÔ¨Åer using a weight matrix W2R37
and bias vector b2R3. Repeat the training of the multi-logit model, but this time keeping z1
as an arbitrary constant (say z1=0), and thus setting c=0 to be a ‚Äúreference‚Äù class. This
has the e ect of removing a node from the output layer of the network, giving a weight
matrix W2R27and bias vector b2R2of smaller dimensions than in (7.16). +267
14. Consider again Example 9.4, where we used a softmax output function SLin con- +333
junction with the cross-entropy loss: C()= lngy+1(xj):Find formulas for@C
@gand@SL
@zL.
Hence, verify that:
@SL
@zL@C
@g=g(xj) ey+1;
where eiis the unit length vector with an entry of 1 in the i-th position.
15. Derive the formula (B.25) for a diagonal Hessian update in a quasi-Newton method +412
for minimization. In other words, given a current minimizer xtoff(x), a diagonal matrix
Cof approximating the Hessian of f, and a gradient vector u=rf(xt), Ô¨Ånd the solution
to the constrained optimization program:
min
AD(xt;Cjxt Au;A)
subject to: A1>;Ais diagonal;
whereDis the Kullback‚ÄìLeibler distance deÔ¨Åned in (B.22) (see Exercise 4).Chapter 9. Deep Learning 353
16. Consider again the Python implementation of the polynomial regression in Sec-
tion 9.5.1, where the stochastic gradient descent was used for training.
Using the polynomial regression data set, implement and run the following four altern-
ative training methods:
(a) the steepest-descent Algorithm 9.4.1;
(b) the Levenberg‚ÄìMarquardt Algorithm B.3.3, in conjunction with Algorithm 9.4.2 for +415
computing the matrix of Jacobi;
(c) the limited-memory BFGS Algorithm 9.4.4;
(d) the Adam Algorithm 9.4.5, which uses past gradient values to determine the next
search direction.
For each training algorithm, using trial and error, tune any algorithmic parameters so that
the network training is as fast as possible. Comment on the relative advantages and disad-
vantages of each training /optimization method. For example, comment on which optimiz-
ation method makes rapid initial progress, but gets trapped in a suboptimal solution, and
which method is slower, but more consistent in Ô¨Ånding good optima.
17. Consider again the Pytorch code in Section 9.5.2. Repeat all the computations, but
this time using the momentum method for training of the network. Comment on which
method is preferable: the momentum or the Adam method?354APPENDIXA
LINEAR ALGEBRA AND FUNCTIONAL
ANALYSIS
The purpose of this appendix is to review some important topics in linear algebra
and functional analysis. We assume that the reader has some familiarity with matrix
and vector operations, including matrix multiplication and the computation of determ-
inants.
A.1 Vector Spaces, Bases, and Matrices
Linear algebra is the study of vector spaces and linear mappings. Vectors are, by deÔ¨Åni-
tion, elements of some vector space vector space Vand satisfy the usual rules of addition and scalar
multiplication, e.g.,
ifx2V andy2V, thenx+y2V for all;2R(orC).
We will be dealing mostly with vectors in the Euclidean vector space Rnfor some n. That
is, we view the points of Rnas objects that can be added up and multiplied with a scalar,
e.g., ( x1;x2)+(y1;y2)=(x1+y1;x2+y2) for points in R2. Sometimes it is convenient to
work with the complex vector space Cninstead ofRn; see also Section A.3.
Vectors v1;:::; vkare called linearly independent linearly
independentif none of them can be expressed as
a linear combination of the others; that is, if 1v1++nvn=0, then it must hold that
i=0 for all i=1;:::; n.
DeÔ¨Ånition A.1: Basis of a Vector Space
A set of vectorsB=fv1;:::; vngis called a basis basis of the vector space Vif every
vector x2V can be written as a unique linear combination of the vectors in B:
x=1v1++nvn:
The (possibly inÔ¨Ånite) number nis called the dimension dimension ofV.
355356 A.1. Vector Spaces, Bases, and Matrices
Using a basisBofV, we can thus represent each vector x2V as a row or column of
numbers
[1;:::; n] or266666666641
:::
n37777777775: (A.1)
Typically, vectors in Rnare represented via the standard basis standard basis , consisting of unit
vectors (points) e1=(1;0;:::; 0);:::; en=(0;0;:::; 0;1). As a consequence, any point
(x1;:::; xn)2Rncan be represented, using the standard basis, as a row or column vec-
tor of the form (A.1) above, with i=xi;i=1;:::; n. We will also write [ x1;x2;:::; xn]>,
for the corresponding column vector, where>denotes the transpose transpose .
To avoid confusion, we will use the convention from now on that a generic vector x
is always represented via the standard basis as a column vector. The corresponding
row vector is denoted by x>.
Amatrix can be viewed as an array of mrows and ncolumns that deÔ¨Ånes a linearmatrixtransformation fromRntoRm(or for complex matrices, from CntoCm). The matrix is saidlinear
transformation to be square ifm=n. Ifa1;a2;:::; anare the columns of A, that is, A=[a1;a2;:::; an],
and if x=[x1;:::; xn]>, then Ax=x1a1++xnan:In particular, the standard basis
vector ekis mapped to the vector ak,k=1;:::; n. We sometimes use the notation A=[ai j],
to denote a matrix whose ( i;j)-th element is ai j. When we wish to emphasize that a matrix
Ais real-valued with mrows and ncolumns, we write A2Rmn. The rank rank of a matrix is the
number of linearly independent rows or, equivalently, the number of linearly independent
columns.
Example A.1 (Linear Transformation) Take the matrix
A="1 1
 0:5 2#
:
It transforms the two basis vectors [1 ;0]>and [0;1]>, shown in red and blue in the left panel
of Figure A.1, to the vectors [1 ; 0:5]>and [1; 2]>, shown on the right panel. Similarly,
the points on the unit circle are transformed to an ellipse.
-1 -0.5 0 0.5 1
x-1-0.500.51y
-2 0 2
x-3-2-10123y
Figure A.1: A linear transformation of the unit circle.Appendix A. Linear Algebra and Functional Analysis 357
Suppose A=[a1;:::; an], where theA=faigform a basis of Rn. Take any vector x=
[x1;:::; xn]>
Ewith respect to the standard basis E(we write subscript Eto stress this). Then
the representation of this vector with respect to Ais simply
y=A 1x;
where A 1is the inverse inverse ofA; that is, the matrix such that AA 1=A 1A=In, where
Inis the n-dimensional identity matrix. To see this, note that A 1aigives the i-th unit
vector representation, for i=1;:::; n, and recall that each vector in Rnis a unique linear
combination of these basis vectors.
Example A.2 (Basis Representation) Consider the matrix
A="1 2
3 4#
with inverse A 1=" 2 1
3=2 1=2#
: (A.2)
The vector x=[1;1]>
Ein the standard basis has representation y=A 1x=[ 1;1]>
Ain the
basis consisting of the columns of A. Namely,
Ay= "1
3#
+"2
4#
="1
1#
:
Thetranspose transpose of a matrix A=[ai j] is the matrix A>=[aji]; that is, the ( i;j)-th element
ofA>is the ( j;i)-th element of A. The trace trace of a square matrix is the sum of its diagonal
elements. A useful result is the following cyclic property.
Theorem A.1: Cyclic Property
The trace is invariant under cyclic permutations: tr( ABC )=tr(BCA )=tr(CAB ).
Proof: It suces to show that tr( DE) is equal to tr( ED) for any mnmatrix D=[di j]
andnmmatrix E=[ei j]. The diagonal elements of DEarePn
j=1di jeji;i=1;:::; mand
the diagonal elements of EDarePm
i=1ejidi j;j=1;:::; n. They sum up to the same numberPm
i=1Pn
j=1di jeji. 
A square matrix has an inverse if and only if its columns (or rows) are linearly in-
dependent. This is the same as the matrix being of full rank ; that is, its rank is equal to
the number of columns. An equivalent statement is that its determinant is not zero. The
determinant determinant of an nnmatrix A=[ai;j] is deÔ¨Åned as
det(A) :=X
( 1)()nY
i=1ai;i; (A.3)
where the sum is over all permutations =(1;:::; n) of (1;:::; n), and() is the num-
ber of pairs ( i;j) for which i<jandi>  j. For example, (2;3;4;1)=3 for the pairs
(1;4);(2;4);(3;4). The determinant of a diagonal matrix diagonal matrix ‚Äî a matrix with only zero ele-
ments o the diagonal ‚Äî is simply the product of its diagonal elements.358 A.1. Vector Spaces, Bases, and Matrices
Geometrically, the determinant of a square matrix A=[a1;:::; an] is the (signed)
volume of the parallelepiped ( n-dimensional parallelogram) deÔ¨Åned by the columns
a1;:::; an; that is, the set of points x=Pn
i=1iai, where 06i61;i=1;:::; n.
The easiest way to compute a determinant of a general matrix is to apply simple op-
erations to the matrix that potentially reduce its complexity (as in the number of non-zero
elements, for example), while retaining its determinant:
¬àAdding a multiple of one column (or row) to another, does not change the determin-
ant.
¬àMultiplying a column (or row) with a number multiplies the determinant by the same
number.
¬àSwapping two rows changes the sign of the determinant.
By applying these rules repeatedly one can reduce any matrix to a diagonal matrix.
It follows then that the determinant of the original matrix is equal to the product of the
diagonal elements of the resulting diagonal matrix multiplied by a known constant.
Example A.3 (Determinant and Volume) Figure A.2 illustrates how the determinant
of a matrix can be viewed as a signed volume, which can be computed by repeatedly apply-
ing the Ô¨Årst rule above. Here, we wish to compute the area of red parallelogram determined
by the matrix Agiven in (A.2). In particular, the corner points of the parallelogram corres-
pond to the vectors [0 ;0]>;[1;3]>;[2;4]>, and [3;7]>.
0 0.5 1 1.5 2 2.5 3-202468
Figure A.2: The volume of the red parallelogram can be obtained by a number of shear
operations that do not change the volume.
Adding 2 times the Ô¨Årst column of Ato the second column gives the matrix
B="1 0
3 2#
;Appendix A. Linear Algebra and Functional Analysis 359
corresponding to the blue parallelogram. The linear operation that transforms the red to the
blue parallelogram can be thought of as a succession of two linear transformations. The
Ô¨Årst is to transform the coordinates of points on the red parallelogram (in standard basis)
to the basis formed by the columns of A. Second, relative to this new basis, we apply the
matrix Babove. Note that the input of this matrix is with respect to the new basis, whereas
the output is with respect to the standard basis. The matrix for the combined operation is
now
BA 1="1 0
3 2# " 2 1
3=2 1=2#
=" 2 1
 9 4#
;
which maps [1 ;3]>to [1;3]>(does not change) and [2 ;4]>to [0; 2]>. We say that we
apply a shear shear in the direction [1 ;3]>. The signiÔ¨Åcance of such an operation is that a shear
does not alter the volume of the parallelogram . The second (blue) parallelogram has an
easier form, because one of the sides is parallel to the y-axis. By applying another shear,
in the direction [0 ; 2]>, we can obtain a simple (green) rectangle, whose volume is 2. In
matrix terms, we add 3 /2 times the second column of Bto the Ô¨Årst column of B, to obtain
the matrix
C="1 0
0 2#
;
which is a diagonal matrix, whose determinant is  2, corresponding to the volume 2 of all
the parallelograms.
Theorem A.2 summarizes a number of useful matrix rules for the concepts that we have
discussed so far. We leave the proofs, which typically involves ‚Äúwriting out‚Äù the equations,
as an exercise for the reader; see also [116].
Theorem A.2: Useful Matrix Rules
1. (AB)>=B>A>
2. (AB) 1=B 1A 1
3. (A 1)>=(A>) 1=:A >
4. det( AB)=det(A) det( B)
5.x>Ax=tr Axx>
6. det( A)=Q
iaiiifA=[ai j] is triangular
Next, consider an npmatrix Afor which the matrix inverse fails to exist. That is, A
is either non-square ( n,p) or its determinant is 0. Instead of the inverse, we can use its
so-called pseudo-inverse, which always exists.360 A.2. Inner Product
DeÔ¨Ånition A.2: Moore‚ÄìPenrose Pseudo-Inverse
The Moore‚ÄìPenrose pseudo-inverse Moore ‚ÄìPenrose
pseudo -inverseof a real matrix A2Rnpis deÔ¨Åned as the
unique matrix A+2Rpnthat satisÔ¨Åes the conditions:
1.AA+A=A
2.A+AA+=A+
3. (AA+)>=AA+
4. (A+A)>=A+A
We can write A+explicitly in terms of Awhen Ahas a full column or row rank. For
example, we always have
A>AA+=A>(AA+)>=((AA+)A)>=(A)>=A>: (A.4)
IfAhas a full column rank p, then ( A>A) 1exists, so that from (A.4) it follows that
A+=(A>A) 1A>. This is referred to as the left pseudo-inverse left
pseudo -inverse, asA+A=Ip. Similarly, if
Ahas a full row rank n, that is, ( AA>) 1exists, then it follows from
A+AA>=(A+A)>A>=(A(A+A))>=A>
thatA+=A>(AA>) 1. This is the right pseudo-inverse right
pseudo -inverse, asAA+=In. Finally, if Ais of full
rank and square, then A+=A 1.
A.2 Inner Product
The (Euclidean) inner product inner product of two real vectors x=[x1;:::; xn]>andy=[y1;:::; yn]>
is deÔ¨Åned as the number
hx;yi=nX
i=1xiyi=x>y:
Here x>yis the matrix multiplication of the (1 n) matrix x>and the ( n1) matrix y.
The inner product induces a geometry on the linear space Rn, allowing for the deÔ¨Ånition of
length, angle, and so on. The inner product satisÔ¨Åes the following properties:
1.hx+y;zi=hx;zi+hy;zi;
2.hx;yi=hy;xi;
3.hx;xi>0;
4.hx;xi=0 if and only if x=0.
Vectors xandyare called perpendicular (ororthogonal orthogonal ) ifhx;yi=0. The Euclidean
norm Euclidean norm (or length) of a vector xis deÔ¨Åned as
jjxjj=q
x2
1++x2
n=p
hx;xi:Appendix A. Linear Algebra and Functional Analysis 361
Ifxandyare perpendicular, then Pythagoras‚Äô theorem Pythagoras ‚Äô
theoremholds:
jjx+yjj2=hx+y;x+yi=hx;xi+2hx;yi+hy;yi=jjxjj2+jjyjj2: (A.5)
A basisfv1;:::; vngofRnin which all the vectors are pairwise perpendicular and have
norm 1 is called an orthonormal orthonormal (short for orthogonal and normalized) basis. For example,
the standard basis is orthonormal.
Theorem A.3: Orthonormal Basis Representation
Iffv1;:::; vngis an orthonormal basis of Rn, then any vector x2Rncan be expressed
as
x=hx;v1iv1++hx;vnivn: (A.6)
Proof: Observe that, because the fvigform a basis, there exist unique 1;:::; nsuch that
x=1v1++nvn. By the linearity of the inner product and the orthonormality of the
fvigit follows thathx;vji=hP
iivi;vji=j. 
Annnmatrix Vwhose columns form an orthonormal basis is called an orthogonal
matrix orthogonal
matrix.1Note that for an orthogonal matrix V=[v1;:::; vn], we have
V>V=26666666666666664v>
1
v>
2:::
v>
n37777777777777775[v1;v2;:::; vn]=26666666664v>
1v1v>
1v2:::v>
1vn
::::::::::::
v>
nv1v>
nv2:::v>
nvn37777777775=In:
Hence, V 1=V>. Note also that an orthogonal transformation is length preserving length
preserving; that
is,Vxhas the same length as x. This follows from
jjVxjj2=hVx;Vxi=x>V>Vx=x>x=jjxjj2:
A.3 Complex Vectors and Matrices
Instead of the vector space Rnofn-dimensional real vectors, it is sometimes useful to
consider the vector space Cnofn-dimensional complex vectors. In this case the adjoint adjoint
orconjugate transpose operation () replaces the transpose operation ( >). This involves
the usual transposition of the matrix or vector with the additional step that any complex
number z=x+iyis replaced by its complex conjugate z=x iy. For example, if
x="a1+ib1
a2+ib2#
and A="a11+ib11a12+ib12
a21+ib21a22+ib22#
;
then
x=[a1 ib1;a2 ib2] and A="a11 ib11a21 ib21
a12 ib12a22 ib22#
:
1The qualiÔ¨Åer ‚Äúorthogonal‚Äù for such matrices has been Ô¨Åxed by history. A better term would have been
‚Äúorthonormal‚Äù.362 A.4. Orthogonal Projections
The (Euclidean) inner product of xandy(viewed as column vectors) is now deÔ¨Åned as
hx;yi=yx=nX
i=1xiyi;
which is no longer symmetric: hx;yi=hy;xi. Note that this generalizes the real-valued
inner product. The determinant of a complex matrix Ais deÔ¨Åned exactly as in (A.3). As a
consequence, det( A)=det(A).
A complex matrix is said to be Hermitian orself-adjoint ifA=A, and unitary ifHermitian
unitaryAA=I(that is, if A=A 1). For real matrices ‚ÄúHermitian‚Äù is the same as ‚Äúsymmetric‚Äù,
and ‚Äúunitary‚Äù is the same as ‚Äúorthogonal‚Äù.
A.4 Orthogonal Projections
Letfu1;:::; ukgbe a set of linearly independent vectors in Rn. The set
V=Spanfu1;:::; ukg=f1u1++kuk; 1;:::; k2Rg;
is called the linear subspace spanned by fu1;:::; ukg. The orthogonal complement ofV,linear subspace
orthogonal
complementdenoted byV?, is the set of all vectors wthat are orthogonal to V, in the sense that
hw;vi=0 for all v2V. The matrix Psuch that Px=x, for all x2V, and Px=0, for all
x2V?is called the orthogonal projection matrix orthogonal
projection
matrixontoV. Suppose that U=[u1;:::; uk]
has full rank, in which case U>Uis an invertible matrix. The orthogonal projection matrix
PontoV=Spanfu1;:::; ukgis then given by
P=U(U>U) 1U>:
Namely, since PU=U, the matrix Pprojects any vector in Vonto itself. Moreover, P
projects any vector in V?onto the zero vector. Using the pseudo-inverse, it is possible to
specify the projection matrix also for the case where Uis not of full rank, leading to the
following theorem.
Theorem A.4: Orthogonal Projection
LetU=[u1;:::; uk]. Then, the orthogonal projection matrix PontoV=Spanfu1;
:::;ukgis given by
P=U U+; (A.7)
where U+is the (right) pseudo-inverse of U.
Proof: By Property 1 of DeÔ¨Ånition A.2 we have PU=UU+U=U, so that Pprojects any
vector inVonto itself. Moreover, Pprojects any vector in V?onto the zero vector. 
Note that in the special case where u1;:::; ukabove form an orthonormal basis of V,
then the projection onto Vis very simple to describe, namely we have
Px=UU>x=kX
i=1hx;uiiui: (A.8)Appendix A. Linear Algebra and Functional Analysis 363
For any point x2Rn, the point inVthat is closest to xis its orthogonal projection Px,
as the following theorem shows.
Theorem A.5: Orthogonal Projection and Minimal Distance
Letfu1;:::; ukgbe an orthonormal basis of subspace Vand let Pbe the orthogonal
projection matrix onto V. The solution to the minimization program
min
y2Vkx yk2
isy=Px. That is, Px2V is closest to x.
Proof: We can write each point y2V asy=Pk
i=1iui. Consequently,
kx yk2=
x kX
i=1iui;x kX
i=1iui
=kxk2 2kX
i=1ihx;uii+kX
i=12
i:
Minimizing this with respect to the figgivesi=hx;uii;i=1;:::; k. In view of (A.8),
the optimal yis thus Px. 
A.5 Eigenvalues and Eigenvectors
LetAbe an nnmatrix. If Av=vfor some number and non-zero vector v, thenis
called an eigenvalue ofAwith eigenvector v.eigenvalue
eigenvector If (;v) is an (eigenvalue, eigenvector) pair, the matrix I Amaps any multiple of v
to the zero vector. Consequently, the columns of I Aare linearly dependent , and hence
its determinant is 0. This provides a way to identify the eigenvalues, namely as the r6n
dierent roots1;:::; rof the characteristic polynomial characteristic
polynomial
det(I A)=( 1)1( r)r;
where1++r=n. The integer iis called the algebraic multiplicity ofi. Thealgebraic
multiplicity eigenvectors that correspond to an eigenvalue ilie in the kernel ornull space of the matrix
null space iI A; that is, the linear space of vectors vsuch that (iI A)v=0. This space is called
theeigenspace ofi. Its dimension, di2f1;:::; ng, is called the geometric multiplicity ofgeometric
multiplicity i. It always holds that di6i. IfP
idi=n, then we can construct a basis for Rnconsisting
of eigenvectors, as illustrated next.
Example A.4 (Linear Transformation (cont.)) We revisit the linear transformation in
Figure A.1, where
A="1 1
 1=2 2#
:
The characteristic polynomial is (  1)(+2)+1=2, with roots 1= 1=2 p
7=2
 1:8229 and2= 1=2+p
7=20:8229. The corresponding unit eigenvectors are v1
[0:3339; 0:9426]>andv2[0:9847; 0:1744]>. The eigenspace corresponding to 1is364 A.5. Eigenvalues and Eigenvectors
V1=Spanfv1g=fv1:2Rgand the eigenspace corresponding to 2isV2=Spanfv2g.
The algebraic and geometric multiplicities are 1 in this case. Any pair of vectors taken
fromV1andV2forms a basis for R2. Figure A.3 shows how v1andv2are transformed to
Av12V 1andAv22V 2, respectively.
-2 0 2
x-3-2-10123y
Figure A.3: The dashed arrows are the unit eigenvectors v1(blue) and v2(red) of matrix A.
Their transformed values Av1andAv2are indicated by solid arrows.
A matrix for which the algebraic and geometric multiplicities of all its eigenvalues
are the same is called semi-simple . This is equivalent to the matrix being diagonalizable ,semi-simple
diagonalizable meaning that there is a matrix Vand a diagonal matrix Dsuch that
A=VDV 1:
To see that this so-called eigen-decomposition eigen -
decompositionholds, suppose Ais a semi-simple matrix
with eigenvalues
1;:::; 1|     {z     }
d1;;r;:::; r|     {z     }
dr:
LetDbe the diagonal matrix whose diagonal elements are the eigenvalues of A, and let V
be a matrix whose columns are linearly independent eigenvectors corresponding to these
eigenvalues. Then, for each (eigenvalue, eigenvector) pair ( ;v), we have Av=v. Hence,
in matrix notation, we have A V=VD, and so A=VDV 1.
A.5.1 Left- and Right-Eigenvectors
The eigenvector as deÔ¨Åned in the previous section is called a right -eigenvector, as it lies on
the right of Ain the equation Av=v.
IfAis a complex matrix with an eigenvalue , then the eigenvalue‚Äôs complex conjugate
is an eigenvalue of A. To see this, deÔ¨Åne B:=I AandB:=I A. Sinceis
an eigenvalue, we have det( B)=0. Applying the identity det( B)=det(B), we see thatAppendix A. Linear Algebra and Functional Analysis 365
therefore det( B)=0, and hence that is an eigenvalue of A. Let wbe an eigenvector
corresponding to . Then, Aw=wor, equivalently,
wA=w:
For this reason, we call wtheleft-eigenvector left-
eigenvectorofAfor eigenvalue . Ifvis a (right-) ei-
genvector of A, then its adjoint vis usually nota left-eigenvector, unless AA=AA
(such matrices are called normal ; normal matrix a real symmetric matrix is normal). However, the im-
portant property holds that left- and right-eigenvectors belonging to dierent eigenvalues
areorthogonal . Namely, if wis a left-eigenvalue of 1andva right-eigenvalue of 2,1,
then
1wv=wAv=2wv;
which can only be true if wv=0.
Theorem A.6: Schur Triangulation
For any complex matrix A, there exists a unitary matrix Usuch that T=U 1AUis
upper triangular.
Proof: The proof is by induction on the dimension nof the matrix. Clearly, the statement
is true for n=1, as Ais simply a complex number and we can take Uequal to 1. Suppose
that the result is true for dimension n. We wish to show that it also holds for dimension
n+1. Any matrix Aalways has at least one eigenvalue with eigenvector v, normalized
to have length 1. Let Ube any unitary matrix whose Ô¨Årst column is v. Such a matrix can
always be constructed2. AsUis unitary, the Ô¨Årst row of U 1isv, and U 1AUis of the form
"v
#
Ah
vi
|   {z   }
U="
0B#
;
for some matrix B. By the induction hypothesis, there exists a unitary matrix Wand an
upper triangular matrix Tsuch that W 1BW=T. Now, deÔ¨Åne
V:="10>
0W#
:
Then,
V 1
U 1AU
V="10>
0W 1#"
0B#"10>
0W#
="
0W 1BW#
="
0T#
;
which is upper triangular of dimension n+1. Since UVis unitary, this completes the
induction, and hence the result is true for all n. 
The theorem above can be used to prove an important property of Hermitian matrices,
i.e., matrices for which A=A.
2After specifying vwe can complete the rest of the unitary matrix via the Gram‚ÄìSchmidt procedure, for
example; see Section A.6.4.366 A.5. Eigenvalues and Eigenvectors
Theorem A.7: Eigenvalues of a Hermitian Matrix
Any nnHermitian matrix has real eigenvalues. The corresponding matrix of nor-
malized eigenvectors is a unitary matrix.
Proof: LetAbe a Hermitian matrix. By Theorem A.6 there exists a unitary matrix Usuch
thatU 1AU=T, where Tis upper triangular. It follows that the adjoint ( U 1AU)=T
is lower triangular. However, ( U 1AU)=U 1AU, since A=AandU=U 1. Hence,
TandTmust be the same, which can only be the case if Tis areal diagonal matrix D.
Since AU=DU, the diagonal elements are exactly the eigenvalues and the corresponding
eigenvectors are the columns of U. 
In particular, the eigenvalues of a real symmetric matrix are real. We can now repeat
the proof of Theorem A.6 with real eigenvalues and eigenvectors, so that there exists an
orthogonal matrix Qsuch that Q 1AQ=Q>AQ=D. The eigenvectors can be chosen as
the columns of Q, which form an orthonormal basis. This proves the following theorem.
Theorem A.8: Real Symmetric Matrices are Orthogonally Diagonizable
Any real symmetric matrix Acan be written as
A=QDQ>;
where Dis the diagonal matrix of (real) eigenvalues and Qis an orthogonal matrix
whose columns are eigenvectors of A.
Example A.5 (Real Symmetric Matrices and Ellipses) As we have seen, linear trans-
formations map circles into ellipses. We can use the above theory for real symmetric
matrices to identify the principal axes. Consider, for example, the transformation with mat-
rixA=[1;1; 1=2; 2] in (A.1). A point xon the unit circle is mapped to a point y=Ax.
Since for such points kxk2=x>x=1, we have that ysatisÔ¨Åes y>(A 1)>A 1y=1, which
gives the equation for the ellipse
17y2
1
9+20y1y2
9+8y2
2
9=1:
LetQbe the orthogonal matrix of eigenvectors of the symmetric matrix ( A 1)>A 1=
(AA>) 1, soQ>(AA>) 1Q=Dfor some diagonal matrix D. Taking the inverse on both
sides of the previous equation, we have Q>AA>Q=D 1, which shows that Qis also the
matrix of eigenvectors of AA>. These eigenvectors point precisely in the direction of the
principal axes, as shown in Figure A.4. It turns out, see Section A.6.5, that the square roots
of the eigenvalues of AA>, here approximately 2 :4221 and 0:6193, correspond to the sizes
of the principal axes of the ellipse, as illustrated in Figure A.4.Appendix A. Linear Algebra and Functional Analysis 367
-1 0 1-2-1012
Figure A.4: The eigenvectors and eigenvalues of AA>determine the principal axes of the
ellipse.
The following deÔ¨Ånition generalizes the notion of positivity of a real variable to that of a
(Hermitian) matrix, providing a crucial concept for multivariate di erentiation and optim-
ization; see Appendix B. +397
DeÔ¨Ånition A.3: Positive (Semi)DeÔ¨Ånite Matrix
A Hermitian matrix Ais called positive semideÔ¨Ånite positive
semidefinite(we write A0) ifhAx;xi>0
for all x. It is called positive deÔ¨Ånite (we write A0) ifhAx;xi>0 for all x,0.
The positive (semi)deÔ¨Åniteness of a matrix can be directly related to the positivity of
its eigenvalues, as follows:
Theorem A.9: Eigenvalues of a Positive SemideÔ¨Ånite Matrix
All eigenvalues of a positive semideÔ¨Ånite matrix are non-negative and all eigenval-
ues of a positive deÔ¨Ånite matrix are strictly positive.
Proof: LetAbe a positive semideÔ¨Ånite matrix. By Theorem A.7, the eigenvalues of Aare
allreal. Supposeis an eigenvalue with eigenvector v. As Ais positive semideÔ¨Ånite, we
have
06hAv;vi=hv;vi=kvk2;
which can only be true if >0. Similarly, for a positive deÔ¨Ånite matrix, must be strictly
greater than 0. 
Corollary A.1 Any real positive semideÔ¨Ånite matrix Acan be written as
A=BB>
for some real matrix B. Conversely, for any real matrix B, the matrix BB>is positive
semideÔ¨Ånite.368 A.6. Matrix Decompositions
Proof: The matrix Ais both Hermitian (by deÔ¨Ånition) and real (by assumption) and hence
it is symmetric. By Theorem A.8, we can write A=QDQ>, where Dis the diagonal
matrix of (real) eigenvalues of A. By Theorem A.9 all eigenvalues are non-negative, and
thus their square root is real-valued. Now, deÔ¨Åne B=Qp
D, wherep
Dis deÔ¨Åned as the
diagonal matrix whose diagonal elements are the square roots of the eigenvalues of A.
Then, BB>=Qp
D(p
D)>Q>=QDQ>=A. The converse statement follows from the
fact that x>BB>x=kB>xk2>0 for all x. 
A.6 Matrix Decompositions
Matrix decompositions are frequently used in linear algebra to simplify proofs, avoid nu-
merical instability, and to speed up computations. We mention three important matrix de-
compositions: (P)LU, QR, and SVD.
A.6.1 (P)LU Decomposition
Every invertible matrix Acan be written as the product of three matrices:
A=PLU; (A.9)
where Lis a lower triangular matrix, Uan upper triangular matrix, and Papermutation
matrix permutation
matrix. A permutation matrix is a square matrix with a single 1 in each row and column,
and zeros otherwise. The matrix product PBsimply permutes the rows of a matrix Band,
likewise, BPpermutes its columns. A decomposition of the form (A.9) is called a PLU
decomposition PLU
decomposition. As a permutation matrix is orthogonal, its transpose is equal to its inverse,
and so we can write (A.9) as
P>A=LU:
The decomposition is not unique, and in many cases Pcan be taken to be the identity
matrix, in which case we speak of the LU decomposition ofA, also called the LR for
left‚Äìright (triangular) decomposition.
A PLU decomposition of an invertible nnmatrix A0can be obtained recursively as
follows. The Ô¨Årst step is to swap the rows of A0such that the element in the Ô¨Årst column and
Ô¨Årst row of the pivoted matrix is as large as possible in absolute value. Write the resulting
matrix as
eP0A0="a1b>
1
c1D1#
;
whereeP0is the permutation matrix that swaps the Ô¨Årst and k-th row, where kis the row
that contains the largest element in the Ô¨Årst column. Next, add the matrix  c1[1;b>
1=a1] to
the last n 1 rows of eP0A0, to obtain the matrix
"a1 b>
1
0 D 1 c1b>
1=a1#
=:"a1b>
1
0 A 1#
:
In eect, we add some multiple of the Ô¨Årst row to each of the remaining rows in order to
obtain zeros in the Ô¨Årst column, except for the Ô¨Årst element.
We now apply the same procedure to A1as we did to A0and then to subsequent smaller
matrices A2;:::; An 1:Appendix A. Linear Algebra and Functional Analysis 369
1. Swap the Ô¨Årst row with the row having the maximal absolute value element in the
Ô¨Årst column.
2. Make every other element in the Ô¨Årst column equal to 0 by adding appropriate mul-
tiples of the Ô¨Årst row to the other rows.
Suppose that Athas a PLU decomposition PtLtUt. Then it is easy to check that
eP>
t 1"10>
0 P t#
|        {z        }
Pt 1"1 0>
P>
tct=atLt#
|           {z           }
Lt 1"atb>
t
0 U t#
|   {z   }
Ut 1(A.10)
is a PLU decomposition of At 1. Since the PLU decomposition for the scalar An 1is trivial,
by working backwards we obtain a PLU decomposition P0L0U0ofA.
Example A.6 (PLU Decomposition) Take
A=26666666640 1 7
3 2 0
1 1 13777777775:
Our goal is to modify Avia Steps 1 and 2 above so as to obtain an upper triangular matrix
with maximal elements on the diagonal. We Ô¨Årst swap the Ô¨Årst and second row. Next, we
add 1=3 times the Ô¨Årst row to the third row and 1 =3 times the second row to the third row:
26666666640 1 7
3 2 0
1 1 13777777775 !26666666643 2 0
0 1 7
1 1 13777777775 !26666666643 2 0
0 1 7
0 1=3 13777777775 !26666666643 2 0
0 1 7
0 0 10=33777777775:
The Ô¨Ånal matrix is U0, and in the process we have applied the permutation matrices
eP0=26666666640 1 0
1 0 0
0 0 13777777775;eP1="1 0
0 1#
:
Using the recursion (A.10) we can now recover P0andL0. Namely, at the Ô¨Ånal iteration
we have P2=1;L2=1, and U2=10=3. And subsequently,
P1="1 0
0 1#
;L1="1 0
 1=3 1#
;P0=26666666640 1 0
1 0 0
0 0 13777777775;L0=26666666641 0 0
0 1 0
1=3 1=3 13777777775;
observing that a1=3;c1=[0;1]>,a2= 1, and c2=1=3.
PLU decompositions can be used to solve large systems of linear equations of the form
Ax=beciently, especially when such an equation has to be solved for many di erent b.
This is done by Ô¨Årst decomposing AintoPLU , and then solving two triangular systems:
1.Ly=P>b.
2.Ux=y.370 A.6. Matrix Decompositions
The Ô¨Årst equation can be solved e ciently via forward substitution , and the second viaforward
substitution backward substitution , as illustrated in the following example.
backward
substitution Example A.7 (Solving Linear Equations with an LU Decomposition) LetA=PLU
be the same as in Example A.6. We wish to solve Ax=[1;2;3]>. First, solving
26666666641 0 0
0 1 0
1=3 1=3 137777777752666666664y1
y2
y33777777775=26666666642
1
33777777775
gives, y1=2,y2=1 and y3=3 2=3+1=3=8=3, by forward substitution. Next,
26666666643 2 0
0 1 7
0 0 10=337777777752666666664x1
x2
x33777777775=26666666642
1
8=33777777775
gives x3=4=5;x2= 1+28=5=23=5, and x1=2(1 23=5)=3= 12=5, so x=
[ 12;23;4]>=5.
A.6.2 Woodbury Identity
LU (or more generally PLU) decompositions can also be applied to block matrices. A
starting point is the following LU decomposition for a general 2 2 matrix:
"a b
c d#
="a 0
c d bc=a#"1b=a
0 1#
;
which holds as long as a,0; this can be seen by simply writing out the matrix product.
The block matrix generalization for matrices A2Rnn;B2Rnk;C2Rkn;D2Rkkis
:="A B
C D#
="A O nk
C D CA 1B#"InA 1B
Okn Ik#
; (A.11)
provided that Ais invertible (again, write out the block matrix product). Here, we use the
notation Opqto denote the pqmatrix of zeros. We can further rewrite this as:
="In Onk
CA 1Ik#"A O nk
OknD CA 1B#"InA 1B
Okn Ik#
:
Thus, inverting both sides, we obtain
 1="InA 1B
Okn Ik# 1"A O nk
OknD CA 1B# 1"In Onk
CA 1Ik# 1
:
Inversion of the above block matrices gives (again write out)
 1="In A 1B
Okn Ik#"A 1Onk
Okn(D CA 1B) 1#"In Onk
 CA 1Ik#
: (A.12)
Assuming that Dis invertible, we could also perform a block UL (as opposed to LU)
decomposition:
="A BD 1C B
Okn D#"In Onk
D 1C I k#
; (A.13)Appendix A. Linear Algebra and Functional Analysis 371
which, after a similar calculation as the one above, yields
 1="In Onk
 D 1C I k#"(A BD 1C) 1Onk
Okn D 1#"In BD 1
Okn Ik#
: (A.14)
The upper-left block of  1from (A.14) must be the same as the upper-left block of  1
from (A.12), leading to the Woodbury identity Woodbury
identity:
(A BD 1C) 1=A 1+A 1B(D CA 1B) 1CA 1: (A.15)
From (A.11) and the fact that the determinant of a product is the product of the determ-
inants, we see that det( )=det(A) det( D CA 1B). Similarly, from (A.13) we have
det()=det(A BD 1C) det( D), leading to the identity
det(A BD 1C) det( D)=det(A) det( D CA 1B): (A.16)
The following special cases of (A.16) and (A.15) are of particular importance.
Theorem A.10: Sherman‚ÄìMorrison Formula
Suppose that A2Rnnis invertible and x;y2Rn. Then,
det(A+xy>)=det(A)(1+y>A 1x):
If in addition y>A 1x, 1, then the Sherman‚ÄìMorrison formula Sherman ‚Äì
Morrison
formulaholds:
(A+xy>) 1=A 1 A 1xy>A 1
1+y>A 1x:
Proof: Take B=x,C= y>, and D=1 in (A.16) and (A.15). 
One important application of the Sherman‚ÄìMorrison formula is in the e cient solution
of the linear system Ax=b, where Ais an nnmatrix of the form:
A=A0+pX
j=1aja>
j
for some column vectors a1;:::; ap2Rnandnndiagonal (or otherwise easily invertible)
matrix A0. Such linear systems arise, for example, in the context of ridge regression and +217
optimization. +414
To see how the Sherman‚ÄìMorrison formula can be exploited, deÔ¨Åne the matrices
A0;:::; Apvia the recursion:
Ak=Ak 1+aka>
k;k=1;:::; p:
Application of Theorem A.10 for k=1;:::; pyields the identities:3
A 1
k=A 1
k 1 A 1
k 1aka>
kA 1
k 1
1+a>
kA 1
k 1ak
jAkj=jAk 1j
1+a>
kA 1
k 1ak
:
3HerejAjis a shorthand notation for det( A).372 A.6. Matrix Decompositions
Therefore, by evolving the recursive relationships up until k=p, we obtain:
A 1
p=A 1
0 pX
j=1A 1
j 1aja>
jA 1
j 1
1+a>
jA 1
j 1aj
jApj=jA0jpY
j=1
1+a>
jA 1
j 1aj
:
These expressions will allow us to easily compute A 1=A 1
pandjAj=jApjprovided the
following quantities are available:
ck;j:=A 1
k 1aj;k=1;:::; p 1;j=k+1;:::; p:
Since, by Theorem A.10, we can write:
A 1
k 1aj=A 1
k 2aj A 1
k 2ak 1a>
k 1A 1
k 2
1+a>
k 1A 1
k 2ak 1aj;
the quantitiesfck;jgcan be computed from the recursion:
c1;j=A 1
0aj;j=1;:::; p
ck;j=ck 1;j a>
k 1ck 1;j
1+a>
k 1ck 1;k 1ck 1;k 1;k=2;:::; p;j=k;:::; p:(A.17)
Observe that this recursive computation takes O(p2n) time and that once fck;jgare available,
we can express A 1andjAjas:
A 1=A 1
0 pX
j=1cj;jc>
j;j
1+a>
jcj;j
jAj=jA0jpY
j=1
1+a>
jcj;j
:
In summary, we have proved the following.
Theorem A.11: Sherman‚ÄìMorrison Recursion
The inverse and determinant of the nnmatrix A=A0+Pp
k=1aka>
kare given
respectively by:
A 1=A 1
0 CD 1C>
det(A)=det(A0) det( D);
where C2RnpandD2Rppare the matrices
C:=h
c1;1;:::; cp;pi
;D:=diag
1+a>
1c1;1;;1+a>
pcp;p
;
and all thefcj;kgare computed from the recursion (A.17) in O(p2n) time.Appendix A. Linear Algebra and Functional Analysis 373
As a consequence of Theorem A.11, the solution to the linear system Ax=bcan be
computed inO(p2n) time via:
x=A 1
0b CD 1[C>b]:
Ifn>p, the Sherman‚ÄìMorrison recursion can frequently be much faster than the O(n3)
direct solution via the LU decomposition method in Section A.6.1. +368
In summary, the following algorithm computes the matrices CandDin Theorem A.11
via the recursion (A.17).
Algorithm A.6.1: Sherman‚ÄìMorrison Recursion
input: Easily invertible matrix A0and column vectors a1;:::; ap.
output: Matrices CandDsuch that CD 1C>=A 1
0 
A0+P
jaja>
j 1.
1ck A 1
0akfork=1;:::; p(assuming A0is diagonal or easily invertible matrix)
2fork=1;:::; p 1do
3 dk 1+a>
kck
4 forj=k+1;:::; pdo
5 cj cj a>
kcj
dkck
6dp 1+a>
pcp
7C [c1;:::; cp]
8D diag( d1;:::; dp)
9return C andD
Finally, note that if A0is a diagonal matrix and we only store the diagonal elements of
DandA0(as opposed to storing the full matrices DandA0), then the storage or memory
requirements of Algorithm A.6.1 are only O(p n).
A.6.3 Cholesky Decomposition
IfAis a real-valued positive deÔ¨Ånite matrix (and therefore symmetric), e.g., a covariance
matrix, then an LU decomposition can be achieved with matrices LandU=L>.
Theorem A.12: Cholesky Decomposition
A real-valued positive deÔ¨Ånite matrix A=[ai j]2Rnncan be decomposed as
A=LL>;
where the real nnlower triangular matrix L=[lk j] satisÔ¨Åes the recursive formula
lk j=ak j Pj 1
i=1ljilkiq
aj j Pj 1
i=1l2
ji;where0X
i=1ljilki:=0 (A.18)
fork=1;:::; nandj=1;:::; k.374 A.6. Matrix Decompositions
Proof: The proof is by inductive construction. For k=1;:::; n, let Akbe the left-upper
kksubmatrix of A=An. With e1:=[1;0;:::; 0]>, we have A1=a11=e>
1Ae1>0 by the
positive-deÔ¨Åniteness of A. It follows that l11=pa11. Suppose that Ak 1has a Cholesky fac-
torization Lk 1L>
k 1with Lk 1having strictly positive diagonal elements, we can construct
a Cholesky factorization of Akas follows. First write
Ak="Lk 1L>
k 1ak 1
a>
k 1akk#
and propose Lkto be of the form
Lk="Lk 10
l>
k 1lkk#
for some vector lk 12Rk 1and scalar lkk, for which it must hold that"Lk 1L>
k 1ak 1
a>
k 1akk#
="Lk 10
l>
k 1lkk#"L>
k 1lk 1
0>lkk#
:
To establish that such an lk 1andlkkexist, we must verify that the set of equations
Lk 1lk 1=ak 1
l>
k 1lk 1+l2
kk=akk(A.19)
has a solution. The system Lk 1lk 1=ak 1has a unique solution, because (by assump-
tion) Lk 1is lower diagonal with strictly positive entries down the main diagonal and we
can solve for lk 1using forward substitution: lk 1=L 1
k 1ak 1. We can solve the second
equation as lkk=p
akk klk 1k2, provided that the term within the square root is positive.
We demonstrate this using the fact that Ais a positive deÔ¨Ånite matrix. In particular, for
x2Rnof the form [ x>
1;x2;0>]>, where x1is a non-zero ( k 1)-dimensional vector and x2
a non-zero number, we have
0<x>Ax=x>
1;x2"Lk 1L>
k 1ak 1
a>
k 1akk#"x1
x2#
=kL>
k 1x1k2+2x>
1ak 1x2+akkx2
2:
Now take x1= x2L >
k 1lk 1to obtain 0 <x>Ax=x2
2(akk klk 1k2). Therefore, (A.19)
can be uniquely solved. As we have already solved it for k=1, we can solve it for any
k=1;:::; n, leading to the recursive formula (A.18) and Algorithm A.6.2 below. 
An implementation of Cholesky‚Äôs decomposition that uses the notation in the proof of
Theorem A.6.3 is the following algorithm, whose running cost is O(n3).
Algorithm A.6.2: Cholesky Decomposition
input: Positive-deÔ¨Ånite nnmatrix Anwith entriesfai jg.
output: Lower triangular Lnsuch that LnL>
n=An.
1L1 pa11
2fork=2;:::; ndo
3 ak 1 [a1k;:::; ak 1;k]>
4 lk 1 L 1
k 1ak 1(computed inO(k2) time via forward substitution)
5 lkk p
akk l>
k 1lk 1
6 Lk "Lk 10
l>
k 1lkk#
7return L nAppendix A. Linear Algebra and Functional Analysis 375
A.6.4 QR Decomposition and the Gram‚ÄìSchmidt Procedure
LetAbe an npmatrix, where p6n. Then, there exists a matrix Q2Rnpsatisfying
Q>Q=Ip, and an upper triangular matrix R2Rpp, such that
A=QR:
This is the QR decomposition for real-valued matrices. When Ahas full column rank, such
a decomposition can be obtained via the Gram‚ÄìSchmidt procedure, which constructs anGram‚ÄìSchmidtorthonormal basis fu1;:::; upgof the column space of Aspanned byfa1;:::; apg, in the
following way (see also Figure A.5):
1. Take u1=a1=ka1k.
2. Let p1be the projection of a2onto Spanfu1g. That is, p1=hu1;a2iu1. Now take
u2=(a2 p1)=ka2 p1k. This vector is perpendicular to u1and has unit length.
3. Let p2be the projection of a3onto Spanfu1;u2g. That is, p2=hu1;a3iu1+hu2;a3iu2.
Now take u3=(a3 p2)=ka3 p2k. This vector is perpendicular to both u1andu2
and has unit length.
4. Continue this process to obtain u4;:::; up.
0 1 2-10123
p1a1
a2a2!p1u1
u2
Figure A.5: Illustration of the Gram‚ÄìSchmidt procedure.
At the end of the procedure, a set fu1;:::; upgofporthonormal vectors are obtained.
Consequently, as a result of Theorem A.3,
aj=jX
i=1haj;uii| {z }
ri jui;j=1;:::; p;376 A.6. Matrix Decompositions
for some numbers ri j;j=1;:::; i;i=1;:::; p. Denoting the corresponding upper triangu-
lar matrix [ ri j] byR, we have in matrix notation:
QR=[u1;:::; up]26666666666666664r11r12r13::: r1p
0r22r23::: r2p
:::0:::::::::
0 0 0 ::: rpp37777777777777775=[a1;:::; ap]=A;
which yields a QR decomposition. The QR decomposition can be used to e ciently solve
least-squares problems; this will be shown shortly. It can also be used to calculate the
determinant of the matrix A, whenever Ais square. Namely, det( A)=det(Q) det( R)=
det(R); and since Ris triangular, its determinant is the product of its diagonal elements.
There exist various improvements of the Gram‚ÄìSchmidt process (for example, the House-
holder transformation [52]) that not only improve the numerical stability of the QR de-
composition, but also can be applied even when Ais not full rank.
An important application of the QR decomposition is found in solving the least-squares
problem inO(p2n) time:
min
2RpkX yk2
for some X2Rnp(model) matrix. Using the deÔ¨Åning properties of the pseudo-inverse in
DeÔ¨Ånition A.2, one can show that kXX+y yk26kX yk2for any. In other words, b:= +360
X+yminimizeskX yk. If we have the QR decomposition X=QR, then a numerically
stable way to calculate bwith anO(p2n) cost is via
b=(QR)+y=R+Q+y=R+Q>y:
IfXhas full column rank, then R+=R 1.
Note that while the QR decomposition is the method of choice for solving the ordinary
least-squares regression problem, the Sherman‚ÄìMorrison recursion is the method of choice +372
for solving the regularized least-squares (or ridge) regression problem. +217
A.6.5 Singular Value Decomposition
One of the most useful matrix decompositions is the singular value decomposition singular value
decomposition(SVD).
Theorem A.13: Singular Value Decomposition
Any (complex) matrix mnmatrix Aadmits a unique decomposition
A=UV;
where UandVare unitary matrices of dimension mandn, respectively, and is a
realmndiagonal matrix. If Ais real, then UandVare both orthogonal matrices.
Proof: Without loss of generality we can assume that m>n(otherwise consider the trans-
pose of A). Then AAis a positive semideÔ¨Ånite Hermitian matrix, because hAAv;vi=
vAAv=kAvk2>0 for all v. Hence, AAhas non-negative real eigenvalues, 1>2>Appendix A. Linear Algebra and Functional Analysis 377
>n>0. By Theorem A.7 the matrix V=[v1;:::; vn] of right-eigenvectors is a unit-
ary matrix. DeÔ¨Åne the i-thsingular value singular value asi=pi,i=1;:::; nand suppose 1;:::; r
are all greater than 0, and r+1;:::; n=0. In particular, Avi=0fori=r+1;:::; n. Let
ui=Avi=i,i=1;:::; r. Then, for i;j6r,
hui;uji=u
jui=v
jAAvi
ij=i1fi=jg
ij=1fi=jg:
We can extend u1;:::; urto an orthonormal basis fu1;:::; umgofCm(e.g., using the Gram‚Äì
Schmidt procedure). Let U=[u1;:::; un] be the corresponding unitary matrix. DeÔ¨Åning 
to be the mndiagonal matrix with diagonal ( 1;:::; r;0;:::; 0), we have,
U=[Av1;:::; Avr;0;:::; 0]=A V;
and hence A=UV. 
Note that
AA=UVVU=U>Uand AA=VUUV=V>V:
So,Uis a unitary matrix whose columns are eigenvectors of AAandVis a unitary matrix
whose columns are eigenvectors of AA.
The SVD makes it possible to write the matrix Aas a sum of rank-1 matrices, weighted
by the singular values fig:
A=h
u1;u2;:::; umi26666666666666666666666410::: ::: 0
0:::0::: 0
0:::  r::: 0
0::: ::: 0::: 0
0::: ::: ::::::037777777777777777777777526666666666666664v
1
v
2:::
v
n37777777777777775=rX
i=1iuiv
i; (A.20)
which is called the dyade orspectral representation spectral
representationofA.
For real-valued matrices, the SVD has a nice geometric interpretation, illustrated in
Figure A.6. The linear mapping deÔ¨Åned by matrix Acan be thought of as a succession of
three linear operations: (1) an orthogonal transformation (i.e., a rotation with a possible
Ô¨Çipping of some axes), corresponding to matrix V>, followed by (2) a simple scaling of
the unit vectors, corresponding to , followed by (3) another orthogonal transformation,
corresponding to U.
-4 -2 0 2 4-202
-4 -2 0 2 4-202
-4 -2 0 2 4-202
-4 -2 0 2 4-202
Figure A.6: The Ô¨Ågure shows how the unit circle and unit vectors (Ô¨Årst panel) are Ô¨Årst
rotated (second panel), then scaled (third panel), and Ô¨Ånally rotated and Ô¨Çipped.378 A.6. Matrix Decompositions
Example A.8 (Ellipses) We continue Example A.5. Using the svdmethod of the mod-
ulenumpy.linalg , we obtain the following SVD matrices for matrix A:
U=" 0:5430 0:8398
0:8398 0:5430#
;="2:4221 0
0 0:6193#
;and V=" 0:3975 0:9176
 0:9176 0:3975#
:
Figure A.4 shows the columns of the matrix Uas the two principal axes of the ellipse that +367
is obtained by applying matrix Ato the points of the unit circle.
A practical method to compute the pseudo-inverse of a real-valued matrix Ais via the
singular value decomposition A=UV>, where is the diagonal matrix collecting all the
positive singular values, say 1;:::; r, as in Theorem A.13. In this case, A+=V+U>,
where +is the nmdiagonal (pseudo-inverse) matrix:
+=266666666666666666666664 1
10::: ::: 0
0:::0::: 0
0:::  1
r::: 0
0::: ::: 0::: 0
0::: ::: ::::::0377777777777777777777775:
We conclude with a typical application of the pseudo-inverse for a least-squares optim-
ization problem from data science.
Example A.9 (Rank-DeÔ¨Åcient Least Squares) Given is an npdata matrix
X=26666666666666664x11x12 x1p
x21x22 x2p
::::::::::::
xn1xn2 xnp37777777777777775:
It is assumed that the matrix is of full row rank (all rows of Xare linearly independent) and
that the number of rows is less than the number of columns: n<p. Under this setting, any
solution to the equation X=yprovides a perfect Ô¨Åt to the data and minimizes (to 0) the
least-squares problem
b=argmin
2RpkX yk2: (A.21)
In particular, if minimizeskX yk2then so does +ufor all uin the null space
NX:=fu:Xu=0g, which has dimension p n. To cope with the non-uniqueness of
solutions, a possible approach is to solve instead the following optimization problem:
minimize
2Rp>
subject to X y=0:
That is, we are interested in a solution with the smallest squared norm (or, equival-
ently, the smallest norm). The solution can be obtained via Lagrange‚Äôs method (see Sec-
tion B.2.2). SpeciÔ¨Åcally, set L(;)=> >(X y), and solve +406
rL(;)=2 X>=0; (A.22)Appendix A. Linear Algebra and Functional Analysis 379
and
rL(;)=X y=0: (A.23)
From (A.22) we get =X>=2. By substituting it in (A.23), we arrive at =2(XX>) 1y,
and henceis given by
=X>
2=X>2(XX>) 1y
2=X>(XX>) 1y=X+y:
An example Python code is given below.
svdexample.py
from numpy import diag , zeros ,vstack
from numpy.random import rand , seed
from numpy.linalg import svd, pinv
seed(12345)
n = 5
p = 8
X = rand(n,p)
y = rand(n,1)
U,S,VT = svd(X)
SI = diag(1/S)
# compute pseudo inverse
pseudo_inv = VT.T @ vstack((SI, zeros((p-n,n)))) @ U.T
b = pseudo_inv @ y
#b = pinv(X) @ y #remove comment for the built -in pseudo inverse
print (X @ b - y)
[[5.55111512e-16]
[1.11022302e-16]
[5.55111512e-16]
[8.60422844e-16]
[2.22044605e-16]]
A.6.6 Solving Structured Matrix Equations
For a general matrix A2Cnn, performing matrix‚Äìvector multiplications takes O(n2) op-
erations; and solving linear systems Ax=b, and carrying out LU decompositions takes
O(n3) operations. However, when Aissparse (i.e., has relatively few non-zero elements)
or has a special structure, the computational complexity for these operations can often be
reduced. Matrices Athat are ‚Äústructured‚Äù in this way often satisfy a Sylvester equation Sylvester
equation, of
the form
M1A AM
2=G1G
2; (A.24)
where Mi2Cnn;i=1;2 are sparse matrices and Gi2Cnr,i=1;2 are matrices of rank
rn. The elements of Amust be easy to recover from these matrices, e.g., with O(1) op-
erations. A typical example is a (square) Toeplitz matrix Toeplitz matrix , which has the following structure:380 A.6. Matrix Decompositions
A=266666666666666666666664a0a 1 a (n 2)a (n 1)
a1 a0a 1 a (n 2)
::: a1a0::::::
an 2:::::: a 1
an 1an 2 a1 a0377777777777777777777775:
A general square Toeplitz matrix Ais completely determined by the 2 n 1 elements along
its Ô¨Årst row and column. If Ais also Hermitian (i.e., A=A), then clearly it is determined
by only nelements. If we deÔ¨Åne the matrices:
M1=2666666666666666666666640 0 0 1
1 0 0 0
:::1 0::::::
0::::::0
0 0 1 0377777777777777777777775and M2=2666666666666666666666640 1 0 0
0 0 1 0
:::0 0::::::
0::::::1
 1 0 0 0377777777777777777777775;
then (A.24) is satisÔ¨Åed with
G1G
2:=26666666666666666666641 0
0a1+a (n 1)
0a2+a (n 2)
::::::
0an 1+a 13777777777777777777775"an 1 a 1an 2 a 2::: a1 a (n 1)2a0
0 0 ::: 0 1#
=266666666666666666666664an 1 a 1an 2 a 2::: a1 a (n 1) 2a0
0 0 ::: 0 a1+a (n 1)
:::::: :::::: a2+a (n 2)
:::::: :::::::::
0 0 ::: 0 an 1+a 1377777777777777777777775;
which has rank r62.
Example A.10 (Discrete Convolution of Vectors) The convolution of two vectors
can be represented as multiplication of one of the vectors by a Toeplitz matrix. Sup-
pose a=[a1;:::; an]>andb=[b1;:::; bn]>are two complex-valued vectors. Then, their
convolution convolution is deÔ¨Åned as the vector abwith i-th element
[ab]i=nX
k=1akbi k+1;i=1;:::; n;
where bj:=0 for j60. It is easy to verify that the convolution can be written as
ab=Ab;Appendix A. Linear Algebra and Functional Analysis 381
where, denoting the d-dimensional column vector of zeros by 0d, we have that
A=26666666666666666666666664a 0
0n 1a:::
0n 2:::0n 2
:::a0n 1
0 a37777777777777777777777775:
Clearly, the matrix Ais a (sparse) Toeplitz matrix.
Acirculant matrix circulant
matrixis a special Toeplitz matrix which is obtained from a vector cby
circularly permuting its indices as follows:
C=266666666666666666666664c0cn 1::: c2c1
c1 c0cn 1 c2
::: c1 c0::::::
cn 2::::::cn 1
cn 1cn 2::: c1c0377777777777777777777775: (A.25)
Note that Cis completely determined by the nelements of its Ô¨Årst column, c.
To illustrate how structured matrices allow for faster matrix computations, consider
solving the nnlinear system:
Anxn=an
forxn=[x1;:::; xn]>, where an=[a1;:::; an]>, and
An:=266666666666666666666666641 a1::: an 2an 1
a1 1::: an 2
:::::::::::::::
an 2::::::a1
an 1an 2 a1 137777777777777777777777775(A.26)
is a real-valued symmetric positive-deÔ¨Ånite Toeplitz matrix (so that it is invertible). Note
that the entries of Anare completely determined by the right-hand side of the linear equa-
tion: vector an. As we shall see shortly in Example A.11, the solution to the more general
linear equation Anxn=bn, where bnis arbitrary, can be e ciently computed using the
solution to this speciÔ¨Åc system Anxn=an, obtained via a special recursive algorithm
(Algorithm A.6.3 below).
For every k=1;:::; nthekkToeplitz matrix AksatisÔ¨Åes
Ak=PkAkPk;
where Pkis a permutation matrix that ‚ÄúÔ¨Çips‚Äù the order of elements ‚Äî rows when pre-
multiplying and columns when post-multiplying. For example,
"1 2 3 4 5
6 7 8 9 10#
P5="5 4 3 2 1
10 9 8 7 6#
;where P5=266666666666666666640 0 0 0 1
0 0 0 1 0
0 0 1 0 0
0 1 0 0 0
1 0 0 0 037777777777777777775:382 A.6. Matrix Decompositions
Clearly, Pk=P>
kandPkPk=Ikhold, so that in fact Pkis an orthogonal matrix.
We can solve the nnlinear system Anxn=aninO(n2) time recursively, as follows.
Assume that we have somehow solved for the upper kkblock Akxk=akand now we
wish to solve for the ( k+1)(k+1) block:
Ak+1xk+1=ak+1()"AkPkak
a>
kPk 1#"z
#
="ak
ak+1#
:
Therefore,
=ak+1 a>
kPkz
Akz=ak Pkak:
Since A 1
kPk=PkA 1
k, the second equation above simpliÔ¨Åes to
z=A 1
kak A 1
kPkak
=xk Pkxk:
Substituting z=xk Pkxkinto=ak+1 a>
kPkzand solving for yields:
=ak+1 a>
kPkxk
1 a>
kxk:
Finally, with the value of computed above, we have
xk+1="xk Pkxk
#
:
This gives the following Levinson‚ÄìDurbin Levinson ‚Äì
Durbinrecursive algorithm for solving Anxn=an.
Algorithm A.6.3: Levinson‚ÄìDurbin Recursion for Solving Anxn=an
input: First row [1 ;a1;:::; an 1]=[1;a>
n 1] of matrix An.
output: Solution xn=A 1
nan.
1x1 a1
2fork=1;:::; n 1do
3k 1 a>
kxk
4 Àòx [xk;k;xk;k 1;:::; xk;1]>
5 (ak+1 a>
kÀòx)=k
6 xk+1 "xk Àòx
#
7return xn
In the algorithm above, we have identiÔ¨Åed xk=[xk;1;xk;2;:::; xk;k]>. The advantage of
the Levinson‚ÄìDurbin algorithm is that its running cost is O(n2), instead of the usual O(n3).
Using thefxk;kgcomputed in Algorithm A.6.3, we construct the following lower tri-
angular matrix recursively, setting L1=1 and
Lk+1="Lk 0k
 (Pkxk)>1#
; k=1;:::; n 1: (A.27)Appendix A. Linear Algebra and Functional Analysis 383
Then, we have the following factorization of An.
Theorem A.14: Diagonalization of Toeplitz Correlation Matrix A n
For a real-valued symmetric positive-deÔ¨Ånite Toeplitz matrix Anof the form (A.26),
we have
LnAnL>
n=Dn;
where Lnis a the lower diagonal matrix (A.27) and Dn:=diag(1;1;:::; n 1) is a
diagonal matrix.
Proof: We give a proof by induction. Obviously, L1A1L>
1=111=1=D1is true. Next,
assume that the factorization LkAkL>
k=Dkholds for a given k. Observe that
Lk+1Ak+1="Lk 0k
 (Pkxk)>1#"AkPkak
a>
kPk 1#
="LkAk; LkPkak
 (Pkxk)>Ak+a>
kPk; (Pkxk)>Pkak+1#
:
It is straightforward to verify that [  (Pkxk)>Ak+a>
kPk; (Pkxk)>Pkak+1]=[0>
k;k],
yielding the recursion
Lk+1Ak+1="LkAkLkPkak
0>
kk#
:
Secondly, observe that
Lk+1Ak+1L>
k+1="LkAkLkPkak
0>
kk#"L>
k Pkxk
0>
k 1#
="LkAkL>
k; LkAkPkxk+LkPkak
0>
k;  k#
:
By noting that AkPkxk=PkPkAkPkxk=PkAkxk=Pkak, we obtain:
Lk+1Ak+1L>
k+1="LkAkL>
k0k
0>
kk#
:
Hence, the result follows by induction. 
Example A.11 (Solving A nxn=bninO(n2)Time) One application of the factoriza-
tion in Theorem A.14 is in the fast solution of a linear system Anxn=bn, where the right-
hand side is an arbitrary vector bn. Since the solution xncan be written as
xn=A 1
nbn=L>
nD 1
nLnbn;
we can compute xninO(n2) time, as follows.
Algorithm A.6.4: Solving Anxn=bnfor a General Right-Hand Side
input: First row [1 ;a>
n 1] of matrix Anand right-hand side bn.
output: Solution xn=A 1
nbn.
1Compute Lnin (A.27) and the numbers 1;:::; n 1via Algorithm A.6.3.
2[x1;:::; xn]> Lnbn(computed inO(n2) time)
3xi xi=i 1fori=2;:::; n(computed inO(n) time)
4[x1;:::; xn] [x1;:::; xn]Ln(computed inO(n2) time)
5return xn [x1;:::; xn]>384 A.7. Functional Analysis
Note that it is possible to avoid the explicit construction of the lower triangular matrix
in (A.27) via the following modiÔ¨Åcation of Algorithm A.6.3, which only stores an extra
vector yat each recursive step of the Levinson‚ÄìDurbin algorithm.
Algorithm A.6.5: Solving Anxn=bnwithO(n) Memory Cost
input: First row [1 ;a>
n 1] of matrix Anand right-hand side bn.
output: Solution xn=A 1
nbn.
1x b1
2y a1
3fork=1;:::; n 1do
4 Àòx [xk;xk 1;:::; x1]
5 Àòy [yk;yk 1;:::; y1]
6 1 a>
ky
7x (bk+1 b>
kÀòx)=
8y (ak+1 a>
kÀòy)=
9 x [x xÀòx;x]>
10 y [y yÀòy;y]>
11return x
A.7 Functional Analysis
Much of the previous theory on Euclidean vector spaces can be generalized to vector spaces
offunctions . Every element of a (real-valued) function spaceHis a function from somefunction spacesetXtoR, and elements can be added and scalar multiplied as if they were vectors. In
other words, if f2H andg2H, thenf+g2H for all;2R. OnHwe can impose
an inner product as a mapping h;ifromHH toRthat satisÔ¨Åes
1.hf1+f2;gi=hf1;gi+hf2;gi;
2.hf;gi=hg;fi;
3.hf;fi>0;
4.hf;fi=0 if and only if f=0 (the zero function).
We focus on real-valued function spaces, although the theory for complex-valued
function spaces is similar (and sometimes easier), under suitable modiÔ¨Åcations (e.g.,
hf;gi=hg;fi).
Similar to the linear algebra setting in Section A.2, we say that two elements fandg
inHareorthogonal to each other with respect to this inner product if hf;gi=0. Given an
inner product, we can measure distances between elements of the function space Husing
thenorm norm
kfk:=p
hf;fi:
For example, the distance between two functions fmandfnis given bykfm fnk. The space
His said to be complete complete if every sequence of functions f1;f2;:::2H for which
kfm fnk! 0 asm;n!1; (A.28)Appendix A. Linear Algebra and Functional Analysis 385
converges to some f2H; that is,kf fnk! 0 asn!1 . A sequence that satisÔ¨Åes (A.28)
is called a Cauchy sequence .Cauchy
sequence A complete inner product space is called a Hilbert space . The most fundamental Hilbert
Hilbert space space of functions is the space L2. An in-depth introduction to L2requires some measure
theory [6]. For our purposes, it su ces to assume that XRdand that onXameasure measure is
deÔ¨Åned which assigns to each suitable4setAa positive number (A)>0 (e.g., its volume).
In many cases of interest is of the form
(A)=Z
Aw(x) dx (A.29)
where w>0 is a positive function on Xwhich is called the density density ofwith respect to
the Lebesgue measure (the natural volume measure on Rd). We write (dx)=w(x) dxto
indicate that has density w. Another important case is where
(A)=X
x2A\Zdw(x); (A.30)
where w>0 is again called the density of , but now with respect to the counting measure
onZd(which counts the points of Zd). Integrals with respect to measures in (A.29) and
(A.30) can now be deÔ¨Åned as
Z
f(x)(dx)=Z
f(x)w(x) dx;
and Z
f(x)(dx)=X
xf(x)w(x);
respectively. We assume for simplicity that has the form (A.29). For measures of the
form (A.30) (so-called discrete measures), replace integrals by sums in what follows.
DeÔ¨Ånition A.4: L2Space
LetXbe a subset of Rdwith measure (dx)=w(x) dx. The Hilbert space L2(X;)
is the linear space of functions from XtoRthat satisfy
Z
Xf(x)2w(x) dx<1; (A.31)
and with inner product
hf;gi=Z
Xf(x)g(x)w(x) dx: (A.32)
LetHbe a Hilbert space. A set of functions ffi;i2Ig is called an orthonormal system orthonormal
systemif
4Not all sets have a measure. Suitable sets are Borel sets, which can be thought of as countable unions
of rectangles.386 A.7. Functional Analysis
1. the norm of every fiis 1; that is,hfi;fii=1 for all i2I,
2. theffigare orthogonal; that is, hfi;fji=0 for i,j.
It follows then that the ffigare linearly independent; that is, the only linear combinationP
jjfj(x) that is equal to fi(x) for all xis the one where i=1 andj=0 for j,i. An
orthonormal system ffigis called an orthonormal basis if there is no other f2H that isorthonormal
basis orthogonal to all the ffi;i2Ig (other than the zero function). Although the general theory
allows for uncountable bases, in practice5the setIis taken to be countable.
Example A.12 (Trigonometric Orthonormal Basis) LetHbe the Hilbert space
L2((0;2);), where(dx)=w(x) dxandwis the constant function w(x)=1, 0<x<2.
Alternatively, take X=Randwthe indicator function on (0 ;2). The trigonometric func-
tions
g0(x)=1p
2;gk(x)=1pcos(kx);hk(x)=1psin(kx);k=1;2;:::
form a countable inÔ¨Ånite-dimensional orthonormal basis of H.
A Hilbert spaceHwith an orthonormal basis ff1;f2;:::gbehaves very similarly to the
familiar Euclidean vector space. In particular, every element (i.e., function) f2H can be
written as a unique linear combination of the basis vectors:
f=X
ihf;fiifi; (A.33)
exactly as in Theorem A.3. The right-hand side of (A.33) is called a (generalized) Fourier
expansion off. Note that such a Fourier expansion does not require a trigonometric basis;Fourier
expansion any orthonormal basis will do.
Example A.13 (Example A.12 (cont.)) Consider the indicator function f(x)=1f0<
x< g. As the trigonometric functions fgkgandfhkgform a basis for L2((0;2);1dx), we
can write
f(x)=a01p
2+1X
k=1ak1pcos(kx)+1X
k=1bk1psin(kx); (A.34)
where a0=R
01=p
2dx=p=2,ak=R
0cos(kx)=pdxandbk=R
0sin(kx)=pdx,k=
1;2;:::. This means that ak=0 for all k,bk=0 for even k, and bk=2=(kp) for odd k.
Consequently,
f(x)=1
2+2
1X
k=1sin(kx)
k: (A.35)
Figure A.7 shows several Fourier approximations obtained by truncating the inÔ¨Ånite sum
in (A.35).
5The function spaces typically encountered in machine learning and data science are usually separable
spaces , which allows for the set Ito be considered countable; see, e.g., [106].Appendix A. Linear Algebra and Functional Analysis 387
0 0.5 1 1.5 2 2.5 3 3.500.51
Figure A.7: Fourier approximations of the unit step function fon the interval (0 ;), trun-
cating the inÔ¨Ånite sum in (A.35) to i=2, 4, and 14 terms, giving the dotted blue, dashed
red, and solid green curves, respectively.
Starting from any countable basis, we can use the Gram‚ÄìSchmidt procedure to obtain +375
an orthonormal basis, as illustrated in the following example.
Example A.14 (Legendre Polynomials) Take the function space L2(R;w(x) dx), where
w(x)=1f 1<x<1g. We wish to construct an orthonormal basis of polynomial functions
g0;g1;g2;:::, starting from the collection of monomials: 0;1;2;:::, wherek:x7!xk. Us-
ing Gram‚ÄìSchmidt, the Ô¨Årst normalized zero-degree polynomial is g0=0=k0k=p1=2.
To Ô¨Ånd g1(a polynomial of degree 1), project 1(the identity function) onto the space
spanned by g0. The resulting projection is p1:=hg0;1ig0, written out as
p1(x)= Z1
 1x g0(x) dx!
g0(x)=1
2Z1
 1xdx=0:
Hence, g1=(1 p1)=k1 p1kis a linear function; that is, of the form g1(x)=ax. The
constant ais found by normalization:
1=kg1k2=Z1
 1g2
1(x) dx=a2Z1
 1x2dx=a22
3;
so that g1(x)=p3=2x. Continuing the Gram‚ÄìSchmidt procedure, we Ô¨Ånd g2(x)=p5=8(3x2 1);g3(x)=p7=8(5x3 3x) and, in general,
gk(x)=p
2k+1
2k+1
2k!dk
dxk(x2 1)k;k=0;1;2;::::
These are the (normalized) Legendre polynomials . The graphs of g0;g1;g2;andg3are givenLegendre
polynomials in Figure A.8.388 A.7. Functional Analysis
-1 -0.5 0 0.5 1
-11
Figure A.8: The Ô¨Årst 4 normalized Legendre polynomials.
As the Legendre polynomials form an orthonormal basis of L2(R;1f 1<x<1gdx),
they can be used to approximate arbitrary functions in this space. For example, Figure A.9
shows an approximation using the Ô¨Årst 51 Legendre polynomials ( k=0;1;:::; 50) of the
Fourier expansion of the indicator function on the interval (  1=2;1=2). These Legendre
polynomials form the basis of a 51-dimensional linear subspace onto which the indicator
function is orthogonally projected.
-1 -0.5 0 0.5 100.20.40.60.81
Figure A.9: Approximation of the indicator function on the interval (  1=2;1=2), using the
Legendre polynomials g0;g1;:::; g50.
The Legendre polynomials were produced in the following way: We started with an
unnormalized probability density on R‚Äî in this case the probability density of the uniform +424
distribution on ( 1;1). We then constructed a sequence of polynomials by applying the
Gram‚ÄìSchmidt procedure to the monomials 1 ;x;x2;:::.
By using exactly the same procedure, but with a di erent probability density, we can
produce other such orthogonal polynomials . For example, the density of the standard expo-orthogonal
polynomials nential6distribution, w(x)=e x;x>0, gives the Laguerre polynomials , which are deÔ¨Åned
Laguerre
polynomials
6This can be further generalized to the density of a gamma distribution.Appendix A. Linear Algebra and Functional Analysis 389
by the recurrence
(n+1)gn+1(x)=(2n+1 x)gn(x) ngn 1(x);n=1;2;:::;
with g0(x)=1 and g1(x)=1 x, for x>0. The Hermite polynomials Hermite
polynomialsare obtained when
using instead the density of the standard normal distribution: w(x)=e x2=2=p
2;x2R.
These polynomials satisfy the recursion
gn+1(x)=xgn(x) dgn(x)
dx;n=0;1;:::;
with g0(x)=1,x2R. Note that the Hermite polynomials as deÔ¨Åned above have not been
normalized to have norm 1. To normalize, use the fact that kgnk2=n!.
We conclude with a number of key results in functional analysis. The Ô¨Årst one is the
celebrated Cauchy‚ÄìSchwarz Cauchy ‚Äì
Schwarzinequality.
Theorem A.15: Cauchy‚ÄìSchwarz
LetHbe a Hilbert space. For every f;g2H it holds that
jhf;gij6kfkkgk:
Proof: The inequality is trivially true for g=0 (zero function). For g,0, we can write
f=g+h, where h?gand=hf;gi=kgk2. Consequently,kfk2=jj2kgk2+khk2>
jj2kgk2. The result follows after rearranging this last inequality. 
LetVandWbe two linear vector spaces (for example, Hilbert spaces) on which norms
kkVandkkWare deÔ¨Åned. Suppose A:V!W is a mapping from VtoW. When
W=V, such a mapping is often called an operator ; whenW=Rit is called a functional .operator
functional Mapping Ais said to be linear ifA(f+g)=A(f)+A(g). In this case we write A f
linear mapping instead of A(f). If there exists <1such that
kA fkW6kfkV;f2V; (A.36)
then Ais said to be a bounded mapping . The smallest for which (A.36) holds is called thebounded
mapping norm ofA; denoted bykAk. A (not necessarily linear) mapping A:V!W is said to be
norm continuous atfif for any sequence f1;f2;:::converging to fthe sequence A(f1);A(f2);:::
continuous
mappingconverges to A(f). That is, if
8">0;9>0 :8g2V;kf gkV<)kA(f) A(g)kW<": (A.37)
If the above property holds for every f2V, then the mapping Aitself is called continuous .
Theorem A.16: Continuity and Boundedness for Linear Mappings
For a linear mapping, continuity and boundedness are equivalent.
Proof: LetAbe linear and bounded. We may assume that Ais non-zero (otherwise the
statement holds trivially), and that therefore 0 <kAk<1. Taking<"=kAkin (A.37) now
ensures thatkA f AgkW6kAkkf gkV<kAk<" . This shows that Ais continuous.390 A.8. Fourier Transforms
Conversely, suppose Ais continuous. In particular, it is continuous at f=0 (the zero-
element ofV). Thus, take f=0 and let"andbe as in (A.37). For any g,0, let h=
=(2kgkV)g. AskhkV==2<, it follows from (A.37) that
kAhkW=
2kgkVkAgkW<":
Rearranging the last inequality gives kAgkW<2"=kgkV, showing that Ais bounded.
Theorem A.17: Riesz Representation Theorem
Any bounded linear functional on a Hilbert space Hcan be represented as (h)=
hh;gi, for some g2H (depending on ).
Proof: LetPbe the projection of Honto the nullspace Nof; that is,N=fg2H :
(g)=0g. Ifis not the 0-functional, then there exists a g0,0 with(g0),0. Let
g1=g0 Pg0. Then g1?N and(g1)=(g0). Take g2=g1=(g1). For any h2H ,
f:=h (h)g2lies inN. As g2?N it holds thathf;g2i=0, which is equivalent to
hh;g2i=(h)kg2k2. By deÔ¨Åning g=g2=kg2k2we have found our representation. 
A.8 Fourier Transforms
We will now brieÔ¨Çy introduce the Fourier transform. Before doing so, we will extend the
concept of L2space of real-valued functions as follows. +385
DeÔ¨Ånition A.5: LpSpace
LetXbe a subset of Rdwith measure (dx)=w(x) dxandp2[1;1). Then Lp(X;)
is the linear space of functions from XtoCthat satisfy
Z
Xjf(x)jpw(x) dx<1: (A.38)
When p=2,L2(X;) is in fact a Hilbert space equipped with inner product
hf;gi=Z
Xf(x)g(x)w(x) dx: (A.39)
We are now in a position to deÔ¨Åne the Fourier transform (with respect to the Lebesgue
measure). Note that in the following DeÔ¨Ånitions A.6 and A.7 we have chosen a particular
convention. Equivalent (but not identical) deÔ¨Ånitions exist that include scaling constants
(2)dor (2) dand where 2tis replaced with 2 t,t, or t.Appendix A. Linear Algebra and Functional Analysis 391
DeÔ¨Ånition A.6: (Multivariate) Fourier Transform
TheFourier transform Fourier
transformF[f] of a (real- or complex-valued) function f2L1(Rd) is
the function efdeÔ¨Åned as
ef(t) :=Z
Rde i 2t>xf(x) dx;t2Rd:
The Fourier transform efis continuous, uniformly bounded (since f2L1(Rd) im-
plies thatjef(t)j6R
Rdjf(x)jdx<1), and satisÔ¨Åes lim ktk!1ef(t)=0 (a result known as
theRiemann‚ÄìLebesgue lemma ). However,jefjdoes not necessarily have a Ô¨Ånite integ-
ral. A simple example in R1is the Fourier transform of f(x)=1f 1=2<x<1=2g. Then
ef(t)=sin(t)=(t)=sinc(t), which is not absolutely integrable.
DeÔ¨Ånition A.7: (Multivariate) Inverse Fourier Transform
The inverse Fourier transform inverse Fourier
transformF 1[ef] of a (real- or complex-valued) function
ef2L1(Rd) is the function ÀòfdeÔ¨Åned as
Àòf(x) :=Z
Rdei 2t>xef(t) dt;x2Rd:
As one would hope, it holds that if fandF[f] are both in L1(Rd), then f=F 1[F[f]]
almost everywhere.
The Fourier transform enjoys many interesting and useful properties, some of which
we list below.
1.Linearity : For f;g2L1(Rd) and constants a;b2R,
F[a f+bg]=aF[f]+bF[g]:
2.Space Shifting and Scaling : Let A2Rddbe an invertible matrix and b2Rda con-
stant vector. Let f2L1(Rd) and deÔ¨Åne h(x) :=f(Ax+b). Then
F[h](t)=ei 2(A >t)>bef(A >t)=jdet(A)j;
where A >:=(A>) 1=(A 1)>.
3.Frequency Shifting and Scaling : Let A2Rddbe an invertible matrix and b2Rda
constant vector. Let f2L1(Rd) and deÔ¨Åne
h(x) :=e i 2b>A >xf(A >x)=jdet(A)j:
ThenF[h](t)=ef(At+b).
4.Dierentiation : Let f2L1(Rd)\C1(Rd) and let fk:=@f=@xkbe the partial derivat-
ive of fwith respect to xk. Iffk2L1(Rd) for k=1;:::; d, then
F[fk](t)=(i 2tk)ef(t):392 A.8. Fourier Transforms
5.Convolution : Let f;g2L1(Rd) be real or complex valued functions. Their convolu-
tion, fg, is deÔ¨Åned as
(fg)(x)=Z
Rdf(y)g(x y) dy;
and is also in L1(Rd). Moreover, the Fourier transform satisÔ¨Åes
F[fg]=F[f]F[g]:
6.Duality : Let fandF[f] both be in L1(Rd). ThenF[F[f]](t)=f( t).
7.Product Formula : Let f;g2L1(Rd) and denote by ef;egtheir respective Fourier trans-
forms. Then ef g;feg2L1(Rd), and
Z
Rdef(z)g(z) dz=Z
Rdf(z)eg(z) dz:
There are many additional properties which hold if f2L1(Rd)\L2(Rd). In particular,
iff;g2L1(Rd)\L2(Rd), then ef;eg2L2(Rd) andhef;egi=hf;gi, a result often known as
Parseval‚Äôs formula . Putting g=fgives the result often referred to as Plancherel‚Äôs theorem .
The Fourier transform can be extended in several ways, in the Ô¨Årst instance to functions
inL2(Rd) by continuity. A substantial extension of the theory is realized by replacing integ-
ration with respect to the Lebesgue measure (i.e.,R
Rddx) with integration with respect
to a (Ô¨Ånite Borel) measure (i.e.,R
Rd(dx)). Moreover, there is a close connection
between the Fourier transform and characteristic functions arising in probability theory.
Indeed, if Xis a random vector with pdf f, then its characteristic function  satisÔ¨Åes +441
 (t) :=Eeit>X=F[f]( t=(2)):
A.8.1 Discrete Fourier Transform
Here, we introduce the (univariate) discrete Fourier transform, which can be viewed as a
special case of the Fourier transform introduced in DeÔ¨Ånition A.6, where d=1, integration
is with respect to the counting measure, and f(x)=0 for x<0 and x>(n 1).
DeÔ¨Ånition A.8: Discrete Fourier Transform
The discrete Fourier transform discrete
Fourier
transform(DFT) of a vector x=[x0;:::; xn 1]>2Cnis the
vectorex=[ex0;:::;exn 1]>whose elements are given by
ext=n 1X
s=0!stxs;t=0;:::; n 1; (A.40)
where!=exp( i 2=n).
In other words, exis obtained from xvia the linear transformation
ex=Fx;Appendix A. Linear Algebra and Functional Analysis 393
where
F=26666666666666666666641 1 1 ::: 1
1! !2::: !n 1
1!2!4::: !2(n 1)
:::::::::::::::
1!n 1!2(n 1)::: !(n 1)23777777777777777777775:
The matrix Fis a so-called Vandermonde matrix , and is clearly symmetric (i.e., F=F>).
Moreover, F=pnis in fact a unitary matrix and hence its inverse is simply its complex
conjugate F=pn. Thus, F 1=F=nand we have that the inverse discrete Fourier transform inverse discrete
Fourier
transform(IDFT) is given by
xt=1
nn 1X
s=0! stexs;t=0;:::; n 1; (A.41)
or in terms of matrices and vectors,
x=Fex=n:
Observe that the IDFT of a vector yis related to the DFT of its complex conjugate y, since
Fy=n=Fy=n:
Consequently, an IDFT can be computed via a DFT.
There is a close connection between circulant matrices Cand the DFT. To make this
connection concrete, let Cbe the circulant matrix corresponding to the vector c2Cnand
denote by ftthet-th column of the discrete Fourier matrix F,t=0;1;:::; n 1. Then, the
s-th element of Cftis
n 1X
k=0c(s k) mod n!tk=n 1X
y=0cy!t(s y)=!ts|{z}
s-th element of ftn 1X
y=0cy! ty
|      {z      }
s:
Hence, the eigenvalues of Care
t=c>ft;t=0;1;:::; n 1;
with corresponding eigenvectors ft. Collecting the eigenvalues into the vector =
[0;:::; n 1]>=Fc, we therefore have the eigen-decomposition
C=Fdiag()F=n:
Consequently, one can compute the circular convolution of a vector a=[a1;:::; an]>
andc=[c0;:::; cn 1]>by a series of DFTs as follows. Construct the circulant matrix C
corresponding to c. Then, the circular convolution of aandcis given by y=Ca. Proceed
in four steps:
1. Compute z=Fa=n.
2. Compute =Fc.394 A.8. Fourier Transforms
3. Compute p=z=[z10;:::; znn 1]>.
4. Compute y=Fp.
Steps 1 and 2 are (up to constants) in the form of an IDFT, and step 4 is in the form of a
DFT. These are computable via the FFT (Section A.8.2) in O(nlnn) time. Step 3 is a dot +394
product computable in O(n) time. Thus, the circular convolution can be computed with the
aid of the FFT inO(nlnn) time.
One can also e ciently compute the product of an nnToeplitz matrix Tand an n1
vector ainO(nlnn) time by embedding Tinto a circulant matrix Cof size 2 n2n. Namely,
deÔ¨Åne
C="T B
B T#
;
where
B=2666666666666666666666640 tn 1 t2 t1
t (n 1) 0 tn 1 t2
::: t (n 1) 0::::::
t 2::::::tn 1
t 1 t 2 t (n 1) 0377777777777777777777775:
Then a product of the form y=Tacan be computed in O(nlnn) time, since we may write
C"a
0#
="T B
B T#"a
0#
="Ta
Ba#
:
The left-hand side is a product of a 2 n2ncirculant matrix with vector of length 2 n, and
so can be computed in O(nlnn) time via the FFT, as previously discussed.
Conceptually, one can also solve equations of the form Cx=bfor a given vector b2Cn
and circulant matrix C(corresponding to c2Cn, assuming all its eigenvalues are non-zero)
via the following four steps:
1. Compute z=Fb=n.
2. Compute =Fc.
3. Compute p=z==[z1=0;:::; zn=n 1]>.
4. Compute x=Fp.
Once again, Steps 1 and 2 are (up to constants) in the form of an IDFT, and Step 4 is in
the form of a DFT, all of which are computable via the FFT in O(nlnn) time, and Step 3
is computable inO(n) time, meaning the solution xcan be computed using the FFT in
O(nlnn) time.
A.8.2 Fast Fourier Transform
Thefast Fourier transform fast Fourier
transform(FFT) is a numerical algorithm for the fast evaluation of (A.40)
and (A.41). By using a divide-and-conquer strategy, the algorithm reduces the compu-
tational complexity from O(n2) (for the na√Øve evaluation of the linear transformation) to
O(nlnn) [60].Appendix A. Linear Algebra and Functional Analysis 395
The essence of the algorithm lies in the following observation. Suppose n=r1r2. Then
one can express any index tappearing in (A.40) via a pair ( t0;t1), with t=t1r1+t0,
where t02f0;1;:::; r1 1gandt12f0;1;:::; r2 1g. Similarly, one can express any index
sappearing in (A.40) via a pair ( s0;s1), with s=s1r2+s0, where s02f0;1;:::; r2 1gand
s12f0;1;:::; r1 1g.
Identifying extext1;t0andxsxs1;s0, we may re-express (A.40) as
ext1;t0=r2 1X
s0=0!s0tr1 1X
s1=0!s1r2txs1;s0;t0=0;1;:::; r1 1;t1=0;1;:::; r2 1: (A.42)
Observe that !s1r2t=!s1r2t0(because!r1r2=1), so that the inner sum over s1depends only
ons0andt0. DeÔ¨Åne
yt0;s0:=r1 1X
s1=0!s1r2t0xs1;s0;t0=0;1;:::; r1 1;s0=0;1;:::; r2 1:
Computing each yt0;s0requiresO(n r1) operations. In terms of the fyt0;s0g, (A.42) can be
written as
ext1;t0=r2 1X
s0=0!s0tyt0;s0;t1=0;1;:::; r2 1;t0=0;1;:::; r1 1;
requiringO(n r2) operations to compute. Thus, calculating the DFT using this two-step
procedure requires O(n(r1+r2)) operations, rather than O(n2).
Now supposing n=r1r2rm, repeated application the above divide-and-conquer idea
yields an m-step procedure requiring O(n(r1+r2++rm)) operations. In particular, if
rk=rfor all k=1;2;:::; m, we have that n=rmandm=logrn, so that the total number
of operations isO(r n m )O(r nlogr(n)). Typically, the radix r is a small (not necessarily
prime) number, for instance r=2.
Further Reading
A good reference book on matrix computations is Golub and Van Loan [52]. A useful list
of many common vector and matrix calculus identities can be found in [95]. Strang‚Äôs in-
troduction to linear algebra [116] is a classic textbook, and his recent book [117] combines
linear algebra with the foundations of deep learning. Fast reliable algorithms for matrices
with structure can be found in [64]. Kolmogorov and Fomin‚Äôs masterpiece on the theory
of functions and functional analysis [67] still provides one of the best introductions to the
topic. A popular choice for an advanced course in functional analysis is Rudin [106].396APPENDIXB
MULTIVARIATE DIFFERENTIATION AND
OPTIMIZATION
The purpose of this appendix is to review various aspects of multivariate di eren-
tiation and optimization. We assume the reader is familiar with di erentiating a real-
valued function.
B.1 Multivariate Differentiation
For a multivariate function fthat maps a vector x=[x1;:::; xn]>to a real number f(x),
thepartial derivative with respect to xi, denoted@f
@xi, is the derivative taken with respectpartial
derivative toxiwhile all other variables are held constant. We can write all the npartial derivatives
neatly using the ‚Äúscalar /vector‚Äù derivative notation:
scalar /vector:@f
@x:=2666666666664@f
@x1:::
@f
@xn3777777777775: (B.1)
This vector of partial derivatives is known as the gradient offatxand is sometimes writtengradientasrf(x).
Next, suppose that fis a multivalued (vector-valued) function taking values in Rm,
deÔ¨Åned by
x=26666666666666664x1
x2
:::
xn377777777777777757!26666666666666664f1(x)
f2(x)
:::
fm(x)37777777777777775=:f(x):
We can compute each of the partial derivatives @fi=@xjand organize them neatly in a ‚Äúvec-
tor/vector‚Äù derivative notation:
vector /vector:@f
@x:=2666666666666666664@f1
@x1@f2
@x1@fm
@x1@f1
@x2@f2
@x2@fm
@x2:::::::::
@f1
@xn@f2
@xn@fm
@xn3777777777777777775: (B.2)
397398 B.1. Multivariate Differentiation
The transpose of this matrix is known as the matrix of Jacobi matrix of Jacobi offatx(sometimes
called the Fr√©chet derivative offatx); that is,
Jf(x) :="@f
@x#>
=2666666666666666664@f1
@x1@f1
@x2@f1
@xn@f2
@x1@f2
@x2@f2
@xn:::::::::
@fm
@x1@fm
@x2@fm
@xn3777777777777777775: (B.3)
If we deÔ¨Åne g(x) :=rf(x) and take the ‚Äúvector /vector‚Äù derivative of gwith respect to
x, we obtain the matrix of second-order partial derivatives of f:
Hf(x) :=@g
@x=266666666666666666664@2f
@2x1@2f
@x1@x2@2f
@x1@xm
@2f
@x2@x1@2f
@2x2@2f
@x2@xm:::::::::
@2f
@xm@x1@2f
@xm@x2@2f
@2xm377777777777777777775; (B.4)
which is known as the Hessian matrix Hessian matrix offatx, also denoted asr2f(x). If these second-
order partial derivatives are continuous in a region around x, then@f
@xi@xj=@f
@xj@xiand, hence,
the Hessian matrix Hf(x) issymmetric .
Finally, note that we can also deÔ¨Åne a ‚Äúscalar /matrix‚Äù derivative of ywith respect to
X2Rmnwith ( i;j)-th entry xi j:
@y
@X:=2666666666666666664@y
@x11@y
@x12@y
@x1n@y
@x21@y
@x22@y
@x2n:::::::::
@y
@xm1@y
@xm2@y
@xmn3777777777777777775
and a ‚Äúmatrix /scalar‚Äù derivative:
@X
@y:=2666666666666666664@x11
@y@x12
@y@x1n
@y
@x21
@y@x22
@y@x2n
@y:::::::::
@xm1
@y@xm2
@y@xmn
@y3777777777777777775:
Example B.1 (Scalar /Matrix Derivative) Lety=a>Xb, where X2Rmn,a2Rm,
andb2Rn. Since yis a scalar, we can write y=tr(y)=tr(Xba>), using the cyclic property
of the trace (see Theorem A.1). DeÔ¨Åning C:=ba>, we have +357
y=mX
i=1[XC]ii=mX
i=1nX
j=1xi jcji;
so that@y=@xi j=cjior, in matrix form,
@y
@X=C>=ab>:Appendix B. Multivariate Differentiation and Optimization 399
Example B.2 (Scalar /Matrix Derivative via the Woodbury Identity) Lety=tr
X 1A
,
where X;A2Rnn. We now prove that
@y
@X= X >A>X >:
To show this, apply the Woodbury matrix identity to an inÔ¨Ånitesimal perturbation, X+"U,
ofX, and take"#0 to obtain the following: +371
(X+"U) 1 X 1
"= X 1U(I+"X 1U) 1X 1 !  X 1U X 1:
Therefore, as "#0
tr
(X+"U) 1A
 tr
X 1A
" !  tr
X 1U X 1A
= tr
U X 1AX 1
:
Now, suppose that Uis an all zero matrix with a one in the ( i;j)-th position. We can write,
@y
@xi j=lim
"#0tr
(X+"U) 1A
 tr
X 1A
"= tr
UX 1AX 1
= h
X 1AX 1i
ji:
Therefore,@y
@X= 
X 1AX 1>.
The following two examples specify multivariate derivatives for the important special
cases of linear and quadratic functions.
Example B.3 (Gradient of a Linear Function) Letf(x)=Axfor some mnconstant
matrix A. Then, its vector /vector derivative (B.2) is the matrix
@f
@x=A>: (B.5)
To see this, let ai jdenote the ( i;j)-th element of A, so that
f(x)=Ax=26666666664Pn
k=1a1kxk
:::Pn
k=1amkxk37777777775:
To Ô¨Ånd the ( j;i)-th element of@f
@x, we di erentiate the i-th element of fwith respect to xj:
@fi
@xj=@
@xjnX
k=1aikxk=ai j:
In other words, the ( i;j)-th element of@f
@xisaji, the ( i;j)-th element of A>.
Example B.4 (Gradient and Hessian of a Quadratic Function) Letf(x)=x>Axfor
some nnconstant matrix A. Then,
rf(x)=(A+A>)x: (B.6)400 B.1. Multivariate Differentiation
It follows immediately that if Aissymmetric , that is, A=A>, thenr(x>Ax)=2Axand
r2(x>Ax)=2A.
To prove (B.6), Ô¨Årst observe that f(x)=x>Ax=Pn
i=1Pn
j=1ai jxixj, which is a quadratic
form in x, is real-valued, with
@f
@xk=@
@xknX
i=1nX
j=1ai jxixj=nX
j=1ak jxj+nX
i=1aikxi:
The Ô¨Årst term on the right-hand side is equal to the k-th element of Ax, whereas the second
term equals the k-th element of x>A;or equivalently the k-th element of A>x.
B.1.1 Taylor Expansion
The matrix of Jacobi and the Hessian matrix feature prominently in multidimensional
Taylor expansions.
Theorem B.1: Multidimensional Taylor Expansions
LetXbe an open subset of Rnand let a2X. Iff:X!Ris a continuously twice
dierentiable function with Jacobian matrix Jf(x) and Hessian matrix Hf(x), then
for every x2Xwe have the following Ô¨Årst- and second-order Taylor expansions:
f(x)=f(a)+Jf(a) (x a)+O(kx ak2) (B.7)
and
f(x)=f(a)+Jf(a) (x a)+1
2(x a)>Hf(a) (x a)+O(kx ak3) (B.8)
askx ak! 0. By dropping the Oremainder terms, one obtains the corresponding
Taylor approximations.
The result is essentially saying that a smooth enough function behaves locally (in the
neighborhood of a point x) like a linear and quadratic function. Thus, the gradient or Hes-
sian of an approximating linear or quadratic function is a basic building block of many
approximation and optimization algorithms.
Remark B.1 (Version Without Remainder Terms) An alternative version of Taylor‚Äôs
theorem states that there exists an a0that lies on the line segment between xandasuch
that (B.7) and (B.8) hold without remainder terms, with Jf(a) in (B.7) replaced by Jf(a0)
andHf(a) in (B.8) replaced by Hf(a0).
B.1.2 Chain Rule
Consider the functions f:Rk!Rmandg:Rm!Rn. The function x7!g(f(x)) is called
thecomposition composition ofgandf, written as gf, and is a function from RktoRn. Suppose
y=f(x) and z=g(y), as in Figure B.1. Let Jf(x) and Jg(y) be the (Fr√©chet) derivatives
off(atx) and g(aty), respectively. We may think of Jf(x) as the matrix that describesAppendix B. Multivariate Differentiation and Optimization 401
how, in a neighborhood of x, the function fcan be approximated by a linear function:
f(x+h)f(x)+Jf(x)h, and similarly for Jg(y). The well-known chain rule chain rule of calculus
simply states that the derivative of the composition gfis the matrix product of the
derivatives of gandf; that is,
Jgf(x)=Jg(y)Jf(x):
g‚ó¶f
x yz
fg
RnRmRk
Figure B.1: Function composition. The blue arrows symbolize the linear mappings.
In terms of our vector /vector derivative notation, we have
"@z
@x#>
="@z
@y#>"@y
@x#>
or, more simply,
@z
@x=@y
@x@z
@y: (B.9)
In a similar way we can establish a scalar /matrix chain rule. In particular, suppose Xis
annpmatrix, which is mapped to y:=Xfor a Ô¨Åxed p-dimensional vector . In turn, y
is mapped to a scalar z:=g(y) for some function g. Denote the columns of Xbyx1;:::; xp.
Then,
y=X=pX
j=1jxj;
and, therefore, @y=@xj=jIn. It follows by the chain rule (B.9) that
@z
@xi=@y
@xi@z
@y=iIn@z
@y=i@z
@y:
Therefore,
@z
@X=h@z
@x1;:::;@z
@xpi
=h
1@z
@y;:::; p@z
@yi
=@z
@y>: (B.10)
Example B.5 (Derivative of the Log-Determinant) Suppose we are given a positive
deÔ¨Ånite matrix A2Rppand wish to compute the scalar /matrix derivative@lnjAj
@A. The result
is@lnjAj
@A=A 1:
To see this, we can reason as follows. By Theorem A.8, we can write A=Q D Q>, where +366402 B.2. Optimization Theory
Qis an orthogonal matrix and D=diag(1;:::; p) is the diagonal matrix of eigenvalues of
A. The eigenvalues are strictly positive, since Ais positive deÔ¨Ånite. Denoting the columns
ofQby (qi), we have
i=q>
iAqi=tr qiAq>
i;i=1;:::; p: (B.11)
From the properties of determinants, we have y:=lnjAj=lnjQ D Q>j=ln(jQjjDjjQ>j)=
lnjDj=Pp
i=1lni. We can thus write
@lnjAj
@A=pX
i=1@lni
@A=pX
i=1@i
@A@lni
@i=pX
i=1@i
@A1
i;
where the second equation follows from the chain rule applied to the function composition
A7!i7!y. From (B.11) and Example B.1 we have @i=@A=qiq>
i. It follows that
@y
@A=pX
i=1qiq>
i1
i=Q D 1Q>=A 1:
B.2 Optimization Theory
Optimization is concerned with Ô¨Ånding minimal or maximal solutions of a real-valued
objective function objective
functionfin some setX:
min
x2Xf(x) or max
x2Xf(x): (B.12)
Since any maximization problem can easily be converted into a minimization problem via
the equivalence max xf(x) min x f(x), we focus only on minimization problems. We
use the following terminology. A local minimizer local minimizer off(x) is an element x2Xsuch that
f(x)6f(x) for all xin some neighborhood of x. Iff(x)6f(x) for all x2X, then xis
called a global minimizer global
minimizerorglobal solution . The set of global minimizers is denoted by
argmin
x2Xf(x):
The function value f(x) corresponding to a local /global minimizer xis referred to as the
local/global minimum local/global
minimumoff(x).
Optimization problems may be classiÔ¨Åed by the set Xand the objective function f.
IfXis countable, the optimization problem is called discrete orcombinatorial . If instead
Xis a nondenumerable set such as Rnand ftakes values in a nondenumerable set, then
the problem is said to be continuous . Optimization problems that are neither discrete nor
continuous are said to be mixed .
The search setXis often deÔ¨Åned by means of constraints . A standard setting for con-
strained optimization (minimization) is the following:
min
x2Yf(x)
subject to: hi(x)=0;i=1;:::; m;
gi(x)60;i=1;:::; k:(B.13)Appendix B. Multivariate Differentiation and Optimization 403
Here, fis the objective function, and fgigandfhigare given functions so that hi(x)=0
andgi(x)60 represent the equality andinequality constraints, respectively. The region
XY where the objective function is deÔ¨Åned and where all the constraints are satisÔ¨Åed
is called the feasible region feasible region . An optimization problem without constraints is said to be an
unconstrained problem.
For an unconstrained continuous optimization problem, the search space Xis often
taken to be (a subset of) Rn, and fis assumed to be a Ckfunction for su ciently high
k(typically k=2 or 3 su ces); that is, its k-th order derivative is continuous. For a C1
function the standard approach to minimizing f(x) is to solve the equation
rf(x)=0; (B.14)
whererf(x) is the gradient offatx. The solutions xto (B.14) are called station- +397
ary points stationary
points. Stationary points can be local /global minimizers, local /global maximizers, or
saddle points (which are neither). If, in addition, the function is C2, the conditionsaddle points
y>(r2f(x))y>0 for all y,0 (B.15)
ensures that the stationary point xis a local minimizer of f. The condition (B.15) states
that the Hessian matrix of fatxispositive deÔ¨Ånite . Recall that we write H0 to indicate +398
that a matrix His positive deÔ¨Ånite.
In Figure B.2 we have a multiextremal objective function on X=R. There are four
stationary points: two are local minimizers, one is a local maximizer, and one is neither a
minimizer nor a maximizer, but a saddle point.
xLocal minimumLocal maximum
Global minimumSaddle pointf(x)
Figure B.2: A multiextremal objective function in one dimension.
B.2.1 Convexity and Optimization
An important class of optimization problems is related to the notion of convexity . A setX
is said to be convex if for all x1;x22Xit holds that x1+(1 )x22Xfor all 0661.
In addition, the objective function fis aconvex function convex
functionprovided that for each xin the
interior ofXthere exists a vector vsuch that
f(y)>f(x)+(y x)>v;y2X: (B.16)404 B.2. Optimization Theory
The vector vin (B.16) may not be unique and is referred to as a subgradient off.subgradientOne of the crucial properties of a convex function fis that Jensen‚Äôs inequality holds
(see Exercise 14 in Chapter 2): +62
Ef(X)>f(EX);
for any random vector X.
Example B.6 (Convexity and Directional Derivative) Thedirectional derivative directional
derivativeof a
multivariate function fatxin the direction dis deÔ¨Åned as the right derivative of g(t) :=
f(x+td) att=0:
lim
t#0f(x+td) f(x)
t=lim
t"1t(f(x+d=t) f(x)):
This right derivative may not always exist. However, if fis a convex function, then the
directional derivative of fatxin the interior of its domain always exists (in any direction
d).
To see this, let t1>t2>0. By Jensen‚Äôs inequality we have for any xandyin the interior
of the domain:
t2
t1f(y)+ 
1 t2
t1!
f(x)>f t2
t1y+ 
1 t2
t1!
x!
:
Making the substitution y=x+t1dand rearranging the last equation yields:
f(x+t1d) f(x)
t1>f(x+t2d) f(x)
t2:
In other words, the function t7!(f(x+td) f(x))=tis increasing for t>0 and therefore
the directional derivative satisÔ¨Åes:
lim
t#0f(x+td) f(x)
t=inf
t>0f(x+td) f(x)
t:
Hence, to show existence it is enough to show that ( f(x+td) f(x))=tis bounded from
below.
Since xlies in the interior of the domain of f, we can choose tsmall enough so that
x+tdalso lies in the interior. Therefore, the convexity of fimplies that there exists a
subgradient vector vsuch that f(x+td)>f(x)+v>(td). In other words,
f(x+td) f(x)
t>v>d
provides a lower bound for all t>0, and the directional derivative of fat an interior x
always exists (in any direction).
A function fsatisfying (B.16) with strict inequality is said to be strictly convex . It is
said to be a (strictly) concave function concave
functionif fis (strictly) convex. Assuming that Xis an
open set, convexity for f2C1is equivalent to
f(y)>f(x)+(y x)>rf(x) for all x;y2X:
Moreover, for f2C2strict convexity is equivalent to the Hessian matrix being positive
deÔ¨Ånite for all x2X, and convexity is equivalent to the Hessian matrix being positive
semideÔ¨Ånite for all x; that is, y>
r2f(x)
y>0 for all yandx. Recall that we write H0
to indicate that a matrix His positive semideÔ¨Ånite. +367Appendix B. Multivariate Differentiation and Optimization 405
Example B.7 (Convexity and Di erentiability) Iffis a continuously di erentiable
multivariate function, then fis convex if and only if the univariate function
g(t) :=f(x+td);t2[0;1]
is a convex function for any xandx+din the interior of the domain of f. This property
provides an alternative deÔ¨Ånition for convexity of a multivariate and di erentiable function.
To see why it is true, Ô¨Årst assume that fis convex and t1;t22[0;1]. Then, using the
subgradient deÔ¨Ånition of convexity in (B.16), we have f(a)>f(b)+(a b)>vfor some
subgradient v. Substituting with a=x+t1dandb=x+t2d, we obtain
g(t1)>g(t2)+(t1 t2)v>d
for any two points t1;t22[0;1]. Therefore, gis convex, because we have identiÔ¨Åed the
existence of a subgradient v>dfor each t2.
Conversely, assume that gis convex for t2[0;1]. Since fis dierentiable, then so is g.
Then, the convexity of gimplies that there is a subgradient vat 0 such that: g(t)>g(0)+t v
for all t2[0;1]. Rearranging,
v>g(t) g(0)
t;
and taking the right limit as t#0 we obtain v>g0(0)=d>rf(x):Therefore,
g(t)>g(0)+t v>g(0)+td>rf(x)
and substituting t=1 yields:
f(x+d)>f(x)+d>rf(x);
so that there exists a subgradient vector, namely rf(x), for each x. Hence, fis convex by
the deÔ¨Ånition in (B.16).
An optimization program of the form (B.13) is said to be a convex programming prob-
lemif:convex
programming
problem1. The objective fis aconvex function .
2. The inequality constraint functions fgigare convex.
3. The equality constraint functions fhigareane, that is, of the form a>
ix bi. This is
equivalent to both hiand hibeing convex for all i.
Table B.1 summarizes some commonly encountered problems, all of which are convex,
with the exception of the quadratic programs with A0.406 B.2. Optimization Theory
Table B.1: Some common classes of optimization problems.
Name f(x) Constraints
Linear Program (LP) c>x Ax=bandx>0
Inequality Form LP c>x Ax6b
Quadratic Program (QP)1
2x>Ax+b>xDx6d,Ex=e
Convex QP1
2x>Ax+b>xDx6d,Ex=e(A0)
Convex Program f(x) convexfgi(x)gconvex,fhi(x)gof the form a>
ix bi
Recognizing convex optimization problems or those that can be transformed to convex
optimization problems can be challenging. However, once formulated as convex optimiz-
ation problems, these can be e ciently solved using subgradient [112], bundle [57], and
cutting-plane methods [59].
B.2.2 Lagrangian Method
The main components of the Lagrangian method are the Lagrange multipliers and the
Lagrange function. The method was developed by Lagrange in 1797 for the optimization
problem (B.13) with only equality constraints. In 1951 Kuhn and Tucker extended Lag-
range‚Äôs method to inequality constraints. Given an optimization problem (B.13) containing
only equality constraints hi(x)=0;i=1;:::; m, the Lagrange function Lagrange
functionis deÔ¨Åned as
L(x;)=f(x)+mX
i=1ihi(x);
where the coe cientsfigare called the Lagrange multipliers Lagrange
multipliers. A necessary condition for a
point xto be a local minimizer of f(x) subject to the equality constraints hi(x)=0;i=
1;:::; m, is
rxL(x;)=0;
rL(x;)=0;
for some value . The above conditions are also su cient ifL(x;) is a convex function
ofx.
Given the original optimization problem (B.13), containing both the equality and in-
equality constraints, the generalized Lagrange function , orLagrangian Lagrangian , is deÔ¨Åned as
L(x;;)=f(x)+kX
i=1igi(x)+mX
i=1ihi(x):Appendix B. Multivariate Differentiation and Optimization 407
Theorem B.2: Karush‚ÄìKuhn‚ÄìTucker (KKT) Conditions
A necessary condition for a point xto be a local minimizer of f(x) in the optimiz-
ation problem (B.13) is the existence of an andsuch that
rxL(x;;)=0;
rL(x;;)=0;
gi(x)60;i=1;:::; k;

i>0;i=1;:::; k;

igi(x)=0;i=1;:::; k:
Forconvex programs we have the following important results [18, 43]:
1. Every local solution xto a convex programming problem is a global solution and
the set of global solutions is convex. If, in addition, the objective function is strictly
convex, then any global solution is unique.
2. For a strictly convex programming problem with C1objective and constraint func-
tions, the KKT conditions are necessary and su cient for a unique global solution.
B.2.3 Duality
The aim of duality is to provide an alternative formulation of an optimization problem
which is often more computationally e cient or has some theoretical signiÔ¨Åcance (see [43,
Page 219]). The original problem (B.13) is referred to as the primal primal problem whereas the
reformulated problem, based on Lagrange multipliers, is called the dual dual problem. Duality
theory is most relevant to convex optimization problems. It is well known that if the primal
optimization problem is (strictly) convex then the dual problem is (strictly) concave and
has a (unique) solution from which the (unique) optimal primal solution can be deduced.
TheLagrange dual program Lagrange dual
program(also called the Wolfe dual ) of the primal program (B.13),
is:
max
;L(;)
subject to: >0;
whereLis the Lagrange dual function :
L(;)=inf
x2XL(x;;); (B.17)
giving the greatest lower bound (inÔ¨Åmum) of L(x;;) over all possible x2X.
It is not di cult to see that if fis the minimal value of the primal problem, then
L(;)6ffor any>0and any. This property is called weak duality . The Lag-
rangian dual program thus determines the best lower bound on f. Ifdis the optimal
value for the dual problem then d6f. The di erence f dis called the duality gap .
The duality gap is extremely useful for providing lower bounds for the solutions of
primal problems that may be impossible to solve directly. It is important to note that for408 B.3. Numerical Root-Finding and Minimization
linearly constrained problems, if the primal is infeasible (does not have a solution satisfying
the constraints), then the dual is either infeasible or unbounded. Conversely, if the dual
is infeasible then the primal has no solution. Of crucial importance is the strong duality strong duality
theorem, which states that for convex programs (B.13) with linear constrained functions hi
andgithe duality gap is zero, and any xand (;) satisfying the KKT conditions are
(global) solutions to the primal and dual programs, respectively. In particular, this holds for
linear and convex quadratic programs (note that not all quadratic programs are convex).
For a convex primal program with C1objective and constraint functions, the Lagrangian
dual function (B.17) can be obtained by simply setting the gradient (with respect to x) of
the LagrangianL(x;;) to zero. One can further simplify the dual program by substitut-
ing into the Lagrangian the relations between the variables thus obtained.
Further, for a convex primal problem, if there is a strictly feasible pointex(that is, a
feasible point satisfying all of the inequality constraints with strict inequality), then the
duality gap is zero, and strong duality holds. This is known as Slater‚Äôs condition [18, Page
226].
The Lagrange dual problem is an important example of a saddle-point problem ormin-
imax problem. In such problems the aim is to locate a point ( x;y)2XY that satisÔ¨Åes
sup
y2Yinf
x2Xf(x;y)=inf
x2Xf(x;y)=f(x;y)=sup
y2Yf(x;y)=inf
x2Xsup
y2Yf(x;y):
The equation
sup
y2Yinf
x2Xf(x;y)=inf
x2Xsup
y2Yf(x;y)
is known as the minimax minimax equality. Other problems that fall into this framework are zero-
sum games in game theory; see also [24] for a number of combinatorial optimization prob-
lems that can be viewed as minimax problems.
B.3 Numerical Root-Finding and Minimization
In order to minimize a C1function f:Rn!Rone may solve
rf(x)=0;
which gives a stationary point of f. As a consequence, any technique for root-Ô¨Ånding can
be transformed into an unconstrained optimization method by attempting to locate roots
of the gradient. However, as noted in Section B.2, not all stationary points are minima,
and so additional information (such as is contained in the Hessian, if fisC2) needs to be
considered in order to establish the type of stationary point.
Alternatively, a root of a continuous function g:Rn!Rnmay be found by minimizing
the norm of g(x) over all x; that is, by solving min xf(x), with f(x) :=kg(x)kp, where for
p>1 the p-norm p-norm ofy=[y1;:::; yn]>is deÔ¨Åned as
kykp:=nX
i=1jyijp1=p
:
Hence, any (un)constrained optimization method can be transformed into a technique for
locating the roots of a function.Appendix B. Multivariate Differentiation and Optimization 409
Starting with an initial guess x0, most minimization and root-Ô¨Ånding algorithms create
a sequence x0;x1;:::using the iterative updating rule:
xt+1=xt+tdt;t=0;1;2;:::; (B.18)
wheret>0 is a (typically small) step size, called the learning rate learning rate , and the vector dt
is the search direction at step t. The iteration (B.18) continues until the sequence fxtgis
deemed to have converged to a solution, or a computational budget has been exhausted.
The performance of all such iterative methods depends crucially on the quality of the initial
guess x0.
There are two broad categories of iterative optimization algorithms of the form (B.18):
¬àThose of line search line search type, where at iteration twe Ô¨Årst compute a direction dtand
then determine a reasonable step size talong this direction. For example, in the
case of minimization, t>0 may be chosen to approximately minimize f(xt+dt)
for Ô¨Åxed xtanddt.
¬àThose of trust region trust region type, where at each iteration twe Ô¨Årst determine a suitable step
sizetand then compute an approximately optimal direction dt.
In the following sections, we review several widely-used root-Ô¨Ånding and optimization
algorithms of the line search type.
B.3.1 Newton-Like Methods
Suppose we wish to Ô¨Ånd roots of a function f:Rn!Rn. Iffis inC1, we can approximate
faround a point xtas
f(x)f(xt)+Jf(xt)(x xt);
where Jfis the matrix of Jacobi ‚Äî the matrix of partial derivatives of f; see (B.3). When +398
Jf(xt) is invertible, this linear approximation has root xt J 1
f(xt)f(xt):This gives the
iterative updating formula (B.18) for Ô¨Ånding roots of fwith direction dt= J 1
f(xt)f(xt)
and learning rate t=1. This is known as Newton‚Äôs method Newton ‚Äôs
method(or the Newton‚ÄìRaphson
method ) for root-Ô¨Ånding.
Instead of a unit learning rate, sometimes it is more e ective to use an tthat satisÔ¨Åes
theArmijo inexact line search Armijo inexact
line searchcondition:
kf(xt+tdt)k<(1 "1t)kf(xt)k;
where"1is a small heuristically chosen constant, say "1=10 4. For C1functions, such an
talways exists by continuity and can be computed as in the following algorithm.410 B.3. Numerical Root-Finding and Minimization
Algorithm B.3.1: Newton‚ÄìRaphson for Finding Roots of f(x)=0
input: An initial guess xand stopping error ">0.
output: The approximate root of f(x)=0.
1whilekf(x)k>"and budget is not exhausted do
2 Solve the linear system Jf(x)d= f(x).
3 1
4 whilekf(x+d)k>(1 10 4)kf(x)kdo
5 =2
6 x x+d
7return x
We can adapt a root-Ô¨Ånding Newton-like method in order to minimize a di erentiable
function f:Rn!R. We simply try to locate a zero of the gradient of f. When fis a
C2function, the function rf:Rn!Rnis continuous, and so the root of rfleads to the
search direction
dt= H 1
trf(xt); (B.19)
where Htis the Hessian matrix at xt(the matrix of Jacobi of the gradient is the Hessian).
When the learning rate tis equal to 1, the update xt H 1
trf(xt) can alternatively be
derived by assuming that f(x) is approximately quadratic and convex in the neighborhood
ofxt, that is,
f(x)f(xt)+(x xt)>rf(xt)+1
2(x xt)>Ht(x xt); (B.20)
and then minimizing the right-hand side of (B.20) with respect to x.
The following algorithm uses an Armijo inexact line search for minimization and
guards against the possibility that the Hessian may not be positive deÔ¨Ånite (that is, its
Cholesky decomposition does not exist). +373
Algorithm B.3.2: Newton‚ÄìRaphson for Minimizing f(x)
input: An initial guess x; stopping error ">0; line search parameter 2(0;1).
output: An approximate minimizer of f(x).
1L In(the identity matrix)
2whilekrf(x)k>"and budget is not exhausted do
3 Compute the Hessian Hatx.
4 if H0 then // Cholesky is successful
5 Update Lto be the Cholesky factor satisfying LL>=H.
6 else
7 Do not update the lower triangular L.
8 d   L 1rf(x) (computed by forward substitution)
9 d L >d(computed by backward substitution)
10 1
11 while f(x+d)>f(x)+10 4rf(x)>ddo
12 
13 x x+d
14return xAppendix B. Multivariate Differentiation and Optimization 411
A downside with all Newton-like methods is that at each step they require the calcu-
lation and inversion of an nnHessian matrix, which has computing time of O(n3), and
is thus infeasible for large n. One way to avoid this cost is to use quasi-Newton methods,
described next.
B.3.2 Quasi-Newton Methods
The idea behind quasi-Newton methods is to replace the inverse Hessian in (B.19) at iter-
ation tby an nnmatrix Csatisfying the secant condition secant
condition:
C1=; (B.21)
where xt xt 1and1 r f(xt) rf(xt 1) are vectors stored in memory at each iter-
ation t. The secant condition is satisÔ¨Åed, for example, by the Broyden‚Äôs family of matrices:
A+1
u>1( A1)u>
for some u,0andA. Since there is an inÔ¨Ånite number of matrices that satisfy the condi-
tion (B.21), we need a way to determine a unique Cat each iteration tsuch that computing
and storing Cfrom one step to the next is fast and avoids any costly matrix inversion. The
following examples illustrate how, starting with an initial guess C=Iatt=0, such a
matrix Ccan be e ciently updated from one iteration to the next.
Example B.8 (Low-Rank Hessian Update) The quadratic model (B.20) can be
strengthened by further assuming that exp(  f(x)) is proportional to a probability density
that can be approximated in the neighborhood of xtby the pdf of the N(xt+1;H 1
t) dis-
tribution. This normal approximation allows us to measure the discrepancy between two
pairs ( x1;H0) and ( x2;H1) using the Kullback‚ÄìLeibler divergence between the pdfs of the + 42
N(x1;H 1
0) andN(x2;H 1
1) distributions (see Exercise 4 on page 350):
D(x1;H 1
0jx2;H 1
1) :=1
2
tr(H1H 1
0) lnjH1H 1
0j+(x2 x1)>H1(x2 x1) n
:(B.22)
Suppose that the latest approximation to the inverse Hessian is Cand we wish to com-
pute an updated approximation for step t. One approach is to Ô¨Ånd the symmetric matrix
that minimizes its Kullback‚ÄìLeibler discrepancy from C, as deÔ¨Åned above, subject to the
constraint (B.21). In other words,
min
AD(0;Cj0;A)
subject to: A1=;A=A>:
The solution to this constrained optimization (see Exercise 10 on page 352) yields the
Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno or BFGS formula bfgs formula for updating the matrix Cfrom one
iteration to the next:
CBFGS=C+1>+1>C1
(1>)2> 1
1> 1>C+(1>C)>
|                                                     {z                                                     }
BFGS update: (B.23)412 B.3. Numerical Root-Finding and Minimization
In a practical implementation, we keep a single copy of Cin memory and apply the BFGS
update to it at every iteration. Note that if the current Cis symmetric, then so is the updated
matrix. Moreover, the BFGS update is a matrix of rank two.
Since the Kullback‚ÄìLeibler divergence is not symmetric, it is possible to Ô¨Çip the roles
ofH0andH1in (B.22) and instead solve
min
AD(0;Aj0;C)
subject to: A1=;A=A>:
The solution (see Exercise 10 on page 352) gives the Davidon‚ÄìFletcher‚ÄìPowell or DFP
formula dfp formula for updating the matrix Cfrom one iteration to the next:
CDFP=C+>
1> C11>C
1>C1|             {z             }
DFP update: (B.24)
Note that if the curvature condition 1>>0 holds and the current Cis symmetric positive
deÔ¨Ånite, then so is its update.
Example B.9 (Diagonal Hessian Update) The original BFGS formula requires O(n2)
storage and computation, which may be unmanageable for large n. One way to circumvent
the prohibitive quadratic cost is to only store and update a diagonal Hessian matrix from
one iteration to the next. If Cis diagonal, then we may not be able to satisfy the secant
condition (B.21) and maintain positive deÔ¨Åniteness. Instead the secant condition (B.21)
can be relaxed to the set of inequalities 1>C 1, which are related to the deÔ¨Ånition of a
subgradient for convex functions. We can then Ô¨Ånd a unique diagonal matrix by minimizing +403
D(xt;Cjxt+1;A) with respect to Aand subject to the constraints that A1>andAis
diagonal. The solution (Exercise 15 on page 352) yields the updating formula for a diagonal
element ciofC:
ci 8>>>>><>>>>>:2ci
1+q
1+4ciu2
i;if2ci
1+q
1+4ciu2
i>i=1i
i=1i; otherwise;(B.25)
where u:=rf(xt) and we assume a unit learning rate: xt+1=xt Au.
Example B.10 (Scalar Hessian Update) If the identity matrix is used in place of the
Hessian in (B.19), one obtains steepest descent steepest
descentorgradient descent methods, in which the
iteration (B.18) reduces to xt+1=xt trf(xt).
The rationale for the name steepest descent is as follows. If we start from any point
xand make an inÔ¨Ånitesimal move in some direction, then the function value is reduced
by the largest magnitude in the (unit norm) direction: u:= rf(x)=krf(x)k. This is seen
from the following inequality for all unit vectors u(that is,kuk=1):
d
dtf(x+tu)t=06d
dtf(x+tu)t=0:
Observe that equality is achieved if and only if u=u. This inequality is an easy con-
sequence of the Cauchy‚ÄìSchwarz inequality: +389Appendix B. Multivariate Differentiation and Optimization 413
 rf>u6jrf>uj6|{z}
Cauchy‚ÄìSchwartzkukkrfk=krfk= rf>u:
The steepest descent iteration, xt+1=xt trf(xt), still requires a suitable choice of the
learning rate t. An alternative way to think about the iteration is to assume that the learning
rate is always unity, and that at each iteration we use an inverse Hessian matrix of the form
tIfor some positive constant t. Satisfying the secant condition (B.21) with a matrix of the
form C=Iis not possible. However, it is possible to choose so that the secant condition
(B.21) is satisÔ¨Åed in the direction of 1(or alternatively ). This gives the Barzilai‚ÄìBorwein
formulas Barzilai ‚Äì
Borwein
formulasfor the learning rate at iteration t:
t=1>
k1k2 
or alternatively t=kk2
>1!
: (B.26)
B.3.3 Normal Approximation Method
Let'H 1
t(x xt+1) denote the pdf of the N(xt+1;H 1
t) distribution. As we already saw in Ex-
ample B.8, the quadratic approximation (B.20) of fin the neighborhood of xtis equivalent
(up to a constant) to the minus of the logarithm of the pdf 'H 1
t(x xt+1). In other words,
we use'H 1
t(x xt+1) as a simple model for the density
exp( f(x)).Z
exp( f(y)) dy:
One consequence of the normal approximation is that for xin the neighborhood of xt+1,
we can write:
 rf(x)@
@xln'H 1
t(x xt+1)= Ht(x xt+1):
In other words, using the fact that H>
t=Ht,
rf(x)[rf(x)]>Ht(x xt+1)(x xt+1)>Ht;
and taking expectations on both sides with respect to XN(xt+1;H 1
t) gives:
Erf(X) [rf(X)]>Ht:
This suggests that, given the gradient vectors computed in the past h(where hstands for
history) of Newton iterations:
ui:=rf(xi);i=t (h 1);:::; t;
the Hessian matrix Htcan be approximated via the average
1
htX
i=t h+1uiu>
i:
A shortcoming of this approximation is that, unless his large enough, the Hessian approx-
imationPt
i=t h+1uiu>
imay not be full rank and hence not invertible. To ensure that the414 B.3. Numerical Root-Finding and Minimization
Hessian approximation is invertible, we add a suitable diagonal matrix A0to obtain the
regularized version of the approximation: +217
HtA0+1
htX
i=t h+1uiu>
i:
With this full-rank approximation of the Hessian, the Newton search direction in (B.19)
becomes:
dt= 0BBBBB@A0+1
htX
i=t h+1uiu>
i1CCCCCA 1
ut: (B.27)
Thus, dtcan be computed in O(h2n) time via the Sherman‚ÄìMorrison Algorithm A.6.1. +373
Further to this, the search direction (B.27) can be e ciently updated to the next one:
dt+1= 0BBBBB@A0+1
ht+1X
i=t h+2uiu>
i1CCCCCA 1
ut+1
inO(h n) time, thus avoiding the usual O(h2n) cost (see Exercise 6 on page 351).
B.3.4 Nonlinear Least Squares
Consider the squared-error training loss in nonlinear regression: +188
`(g(j))=1
nnX
i=1(g(xij) yi)2;
where g(j) is a nonlinear prediction function that depends on the parameter (for ex-
ample, (5.29) shows the nonlinear logistic prediction function). The training loss can be
written as1
nkg(j) yk2, where g(j) :=[g(x1j);:::; g(xnj)]>is the vector of out-
puts.
We wish to minimize the training loss in terms of . In the Newton-like methods in
Section B.3.1, one derives an iterative minimization algorithm that is inspired by a Taylor
expansion of `(g(j)). Instead, given a current guess t, we can consider the Taylor ex-
pansion of the nonlinear prediction function g:
g(j)g(jt)+Gt( t);
where Gt:=Jg(t) is the matrix of Jacobi of g(j) att. Denoting the residual et:= +398
g(jt) yand replacing g(j) with its Taylor approximation in `(g(j)), we obtain
the approximation to the training loss in the neighborhood of t:
`(g(j))1
nGt( t)+et2:
The minimization of the right-hand side is a linear least-squares problem and therefore
dt:= tsatisÔ¨Åes the normal equations: G>
tGtdt=G>
t( et). Assuming that G>
tGtis +28
invertible, the normal equations yield the Gauss‚ÄìNewton Gauss‚ÄìNewton search direction:
dt= (G>
tGt) 1G>
tet:Appendix B. Multivariate Differentiation and Optimization 415
Unlike the search direction (B.19) for Newton-like algorithms, the search direction of a
Gauss‚ÄìNewton algorithm does not require the computation of a Hessian matrix.
Observe that in the Gauss‚ÄìNewton approach we determine dtby viewing the search
direction as coe cients in a linear regression with feature matrix Gtand response et. This
suggests that instead of using a linear regression, we can compute dtvia a ridge regression
with a suitable choice for the regularization parameter : +217
dt= (G>
tGt+nIp) 1G>
tet:
If we replace nIpwith the diagonal matrix diag( G>
tGt), we then obtain the Levenberg‚Äì
Marquardt Levenberg ‚Äì
Marquardtsearch direction:
dt= (G>
tGt+diag( G>
tGt)) 1G>
tet: (B.28)
Recall that the ridge regularization parameter has the following e ect on the least-squares
solution: When it is zero, then the solution dtcoincides with the search direction of the
Gauss‚ÄìNewton method, and when tends to inÔ¨Ånity, then kdtktends to zero. Thus, 
controls both the magnitude and direction of vector dt. A simple version of the Levenberg‚Äì
Marquardt algorithm is the following.
Algorithm B.3.3: Levenberg‚ÄìMarquardt for Minimizing1
nkg(j) yk2
input: An initial guess 0; stopping error ">0; training set .
output: An approximate minimizer of1
nkg(j) yk2.
1t 0 and 0:01 (or another default value)
2while stopping condition is not met do
3 Compute the search direction dtvia (B.28).
4 et+1 g(jt+dt) y
5 ifket+1k<ketkthen
6 =10, et+1 et,t+1 t+dt
7 else
8 10
9 t t+1
10returnt
B.4 Constrained Minimization via Penalty Functions
A constrained optimization problem of the form (B.13) can sometimes be reformulated as a
simpler unconstrained problem ‚Äî for example, the unconstrained set Ycan be transformed
to the feasible region Xof the constrained problem via a function :Rn!Rnsuch that
X=(Y). Then, (B.13) is equivalent to the minimization problem
min
y2Yf((y));
in the sense that a solution xof the original problem is obtained from a transformed
solution yviax=(y). Table B.2 lists some examples of possible transformations.416 B.4. Constrained Minimization via Penalty Functions
Table B.2: Some transformations to eliminate constraints.
Constrained Unconstrained
x>0 exp( y)
x>0 y2
a6x6b a +(b a) sin2(y)
Unfortunately, an unconstrained minimization method used in combination with these
transformations is rarely e ective. Instead, it is more common to use penalty functions.
The overarching idea of penalty functions penalty
functionsis to transform a constrained problem into
an unconstrained problem by adding weighted constraint-violation terms to the original
objective function, with the premise that the new problem has a solution that is identical or
close to the original one.
For example, if there are only equality constraints, then
ef(x) :=f(x)+mX
i=1aijhi(x)jp
for some constants a1;:::; am>0 and integer p2f1;2g, gives an exact penalty function ,
in the sense that the minimizer of the penalized function efis equal to the minimizer of f
subject to the mequality constraints h1;:::; hm. With the addition of inequality constraints,
one could use
ef(x)=f(x)+mX
i=1aijhi(x)jp+kX
j=1bjmaxfgj(x);0g
for some constants a1;:::; am;b1;:::; bk>0.
Example B.11 (Alternating Direction Method of Multipliers) The Lagrange method
is designed to handle convex minimization subject to equality constraints. Nevertheless, +406
some practical algorithms may still use the penalty function approach in combination with
the Lagrangian method. An example is the alternating direction method of multipliers alternating
direction
method of
multipliers(ADMM) [17]. The ADMM solves problems of the form:
min
x2Rn;z2Rmf(x)+g(z)
subject to: Ax+Bz=c;(B.29)
where A2Rpn,B2Rpm, and c2Rp, and f:Rn!Randg:Rm!Rare convex func-
tions. The approach is to form an augmented Lagrangian
L%(x;z;) :=f(x)+g(z)+>(Ax+Bz c)+%
2kAx+Bz ck2;
where%>0 is a penalty parameter, and 2Rpare dual variables. The ADMM then iterates
through updates of the following form:
x(t+1)=argmin
x2RnL%(x;z(t);(t))
z(t+1)=argmin
z2RmL%(x(t+1);z;(t))
(t+1)=(t)+%
Ax(t+1)+Bz(t+1) c
:Appendix B. Multivariate Differentiation and Optimization 417
Suppose that (B.13) has inequality constraints only. Barrier functions Barrier
functionsare an important ex-
ample of penalty functions that can handle inequality constraints. The prototypical example
is alogarithmic barrier function which gives the unconstrained optimization:
ef(x)=f(x) kX
j=1ln( gj(x)); > 0;
such that the minimizer of eftends to the minimizer of fas!0. Direct minimization
ofefvia an unconstrained minimization algorithm is frequently too di cult. Instead, it
is common to combine the logarithmic barrier function with the Lagrangian method as
follows.
The idea is to introduce knonnegative auxiliary or slack variables slack variables s1;:::; skthat satisfy
the equalities gj(x)+sj=0 for all j. These equalities ensure that the inequality constraints
are maintained: gj(x)= sj60 for all j. Then, instead of the unconstrained optimization
ofef, we consider the unconstrained optimization of the Lagrangian:
L(x;s;)=f(x) kX
j=1lnsj+kX
j=1j(gj(x)+sj); (B.30)
where>0 andare the Lagrange multipliers for the equalities gj(x)+sj=0;j=1;:::; k.
Observe how the logarithmic barrier function keeps the slack variables positive. In
addition, while the optimization of efis over ndimensions (recall that x2Rn), the optimiz-
ation of the Lagrangian function Lis over n+2kdimensions. Despite this enlargement of
the search space with the variables sand, the optimization of the Lagrangian Lis easier
in practice than the direct optimization of ef.
Example B.12 (Interior-Point Method for Nonnegativity) One of the simplest and
most common constrained optimization problems can be formulated as the minimization
off(x) subject to nonnegative x, that is: min x>0f(x). In this case, the Lagrangian with
logarithmic barrier (B.30) is:
L(x;s;)=f(x) X
klnsk+>(s x):
The KKT conditions in Theorem B.2 are a necessary condition for a minimizer, and yield
the nonlinear system for [ x>;s>;>]>2R3n:
2666666664rf(x) 
 =s+
s x3777777775=0;
where=sis a shorthand notation for a column vector with components f=sjg. To solve this
system, we can use Newton‚Äôs method for root Ô¨Ånding (see, for example, Algorithm B.3.1),
which requires a formula for the matrix of Jacobi of L. Here, this (3 n)(3n) matrix is:
JL(x;s;)=2666666664H O I
O D I
 I I O3777777775="H B
B>E#
;418 B.4. Constrained Minimization via Penalty Functions
where His the nnHessian of fatx;D:=diag (=(ss))is an nndiagonal matrix;
B:=[O; I] is an n(2n) matrix, and1
E:="D I
I O#
="O I
I D# 1
:
Further, we deÔ¨Åne
H:=(H BE 1B>) 1=(H+D) 1:
Using this notation and applying the matrix blockwise inversion formula (A.14), we obtain +371
the inverse of the matrix of Jacobi:
"H B
B>E# 1
="H HBE 1
 E 1B>HE 1+E 1B>HBE 1#
=2666666664H H HD
H H I HD
 DHI DHDHD D3777777775:
Therefore, the search direction in Newton‚Äôs root-Ô¨Ånding method is given by:
 J 1
L2666666664rf(x) 
 =s+
s x3777777775=2666666664dx
dx+x s
=s  D(dx+x s)3777777775;
where
dx:= (H+D) 1h
rf(x) 2=s+Dxi
;
and we have assumed that H+Dis a positive-deÔ¨Ånite matrix. If at any step of the iteration
the matrix H+Dfails to be positive-deÔ¨Ånite, then Newton‚Äôs root-Ô¨Ånding algorithm may
fail to converge. Thus, any practical implementation will have to include a fail-safe feature
to guard against this possibility.
In summary, for a given penalty parameter  >0, we can locate the approximate non-
negative minimizer of fusing, for example, the version of the Newton‚ÄìRaphson root-
Ô¨Ånding method given in Algorithm B.4.1.
In practice, one needs to choose a su ciently small value for , so that the output x
of Algorithm B.4.1 is a good approximation to x=argminx>0f(x). Alternatively, one
can create a decreasing sequence of penalty parameters 1>  2>and compute the
corresponding solutions x1;x2;:::of the penalized problems. In the so-called interior-
point method , a given xiis used as an initial guess for computing xi+1and so on until theinterior -point
method approximation to the minimizer x=argminx>0f(x) is deemed accurate.
1Here Ois an nnmatrix of zeros and Iis the nnidentity matrix.Appendix B. Multivariate Differentiation and Optimization 419
Algorithm B.4.1: Approximating x=argminx>0f(x) with Logarithmic Barrier
input: An initial guess xand stopping error ">0.
output: The approximate nonnegative minimizer xoff.
1s x, =s, dx 
2whilekdxk>"and budget is not exhausted do
3 Compute the gradient uand the Hessian Hoffatx.
4 s1 =s,s2 s1=s,w 2s1 u s2x
5 if(H+diag( s2))0 then // if Cholesky successful
6 Compute the Cholesky factor Lsatisfying LL>=H+diag( s2).
7 dx L 1w(computed by forward substitution)
8 dx L >dx(computed by backward substitution)
9 else
10 dx w=s2 // if Cholesky fails, do steepest descent
11 ds dx+x s, d s1  s2ds, 1
12 while min jfsj+dsjg<0do
13 =2 // ensure nonnegative slack variables
14 x x+dx,s s+ds, +d
15return x x
Further Reading
For an excellent introduction to convex optimization and Lagrangian duality see [18]. A
classical text on optimization algorithms and, in particular, on quasi-Newton methods is
[43]. For more details on the alternating direction method of multipliers see [17].420APPENDIXC
PROBABILITY AND STATISTICS
The purpose of this chapter is to establish the baseline probability and statistics
background for this book. We review basic concepts such as the sum and product rules
of probability, random variables and their probability distributions, expectations, in-
dependence, conditional probability, transformation rules, limit theorems, and Markov
chains. The properties of the multivariate normal distribution are discussed in more de-
tail. The main ideas from statistics are also reviewed, including estimation techniques
(such as maximum likelihood estimation), conÔ¨Ådence intervals, and hypothesis testing.
C.1 Random Experiments and Probability Spaces
The basic notion in probability theory is that of a random experiment random
experiment: an experiment
whose outcome cannot be determined in advance. Mathematically, a random experiment is
modeled via a triplet ( 
;H;P), where:
¬à
is the set of all possible outcomes of the experiment, called the sample space sample space .
¬àHis the collection of all subsets of 
to which a probability can be assigned; such
subsets are called events events .
¬àPis aprobability measure probability
measure, which assigns to each event Aa numberP[A] between 0
and 1, indicating the likelihood that the outcome of the random experiment lies in A.
Any probability measure Pmust satisfy the following Kolmogorov axioms Kolmogorov
axioms:
1.P[A]>0 for every event A.
2.P[
]=1.
3. For any sequence A1;A2;:::of events,
Ph[
iAii
6X
iP[Ai]; (C.1)
with strict equality whenever the events are disjoint (that is, non-overlapping).
421422 C.2. Random Variables and Probability Distributions
When (C.1) holds as an equality, it is often referred to as the sum rule sum rule of probability. It
simply states that if an event can happen in a number of di erent but not simultaneous
ways, the probability of that event is the sum of the probabilities of the comprising events.
If the events are allowed to overlap, then the inequality (C.1) is called the union bound union bound .
In many applications the sample space is countable ; that is, 
 =fa1;a2;:::g. In this
case the easiest way to specify a probability measure Pis to Ô¨Årst assign a number pito
each elementary event elementary
eventfaig, withP
ipi=1, and then to deÔ¨Åne
P[A]=X
i:ai2Apifor all A
:
Here the collection of events Hcan be taken to be equal to the collection of allsubsets
of
. The triple ( 
;H;P) is called a discrete probability space discrete
probability
space. This idea is graphically
represented in Figure C.1. Each element ai, represented by a dot, is assigned a weight (that
is, probability) pi, indicated by the size of the dot. The probability of the event Ais simply
the sum of the weights of all the outcomes in A.
Œ©
A
Figure C.1: A discrete probability space.
Remark C.1 (Equilikely Principle) A special case of a discrete probability space oc-
curs when a random experiment has Ô¨Ånitely many outcomes that are all equally likely . In
this case the probability measure is given by
P[A]=jAj
j
j; (C.2)
wherejAjdenotes the number of outcomes in Aandj
jis the total number of outcomes.
Thus, the calculation of probabilities reduces to counting numbers of outcomes in events.
This is called the equilikely principle equilikely
principle.
C.2 Random Variables and Probability Distributions
It is often convenient to describe a random experiment via ‚Äúrandom variables‚Äù, repres-
enting numerical measurements of the experiment. Random variables are usually denoted
by capital letters from the last part of the alphabet. From a mathematical point of view, a
random variable random
variableXis a function from 
toRsuch that sets of the form fa<X6bg:=
f!2
:a<X(!)6bgare events (and so can be assigned a probability).Appendix C. Probability and Statistics 423
All probabilities involving a random variable Xcan be computed, in principle, from its
cumulative distribution function cumulative
distribution
function(cdf), deÔ¨Åned by
F(x)=P[X6x];x2R:
For example P[a<X6b]=P[X6b] P[X6a]=F(b) F(a). Figure C.2 shows a
generic cdf. Note that any cdf is right-continuous, increasing, and lies between 0 and 1.
01
Figure C.2: A cumulative distribution function (cdf).
A cdf Fdis called discrete discrete cdf if there exist numbers x1;x2;:::and probabilities 0 <f(xi)6
1 summing up to 1, such that for all x
Fd(x)=X
xi6xf(xi): (C.3)
Such a cdf is piecewise constant and has jumps of sizes f(x1);f(x2);:::at points x1;x2;:::,
respectively. The function f(x) is called a probability mass function ordiscrete probability
density function (pdf). discrete pdf It is often easier to use the pdf rather than the cdf, since probabilities
can simply be calculated from it via summation:
P[X2B]=X
x2Bf(x);
as illustrated in Figure C.3.
Figure C.3: Discrete probability density function (pdf). The darker area corresponds to the
probabilityP[X2B].424 C.2. Random Variables and Probability Distributions
A cdf Fcis called continuous1, continuous cdf if there exists a positive function fsuch that for all x
Fc(x)=Zx
 1f(u) du: (C.4)
Note that such an Fcis dierentiable (and hence continuous) with derivative f. The func-
tion fis called the probability density function (continuous pdf) . pdf By the fundamental the-
orem of integration, we have
P[a<X6b]=F(b) F(a)=Zb
af(x) dx:
Thus, calculating probabilities reduces to integration, as illustrated in Figure C.4.
Figure C.4: Continuous probability density function (pdf). The shaded area corresponds to
the probability P[X2B], with Bbeing here the interval ( a;b].
Remark C.2 (Probability Density and Probability Mass) It is important to note that
we deliberately use the same name, ‚Äúpdf‚Äù, and symbol, f, in both the discrete and the
continuous case, rather than distinguish between a probability mass function (pmf) and
probability density function (pdf). From a theoretical point of view the pdf plays exactly
the same role in the discrete and continuous cases. We use the notation XDist,Xf,
andXFto indicate that Xhas distribution Dist, pdf f, and cdf F.
Tables C.1 and C.2 list a number of important continuous and discrete distributions.
Note that in Table C.1,  is the gamma function:  ()=R1
0e xx 1dx; > 0.
1In advanced probability, we would say ‚Äú absolutely continuous with respect to the Lebesgue measure ‚Äù.Appendix C. Probability and Statistics 425
Table C.1: Commonly used continuous distributions.
Name Notation f(x) x2 Parameters
Uniform U[;]1
 [;]<
Normal N(;2)1
p
2e 1
2(x 
)2
R>0; 2R
Gamma Gamma (;)x 1e x
 ()R+;> 0
Inverse Gamma InvGamma (;)x  1e x 1
 ()R+;> 0
Exponential Exp() e xR+>0
Beta Beta (;) (+)
 () ()x 1(1 x) 1[0;1];> 0
Weibull Weib (;) (x) 1e (x)R+;> 0
Pareto Pareto (;) (1+x) (+1)R+;> 0
Student t (+1
2)
p (
2) 
1+x2
! (+1)=2
R>0
F F(m;n) (m+n
2) (m=n)m=2x(m 2)=2
 (m
2) (n
2) [1+(m=n)x](m+n)=2R+ m;n2N+
TheGamma (n=2;1=2) distribution is called the chi-squared distribution 2
ndistribution with ndegrees
of freedom, denoted 2
n. The t1distribution is also called the Cauchy distribution.
Table C.2: Commonly used discrete distributions.
Name Notation f(x) x2 Parameters
Bernoulli Ber(p) px(1 p)1 xf0;1g 06p61
Binomial Bin(n;p) n
x!
px(1 p)n xf0;1;:::; ng06p61;
n2N
Discrete
uniformUf1;:::; ng1
nf1;:::; ng n2f1;2;:::g
Geometric Geom (p) p(1 p)x 1f1;2;:::g 06p61
Poisson Poi() e x
x!N >0426 C.3. Expectation
C.3 Expectation
It is often useful to consider di erent kinds of numerical characteristics of a random vari-
able. One such quantity is the expectation, which measures the ‚Äúaverage‚Äù value of the
distribution.
Theexpectation expectation (or expected value or mean) of a random variable Xwith pdf f, denoted
byEXor2E[X] (and sometimes ), is deÔ¨Åned by
EX=8>><>>:P
xx f(x) discrete case,R1
 1x f(x) dxcontinuous case :
IfXis a random variable, then a function of X, such as X2or sin( X), is again a random
variable. Moreover, the expected value of a function of Xis simply a weighted average of
the possible values that this function can take. That is, for any real function h
Eh(X)=8>><>>:P
xh(x)f(x) discrete case ;R1
 1h(x)f(x) dxcontinuous case ;
provided that the sum or integral are well-deÔ¨Åned.
Thevariance variance of a random variable X, denoted by VarX(and sometimes 2), is deÔ¨Åned
by
VarX=E(X E[X])2=EX2 (EX)2:
The square root of the variance is called the standard deviation standard
deviation. Table C.3 lists the expect-
ations and variances for some well-known distributions. Both variance and standard devi-
ation measure the spread or dispersion of the distribution. Note, however, that the standard
deviation measures the dispersion in the same units as the random variable, unlike the
variance, which uses squared units.
Table C.3: Expectations and variances for some well-known distributions.
Dist. EXVarX
Bin(n;p) np np (1 p)
Geom (p)1
p1 p
p2
Poi() 
U[;]+
2( )2
12
Exp()1
1
2
t 0 (>1)
 2(>2)Dist. EX VarX
Gamma (;)

2
N(;2)  2
Beta (;)
+
(+)2(1++)
Weib (;) (1=)
2 (2=)
  (1=)
2
F(m;n)n
n 2(n>2)2n2(m+n 2)
m(n 2)2(n 4)(n>4)
2We only use brackets in an expectation if it is unclear with respect to which random variable the ex-
pectation is taken.Appendix C. Probability and Statistics 427
It is sometimes useful to consider the moment generating function moment
generating
functionof a random variable
X. This is the function MdeÔ¨Åned by
M(s)=EesX;s2R: (C.5)
The moment generating functions of two random variables coincide if and only if the ran-
dom variables have the same distribution; see also Theorem C.12.
Example C.1 (Moment Generation Function of the Gamma (;)Distribution) Let
XGamma (;). For s<, the moment generating function of Xatsis given by
M(s)=EesX=Z1
0esxe xx 1
 ()dx
=
 sZ1
0e ( s)x( s)x 1
 ()|                    {z                    }
pdf of Gamma (; s)dx=
 s
:
Fors>,M(s)=1. Interestingly, the moment generating function has a much simpler
formula than the pdf.
C.4 Joint Distributions
Distributions for random vectors and stochastic processes can be speciÔ¨Åed in much the
same way as for random variables. In particular, the distribution of a random vector X=
[X1;:::; Xn]>is completely determined by specifying the joint cdf joint cdf F, deÔ¨Åned by
F(x1;:::; xn)=P[X16x1;:::; Xn6xn];xi2R;i=1;:::; n:
Similarly, the distribution of a stochastic process stochastic
process, that is, a collection of random vari-
ablesfXt;t2Tg, for some index set T, is completely determined by its Ô¨Ånite-dimensional
distributions; speciÔ¨Åcally, the distributions of the random vectors [ Xt1;:::; Xtn]>for every
choice of nandt1;:::; tn.
By analogy to the one-dimensional case, a random vector X=[X1;:::; Xn]>taking
values inRnis said to have a pdf fif, in the continuous case,
P[X2B]=Z
Bf(x) dx; (C.6)
for all n-dimensional rectangles B. Replace the integral with a sum for the discrete case.
The pdf is also called the joint pdf joint pdf ofX1;:::; Xn. The pdfs of the individual components ‚Äî
called marginal pdfs ‚Äî marginal pdf can be recovered from the joint pdf by ‚Äúintegrating out the other
variables‚Äù. For example, for a continuous random vector [ X;Y]>with pdf f, the pdf fXof
Xis given by
fX(x)=Z
f(x;y) dy:428 C.5. Conditioning and Independence
C.5 Conditioning and Independence
Conditional probabilities and conditional distributions are used to model additional inform-
ation on a random experiment. Independence is used to model lack of such information.
C.5.1 Conditional Probability
Suppose some event B
occurs. Given this fact, event Awill occur if and only if
A\Boccurs, and the relative chance of Aoccurring is therefore P[A\B]=P[B], provided
P[B]>0. This leads to the deÔ¨Ånition of the conditional probability conditional
probabilityofAgiven B:
P[AjB]=P[A\B]
P[B];ifP[B]>0: (C.7)
The above deÔ¨Ånition breaks down if P[B]=0. Such conditional probabilities must be
treated with more care [11].
Three important consequences of the deÔ¨Ånition of conditional probability are:
1.Product rule Product rule : For any sequence of events A1;A2;:::; An,
P[A1An]=P[A1]P[A2jA1]P[A3jA1A2]P[AnjA1An 1]; (C.8)
using the abbreviation A1A2Ak:=A1\A2\\ Ak.
2.Law of total probability Law of total
probability: IffBigforms a partition of
(that is, Bi\Bj=;;i,jand
[iBi= 
), then for any event A
P[A]=X
iP[AjBi]P[Bi]: (C.9)
3.Bayes‚Äô rule Bayes‚Äôrule : LetfBigform a partition of 
. Then, for any event AwithP[A]>0,
P[BjjA]=P[AjBj]P[Bj]P
iP[AjBi]P[Bi]: (C.10)
C.5.2 Independence
Two events AandBare said to be independent independent
eventsif the knowledge that Bhas occurred does
not change the probability that Aoccurs. That is, A,Bindependent,P[AjB]=P[A].
SinceP[AjB]P[B]=P[A\B], an alternative deÔ¨Ånition of independence is
A,Bindependent,P[A\B]=P[A]P[B]:
This deÔ¨Ånition covers the case where P[B]=0 and can be extended to arbitrarily many
events: events A1;A2;:::are said to be (mutually) independent if for any kand any choice
of distinct indices i1;:::; ik,
P[Ai1\Ai2\\ Aik]=P[Ai1]P[Ai2]P[Aik]:Appendix C. Probability and Statistics 429
The concept of independence can also be formulated for random variables. Random
variables X1;X2;:::are said to be independent independent
random
variablesif the eventsfXi16xi1g;:::;fXin6xingare
independent for all Ô¨Ånite choices of ndistinct indices i1;:::; inand values xi1;:::; xin.
An important characterization of independent random variables is the following (for a
proof, see [101], for example).
Theorem C.1: Independence Characterization
Random variables X1;:::; Xnwith marginal pdfs fX1;:::; fXnand joint pdf fare in-
dependent if and only if
f(x1;:::; xn)=fX1(x1)fXn(xn) for all x1;:::; xn: (C.11)
Many probabilistic models involve random variables X1;X2;:::that are independent
and identically distributed , abbreviated as iid iid . We use this abbreviation throughout this
book.
C.5.3 Expectation and Covariance
Similar to the univariate case, the expected value of a real-valued function hof a random
vector Xfis a weighted average of all values that h(X) can take. SpeciÔ¨Åcally, in the
continuous case, Eh(X)=R
h(x)f(x) dx. In the discrete case replace this multidimensional
integral with a sum. Using this result, it is not di cult to show that for any collection of
dependent or independent random variables X1;:::; Xn,
E[a+b1X1+b2X2++bnXn]=a+b1EX1++bnEXn (C.12)
for all constants a,b1;:::; bn. Moreover, for independent random variables,
E[X1X2Xn]=EX1EX2EXn: (C.13)
We leave the proofs as an exercise.
Thecovariance covariance of two random variables XandYwith expectations XandY, respect-
ively, is deÔ¨Åned as
Cov(X;Y)=E[(X X)(Y Y)]:
This is a measure of the amount of linear dependency between the variables. Let 2
X=
VarXand2
Y=VarY. A scaled version of the covariance is given by the correlation
coecient correlation
coefficient,
%(X;Y)=Cov(X;Y)
XY:
The following properties follow directly from the deÔ¨Ånitions of variance and covariance.
1.VarX=EX2 2
X:
2.Var[aX+b]=a22
X:
3.Cov(X;Y)=E[XY] XY.
4.Cov(X;Y)=Cov(Y;X).430 C.5. Conditioning and Independence
5. XY6Cov(X;Y)6XY.
6.Cov(aX+bY;Z)=aCov(X;Z)+bCov(Y;Z).
7.Cov(X;X)=2
X.
8.Var[X+Y]=2
X+2
Y+2Cov(X;Y).
9. If XandYare independent, then Cov(X;Y)=0.
As a consequence of Properties 2 and 8 we have that for any sequence of independent
random variables X1, . . . , Xnwith variances 2
1;:::;2
n,
Var[a1X1+a2X2++anXn]=a2
12
1+a2
22
2++a2
n2
n; (C.14)
for any choice of constants a1;:::; an.
For random column vectors, such as X=[X1;:::; Xn]>, it is convenient to write the
expectations and covariances in vector and matrix notation. For a random vector Xwe
deÔ¨Åne its expectation vector expectation
vectoras the vector of expectations
=[1;:::; n]>=[EX1;:::;EXn]>:
Similarly, if the expectation of a matrix is the matrix of expectations, then given two ran-
dom vectors X2RnandY2Rm, the nmmatrix
Cov(X;Y)=E[(X EX)(Y EY)>] (C.15)
has ( i;j)-th element Cov(Xi;Yj)=E[(Xi EXi)(Yj EYj)]:A consequence of this deÔ¨Ånition
is that
Cov(AX;BY)=ACov(X;Y)B>;
where AandBare two matrices with nandmcolumns, respectively.
Thecovariance matrix covariance
matrixof the vector Xis deÔ¨Åned as the nnmatrixCov(X;X). The co-
variance matrix is also denoted as Var(X)=Cov(X;X), in analogy with the scalar identity
Var(X)=Cov(X;X).
A useful application of the cyclic property of the trace of a matrix (see Theorem A.1)
is the following. +357
Theorem C.2: Expectation of a Quadratic Form
LetAbe an nnmatrix and Xann-dimensional random vector with expectation
vectorand covariance matrix . The random variable Y:=X>AXhas expectation
tr(A)+>A.
Proof: Since Yis a scalar, it is equal to its trace. Now, using the cyclic property: EY=
Etr(Y)=Etr(X>AX)=Etr(AXX>)=tr(AE[XX>])=tr(A(+>))=tr(A)+
tr(A>)=tr(A)+>A. Appendix C. Probability and Statistics 431
C.5.4 Conditional Density and Conditional Expectation
Suppose XandYare both discrete or both continuous, with joint pdf f, and suppose fX(x)>
0. Then, the conditional pdf conditional pdf ofYgiven X=xis given by
fYjX(yjx)=f(x;y)
fX(x)for all y: (C.16)
In the discrete case, the formula is a direct translation of (C.7), with fYjX(yjx)=P[Y=
yjX=x]. In the continuous case, a similar interpretation in terms of densities can be used;
see, for example, [101, Page 221]. The corresponding distribution is called the conditional
distribution ofYgiven X=x. Note that (C.16) implies thatconditional
distribution
f(x;y)=fX(x)fYjX(yjx):
This is useful when the marginal and conditional pdfs are given, rather than the joint one.
More generally, for the n-dimensional case we have
f(x1;:::; xn)=fX1(x1)fX2jX1(x2jx1)fXnjX1;:::;Xn 1(xnjx1;:::; xn 1); (C.17)
which is in essence a rephrasing of the product rule (C.8) in terms of probability densities. +428
As a conditional pdf has all the properties of an ordinary pdf, we may deÔ¨Åne expecta-
tions with respect to it. The conditional expectation of a random variable Ygiven X=xisconditional
expectation deÔ¨Åned as
E[YjX=x]=8>><>>:P
yy fYjX(yjx) discrete case,R
y fYjX(yjx) dycontinuous case.(C.18)
Note thatE[YjX=x] is a function of x. The corresponding random variable is written
asE[YjX]. A similar formalism can be used when conditioning on a sequence of random
variables X1;:::; Xn. The conditional expectation has similar properties to the ordinary
expectation. Other useful properties (see, for example, [127]) are:
1.Tower property : IfEYexists, then
EE[YjX]=EY: (C.19)
2.Taking out what is known : IfEYexists, then
E[XYjX]=XE[YjX]:
C.6 Functions of Random Variables
Letx=[x1;:::; xn]>be a column vector in RnandAanmnmatrix. The mapping x7!z,
with z=Ax, is a linear transformation, as discussed in Section A.1. Now consider a +355
random vector X=[X1;:::; Xn]>and let Z:=AX. Then Zis a random vector in Rm. The
following theorem details how the distribution of Zis related to that of X.432 C.6. Functions of Random Variables
Theorem C.3: Linear Transformation
IfXhas an expectation vector Xand covariance matrix X, then the expectation
vector of Zis
Z=AX (C.20)
and the covariance matrix of Zis
Z=AXA>: (C.21)
If, in addition, Ais an invertible nnmatrix and Xis a continuous random vector
with pdf fX, then the pdf of the continuous random vector Z=AXis given by
fZ(z)=fX(A 1z)
jdet(A)j;z2Rn; (C.22)
wherejdet(A)jdenotes the absolute value of the determinant of A.
Proof: We haveZ=EZ=E[AX]=AEX=AXand
Z=E[(Z Z)(Z Z)>]=E[A(X X)(A(X X))>]
=AE[(X X)(X X)>]A>
=AXA>:
ForAinvertible and Xcontinuous (as opposed to discrete), let z=Axandx=A 1z.
Consider the n-dimensional cube C=[z1;z1+h] [zn;zn+h]. Then,
P[Z2C]hnfZ(z);
by deÔ¨Ånition of the joint density of Z. Let Dbe the image of Cunder A 1‚Äî that is, all
points xsuch that Ax2C. Recall from Section A.1 that any matrix Blinearly transforms an +355
n-dimensional rectangle with volume Vinto an n-dimensional parallelepiped with volume
Vjdet(B)j. Thus, in addition to the above expression for P[Z2C], we also have
P[Z2C]=P[X2D]hnjdet(A 1)jfX(x)=hnjdet(A)j 1fX(x):
Equating these two expressions for P[Z2C], dividing both sides by hn, and letting hgo to
0, we obtain (C.22). 
For a generalization of the linear transformation rule (C.22), consider an arbitrary map-
ping x7!g(x), written out:
26666666666666664x1
x2
:::
xn377777777777777757!26666666666666664g1(x)
g2(x)
:::
gn(x)37777777777777775:Appendix C. Probability and Statistics 433
Theorem C.4: Transformation Rule
LetXbe an n-dimensional vector of continuous random variables with pdf fX. Let
Z=g(X), where gis an invertible mapping with inverse g 1andmatrix of Jacobi
Jg; that is, the matrix of partial derivatives of g. Then, at z=g(x) the random vector
Zhas pdf
fZ(z)=fX(x)
jdet(Jg(x))j=fX(g 1(z))jdet(Jg 1(z))j;z2Rn: (C.23)
Proof: For a Ô¨Åxed x, let z=g(x); and thus x=g 1(z). In the neighborhood of x, the
function gbehaves like a linear function, in the sense that g(x+)g(x)+Jg(x)for
small vectors ; see also Section B.1. Consequently, an inÔ¨Ånitesimally small n-dimensional +397
rectangle at xwith volume Vis transformed into an inÔ¨Ånitesimally small n-dimensional
parallelepiped at zwith volume Vjdet(Jg(x))j. Now, as in the proof of the linear case, let
Cbe a small cube around z=g(x) with volume hn. Let Dbe the image of Cunder g 1.
Then,
hnfZ(z)P[Z2C]hnjdet(Jg 1(z))jfX(x);
and sincejdet(Jg 1(z))j=1=jdet(Jg(x))j, (C.23) follows as hgoes to 0. 
Typically, in coordinate transformations it is g 1that is given ‚Äî that is, an expres-
sion for xas a function of z.
Example C.2 (Polar Transform) Suppose X;Yare independent and have standard nor-
mal distribution. The joint pdf is
fX;Y(x;y)=1
2e 1
2(x2+y2);(x;y)2R2:
In polar coordinates we have
X=Rcos and Y=Rsin; (C.24)
where R>0 is the radius and 2[0;2) the angle of the point ( X;Y). What is the joint pdf
ofRand? By the radial symmetry of the bivariate normal distribution, we would expect
to be uniform on (0 ;2). But what is the pdf of R? To work out the joint pdf, consider
the inverse transformation g 1, deÔ¨Åned by
"r
#
g 1
7 !"rcos
rsin#
="x
y#
:
The corresponding matrix of Jacobi is
Jg 1(r;)="cos rsin
sin rcos#
;434 C.7. Multivariate Normal Distribution
which has determinant r. Since x2+y2=r2(cos2+sin2)=r2, it follows by the trans-
formation rule (C.23) that the joint pdf of Randis given by
fR;(r;)=fX;Y(x;y)r=1
2e 1
2r2r; 2(0;2);r>0:
By integrating out andr, respectively, we Ô¨Ånd fR(r)=re r2=2andf()=1=(2). Since
fR;is the product of fRandf, the random variables Randare independent.
C.7 Multivariate Normal Distribution
The normal (or Gaussian) distribution ‚Äî especially its multidimensional version ‚Äî plays
a central role in data science and machine learning. Recall from Table C.1 that a random
variable Xis said to have a normal normal
distributiondistribution with parameters and2if its pdf is given
by
f(x)=1
p
2e 1
2(x 
)2
;x2R: (C.25)
We write XN(;2). The parameters and2are the expectation and variance of the
distribution, respectively. If =0 and=1 then
f(x)=1p
2e x2=2;
and the distribution is known as the standard normal standard
normaldistribution. The cdf of the standard
normal distribution is often denoted by and its pdf by '. In Figure C.5 the pdf of the
N(;2) distribution for various and2is plotted.
-4 -2 0 2 4 600.20.40.60.8
N(0,1)N(0,1/4)
N(2,1)
Figure C.5: The pdf of the N(;2) distribution for various and2.
We next consider some important properties of the normal distribution.
Theorem C.5: Standardization
LetXN(;2) and deÔ¨Åne Z=(X )=. Then Zhas a standard normal distribu-
tion.Appendix C. Probability and Statistics 435
Proof: The cdf of Zis given by
P[Z6z]=P[(X )=6z]=P[X6+z]
=Z+z
 11
p
2e 1
2(x 
)2
dx=Zz
 11p
2e y2=2dy= (z);
where we make a change of variable y=(x )=in the fourth equation. Hence, Z
N(0;1). 
The rescaling procedure in Theorem C.5 is called standardization standardization . It follows from The-
orem C.5 that any XN(;2) can be written as
X=+Z;where ZN(0;1):
In other words, any normal random variable can be viewed as an ane transformation affine
transformation‚Äî
that is, a linear transformation plus a constant ‚Äî of a standard normal random variable.
We now generalize this to ndimensions. Let Z1;:::; Znbe independent and standard
normal random variables. The joint pdf of Z=[Z1;:::; Zn]>is given by
fZ(z)=nY
i=11p
2e 1
2z2
i=(2) n
2e 1
2z>z;z2Rn: (C.26)
We write ZN(0;I), where Iis the identity matrix. Consider the a ne transformation
X=+BZ (C.27)
for some mnmatrix Bandm-dimensional vector . Note that, by (C.20) and (C.21), X +432
has expectation vector and covariance matrix =BB>:We say that Xhas a multivariate
normal multivariate
normalormultivariate Gaussian distribution with mean vector and covariance matrix .
We write XN(;).
The following theorem states that any a ne combination of independent multivariate
normal random variables is again multivariate normal.
Theorem C.6: A ne Transformation of Normal Random Vectors
LetX1;X2;:::; Xrbe independent mi-dimensional normal random vectors, with
XiN(i;i),i=1;:::; r. Then, for any n1 vector aandnmimatrices
B1;:::; Br,
a+rX
i=1BiXiN
a+rX
i=1Bii;rX
i=1BiiB>
i
: (C.28)
Proof: Denote the n-dimensional random vector in the left-hand side of (C.28) by Y. By
deÔ¨Ånition, each Xican be written as i+AiZi, where thefZigare independent (because the
fXigare independent), so that
Y=a+rX
i=1Bi(i+AiZi)=a+rX
i=1Bii+rX
i=1BiAiZi;436 C.7. Multivariate Normal Distribution
which is an a ne combination of independent standard normal random vectors. Hence, Y
is multivariate normal. Its expectation vector and covariance matrix can be found easily
from Theorem C.3.  +432
The next theorem shows that the distribution of a subvector of a multivariate normal
random vector is again normal.
Theorem C.7: Marginal Distributions of Normal Random Vectors
LetXN(;) be an n-dimensional normal random vector. Decompose X,, and
as
X="Xp
Xq#
;="p
q#
;="pr
>
rq#
; (C.29)
where pis the upper left ppcorner of andqis the lower right qqcorner of
. Then, XpN(p;p).
Proof: We give a proof assuming that is positive deÔ¨Ånite. Let BB>be the (lower)
Cholesky decomposition of . We can write +373
"Xp
Xq#
="p
q#
+"BpO
CrCq#
|     {z     }
B"Zp
Zq#
; (C.30)
where ZpandZqare independent p- and q-dimensional standard normal random vectors.
In particular, Xp=p+BpZp, which means that XpN(p;p), since BpB>
p=p.
By relabeling the elements of Xwe see that Theorem C.7 implies that anysubvector of
Xhas a multivariate normal distribution. For example, XqN(q;q).
The following theorem shows that not only the marginal distributions of a normal ran-
dom vector are normal, but also its conditional distributions .
Theorem C.8: Conditional Distributions of Normal Random Vectors
LetXN(;) be an n-dimensional normal random vector with det( )>0. IfX
is decomposed as in (C.29), then

XqjXp=xp
N(q+>
r 1
p(xp p);q >
r 1
pr): (C.31)
As a consequence, XpandXqareindependent if and only if they are uncorrelated ;
that is, if r=O(zero matrix).
Proof: From (C.30) we see that Xp=p+BpZpandXq=q+CrZp+CqZq. Consequently,
(XqjXp=xp)=q+CrB 1
p(xp p)+CqZq;
where Zqis aq-dimensional multivariate standard normal random vector. It follows that
Xqconditional on Xp=xphas aN(q+CrB 1
p(xp p);CqC>
q) distribution. The proof ofAppendix C. Probability and Statistics 437
(C.31) is completed by observing that >
r 1
p=CrB>
p(B>
p) 1B 1
p=CrB 1
p, and
q >
r 1
pr=CrC>
r+CqC>
q CrB 1
pr|{z}
BpC>r=CqC>
q:
IfXpandXqare independent, then they are obviously uncorrelated, as r=E[(Xp 
p)(Xq q)>]=E(Xp p)E(Xq q)>=O. Conversely, if r=O, then by (C.31) the
conditional distribution of Xqgiven Xpis the same as the unconditional distribution of Xq;
that is, N(q;q). In other words, Xqis independent of Xp. 
The next few results are about the relationships between the normal, chi-squared,
2distribution Student, and Fdistributions, deÔ¨Åned in Table C.1. Recall that the chi-squared family of
distributions, denoted by 2
n, are simply Gamma (n=2;1=2) distributions, where the para-
meter n2f1;2;3;:::gis called the degrees of freedom .
Theorem C.9: Relationship Between Normal and 2Distributions
IfXN(;) is an n-dimensional normal random vector with det( )>0, then
(X )> 1(X )2
n: (C.32)
Proof: LetBB>be the Cholesky decomposition of , where Bis invertible. Since Xcan
be written as +BZ, where Z=[Z1;:::; Zn]>is a vector of independent standard normal
random variables, we have
(X )> 1(X )=(X )>(BB>) 1(X )=Z>Z=nX
i=1Z2
i:
Using the independence of Z1;:::; Zn, the moment generating function of Y=Pn
i=1Z2
iis +427
given by
EesY=Ees(Z2
1++Z2
n)=E[esZ2
1esZ2
n]=
EesZ2n;
where ZN(0;1). The moment generating function of Z2is
EesZ2=Z1
 1esz21p
2e z2=2dz=1p
2Z1
 1e 1
2(1 2s)z2dz=1p
1 2s;
so thatEesY=
1
2=(1
2 s)n
2,s<1
2, which is the moment generating function of the
Gamma (n=2;1=2) distribution; that is, the 2
ndistribution ‚Äî see Example C.1. The res- +427
ult now follows from the uniqueness of the moment generating function. 
A consequence of Theorem C.9 is that if X=[X1;:::; Xn]>isn-dimensional standard
normal, then the squared length kXk2=X2
1++X2
nhas a2
ndistribution. If instead Xi
N(i;1),i=1;:::, thenkXk2is said to have a noncentral2
ndistribution noncentral 2
ndistribution. This distribution
depends on thefigonly through the norm kk. We writekXk22
n(), where=kkis
thenoncentrality parameter noncentrality
parameter.
Such distributions frequently occur when considering projections of multivariate nor-
mal random variables, as summarized in the following theorem.438 C.7. Multivariate Normal Distribution
Theorem C.10: Relationship Between Normal and Noncentral 2Distributions
LetXN(;In) be an n-dimensional normal random vector and let VkV mbe
linear subspaces of dimensions kandm, respectively, with k<m6n. Let Xkand
Xmbe orthogonal projections of XontoVkandVm, and letkandmbe the cor-
responding projections of . Then, the following holds.
1. The random vectors Xk,Xm Xk, and X Xmare independent.
2.kXkk22
k(kkk),kXm Xkk22
m k(km kk), andkX Xmk22
n m(k 
mk).
Proof: Letv1;:::; vnbe an orthonormal basis of Rnsuch that v1;:::; vkspansVkand
v1;:::; vmspansVm. By (A.8) we can write the orthogonal projection matrices onto Vj, +362
asPj=Pj
i=1viv>
i,j=k;m;n, whereVnis deÔ¨Åned as Rn. Note that Pnis simply the iden-
tity matrix. Let V:=[v1;:::; vn] and deÔ¨Åne Z:=[Z1;:::; Zn]>=V>X. Recall from Sec-
tion A.2 that any orthogonal transformation such as z=V>xislength preserving ; that is, +361
kzk=kxk.
To prove the Ô¨Årst statement of the theorem, note that V>Xj=V>PjX=[Z1;:::; Zj;
0;:::; 0]>,j=k;m. It follows that V>(Xm Xk)=[0;:::; 0;Zk+1;:::; Zm;0;:::; 0]>and
V>(X Xm)=[0;:::; 0;Zm+1;:::; Zn]>. Moreover, being a linear transformation of a nor-
mal random vector, Zis also normal, with covariance matrix V>V=In. In particular, the
fZigareindependent . This shows that Xk,Xm XkandX Xmare independent as well.
Next, observe that kXkk=kV>Xkk=kZkk, where Zk:=[Z1;:::; Zk]>. The latter vector
has independent components with variances 1, and its squared norm has therefore (by
deÔ¨Ånition) a 2
k() distribution. The noncentrality parameter is =kEZkk=kEXkk=kkk,
again by the length-preserving property of orthogonal transformations. This shows that
kXkk22
k(kkk). The distributions of kXm Xkk2andkX Xmk2follow by analogy. 
Theorem C.10 is frequently used in the statistical analysis of normal linear models ; see
Section 5.4. In typical situations lies in the subspace Vmor evenVk‚Äî in which case +182
kXm Xkk22
m kandkX Xmk22
n m, independently. The (scaled) quotient then turns
out to have an Fdistribution ‚Äî a consequence of the following theorem.
Theorem C.11: Relationship Between 2andFDistributions
LetU2
mandV2
nbe independent. Then,
U=m
V=nF(m;n):
Proof: For notational simplicity, let c=m=2 and d=n=2. The pdf of W=U=Vis
given by fW(w)=R1
0fU(wv)v fV(v) dv. Substituting the pdfs of the corresponding GammaAppendix C. Probability and Statistics 439
distributions, we have
fW(w)=Z1
0(wv)c 1e wv=2
 (c) 2cvvd 1e v=2
 (d) 2ddv=wc 1
 (c) (d) 2c+dZ1
0vc+d 1e (1+w)v=2dv
= (c+d)
 (c) (d)wc 1
(1+w)c+d;
where the last equality follows from the fact that the integrand is equal to  () times
the density of the Gamma (;) distribution with =c+dand=(1+w)=2. The density
ofZ=n
mU
Vis given by
fZ(z)=fW(z m=n)m=n:
The proof is completed by comparing the resulting expression with the pdf of the Fdistri-
bution given in Table C.1.  +425
Corollary C.1 (Relationship Between Normal, 2, and tDistributions) LetZN(0;1)
andV2
nbe independent. Then,
ZpV=ntn:
Proof: LetT=Z=pV=n. Because Z22
1, we have by Theorem C.11 that T2F(1;n).
The result follows now from the symmetry around 0 of the pdf of Tand the fact that the
square of a tnrandom variable has an F(1;n) distribution. 
C.8 Convergence of Random Variables
Recall that a random variable Xis a function from 
toR. If we have a sequence of random
variables X1;X2;:::(for instance, Xn(!)=X(!)+1
nfor each!2
), then one can consider
the pointwise convergence:
lim
n!1Xn(!)=X(!);for all!2
;
in which case we say that X1;X2;:::converges surely sure
convergencetoX. A more interesting type of
convergence uses the probability measure Passociated with X.
DeÔ¨Ånition C.1: Convergence in Probability
The sequence of random variables X1;X2;::: converges in probability to a random
variable Xif, for all">0,
lim
n!1P[jXn Xj>"]=0:
We denote the convergence in probability convergence in
probabilityasXnP !X:440 C.8. Convergence of Random Variables
Convergence in probability refers only to the distribution of Xn. Instead, if the sequence
X1;X2;:::is deÔ¨Åned on a common probability space, then we can consider the following
mode of convergence that uses the joint distribution of the sequence of random variables.
DeÔ¨Ånition C.2: Almost Sure Convergence
The sequence of random variables X1;X2;::: converges almost surely to a random
variable Xif for every ">0
lim
n!1P"
sup
k>njXk Xj>"#
=0:
We denote the almost sure convergence almost sure
convergenceasXna:s: !X.
Note that in accordance with these deÔ¨Ånitions Xna:s: !0 is equivalent to supk>njXkjP !0.
Example C.3 (Convergence in Probability Versus Almost Sure Convergence) Since
the eventfjXn Xj>"gis contained infsupk>njXk Xj>"g, we can conclude that almost
sure convergence implies convergence in probability. However, the converse is not true in
general. For instance, consider the iid sequence X1;X2;:::with marginal distribution
P[Xn=1]=1 P[Xn=0]=1=n:
Clearly, XnP !0. However, for "<1 and any n=1;2;:::we have,
P"
sup
k>njXkj6"#
=P[Xn6";Xn+16";::: ]
=P[Xn6"]P[Xn+16"] (using independence)
=lim
m!1mY
k=nP[Xk6"]=lim
m!1mY
k=n 
1 1
k!
=lim
m!1n 1
nn
n+1m 1
m=0:
It follows that P[supk>njXk 0j>"]=1 for any 0<"< 1 and all n>1. In other words, it
isnottrue that Xna:s: !0.
Another important type of convergence is useful when we are interested in estimating
expectations or multidimensional integrals via Monte Carlo methodology. +67
DeÔ¨Ånition C.3: Convergence in Distribution
The sequence of random variables X1;X2;::: is said to converge in distribution to a
random variable Xwith distribution function FX(x)=P[X6x] provided that:
lim
n!1P[Xn6x]=FX(x) for all xsuch that lim
a!xFX(a)=FX(x): (C.33)
We denote the convergence in distribution convergence in
distributionas either Xnd !X, orXnd !FX.Appendix C. Probability and Statistics 441
The generalization to random vectors replaces (C.33) with
lim
n!1P[Xn2A]=P[X2A] for all ARnsuch thatP[X2@A]=0; (C.34)
where@Adenotes the boundary of the set A.
A useful tool for demonstrating convergence in distribution is the characteristic func-
tion Xof a random vector X, deÔ¨Åned as the expectation:characteristic
function
+225  X(t) :=Eeit>X;t2Rn: (C.35)
The moment generating function in (C.5) is a special case of the characteristic function
evaluated at t= is. Note that while the moment generating function of a random variable
may not exist, its characteristic function always exists. The characteristic function of a
random vector Xfis closely related to the Fourier transform of its pdf f. +390
Example C.4 (Characteristic Function of a Multivariate Gaussian Random Vector)
The density of the multivariate standard normal distribution is given in (C.26) and thus the
characteristic function of ZN(0;In) is
 Z(t)=Eeit>Z=(2) n=2Z
Rneit>z 1
2kzk2dz
=e ktk2=2(2) n=2Z
Rne 1
2kz it>k2dz=e ktk2=2;t2Rn:
Hence, the characteristic function of the random vector X=+BZin (C.27) with mul-
tivariate normal distribution N(;) is given by +435
 X(t)=Eeit>X=Eeit>(+BZ)
=eit>Eei(B>t)>Z=eit> Z(B>t)
=eit> kB>tk2=2=eit> t>t=2:
The importance of the characteristic function is mainly derived from the following
result, for which a proof can be found, for example, in [11].
Theorem C.12: Characteristic Function
Suppose that  X1(t); X2(t);:::are the characteristic functions of the sequence of
random vectors X1;X2;:::and X(t) is the characteristic function of X. Then, the
following three statements are equivalent:
1. lim n!1 Xn(t)= X(t) for all t2Rn.
2.Xnd !X.
3. lim n!1Eh(Xn)=Eh(X) for all bounded continuous functions h:Rd7!R.442 C.8. Convergence of Random Variables
Example C.5 (Convergence in Distribution) DeÔ¨Åne the random variables Y1;Y2;:::
as
Yn:=nX
k=1Xk 1
2!k
; n=1;2;:::;
where X1;X2;:::iidBer(1=2). We now show that Ynd !U(0;1). First, note that
Eexp(i tYn)=nY
k=1Eexp(i tXk=2k)=2 nnY
k=1(1+exp(i t=2k)):
Second, from the collapsing product, (1  exp(i t=2n))Qn
k=1(1+exp(i t=2k))=1 exp(i t),
we have
Eexp(i tYn)=(1 exp(i t))1=2n
1 exp(i t=2n):
It follows that lim n!1Eexp(i tYn)=(exp(i t) 1)=(it);which we recognize as the charac-
teristic function of the U(0;1) distribution. +441
Yet another mode of convergence is the following.
DeÔ¨Ånition C.4: Convergence in Lp-norm
The sequence of random variables X1;X2;::: converges in Lp-norm to a random
variable Xif
lim
n!1EjXn Xjp=0;p>1:
We denote the convergence in Lp-norm convergence in
Lp-normasXnLp
 !X.
The case for p=2 corresponds to convergence in mean squared error. The following
example illustrates that convergence in Lp-norm is qualitatively di erent from convergence
in distribution.
Example C.6 (Comparison of Modes of Convergence) DeÔ¨Åne Xn:=1 X, where X
has a uniform distribution on the interval (0,1). Clearly, Xnd !U(0;1). However, EjXn 
Xj !Ej1 2Xj=1=2 and so the sequence does not converge in L1-norm. In addition,
P[jXn Xj>"] !1 ",0 and so Xndoes not converge in probability as well.
Thus, in general Xnd !Ximplies neither XnP !X, nor XnL1
 !X.
We mention, however, that if Xnd !cfor some constant c, then XnP !cas well. To
see this, note that Xnd !cstands for
lim
n!1P[Xn6x]=8>><>>:1;x>c
0;x<c:
In other words, we can write:
P[jXn cj>"]61 P[Xn6c+"]+P[Xn6c "] !1 1+0=0;n!1;
which shows that XnP !cby deÔ¨Ånition.Appendix C. Probability and Statistics 443
DeÔ¨Ånition C.5: Complete Convergence
The sequence of random variables X1;X2;::: is said to converge completely toXif
for all">0 X
nP[jXn Xj>"]<1:
We denote the complete convergence complete
convergenceasXncpl: !X.
Example C.7 (Complete and Almost Sure Convergence) We show that complete
convergence implies almost sure convergence. We can bound the criterion for almost sure
convergence as follows:
P[sup
k>njXk Xj>"]=P[[k>nfjXk Xj>"g]
6X
k>nP[jXk Xj>"] by union bound in (C.1)
61X
k=1P[jXk Xj>"]
|                  {z                  }
=c<1from Xncpl. !X n 1X
k=1P[jXk Xj>"]
6c n 1X
k=1P[jXk Xj>"] !c c=0; n!1:
Hence, by deÔ¨Ånition Xna:s: !X.
The next theorem shows how the di erent types of convergence are related to each
other. For example, in the diagram below, the notationp>q)means that Lp-norm convergence
implies Lq-norm convergence under the assumption that p>q>1.
Theorem C.13: Modes of Convergence
The most general relationships among the various modes of convergence for numer-
ical random variables are shown on the following hierarchical diagram:
Xncpl. !X)Xna:s: !X
+
XnP !X)Xnd !X
*
XnLp
 !Xp>q)XnLq
 !X:444 C.8. Convergence of Random Variables
Proof: 1.First, we show that XnP !X)Xnd !Xusing the inequality P[A\B]6P[A]
for any event B. To this end, consider the distribution function FXofX:
FXn(x)=P[Xn6x]=P[Xn6x;jXn Xj>"]+P[Xn6x;jXn Xj6"]
6P[jXn Xj>"]+P[Xn6x;X6Xn+"]
6P[jXn Xj>"]+P[X6x+"]:
Now, in the arguments above we can switch the roles of XnandX(there is a symmetry) to
deduce the analogous result: FX(x)6P[jX Xnj>"]+P[Xn6x+"]. Therefore, making
the switch x!x "gives FX(x ")6P[jX Xnj> "]+FXn(x). Putting it all together
gives:
FX(x ") P[jX Xnj>"]6FXn(x)6P[jXn Xj>"]+FX(x+"):
Taking n!1 on both sides yields for any ">0:
FX(x ")6lim
n!1FXn(x)6FX(x+"):
Since FXis continuous at xby assumption we can take "#0 to conclude that
lim n!1FXn(x)=FX(x).
2.Second, we show that XnLp
 ! X)XnLq
 ! Xforp>q>1. Since the function
f(x)=xq=pis concave for q=p61, Jensen‚Äôs inequality yields: +62
(EjXjp)q=p=f(EjXjp)>Ef(jXjp)=EjXjq:
In other words, ( EjXn Xjq)1=q6(EjXn Xjp)1=p !0, proving the statement of the theorem.
3.Third, we show that XnL1
 !X)XnP !X. First note that for any random variable
Y, we can write: EjYj>E[jYj1fjYj>"g]>E[j"j1fjYj>"g]="P[jYj> "]:Therefore, we obtain
Chebyshev‚Äôs inequality Chebyshev ‚Äôs
inequality:
P[jYj>"]6EjYj
": (C.36)
Using Chebyshev‚Äôs inequality and XnL1
 !X, we can write
P[jXn Xj>"]6EjXn Xj
" !0;n!1:
Hence, by deÔ¨Ånition XnP !X.
4.Finally, Xncpl. !X)Xna:s: !X)XnP !Xis proved in Examples C.7 and C.3. 
Finally, we will make use of the following theorem.
Theorem C.14: Slutsky
Letg(x;y) be a continuous scalar function of vectors xandy. Suppose that Xnd !X
andYnP !cfor some Ô¨Ånite constant c. Then,
g(Xn;Yn)d !g(X;c):Appendix C. Probability and Statistics 445
Proof: We prove the theorem for scalar XandY. The proof for random vectors is analog-
ous. First, we show that Zn:="Xn
Yn#
d !"X
c#
=:Zusing, for example, Theorem C.12. In +441
other words, we wish to show that the characteristic function of the joint distribution of Xn
andYnconverges pointwise as n!1 :
 Xn;Yn(t)=Eei(t1Xn+t2Yn) !eit2cEeit1X= X;c(t);8t2R2:
To show the limit above, consider
j Xn;Yn(t)  X;c(t)j6j Xn;c(t)  X;c(t)j+j Xn;Yn(t)  Xn;c(t)j
=jeit2cE(eit1Xn eit1X)j+jEei(t1Xn+t2c)(eit2(Yn c) 1)j
6jeit2cjjE(eit1Xn eit1X)j+Ejei(t1Xn+t2c)jjeit2(Yn c) 1j
6j Xn(t1)  X(t1)j+Ejeit2(Yn c) 1j:
Since Xnd !X, Theorem C.12 implies that  Xn(t1) ! X(t1), and the Ô¨Årst term j Xn(t1) 
 X(t1)jgoes to zero. For the second term we use the fact that
jeix 1j=Rx
0i eid6Rx
0ji eijd=jxj;x2R
to obtain the bound:
Ejeit2(Yn c) 1j=Ejeit2(Yn c) 1j1fjYn cj>"g+Ejeit2(Yn c) 1j1fjYn cj6"g
62E1fjYn cj>"g+Ejt2(Yn c)j1fjYn cj6"g
62P[jYn cj>"]+jt2j" !j t2j"; n!1:
Since"is arbitrary, we can let "#0 to conclude that lim n!1j Xn;Yn(t)  X;c(t)j=0. In other
words, Znd !Z, and by the continuity of g, we have g(Zn)d !g(Z) org(Xn;Yn)d !
g(X;c). 
Example C.8 (Necessity of Slutsky‚Äôs Condition) The condition that Ynconverges in
probability to a constant cannot be relaxed. For example, suppose that g(x;y)=x+y,
Xnd !XN(0;1) and Ynd !YN(0;1). Then, our intuition tempts us to incorrectly
conclude that Xn+Ynd !N(0;2). This intuition is false, because we can have Yn= Xn
for all nso that Xn+Yn=0, while both XandYhave the same marginal distribution (in
this case standard normal).
C.9 Law of Large Numbers and Central Limit Theorem
Two main results in probability are the law of large numbers andthe central limit theorem .
Both are limit theorems involving sums of independent random variables. In particular,
consider a sequence X1;X2;:::of iid random variables with Ô¨Ånite expectation and Ô¨Ånite
variance2. For each ndeÔ¨Åne Xn:=(X1++Xn)=n. What can we say about the (random)
sequence of averages X1;X2;X3;:::? By (C.12) and (C.14) we have EXn=andVarXn= +429
2=n:Hence, as nincreases, the variance of the (random) average Xngoes to 0. This means446 C.9. Law of Large Numbers and Central Limit Theorem
that by DeÔ¨Ånition C.8, the average Xnconverges to inL2-norm as n!1 , that is, XnL2
 !
.
In fact, to obtain convergence in probability the variance need not be Ô¨Ånite ‚Äî it is
sucient to assume that =EX<1.
Theorem C.15: Weak Law of Large Numbers
law of large
numbersIfX1;:::; Xnare iid with Ô¨Ånite expectation , then for all ">0
lim
n!1Ph
jXn j>"i
=0:
In other words, XnP !.
The theorem has a natural generalization for random vectors. Namely, if =EX<1,
thenPh
kXn k>"i
!0, wherekkis the Euclidean norm. We give a proof in the scalar +355
case.
Proof: LetZk:=Xk for all k, so thatEZ=0. We thus need to show that ZnP !0.
We use the properties of the characteristic function of Zdenoted as  Z. Due to the iid +441
assumption, we have
 Zn(t)=EeitZn=EnY
i=1eitZi=n=nY
i=1EeiZit=n=nY
i=1 Z(t=n)=[ Z(t=n)]n: (C.37)
An application of Taylor‚Äôs Theorem B.1 in the neighborhood of t=0 yields
 Z(t=n)= Z(0)+o(t=n):
Since Z(0)=1, we have:
 Zn(t)=[ Z(t=n)]n=[1+o(1=n)]n !1;n!1:
The characteristic function of a random variable that always equals zero is 1. Therefore,
Theorem C.12 implies that Znd !0. However, according to Example C.6, convergence in
distribution to a constant implies convergence in probability. Hence, ZnP !0.
There is also a stronger version of this theorem, as follows.
Theorem C.16: Strong Law of Large Numbers
strong law of
large numbersIfX1;:::; Xnare iid with expectation andEX2<1, then for all ">0
lim
n!1P"
sup
k>njXk j>"#
=0:
In other words, Xna:s: !.Appendix C. Probability and Statistics 447
Proof: First, note that any random variable Xcan be written as the di erence of two non-
negative random variables: X=X+ X , where X+:=maxfX;0gandX := minfX;0g.
Thus, without loss of generality, we assume that the random variables in the theorem above
are nonnegative.
Second, from the sequence fX1;X2;X3;:::gwe can pick up the subsequence fX1;X4;X9;
X16;:::g=:fXj2g. Then, from Chebyshev‚Äôs inequality (C.36) and the iid condition, we have
1X
j=1PhXj2 >"i
6VarX
"21X
j=11
j2<1:
Therefore, by deÔ¨Ånition Xn2cpl: !and from Theorem C.13 we conclude that Xn2a:s: !.
Third, for any arbitrary n, we can Ô¨Ånd a k, say k=bpnc, so that k26n6(k+1)2:For
such a kand nonnegative X1;X2;:::, it holds that
k2
(k+1)2Xk26Xn6X(k+1)2(k+1)2
k2:
Since Xk2andX(k+1)2converge almost surely to ask(and hence n) goes to inÔ¨Ånity, we
conclude that Xna:s: !. 
Note that the condition EX2<1in Theorem C.16 can be weakened to EjXj<1and
the iid condition on the variables X1;:::; Xncan be relaxed to mere pairwise independence.
The corresponding proof, however, is signiÔ¨Åcantly more di cult.
TheCentral Limit Theorem Central Limit
Theoremdescribes the approximate distribution of Xn, and it applies
to both continuous and discrete random variables. Loosely, it states that
the average of a large number of iid random variables
approximately has a normal distribution.
SpeciÔ¨Åcally, the random variable Xnhas a distribution that is approximately normal, with
expectation and variance 2=n.
Theorem C.17: Central Limit Theorem
IfX1;:::; Xnare iid with Ô¨Ånite expectation and Ô¨Ånite variance 2, then for all
x2R,
lim
n!1P266664Xn 
=pn6x377775= (x);
where is the cdf of the standard normal distribution.
Proof: LetZk:=(Xk )=for all k, so thatEZ=0 andEZ2=1. We thus need to show
thatpnZnd !N(0;1). We again use the properties of the characteristic function. Let  Z +441
be the characteristic function of an iid copy of Z, then due to the iid assumption a similar
calculation to the one in (C.37) yields:
 pnZn(t)=EeitpnZn=[ Z(t=pn)]n:448 C.9. Law of Large Numbers and Central Limit Theorem
An application of Taylor‚Äôs Theorem B.1 in the neighborhood of t=0 yields
 Z(t=pn)=1+tpn 0
Z(0)+t2
2n 00
Z(0)+o(t2=n):
Since 0
Z(0)=Ed
dteitZt=0=iEZ=0 and 00
Z(0)=i2EZ2= 1, we have:
 pnZn(t)=h
 Z(t=pn)in="
1 t2
2n+o(1=n)#n
 !e t2=2;n!1:
From Example C.4, we recognize e t2=2as the characteristic function of the standard normal
distribution. Thus, from Theorem C.12 we conclude thatpnZnd !N(0;1).
Figure C.6 shows the central limit theorem in action. The left part shows the pdfs of
X1;2X2;:::; 4X4for the case where the fXighave a U[0;1] distribution. The right part
shows the same for the Exp(1) distribution. In both cases, we clearly see convergence to a
bell-shaped curve, characteristic of the normal distribution.
0 1 2 3 400.20.40.60.81
0 2 4 6 800.20.40.60.81
Figure C.6: Illustration of the central limit theorem for (left) the uniform distribution and
(right) the exponential distribution.
The multivariate version of the central limit theorem is the basis for many asymptotic
(in the size of the training set) results in machine learning and data science.
Theorem C.18: Multivariate Central Limit Theorem
LetX1;:::; Xnbe iid random vectors with expectation vector and Ô¨Ånite covariance
matrix . DeÔ¨Åne Xn:=(X1++Xn)=n. Then,
pn(Xn )d !N(0;) as n!1:
One application is as follows. Suppose that a parameter of interest, , is the unique
solution of the system of equations E (Xj)=0, where is a vector-valued (or multi-
valued) function and the distribution of Xdoes not depend on . An M-estimator M-estimator of,Appendix C. Probability and Statistics 449
denoted bn, is the solution to the system of equations that results from approximating the
expectation with respect to Xusing an average of niid copies of X:
 n() :=1
nnX
i=1 (Xij):
Thus, n(bn)=0.
Theorem C.19: M-estimator
The M-estimator is asymptotically normal as n!1 :
pn(bn )d !N(0;A 1BA >); (C.38)
where A:= E@ 
@(Xj) +398 and B:=E (Xj) (Xj)>is the covariance matrix
of (Xj).
Proof: We give a proof under the simplifying assumption3thatbnis a unique root, that is,
for anyand", there exists a >0 such thatkbn k>"implies thatk n()k>.
First, we argue that bnP !; that is,P[kbn k> "]!0. From the multivariate
extension of Theorem C.15, we have that
 n()P !E n()=E (Xj)=0:
Therefore, using the uniqueness of bn, we can show that bnP !via the bound:
Ph
kbn k>"i
6Ph
k n()k>i
=Ph
k n() E n()k>i
!0;n!1:
Second, we take a Taylor expansion of each component of the vector  n(bn) aroundto
obtain:
 n(bn)= n()+Jn(0)(bn );
where Jn() is the Jacobian of  nat, and0lies on the line segment joining bnand.
Rearrange the last equation and multiply both sides bypnA 1to obtain:
 A 1Jn(0)pn(bn )=A 1pn n():
By the central limit theorem,pn () converges in distribution to N(0;B). Therefore,
 A 1Jn(0)pn(bn )d !N(0;A 1BA >):
Theorem C.15 (the weak law of large numbers) applied to the iid random matrices
f@
@ (Xij)gshows that
Jn()P !E@
@ (Xj):
Moreover, since bnP !andJnis continuous in , we have that Jn(0)P !  A. Therefore,
by Slutsky‚Äôs theorem,  A 1Jn(0)pn(bn ) pn(bn )P !0.  +444
3The result holds under far less stringent assumptions.450 C.9. Law of Large Numbers and Central Limit Theorem
Finally, we mention Laplace‚Äôs approximation Laplace ‚Äôs
approximation, which shows how integrals or expecta-
tions behave under the normal distribution with a vanishingly small variance.
Theorem C.20: Laplace‚Äôs Approximation
Suppose that n!, wherelies in the interior of the open set Rpand that
nis appcovariance matrix such that n!. Let g:7!Rbe a continuous
function with g(),0. Then, as n!1 ,
np=2Z
g() e n
2( n)> 1
n( n)d!g()p
j2j: (C.39)
Proof: (Sketch for a bounded domain .) The left-hand side of (C.39) can be written as
the expectation with respect to the N(n;n=n) distribution:
p
j2njZ
g()exp
 n
2( n)> 1
n( n)
j2n=nj1=2d=p
j2njE[g(Xn)1fXn2g];
where XnN(n;n=n). Let ZN(0;I). Then,n+1=2
nZ=pnhas the same distribution
asXnand
n+1=2
nZ=pn
!asn!1 . By continuity of g()1f2gin the interior
of, asn!1 :4
E[g(Xn)1fXn2g]=E
g
n+1=2
nZpn
1
n+1=2
nZpn
2
 !g()1f2g:
Sincelies in the interior of , we have 1f2g=1, completing the proof. 
As an application of Theorem C.20 we can show the following.
Theorem C.21: Approximation of Integrals
Suppose that r:7!Ris twice continuously di erentiable with a unique global
minimum at andg:7!Ris continuous with g()>0. Then, as n!1 ,
lnZ
Rpg() e n r()d' n r() p
2lnn: (C.40)
More generally, if rnhas a unique global minimum nandrn!r)n!, then
lnZ
Rpg() e n rn()d' n r() p
2lnn:
Proof: We only sketch the proof of (C.40). Let H() be the Hessian matrix of rat. By +400
Taylor‚Äôs theorem we can write
r() r()=( )>@r()
@| {z }
=0+1
2( )>H()( );
4We can exchange the limit and expectation, as g()1f2g6max2g() andR
max2g() d=
jjmax2g()<1.Appendix C. Probability and Statistics 451
whereis a point that lies on the line segment joining and. Sinceis a unique
global minimum, there must be a small enough neighborhood of , say , such that ris a
strictly (also known as strongly) convex function on . In other words, H() is a positive +403
deÔ¨Ånite matrix for all 2and there exists a smallest positive eigenvalue 1>0 such
thatx>H()x>1kxk2for all x. In addition, since the maximum eigenvalue of H() is a
continuous function of 2andis bounded, there must exist a constant 2> 1such
thatx>H()x62kxk2for all x. In other words, denoting r:=r(), we have the bounds:
 2
2k k26 (r() r)6 1
2k k2;2:
Therefore,
e n rZ
g() e n2
2k k2d6Z
g() e n r()d6e n rZ
g() e n1
2k k2d:
An application of Theorem C.20 yieldsR
g() e n r()d=O(e n r=np=2) and, more import-
antly,
lnZ
g() e n r()d' n r p
2lnn:
Thus, the proof will be complete once we show thatR
g() e n r()d;with:=Rpn, is
asymptotically negligible compared toR
g() e n r()d. Sinceis a global minimum that
lies outside any neighborhood of , there must exists a constant c>0 such that r() r>c
for all2. Therefore,
Z
g() e n r()d=e (n 1)rZ
g() e r()e (n 1)(r() r)d
6e (n 1)rZ
g() e r()e (n 1)cd
6e (n 1)(r+c)Z
Rpg() e r()d=O(e n(r+c)):
The last expression is of order o(e n r=np=2), concluding the proof. 
C.10 Markov Chains
DeÔ¨Ånition C.6: Markov Chain
AMarkov chain Markov chain is a collectionfXt;t=0;1;2;:::gof random variables (or ran-
dom vectors) whose futures are conditionally independent of their pasts given their
present values. That is,
P[Xt+12AjXs;s6t]=P[Xt+12AjXt] for all t: (C.41)
In other words, the conditional distribution of the future variable Xt+1, given the entire
pastfXs;s6tg, is the same as the conditional distribution of Xt+1given only the present Xt.
Property (C.41) is called the Markov property Markov
property.452 C.10. Markov Chains
The index tinXtis usually seen as a ‚Äútime‚Äù or ‚Äústep‚Äù parameter. The index set
f0;1;2;:::gin the deÔ¨Ånition above was chosen out of convenience. It can be replaced by any
countable index set. We restrict ourselves to time-homogeneous time-
homogeneousMarkov chains ‚Äî Markov
chains for which the conditional pdfs fXt+1jXt(yjx) do not depend on t; we abbreviate these
asq(yjx). Thefq(yjx)gare called the (one-step) transition densities transition
densityof the Markov chain.
Note that the random variables or vectors fXtgmay be discrete (e.g., taking values in some
setf1;:::; rg) orcontinuous (e.g., taking values in an interval [0 ;1] orRd). In particular, in
thediscrete case, each q(yjx) is a probability: q(yjx)=P[Xt+1=yjXt=x].
The distribution of X0is called the initial distribution initial
distributionof the Markov chain. The one-
step transition densities and the initial distribution completely specify the distribution of
the random vector [ X0;X1;:::; Xt]>. Namely, we have by the product rule (C.17) and the +431
Markov property that the joint pdf is given by
fX0;:::;Xt(x0;:::; xt)=fX0(x0)fX1jX0(x1jx0)fXtjXt 1;:::;X0(xtjxt 1;:::; x0)
=fX0(x0)fX1jX0(x1jx0)fXtjXt 1(xtjxt 1)
=fX0(x0)q(x1jx0)q(x2jx1)q(xtjxt 1):
A Markov chain is said to be ergodic ergodic if the probability distribution of Xtconverges to
a Ô¨Åxed distribution as t!1 . Ergodicity is a property of many Markov chains. Intuitively,
the probability of encountering the Markov chain in a state xat a time tfar into the future
should not depend on the t, provided that the Markov chain can reach every state from any
other state ‚Äî such Markov chains are said to be irreducible ‚Äî and does not ‚Äúescape‚Äù to
inÔ¨Ånity. Thus, for an ergodic Markov chain the pdf fXt(x) converges to a Ô¨Åxed limiting pdf limiting pdf
f(x) ast!1 , irrespective of the starting state. For the discrete case, f(x) corresponds to
the long-run fraction of times that the Markov process visits x.
Under mild conditions (such as irreducibility) the limiting pdf f(x) can be found by
solving the global balance equations global balance
equations:
f(x)=8>><>>:P
yf(y)q(xjy) (discrete case) ;R
f(y)q(xjy) dy (continuous case) :(C.42)
For the discrete case the rationale behind this is as follows. Since f(x) is the long-run
proportion of time that the Markov chain spends in x, the proportion of transitions out of
xisf(x). This should be balanced with the proportion of transitions intostate x, which isP
yf(y)q(xjy).
One is often interested in a stronger type of balance equations. Imagine that we have
taken a video of the evolution of the Markov chain, which we may run in forward and
reverse time. If we cannot determine whether the video is running forward or backward
(we cannot determine any systematic ‚Äúlooping‚Äù, which would indicate in which direction
time is Ô¨Çowing), the chain is said to be time-reversible or simply reversible reversible .
Although not every Markov chain is reversible, each ergodic Markov chain, when run
backwards, gives another Markov chain ‚Äî the reverse Markov chain reverse
Markov chain‚Äî with transition
densities eq(yjx)=f(y)q(xjy)=f(x). To see this, Ô¨Årst observe that f(x) is the long-run
proportion of time spent in xfor both the original and reverse Markov chain. Secondly,
the ‚Äúprobability Ô¨Çux‚Äù from xtoyin the reversed chain must be equal to the probability
Ô¨Çux from ytoxin the original chain, meaning f(x)eq(yjx)=f(y)q(xjy), which yields theAppendix C. Probability and Statistics 453
stated transition probabilities for the reversed chain. In particular, for a reversible Markov
chain we have
f(x)q(yjx)=f(y)q(xjy) for all x;y: (C.43)
These are the detailed (or local) balance equations . Note that the detailed balance equa-local balance
equations tions imply the global balance equations. Hence, if a Markov chain is irreducible and there
exists a pdf such that (C.43) holds, then f(x) must be the limiting pdf. In the discrete state
space case an additional condition is that the chain must be aperiodic aperiodic , meaning that the
return times to the same state cannot always be a multiple of some integer >2.
Example C.9 (Random Walk on a Graph) Consider a Markov chain that performs a
‚Äúrandom walk‚Äù on the graph in Figure C.7, at each step jumping from the current vertex
(node) to one of the adjacent vertices, with equal probability. Clearly this Markov chain is
reversible. It is also irreducible and aperiodic. Let f(x) denote the limiting probability that
the chain is in vertex x. By symmetry, f(1)=f(2)=f(7)=f(8),f(4)=f(5) and f(3)=
f(6). Moreover, by the detailed balance equations, f(4)=5=f(1)=3, and f(3)=4=f(1)=3.
It follows that f(1)++f(8)=4f(1)+25=3f(1)+24=3f(1)=10f(1)=1, so
that f(1)=1=10,f(3)=2=15, and f(4)=1=6.
1
234
567
8
Figure C.7: The random walk on this graph is reversible.
C.11 Statistics
Statistics deals with the gathering, summarization, analysis, and interpretation of data. The
two main branches of statistics are:
1.Classical or frequentist statistics frequentist
statistics: Here the observed data is viewed as the out-
come of random data Tdescribed by a probabilistic model ‚Äî usually the model is
speciÔ¨Åed up to a (multidimensional) parameter; that is, T g(j) for some. The
statistical inference is then purely concerned with the model and in particular with
the parameter . For example, on the basis of the data one may wish to
(a) estimate the parameter,
(b) perform statistical tests on the parameter, or454 C.12. Estimation
(c) validate the model.
2.Bayesian statistics Bayesian
statistics: In this approach we average over all possible values of the
parameterusing a user-speciÔ¨Åed weight function g() and obtain the model
TR
g(j)g() d. For practical computations, this means that we can treat as a
random variable with pdf g(). Bayes‚Äô formula g(j)/g(j)g() is used to learn +47
based on the observed data .
Example C.10 (Iid Sample) The most fundamental statistical model is where the data
T=X1;:::; Xnis such that the random variables X1;:::; Xnare assumed to be independent
and identically distributed:
X1;:::; XniidDist;
according to some known or unknown distribution Dist. An iid sample is often called a
random sample random sample in the statistics literature. Note that the word ‚Äúsample‚Äù can refer to both a
collection of random variables and to a single random variable. It should be clear from the
context which meaning is being used.
Often our guess or model for the true distribution is speciÔ¨Åed up to an unknown para-
meter, with2. The most common model is:
X1;:::; XniidN(;2);
in which case =(;2) and  =RR+.
C.12 Estimation
Suppose the model g(j) for the dataTis completely speciÔ¨Åed up to an unknown para-
meter vector . The aim is to estimate on the basis of the observed data only (an altern-
ative goal could be to estimate = () for some vector-valued function  ). SpeciÔ¨Åcally,
the goal is to Ô¨Ånd an estimator T=T(T) that is close to the unknown . The correspond-estimatoring outcome t=T() is the estimate of. The bias of an estimator Tofis deÔ¨Åned asestimate
bias ET . An estimator Tofis said to be unbiased ifET=. We often write bfor both
an estimator and estimate of . The mean squared error (MSE) of a real-valued estimatormean squared
error Tis deÔ¨Åned as
MSE =E(T )2:
An estimator T1is said to be more ecient efficient than an estimator T2if the MSE of T1is smaller
than the MSE of T2. The MSE can be written as the sum
MSE =(ET )2+VarT:
The Ô¨Årst term measures the unbiasedness and the second is the variance of the estimator.
In particular, for an unbiased estimator the MSE of an estimator is simply equal to its
variance.
For simulation purposes it is often important to include the running time of the estim-
ator in e ciency comparisons. One way to compare two unbiased estimators T1andT2is
to compare their relative time variance products relative time
variance
products,
riVarTi
(ETi)2;i=1;2; (C.44)Appendix C. Probability and Statistics 455
where r1andr2are the times required to calculate the estimators T1andT2, respectively.
In this scheme, T1is considered more e cient than T2if its relative time variance product
is smaller. We discuss next two systematic approaches for constructing sound estimators.
C.12.1 Method of Moments
Suppose x1;:::; xnare outcomes from an iid sample X1;:::; Xniidg(xj), where=
[1;:::; k]>is unknown. The moments of the sampling distribution can be easily estim-
ated. Namely, if Xg(xj), then the r-th moment of X, that isr()=EXr(assuming
it exists), can be estimated through the sample r-th moment :1
nPn
i=1xr
i:Themethod of mo-sample r-th
moment ments involves choosing the estimate bofsuch that each of the Ô¨Årst ksample and true
method of
momentsmoments are matched:
1
nnX
i=1xr
i=r(b);r=1;2;:::; k:
In general, this set of equations is nonlinear and so its solution often has to be found
numerically.
Example C.11 (Sample Mean and Sample Variance) Suppose the data is given by
T=fX1;:::; Xng, where thefXigform an iid sample from a general distribution with mean
and variance 2<1. Matching the Ô¨Årst two moments gives the set of equations
1
nnX
i=1xi=;
1
nnX
i=1x2
i=2+2:
The method of moments estimates for and2are therefore the sample mean sample mean
b=x=1
nnX
i=1xi; (C.45)
and
c2=1
nnX
i=1x2
i (x)2=1
nnX
i=1(xi x)2: (C.46)
The corresponding estimator for ,X, is unbiased. However, the estimator for 2is biased:
Ec2=2(n 1)=n. An unbiased estimator is the sample variance sample variance
S2=c2n
n 1=1
n 1nX
i=1(Xi X)2:
Its square root, S=p
S2, is called the sample standard deviation sample
standard
deviation.
Example C.12 (Sample Covariance Matrix) The method of moments can also be
used to estimate the covariance matrix of a random vector. In particular, let the X1;:::; Xn456 C.12. Estimation
be iid copies of a d-dimensional random vector Xwith mean vector and covari-
ance matrix . We assume n>d. The moment estimator for is, as in the d=1 case,
X=(X1++Xn)=n. As the covariance matrix can be written (see (C.15)) as +430
=E(X )(X )>;
the method of moments yields the estimator
b=1
nnX
i=1(Xi X)(Xi X)>: (C.47)
Similar to the one-dimensional case ( d=1), replacing the factor 1 =nwith 1=(n 1) gives
an unbiased estimator, called the sample covariance matrix sample
covariance
matrix.
C.12.2 Maximum Likelihood Method
The concept of likelihood is central in statistics. It describes in a precise way the informa-
tion about model parameters that is contained in the observed data.
LetTbe a (random) data object that is modeled as a draw from the pdf g(j) (dis-
crete or continuous) with parameter vector 2. Letbe an outcome ofT. The function
L(j) :=g(j);2, is called the likelihood function of, based on. The (nat-likelihood
function ural) logarithm of the likelihood function is called the log-likelihood function and is often
log-likelihood
functiondenoted by a lower case l.
Note that L(j) and g(j) have the same formula, but the Ô¨Årst is viewed as a
function offor Ô¨Åxed, where the second is viewed as a function of for Ô¨Åxed.
The concept of likelihood is particularly useful when Tis modeled as an iid sample
fX1;:::; Xngfrom some pdf Àö g. In that case, the likelihood of the data =fx1;:::; xng, as a
function of, is given by the product
L(j)=nY
i=1Àög(xij): (C.48)
Letbe an observation from T g(j), and suppose that g(j) takes its largest
value at=b. In a way this bis our best estimate for , as it maximizes the probability
(density) for the observation . It is called the maximum likelihood estimate (MLE) of.
Note that b=b() is a function of . The corresponding random variable, also denoted bis
themaximum likelihood estimator maximum
likelihood
estimator(also abbreviated as MLE).
Maximization of L(j) as a function of is equivalent (when searching for the max-
imizer) to maximizing the log-likelihood l(j), as the natural logarithm is an increasing
function. This is often easier, especially when Tis an iid sample from some sampling
distribution. For example, for Lof the form (C.48), we have
l(j)=nX
i=1ln Àög(xij):Appendix C. Probability and Statistics 457
Ifl(j) is a di erentiable function with respect to and the maximum is attained in the
interior of,and there exists a unique maximum point , then we can Ô¨Ånd the MLE of by
solving the equations
@
@il(j)=0;i=1;:::; d:
Example C.13 (Bernoulli Random Sample) Suppose we have data n=fx1;:::; xng
and assume the model X1;:::; XniidBer(). Then, the likelihood function is given by
L(j)=nY
i=1xi(1 )1 xi=s(1 )n s;0<< 1; (C.49)
where s:=x1++xn=:nx. The log-likelihood is l()=sln+(n s) ln(1 ). Through
dierentiation with respect to , we Ô¨Ånd the derivative
s
 n s
1 =s
(1 ) n
1 : (C.50)
Solving l0()=0 gives the ML estimate b=xand ML estimator b=X.
C.13 ConÔ¨Ådence Intervals
An essential part in any estimation procedure is to provide an assessment of the accuracy
of the estimate. Indeed, without information on its accuracy the estimate itself would be
meaningless. ConÔ¨Ådence intervals (also called interval estimates interval
estimates) provide a precise way of
describing the uncertainty in the estimate.
LetX1;:::; Xnbe random variables with a joint distribution depending on a parameter
2. Let T1<T2be statistics; that is, Ti=Ti(X1;:::; Xn),i=1;2 are functions of the
data, but not of .
1. The random interval ( T1;T2) is called a stochastic conÔ¨Ådence interval stochastic
confidence
intervalforwith
conÔ¨Ådence 1 if
P[T1<< T2]>1 for all2: (C.51)
2. If t1andt2are the observed values of T1andT2, then the interval ( t1;t2) is called the
(numerical) conÔ¨Ådence interval (numerical )
confidence
intervalforwith conÔ¨Ådence 1  for every2.
3. If the right-hand side of (C.51) is merely a heuristic estimate or approximation of
the true probability, then the resulting interval is called an approximate conÔ¨Ådence
interval .
4. The probability P[T1<  < T2] is called the coverage probability coverage
probability. For a 1 
conÔ¨Ådence interval, it must be at least 1  .
For multidimensional parameters 2Rdthe stochastic conÔ¨Ådence interval is replaced
with a stochastic conÔ¨Ådence region confidence
regionCRdsuch thatP[2C]>1 for all.458 C.14. Hypothesis Testing
Example C.14 (Approximate ConÔ¨Ådence Interval for the Mean) Let X1;X2;:::; Xn
be an iid sample from a distribution with mean and variance 2<1(both assumed
to be unknown). By the central limit theorem and the law of large numbers, +447
T=X 
S=pnapprox:N(0;1);
for large n, where Sis the sample standard deviation. Rearranging the approximate equality
P[jTj6z1 =2]1 , where z1 =2is the 1 =2 quantile of the standard normal
distribution, yields
P"
X z1 =2Spn66X+z1 =2Spn#
1 ;
so that  
X z1 =2Spn;X+z1 =2Spn!
;abbreviated as Xz1 =2Spn; (C.52)
is an approximate stochastic (1  ) conÔ¨Ådence interval for .
Since (C.52) is an asymptotic result only, care should be taken when applying it to
cases where the sample size is small or moderate and the sampling distribution is heavily
skewed.
C.14 Hypothesis Testing
Suppose the model for the data Tis described by a family of probability distributions that
depend on a parameter 2. The aim of hypothesis testing is to decide, on the basis ofhypothesis
testing the observed data , which of two competing hypotheses holds true; these being the null
hypothesis ,H0:20, and the alternative hypothesis ,H1:21.null hypothesis
alternative
hypothesisIn classical statistics the null hypothesis and alternative hypothesis do not play equival-
ent roles. H0contains the ‚Äústatus quo‚Äù statement and is only rejected if the observed data
are very unlikely to have happened under H0.
The decision whether to accept or reject H0is dependent on the outcome of a test
statistic test statistic T=T(T). For simplicity, we discuss only the one-dimensional case TT. Two
(related) types of decision rules are generally used:
1.Decision rule 1 :Reject H 0if T falls in the critical region .
Here the critical region critical region is any appropriately chosen region in R. In practice a critical
region is one of the following:
¬àleft one-sided : ( 1;c],
¬àright one-sided : [c;1),
¬àtwo-sided : ( 1;c1][[c2;1).
For example, for a right one-sided test, H0is rejected if the outcome of the test
statistic is too large. The endpoints c,c1, and c2of the critical regions are called
critical values critical values .Appendix C. Probability and Statistics 459
2.Decision rule 2 :Reject H 0if the P-value is smaller than some signiÔ¨Åcance level .
TheP-value P-value is the probability that, under H0, the (random) test statistic takes a value
as extreme as or more extreme than the one observed. In particular, if tis the observed
outcome of the test statistic T, then
¬àleft one-sided test :P:=PH0[T6t],
¬àright one-sided :P:=PH0[T>t],
¬àtwo-sided :P:=minf2PH0[T6t];2PH0[T>t]g.
The smaller the P-value, the greater the strength of the evidence against H0provided
by the data. As a rule of thumb:
P<0:10 suggestive evidence,
P<0:05 reasonable evidence,
P<0:01 strong evidence.
Whether the Ô¨Årst or the second decision rule is used, one can make two types of errors,
as depicted in Table C.4.
Table C.4: Type I and II errors in hypothesis testing.
True statement
Decision H0is true H1is true
Accept H 0Correct Type II Error
Reject H 0Type I Error Correct
The choice of the test statistic and the corresponding critical region involves a multiob-
jective optimization criterion, whereby both the probabilities of a type I and type II error
should, ideally, be chosen as small as possible. Unfortunately, these probabilities compete
with each other. For example, if the critical region is made larger (smaller), the probability
of a type II error is reduced (increased), but at the same time the probability of a type I
error is increased (reduced).
Since the type I error is considered more serious, Neyman and Pearson [93] suggested
the following approach: choose the critical region such that the probability of a type II error
is as small as possible, while keeping the probability of a type I error below a predetermined
small signiÔ¨Åcance level significance
level.
Remark C.3 (Equivalence of Decision Rules) Note that decision rule 1 and 2 are
equivalent in the following sense:
Reject H0ifTfalls in the critical region, at signiÔ¨Åcance level .
,
Reject H0if the P-value is6signiÔ¨Åcance level .460 C.14. Hypothesis Testing
In other words, the P-value of the test is the smallest level of signiÔ¨Åcance that would lead
to the rejection of H0.
In general, a statistical test involves the following steps:
1. Formulate an appropriate statistical model for the data.
2. Give the null ( H0) and alternative ( H1) hypotheses in terms of the parameters
of the model.
3. Determine the test statistic (a function of the data only).
4. Determine the (approximate) distribution of the test statistic under H0.
5. Calculate the outcome of the test statistic.
6. Calculate the P-value orthe critical region, given a preselected signiÔ¨Åcance
level.
7. Accept or reject H0.
The actual choice of an appropriate test statistic is akin to selecting a good estimator
for the unknown parameter . The test statistic should summarize the information about 
and make it possible to distinguish between the alternative hypotheses.
Example C.15 (Hypothesis Testing) We are given outcomes x1;:::; xmandy1;:::; yn
of two simulation studies obtained via independent runs, with m=100 and n=50. The
sample means and standard deviations are x=1:3,sX=0:1 and y=1:5,sY=0:3. Thus,
thefxigare outcomes of iid random variables fXig, thefyigare outcomes of iid random
variablesfYig, and thefXigandfYigare independent. We wish to assess whether the expect-
ationsX=EXiandY=EYiare the same or not. Going through the 7 steps above, we
have:
1. The model is already speciÔ¨Åed above.
2.H0:X Y=0 versus H1:X Y,0.
3. For similar reasons as in Example C.14, take
T=X Yq
S2
X=m+S2
Y=n:
4. By the central limit theorem, the statistic Thas, under H0, approximately a standard
normal distribution (assuming the variances are Ô¨Ånite).
5. The outcome of Tist=(x y)=q
s2
X=m+s2
Y=n 4:59.
6. As this is a two-sided test, the P-value is 2 PH0[T6 4:59]410 6.
7. Because the P-value is extremely small, there is overwhelming evidence that the two
expectations are not the same.Appendix C. Probability and Statistics 461
Further Reading
Accessible treatises on probability and stochastic processes include [27, 26, 39, 54, 101].
Kallenberg‚Äôs book [61] provides a complete graduate-level overview of the foundations of
modern probability. Details on the convergence of probability measures and limit theorems
can be found in [11]. For an accessible introduction to mathematical statistics with simple
applications see, for example, [69, 74, 124]. For a more detailed overview of statistical
inference, see [10, 25]. A standard reference for classical (frequentist) statistical inference
is [78].462APPENDIXD
PYTHON PRIMER
Python has become the programming language of choice for many researchers and
practitioners in data science and machine learning. This appendix gives a brief intro-
duction to the language. As the language is under constant development and each year
many new packages are being released, we do not pretend to be exhaustive in this in-
troduction. Instead, we hope to provide enough information for novices to get started
with this beautiful and carefully thought-out language.
D.1 Getting Started
The main website for Python is
https://www.python.org/ ,
where you will Ô¨Ånd documentation, a tutorial, beginners‚Äô guides, software examples, and
so on. It is important to note that there are two incompatible ‚Äúbranches‚Äù of Python, called
Python 3 and Python 2. Further development of the language will involve only Python 3,
and in this appendix (and indeed the rest of the book) we only consider Python 3. As there
are many interdependent packages that are frequently used with a Python installation, it
is convenient to install a distribution ‚Äî for instance, the Anaconda Anaconda Python distribution,
available from
https://www.anaconda.com/ .
The Anaconda installer automatically installs the most important packages and also
provides a convenient interactive development environment (IDE), called Spyder .
Use the Anaconda Navigator to launch Spyder ,Jupyter notebook , install and update
packages, or open a command-line terminal.
To get started1, try out the Python statements in the input boxes that follow. You can
either type these statements at the IPython command prompt or run them as (very short)
1We assume that you have installed all the necessary Ô¨Åles and have launched Spyder .
463464 D.1. Getting Started
Python programs. The output for these two modes of input can di er slightly. For ex-
ample, typing a variable name in the console causes its contents to be automatically printed,
whereas in a Python program this must be done explicitly by calling the print function.
Selecting (highlighting) several program lines in Spyder and then pressing function key2
F9is equivalent to executing these lines one by one in the console.
In Python, data is represented as an object object or relation between objects (see also Sec-
tion D.2). Basic data types are numeric types (including integers, booleans, and Ô¨Çoats),
sequence types (including strings, tuples, and lists), sets, and mappings (currently, diction-
aries are the only built-in mapping type).
Strings are sequences of characters, enclosed by single or double quotes. We can print
strings via the print function.
print ("Hello World!")
Hello World!
For pretty-printing output, Python strings can be formatted using the format function. The
bracket syntax {i}provides a placeholder for the i-th variable to be printed, with 0 being
the Ô¨Årst index. Individual variables can be formatted separately and as desired; formatting
syntax is discussed in more detail in Section D.9. +475
print ("Name:{1} (height {2} m, age {0})". format (111,"Bilbo" ,0.84))
Name:Bilbo (height 0.84 m, age 111)
Lists can contain di erent types of objects, and are created using square brackets as in the
following example:
x = [1, 'string ',"another string"] # Quote type is not important
[1, 'string ','another string ']
Elements in lists are indexed starting from 0, and are mutable mutable (can be changed):
x = [1,2]
x[0] = 2 # Note that the first index is 0
x
[2,2]
In contrast, tuples (with round brackets) are immutable immutable (cannot be changed). Strings are
immutable as well.
x = (1,2)
x[0] = 2
TypeError: 'tuple 'object does not support item assignment
Lists can be accessed via the slice slice notation [start:end] . It is important to note that end
is the index of the Ô¨Årst element that will notbe selected, and that the Ô¨Årst element has index
0. To gain familiarity with the slice notation, execute each of the following lines.
2This may depend on the keyboard and operating system.Appendix D. Python Primer 465
a = [2, 3, 5, 7, 11, 13, 17, 19, 23]
a[1:4] # Elements with index from 1 to 3
a[:4] # All elements with index less than 4
a[3:] # All elements with index 3 or more
a[-2:] # The last two elements
[3, 5, 7]
[2, 3, 5, 7]
[7, 11, 13, 17, 19, 23]
[19, 23]
Anoperator operator is a programming language construct that performs an action on one or more
operands. The action of an operator in Python depends on the type of the operand(s). For
example, operators such as +,, , and % that are arithmetic operators when the operands
are of a numeric type, can have di erent meanings for objects of non-numeric type (such
as strings).
'hello '+'world '# String concatenation
'helloworld '
'hello '* 2 # String repetition
'hellohello '
[1,2] * 2 # List repetition
[1, 2, 1, 2]
15 % 4 # Remainder of 15/4
3
Some common Python operators are given in Table D.1. +467
D.2 Python Objects
As mentioned in the previous section, data in Python is represented by objects or relations
between objects. We recall that basic data types included strings and numeric types (such
as integers, booleans, and Ô¨Çoats).
As Python is an object-oriented programming language, functions are objects too
(everything is an object!). Each object has an identity (unique to each object and immutable
‚Äî that is, cannot be changed ‚Äî once created), a type (which determines which operations
can be applied to the object, and is considered immutable), and a value (which is either
mutable or immutable). The unique identity assigned to an object obj can be found by
calling id, as in id(obj) .
Each object has a list of attributes attributes , and each attribute is a reference to another object.
The function dir applied to an object returns the list of attributes. For example, a string
object has many useful attributes, as we shall shortly see. Functions are objects with the
__call__ attribute.466 D.3. Types and Operators
A class (see Section D.8) can be thought of as a template for creating a custom type of
object.
s = "hello"
d = dir(s)
print (d,flush=True) # Print the list in "flushed" format
['__add__ ','__class__ ','__contains__ ','__delattr__ ','__dir__ ',
... (many left out) ... 'replace ','rfind ',
'rindex ','rjust ','rpartition ','rsplit ','rstrip ','split ',
'splitlines ','startswith ','strip ','swapcase ','title ',
'translate ','upper ','zfill ']
Any attribute attr of an object objcan be accessed via the dot notation dot notation :obj.attr . To
Ô¨Ånd more information about any object use the help function.
s = "hello"
help (s.replace)
replace(...) method of builtins.str instance
S.replace(old, new[, count]) -> str
Return a copy of S with all occurrences of substring
old replaced by new. If the optional argument count is
given , only the first count occurrences are replaced.
This shows that the attribute replace is in fact a function. An attribute that is a function is
called a method method . We can use the replace method to create a new string from the old one
by changing certain characters.
s = 'hello '
s1 = s.replace( 'e','a')
print (s1)
hallo
In many Python editors, pressing the TAB key, as in objectname.<TAB> , will bring
up a list of possible attributes via the editor‚Äôs autocompletion feature.
D.3 Types and Operators
Each object has a type type . Three basic data types in Python are str (for string), int (for
integers), and float (for Ô¨Çoating point numbers). The function type returns the type of
an object.
t1 = type ([1,2,3])
t2 = type ((1,2,3))
t3 = type ({1,2,3})
print (t1,t2,t3)Appendix D. Python Primer 467
<class 'list '> <class 'tuple '> <class 'set'>
Theassignment assignment operator, =, assigns an object to a variable; e.g., x = 12 . An expression
is a combination of values, operators, and variables that yields another value or variable.
Variable names are case sensitive and can only contain letters, numbers, and under-
scores. They must start with either a letter or underscore. Note that reserved words
such as True andFalse are case sensitive as well.
Python is a dynamically typed language, and the type of a variable at a particular point
during program execution is determined by its most recent object assignment. That is, the
type of a variable does not need to be explicitly declared from the outset (as is the case in
C or Java), but instead the type of the variable is determined by the object that is currently
assigned to it.
It is important to understand that a variable in Python is a reference reference to an object ‚Äî
think of it as a label on a shoe box. Even though the label is a simple entity, the contents
of the shoe box (the object to which the variable refers) can be arbitrarily complex. Instead
of moving the contents of one shoe box to another, it is much simpler to merely move the
label.
x = [1,2]
y = x # y refers to the same object as x
print (id(x) == id(y)) # check that the object id 's are the same
y[0] = 100 # change the contents of the list that y refers to
print (x)
True
[100,2]
x = [1,2]
y = x # y refers to the same object as x
y = [100,2] # now y refers to a different object
print (id(x) == id(y))
print (x)
False
[1,2]
Table D.1 shows a selection of Python operators for numerical and logical variables.
Table D.1: Common numerical (left) and logical (right) operators.
+ addition ~ binary NOT
- subtraction & binary AND
* multiplication ^ binary XOR
** power | binary OR
/ division == equal to
// integer division != not equal to
% modulus468 D.4. Functions and Methods
Several of the numerical operators can be combined with an assignment operator, as in
x += 1 to mean x = x + 1 . Operators such as +and*can be deÔ¨Åned for other data types
as well, where they take on a di erent meaning. This is called operator overloading , an
example of which is the use of <List> * <Integer> for list repetition as we saw earlier.
D.4 Functions and Methods
Functions make it easier to divide a complex program into simpler parts. To create a
function function , use the following syntax:
def <function name>(<parameter_list>):
<statements>
A function takes a list of input variables that are references to objects. Inside the func-
tion, a number of statements are executed which may modify the objects, but not the ref-
erence itself. In addition, the function may return an output object (or will return the value
None if not explicitly instructed to return output). Think again of the shoe box analogy. The
input variables of a function are labels of shoe boxes, and the objects to which they refer
are the contents of the shoe boxes. The following program highlights some of the subtleties
of variables and objects in Python.
Note that the statements within a function must be indented. This is Python‚Äôs way to
deÔ¨Åne where a function begins and ends.
x = [1,2,3]
def change_list(y):
y.append(100) # Append an element to the list referenced by y
y[0]=0 # Modify the first element of the same list
y = [2,3,4] # The local y now refers to a different list
# The list to which y first referred does not change
return sum (y)
print (change_list(x))
print (x)
9
[0, 2, 3, 100]
Variables that are deÔ¨Åned inside a function only have local scope ; that is, they are
recognized only within that function. This allows the same variable name to be used in
dierent functions without creating a conÔ¨Çict. If any variable is used within a function,
Python Ô¨Årst checks if the variable has local scope. If this is not the case (the variable has
not been deÔ¨Åned inside the function), then Python searches for that variable outside the
function (the global scope). The following program illustrates several important points.Appendix D. Python Primer 469
from numpy import array , square , sqrt
x = array([1.2,2.3,4.5])
def stat(x):
n = len(x) #the length of x
meanx = sum(x)/n
stdx = sqrt( sum(square(x - meanx))/n)
return [meanx ,stdx]
print (stat(x))
[2.6666666666666665 , 1.3719410418171119]
1. Basic math functions such as sqrt are unknown to the standard Python interpreter
and need to be imported. More on this in Section D.5 below.
2. As was already mentioned, indentation is crucial. It shows where the function begins
and ends.
3. No semicolons3are needed to end lines, but the Ô¨Årst line of the function deÔ¨Ånition
(here line 5) must end with a colon (:).
4. Lists are not arrays (vectors of numbers), and vector operations cannot be performed
on lists. However, the numpy module is designed speciÔ¨Åcally with e cient vec-
tor/matrix operations in mind. On the second code line, we deÔ¨Åne xas a vector
(ndarray ) object. Functions such as square ,sum, and sqrt are then applied to
such arrays. Note that we used the default Python functions lenandsum. More on
numpy in Section D.10.
5. Running the program with stat(x) instead of print(stat(x)) in line 11 will not
show any output in the console.
To display the complete list of built-in functions, type (using double underscores)
dir(__builtin__) .
D.5 Modules
A Python module module is a programming construct that is useful for organizing code into
manageable parts. To each module with name module_name is associated a Python Ô¨Åle
module_name.py containing any number of deÔ¨Ånitions, e.g., of functions, classes, and
variables, as well as executable statements. Modules can be imported into other programs
using the syntax: import <module_name> as <alias_name> , where <alias_name>
is a shorthand name for the module.
3Semicolons can be used to put multiple commands on a single line.470 D.5. Modules
When imported into another Python Ô¨Åle, the module name is treated as a namespace namespace ,
providing a naming system where each object has its unique name. For example, di erent
modules mod1 andmod2 can have di erent sumfunctions, but they can be distinguished by
preÔ¨Åxing the function name with the module name via the dot notation, as in mod1.sum and
mod2.sum . For example, the following code uses the sqrt function of the numpy module.
import numpy as np
np.sqrt(2)
1.4142135623730951
A Python package is simply a directory of Python modules; that is, a collection of
modules with additional startup information (some of which may be found in its __path__
attribute). Python‚Äôs built-in module is called __builtins__ . Of the great many useful
Python modules, Table D.2 gives a few.
Table D.2: A few useful Python modules /packages.
datetime Module for manipulating dates and times.
matplotlib MATLABTM-type plotting package
numpy Fundamental package for scientiÔ¨Åc computing, including random
number generation and linear algebra tools. DeÔ¨Ånes the ubiquitous
ndarray class.
os Python interface to the operating system.
pandas Fundamental module for data analysis. DeÔ¨Ånes the powerful
DataFrame class.
pytorch Machine learning library that supports GPU computation.
scipy Ecosystem for mathematics, science, and engineering, containing
many tools for numerical computing, including those for integration,
solving di erential equations, and optimization.
requests Library for performing HTTP requests and interfacing with the web.
seaborn Package for statistical data visualization.
sklearn Easy to use machine learning library.
statsmodels Package for the analysis of statistical models.
Thenumpy package contains various subpackages, such as random ,linalg , and fft.
More details are given in Section D.10.
When using Spyder , press Ctrl+I in front of any object, to display its help Ô¨Åle in a
separate window.
As we have already seen, it is also possible to import only speciÔ¨Åc functions from a
module using the syntax: from <module_name> import <fnc1, fnc2, ...> .
from numpy import sqrt , cos
sqrt(2)
cos(1)Appendix D. Python Primer 471
1.4142135623730951
0.54030230586813965
This avoids the tedious preÔ¨Åxing of functions via the (alias) of the module name. However,
for large programs it is good practice to always use the preÔ¨Åx /alias name construction, to
be able to clearly ascertain precisely which module a function being used belongs to.
D.6 Flow Control
Flow control in Python is similar to that of many programming languages, with conditional
statements as well as while andforloops. The syntax for if-then-else Ô¨Çow control is
as follows.
if <condition1>:
<statements>
elif <condition2>:
<statements>
else:
<statements>
Here, <condition1> and<condition2> are logical conditions that are either True or
False ; logical conditions often involve comparison operators (such as ==, >, <=, != ).
In the example above, there is one elif part, which allows for an ‚Äúelse if‚Äù conditional
statement. In general, there can be more than one elif part, or it can be omitted. The else
part can also be omitted. The colons are essential, as are the indentations.
Thewhile andforloops have the following syntax.
while <condition>:
<statements>
for <variable> in <collection>:
<statements>
Above, <collection> is an iterable object (see Section D.7 below). For further con-
trol in forandwhile loops, one can use a break statement to exit the current loop, and
thecontinue statement to continue with the next iteration of the loop, while abandoning
any remaining statements in the current iteration. Here is an example.
import numpy as np
ans = 'y'
while ans != 'n':
outcome = np.random.randint(1,6+1)
ifoutcome == 6:
print ("Hooray a 6!")
break
else :
print ("Bad luck , a", outcome)
ans = input ("Again? (y/n) ")472 D.7. Iteration
D.7 Iteration
Iterating over a sequence of objects, such as used in a for loop, is a common operation.
To better understand how iteration works, we consider the following code.
s = "Hello"
for cins:
print (c,'*', end= ' ')
H * e * l * l * o *
A string is an example of a Python object that can be iterated. One of the methods of a
string object is __iter__ . Any object that has such a method is called an iterable iterable . Calling
this method creates an iterator iterator ‚Äî an object that returns the next element in the sequence
to be iterated. This is done via the method __next__ .
s = "Hello"
t = s.__iter__() # t is now an iterator. Same as iter(s)
print (t.__next__() ) # same as next(t)
print (t.__next__() )
print (t.__next__() )
H
e
l
The inbuilt functions next and iter simply call these corresponding double-
underscore functions of an object. When executing a for loop, the sequence /collection
over which to iterate must be an iterable. During the execution of the forloop, an iterator
is created and the next function is executed until there is no next element. An iterator is
also an iterable, so can be used in a forloop as well. Lists, tuples, and strings are so-called
sequence sequence objects and are iterables, where the elements are iterated by their index.
The most common iterator in Python is the range range iterator, which allows iteration over
a range of indices. Note that range returns a range object, not a list.
for iin range (4,20):
print (i, end= ' ')
print (range (4,20))
4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
range(4,20)
Similar to Python‚Äôs slice operator [ i:j], the iterator range (i;j) ranges from itoj,
not including the index j.
Two other common iterables are sets and dictionaries. Python sets sets are, as in mathem-
atics, unordered collections of unique objects. Sets are deÔ¨Åned with curly brackets fg, as
opposed to round brackets ( ) for tuples, and square brackets [ ] for lists. Unlike lists, sets do
not have duplicate elements. Many of the usual set operations are implemented in Python,
including the union A | B and intersection A & B .Appendix D. Python Primer 473
A = {3, 2, 2, 4}
B = {4, 3, 1}
C = A & B
for iinA:
print (i)
print (C)
2
3
4
{3, 4}
A useful way to construct lists is by list comprehension list
comprehension; that is, by expressions of the
form
<expression> for <element> in <list> if <condition>
For sets a similar construction holds. In this way, lists and sets can be deÔ¨Åned using very
similar syntax as in mathematics. Compare, for example, the mathematical deÔ¨Ånition of
the sets A:=f3;2;4;2g=f2;3;4g(no order and no duplication of elements) and B:=fx2:
x2Agwith the Python code below.
setA = {3, 2, 4, 2}
setB = {x**2 for xinsetA}
print (setB)
listA = [3, 2, 4, 2]
listB = [x**2 for xinlistA]
print (listB)
{16, 9, 4}
[9, 4, 16, 4]
Adictionary dictionary is a set-like data structure, containing one or more key:value pairs en-
closed in curly brackets. The keys are often of the same type, but do not have to be; the
same holds for the values. Here is a simple example, storing the ages of Lord of the Rings
characters in a dictionary.
DICT = { 'Gimly ': 140, 'Frodo ':51, 'Aragorn ': 88}
for key inDICT:
print (key, DICT[key])
Gimly 140
Frodo 51
Aragorn 88
D.8 Classes
Recall that objects are of fundamental importance in Python ‚Äî indeed, data types and
functions are all objects. A class class is an object type, and writing a class deÔ¨Ånition can be
thought of as creating a template for a new type of object. Each class contains a number
of attributes, including a number of inbuilt methods. The basic syntax for the creation of a
class is:474 D.8. Classes
class <class_name>:
def __init__(self):
<statements>
<statements>
The main inbuilt method is __init__ , which creates an instance instance of a class object.
For example, str is a class object (string class), but s = str( 'Hello ')or simply
s = 'Hello ', creates an instance, s, of the strclass. Instance attributes are created dur-
ing initialization and their values may be di erent for di erent instances. In contrast, the
values of class attributes are the same for every instance. The variable self in the initializ-
ation method refers to the current instance that is being created. Here is a simple example,
explaining how attributes are assigned.
class shire_person:
def __init__(self ,name): # initialization method
self.name = name # instance attribute
self.age = 0 # instance attribute
address = 'The Shire ' # class attribute
print (dir(shire_person)[1:5], '...',dir(shire_person)[-2:])
# list of class attributes
p1 = shire_person( 'Sam') # create an instance
p2 = shire_person( 'Frodo ')# create another instance
print (p1.__dict__) # list of instance attributes
p2.race = 'Hobbit ' # add another attribute to instance p2
p2.age = 33 # change instance attribute
print (p2.__dict__)
print (getattr (p1, 'address ')) # content of p1 's class attribute
['__delattr__ ','__dict__ ','__dir__ ','__doc__ '] ...
['__weakref__ ','address ']
{'name ':'Sam','age': 0}
{'name ':'Frodo ','age': 33, 'race ':'Hobbit '}
The Shire
It is good practice to create all the attributes of the class object in the __init__ method,
but, as seen in the example above, attributes can be created and assigned everywhere, even
outside the class deÔ¨Ånition. More generally, attributes can be added to any object that has
a__dict__ .
An ‚Äúempty‚Äù class can be created via
class <class_name>:
pass
Python classes can be derived from a parent class by inheritance inheritance , via the following
syntax.
class <class_name>(<parent_class_name>):
<statements>Appendix D. Python Primer 475
The derived class (initially) inherits all of the attributes of the parent class.
As an example, the class shire_person below inherits the attributes name ,age, and
address from its parent class person . This is done using the super function, used here
to refer to the parent class person without naming it explicitly. When creating a new
object of type shire_person , the __init__ method of the parent class is invoked, and
an additional instance attribute Shire_address is created. The dirfunction conÔ¨Årms that
Shire_address is an attribute only of shire_person instances.
class person:
def __init__(self ,name):
self.name = name
self.age = 0
self.address= ' '
class shire_person(person):
def __init__(self ,name):
super ().__init__(name)
self.Shire_address = 'Bag End '
p1 = shire_person("Frodo")
p2 = person("Gandalf")
print (dir(p1)[:1], dir(p1)[-3:] )
print (dir(p2)[:1], dir(p2)[-3:] )
['Shire_address '] ['address ','age','name ']
['__class__ '] ['address ','age','name ']
D.9 Files
To write to or read from a Ô¨Åle, a Ô¨Åle Ô¨Årst needs to be opened. The open function in Python
creates a Ô¨Åle object that is iterable, and thus can be processed in a sequential manner in a
fororwhile loop. Here is a simple example.
fout = open ('output.txt ','w')
for iin range (0,41):
ifi%10 == 0:
fout.write( '{:3d}\n '.format (i))
fout.close()
The Ô¨Årst argument of open is the name of the Ô¨Åle. The second argument speciÔ¨Åes
if the Ô¨Åle is opened for reading ( 'r'), writing ( 'w'), appending ( 'a'), and so on. See
help(open) . Files are written in text mode by default, but it is also possible to write in
binary mode. The above program creates a Ô¨Åle output.txt with 5 lines, containing the
strings 0, 10, . . . , 40. Note that if we had written fout.write(i) in the fourth line of the
code above, an error message would be produced, as the variable iis an integer, and not a
string. Recall that the expression string.format() is Python‚Äôs way to specify the format
of the output string.
The formatting syntax {:3d} indicates that the output should be constrained to a spe-
ciÔ¨Åc width of three characters, each of which is a decimal value. As mentioned in the476 D.9. Files
introduction, bracket syntax {i}provides a placeholder for the i-th variable to be printed,
with 0 being the Ô¨Årst index. The format for the output is further speciÔ¨Åed by {i:format} ,
where format is typically4of the form:
[width][.precision][type]
In this speciÔ¨Åcation:
¬àwidth speciÔ¨Åes the minimum width of output;
¬àprecision speciÔ¨Åes the number of digits to be displayed after the decimal point for
a Ô¨Çoating point values of type f, or the number of digits before andafter the decimal
point for a Ô¨Çoating point values of type g;
¬àtype speciÔ¨Åes the type of output. The most common types are sfor strings, dfor
integers, bfor binary numbers, ffor Ô¨Çoating point numbers (Ô¨Çoats) in Ô¨Åxed-point
notation, gfor Ô¨Çoats in general notation, efor Ô¨Çoats in scientiÔ¨Åc notation.
The following illustrates some behavior of formatting on numbers.
'{:5d} '.format (123)
'{:.4e} '.format (1234567890)
'{:.2f} '.format (1234567890)
'{:.2f} '.format (2.718281828)
'{:.3f} '.format (2.718281828)
'{:.3g} '.format (2.718281828)
'{:.3e} '.format (2.718281828)
'{0:3.3f}; {2:.4e}; '.format (123.456789, 0.00123456789)
'123'
'1.2346e+09 '
'1234567890.00 '
'2.72 '
'2.718 '
'2.72 '
'2.718e+00 '
'123.457; 1.2346e-03; '
The following code reads the text Ô¨Åle output.txt line by line, and prints the output
on the screen. To remove the newline \ncharacter, we have used the strip method for
strings, which removes any whitespace from the start and end of a string.
fin = open ('output.txt ','r')
for line infin:
line = line.strip() # strips a newline character
print (line)
fin.close()
0
10
20
30
40
4More formatting options are possible.Appendix D. Python Primer 477
When dealing with Ô¨Åle input and output it is important to always close Ô¨Åles. Files that
remain open, e.g., when a program Ô¨Ånishes unexpectedly due to a programming error, can
cause considerable system problems. For this reason it is recommended to open Ô¨Åles via
context management . The syntax is as follows.
with open ('output.txt ','w') as f:
f.write( 'Hi there! ')
Context management ensures that a Ô¨Åle is correctly closed even when the program is
terminated prematurely. An example is given in the next program, which outputs the most-
frequent words in Dicken‚Äôs A Tale of Two Cities , which can be downloaded from the book‚Äôs
GitHub site as ataleof2cities.txt .
Note that in the next program, the Ô¨Åle ataleof2cities.txt must be placed in the cur-
rent working directory. The current working directory can be determined via import os
followed by cwd = os.getcwd() .
numline = 0
DICT = {}
with open ('ataleof2cities.txt ', encoding="utf8") as fin:
for line infin:
words = line.split()
for winwords:
ifwnot in DICT:
DICT[w] = 1
else :
DICT[w] +=1
numline += 1
sd = sorted (DICT ,key=DICT.get,reverse=True) #sort the dictionary
print ("Number of unique words: {}\n". format (len(DICT)))
print ("Ten most frequent words:\n")
print ("{:8} {}". format ("word", "count"))
print (15* '-')
for iin range (0,10):
print ("{:8} {}". format (sd[i], DICT[sd[i]]))
Number of unique words: 19091
Ten most frequent words:
word count
---------------
the 7348
and 4679
of 3949
to 3387
a 2768
in 2390
his 1911
was 1672
that 1650
I 1444478 D.10. NumPy
D.10 NumPy
The package NumPy (module name numpy ) provides the building blocks for scientiÔ¨Åc
computing in Python. It contains all the standard mathematical functions, such as sin,
cos,tan, etc., as well as e cient functions for random number generation, linear algebra,
and statistical computation.
import numpy as np #import the package
x = np.cos(1)
data = [1,2,3,4,5]
y = np.mean(data)
z = np.std(data)
print ('cos(1) = {0:1.8f} mean = {1} std = {2} '.format (x,y,z))
cos(1) = 0.54030231 mean = 3.0 std = 1.4142135623730951
D.10.1 Creating and Shaping Arrays
The fundamental data type in numpy is the ndarray . This data type allows for fast matrix
operations via highly optimized numerical libraries such as LAPACK and BLAS; this in
contrast to (nested) lists. As such, numpy is often essential when dealing with large amounts
of quantitative data.
ndarray objects can be created in various ways. The following code creates a 2 32
array of zeros. Think of it as a 3-dimensional matrix or two stacked 3 2 matrices.
A = np.zeros([2,3,2]) # 2 by 3 by 2 array of zeros
print (A)
print (A.shape) # number of rows and columns
print (type (A)) # A is an ndarray
[[[ 0. 0.]
[ 0. 0.]
[ 0. 0.]]
[[ 0. 0.]
[ 0. 0.]
[ 0. 0.]]]
(2, 3, 2)
<class 'numpy.ndarray '>
We will be mostly working with 2D arrays; that is, ndarrays that represent ordinary
matrices. We can also use the range method and lists to create ndarrays via the array
method. Note that arange isnumpy ‚Äôs version of range , with the di erence that arange
returns an ndarray object.
a = np.array( range (4)) # equivalent to np.arange(4)
b = np.array([0,1,2,3])
C = np.array([[1,2,3],[3,2,1]])
print (a, '\n', b, '\n', C)
[0 1 2 3]
[0 1 2 3]Appendix D. Python Primer 479
[[1 2 3]
[3 2 1]]
The dimension of an ndarray can be obtained via its shape method, which returns a
tuple. Arrays can be reshaped via the reshape method. This does not change the current
ndarray object. To make the change permanent, a new instance needs to be created.
a = np.array( range (9)) #a is an ndarray of shape (9,)
print (a.shape)
A = a.reshape(3,3) #A is an ndarray of shape (3,3)
print (a)
print (A)
[0 1 2 3 4 5 6 7 8]
(9,)
[[0, 1, 2]
[3, 4, 5]
[6, 7, 8]]
One shape dimension for reshape can be speciÔ¨Åed as  1. The dimension is then
inferred from the other dimension(s).
The'T'attribute of an ndarray gives its transpose. Note that the transpose of a ‚Äúvector‚Äù
with shape ( n;) is the same vector. To distinguish between column and row vectors, reshape
such a vector to an n1 and 1narray, respectively.
a = np.arange(3) #1D array (vector) of shape (3,)
print (a)
print (a.shape)
b = a.reshape(-1,1) # 3x1 array (matrix) of shape (3,1)
print (b)
print (b.T)
A = np.arange(9).reshape(3,3)
print (A.T)
[0 1 2]
(3,)
[[0]
[1]
[2]]
[[0 1 2]]
[[0 3 6]
[1 4 7]
[2 5 8]]
Two useful methods of joining arrays are hstack andvstack , where the arrays are
joined horizontally and vertically, respectively.
A = np.ones((3,3))
B = np.zeros((3,2))
C = np.hstack((A,B))
print (C)480 D.10. NumPy
[[ 1. 1. 1. 0. 0.]
[ 1. 1. 1. 0. 0.]
[ 1. 1. 1. 0. 0.]]
D.10.2 Slicing
Arrays can be sliced similarly to Python lists. If an array has several dimensions, a slice for
each dimension needs to be speciÔ¨Åed. Recall that Python indexing starts at '0'and ends
at'len(obj)-1 '. The following program illustrates various slicing operations.
A = np.array( range (9)).reshape(3,3)
print (A)
print (A[0]) # first row
print (A[:,1]) # second column
print (A[0,1]) # element in first row and second column
print (A[0:1,1:2]) # (1,1) ndarray containing A[0,1] = 1
print (A[1:,-1]) # elements in 2nd and 3rd rows , and last column
[[0 1 2]
[3 4 5]
[6 7 8]]
[0 1 2]
[1 4 7]
1
[[1]]
[5 8]
Note that ndarrays are mutable objects, so that elements can be modiÔ¨Åed directly, without
having to create a new object.
A[1:,1] = [0,0] # change two elements in the matrix A above
print (A)
[[0, 1, 2]
[3, 0, 5]
[6, 0, 8]]
D.10.3 Array Operations
Basic mathematical operators and functions act element-wise onndarray objects.
x = np.array([[2,4],[6,8]])
y = np.array([[1,1],[2,2]])
print (x+y)
[[ 3, 5]
[ 8, 10]]
print (np.divide(x,y)) # same as x/y
[[ 2. 4.]
[ 3. 4.]]Appendix D. Python Primer 481
print (np.sqrt(x))
[[1.41421356 2. ]
[2.44948974 2.82842712]]
In order to compute matrix multiplications and compute inner products of vectors,
numpy ‚Äôsdot function can be used, either as a method of an ndarray instance or as a
method of np.
print (np.dot(x,y))
[[10, 10]
[22, 22]]
print (x.dot(x)) # same as np.dot(x,x)
[[28, 40]
[60, 88]]
Since version 3.5 of Python, it is possible to multiply two ndarray s using the @
operator @operator (which implements the np.matmul method). For matrices, this is similar to using
thedotmethod. For higher-dimensional arrays the two methods behave di erently.
print (x @ y)
[[10 10]
[22 22]]
NumPy allows arithmetic operations on arrays of di erent shapes (dimensions). Spe-
ciÔ¨Åcally, suppose two arrays have dimensions ( m1;m2;:::; mp) and ( n1;n2;:::; np), respect-
ively. The arrays or shapes are said to be aligned aligned if for all i=1;:::; pit holds that
¬àmi=ni, or
¬àminfmi;nig=1, or
¬àeither miorni, or both are missing.
For example, shapes (1 ;2;3) and (4;2;1) are aligned, as are (2 ;;) and (1;2;3). However,
(2;2;2) and (1;2;3) are not aligned. NumPy ‚Äúduplicates‚Äù the array elements across the
smaller dimension to match the larger dimension. This process is called broadcasting broadcasting and
is carried out without actually making copies, thus providing e cient memory use. Below
are some examples.
import numpy as np
A= np.arange(4).reshape(2,2) # (2,2) array
x1 = np.array([40,500]) # (2,) array
x2 = x1.reshape(2,1) # (2,1) array
print (A + x1) # shapes (2,2) and (2,)
print (A * x2) # shapes (2,2) and (2,1)482 D.10. NumPy
[[ 40 501]
[ 42 503]]
[[ 0 40]
[1000 1500]]
Note that above x1is duplicated row-wise and x2column-wise. Broadcasting also applies
to the matrix-wise operator @, as illustrated below. Here, the matrix bis duplicated across
the third dimension resulting in the two matrix multiplications
"0 1
2 3#"0 1
2 3#
and"4 5
6 7#"0 1
2 3#
:
B = np.arange(8).reshape(2,2,2)
b = np.arange(4).reshape(2,2)
print (B@b)
[[[ 2 3]
[ 6 11]]
[[10 19]
[14 27]]]
Functions such as sum,mean , and stdcan also be executed as methods of an ndarray
instance. The argument axis can be passed to specify along which dimension the function
is applied. By default axis=None .
a = np.array( range (4)).reshape(2,2)
print (a.sum(axis=0)) #summing over rows gives column totals
[2, 4]
D.10.4 Random Numbers
One of the sub-modules in numpy israndom . It contains many functions for random vari-
able generation.
import numpy as np
np.random.seed(123) # set the seed for the random number generator
x = np.random.random() # uniform (0,1)
y = np.random.randint(5,9) # discrete uniform 5,...,8
z = np.random.randn(4) # array of four standard normals
print (x,y, '\n',z)
0.6964691855978616 7
[ 1.77399501 -0.66475792 -0.07351368 1.81403277]
For more information on random variable generation in numpy , see
https://docs.scipy.org/doc/numpy/reference/random/index.html .Appendix D. Python Primer 483
D.11 Matplotlib
The main Python graphics library for 2D and 3D plotting is matplotlib , and its subpack-
agepyplot contains a collection of functions that make plotting in Python similar to that
in M ATLAB .
D.11.1 Creating a Basic Plot
The code below illustrates various possibilities for creating plots. The style and color of
lines and markers can be changed, as well as the font size of the labels. Figure D.1 shows
the result.
sqrtplot.py
import matplotlib.pyplot as plt
import numpy as np
x = np.arange(0, 10, 0.1)
u = np.arange(0,10)
y = np.sqrt(x)
v = u/3
plt.figure(figsize = [4,2]) # size of plot in inches
plt.plot(x,y, 'g--') # plot green dashed line
plt.plot(u,v, 'r.') # plot red dots
plt.xlabel( 'x')
plt.ylabel( 'y')
plt.tight_layout()
plt.savefig( 'sqrtplot.pdf ',format ='pdf')# saving as pdf
plt.show() # both plots will now be drawn
0
 2
 4
 6
 8
 10
x
0
1
2
3y
Figure D.1: A simple plot created using pyplot.
The library matplotlib also allows the creation of subplots. The scatterplot and histogram
in Figure D.2 have been produced using the code below. When creating a histogram there
are several optional arguments that a ect the layout of the graph. The number of bins is
determined by the parameter bins (the default is 10). Scatterplots also take a number of
parameters, such as a string cwhich determines the color of the dots, and alpha which
aects the transparency of the dots.484 D.11. Matplotlib
histscat.py
import matplotlib.pyplot as plt
import numpy as np
x = np.random.randn(1000)
u = np.random.randn(100)
v = np.random.randn(100)
plt.subplot(121) # first subplot
plt.hist(x,bins=25, facecolor= 'b')
plt.xlabel( 'X Variable ')
plt.ylabel( 'Counts ')
plt.subplot(122) # second subplot
plt.scatter(u,v,c= 'b', alpha=0.5)
plt.show()
2
 0
 2
X Variable
0
20
40
60
80
100
120Counts
2
 0
 2
2
1
0
1
2
Figure D.2: A histogram and scatterplot.
One can also create three-dimensional plots as illustrated below.
surf3dscat.py
import matplotlib.pyplot as plt
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
def npdf(x,y):
return np.exp(-0.5*( pow(x,2)+ pow(y,2)))/np.sqrt(2*np.pi)
x, y = np.random.randn(100), np.random.randn(100)
z = npdf(x,y)
xgrid , ygrid = np.linspace(-3,3,100), np.linspace(-3,3,100)
Xarray , Yarray = np.meshgrid(xgrid ,ygrid)Appendix D. Python Primer 485
Zarray = npdf(Xarray ,Yarray)
fig = plt.figure(figsize=plt.figaspect(0.4))
ax1 = fig.add_subplot(121, projection= '3d')
ax1.scatter(x,y,z, c= 'g')
ax1.set_xlabel( '$x$')
ax1.set_ylabel( '$y$')
ax1.set_zlabel( '$f(x,y)$ ')
ax2 = fig.add_subplot(122, projection= '3d')
ax2.plot_surface(Xarray ,Yarray ,Zarray ,cmap= 'viridis ',
edgecolor= 'none ')
ax2.set_xlabel( '$x$')
ax2.set_ylabel( '$y$')
ax2.set_zlabel( '$f(x,y)$ ')
plt.show()
x210123y
21012f(x,y)
0.00.10.20.30.4
x3210123y
3210123f(x,y)
0.050.100.150.200.250.300.35
Figure D.3: Three-dimensional scatter- and surface plots.
D.12 Pandas
The Python package Pandas (module name pandas ) provides various tools and data struc-
tures for data analytics, including the fundamental DataFrame class.
For the code in this section we assume that pandas has been imported via
import pandas as pd .
D.12.1 Series and DataFrame
The two main data structures in pandas areSeries andDataFrame . ASeries object can
be thought of as a combination of a dictionary and an 1-dimensional ndarray . The syntax486 D.12. Pandas
for creating a Series object is
series = pd.Series(<data>, index=[ 'index '])
Here, <data> some 1-dimensional data structure, such as a 1-dimensional ndarray , a list,
or a dictionary, and index is a list of names of the same length as <data> . When <data>
is a dictionary, the index is created from the keys of the dictionary. When <data> is an
ndarray andindex is omitted, the default index will be [0, ..., len(data)-1] .
DICT = { 'one':1, 'two':2, 'three ':3, 'four ':4}
print (pd.Series(DICT))
one 1
two 2
three 3
four 4
dtype: int64
years = [ '2000 ','2001 ','2002 ']
cost = [2.34, 2.89, 3.01]
print (pd.Series(cost ,index = years , name = 'MySeries '))#name it
2000 2.34
2001 2.89
2002 3.01
Name: MySeries , dtype: float64
The most commonly-used data structure in pandas is the two-dimensional DataFrame ,
which can be thought of as pandas ‚Äô implementation of a spreadsheet or as a diction-
ary in which each ‚Äúkey‚Äù of the dictionary corresponds to a column name and the dic-
tionary ‚Äúvalue‚Äù is the data in that column. To create a DataFrame one can use the
pandas DataFrame method, which has three main arguments: data, index (row labels),
and columns (column labels).
DataFrame(<data>, index=[ '<row_name> '], columns=[ '<column_name> '])
If the index is not speciÔ¨Åed, the default index is [0, ..., len(data)-1] . Data can
also be read directly from a CSV or Excel Ô¨Åle, as is done in Section 1.1. If a dictionary is +1
used to create the data frame (as below), the dictionary keys are used as the column names.
DICT = { 'numbers ':[1,2,3,4], 'squared ':[1,4,9,16] }
df = pd.DataFrame(DICT , index = list ('abcd '))
print (df)
numbers squared
a 1 1
b 2 4
c 3 9
d 4 16Appendix D. Python Primer 487
D.12.2 Manipulating Data Frames
Often data encoded in DataFrame orSeries objects need to be extracted, altered, or com-
bined. Getting, setting, and deleting columns works in a similar manner as for dictionaries.
The following code illustrates various operations.
ages = [6,3,5,6,5,8,0,3]
d={'Gender ':['M','F']*4, 'Age': ages}
df1 = pd.DataFrame(d)
df1.at[0, 'Age']= 60 # change an element
df1.at[1, 'Gender '] = 'Female '# change another element
df2 = df1.drop( 'Age',1) # drop a column
df3 = df2.copy(); # create a separate copy of df2
df3[ 'Age'] = ages # add the original column
dfcomb = pd.concat([df1,df2,df3],axis=1) # combine the three dfs
print (dfcomb)
Gender Age Gender Gender Age
0 M 60 M M 6
1 Female 3 Female Female 3
2 M 5 M M 5
3 F 6 F F 6
4 M 5 M M 5
5 F 8 F F 8
6 M 0 M M 0
7 F 3 F F 3
Note that the above DataFrame object has two Age columns. The expression
dfcomb['Age'] will return a DataFrame with both these columns.
Table D.3: Useful pandas methods for data manipulation.
agg Aggregate the data using one or more functions.
apply Apply a function to a column or row.
astype Change the data type of a variable.
concat Concatenate data objects.
replace Find and replace values.
read_csv Read a CSV Ô¨Åle into a DataFrame.
sort_values Sort by values along rows or columns.
stack Stack a DataFrame.
to_excel Write a DataFrame to an Excel Ô¨Åle.
It is important to correctly specify the data type of a variable before embarking on
data summarization and visualization tasks, as Python may treat di erent types of objects
in dissimilar ways. Common data types for entries in a DataFrame object are float,
category, datetime, bool , and int. A generic object type is object .
d={'Gender ':['M','F','F']*4, 'Age': [6,3,5,6,5,8,0,3,6,6,7,7]}
df=pd.DataFrame(d)
print (df.dtypes)
df['Gender '] = df[ 'Gender '].astype( 'category ')#change the type
print (df.dtypes)488 D.12. Pandas
Gender object
Age int64
dtype: object
Gender category
Age int64
dtype: object
D.12.3 Extracting Information
Extracting statistical information from a DataFrame object is facilitated by a large col-
lection of methods (functions) in pandas . Table D.4 gives a selection of data inspection
methods. See Chapter 1 for their practical use. The code below provides several examples +1
of useful methods. The apply method allows one to apply general functions to columns
or rows of a DataFrame. These operations do not change the data. The locmethod allows
for accessing elements (or ranges) in a data frame and acts similar to the slicing operation
for lists and arrays, with the di erence that the ‚Äústop‚Äù value is included , as illustrated in
the code below.
import numpy as np
import pandas as pd
ages = [6,3,5,6,5,8,0,3]
np.random.seed(123)
df = pd.DataFrame(np.random.randn(3,4), index = list ('abc'),
columns = list ('ABCD '))
print (df)
df1 = df.loc["b":"c","B":"C"] # create a partial data frame
print (df1)
meanA = df[ 'A'].mean() # mean of 'A'column
print ('mean of column A = {} '.format (meanA))
expA = df[ 'A'].apply (np.exp) # exp of all elements in 'A'column
print (expA)
A B C D
a -1.085631 0.997345 0.282978 -1.506295
b -0.578600 1.651437 -2.426679 -0.428913
c 1.265936 -0.866740 -0.678886 -0.094709
B C
b 1.651437 -2.426679
c -0.866740 -0.678886
mean of column A = -0.13276486552118785
a 0.337689
b 0.560683
c 3.546412
Name: A, dtype: float64
Thegroupby method of a DataFrame object is useful for summarizing and displaying
the data in manipulated ways. It groups data according to one or more speciÔ¨Åed columns,
such that methods such as count andmean can be applied to the grouped data.Appendix D. Python Primer 489
Table D.4: Useful pandas methods for data inspection.
columns Column names.
count Counts number of non-NA cells.
crosstab Cross-tabulate two or more categories.
describe Summary statistics.
dtypes Data types for each column.
head Display the top rows of a DataFrame.
groupby Group data by column(s).
info Display information about the DataFrame.
loc Access a group or rows or columns.
mean Column /row mean.
plot Plot of columns.
std Column /row standard deviation.
sum Returns column /row sum.
tail Display the bottom rows of a DataFrame.
value_counts Counts of di erent non-null values.
var Variance.
df = pd.DataFrame({ 'W':['a','a','b','a','a','b'],
'X':np.random.rand(6),
'Y':['c','d','d','d','c','c'],'Z':np.random.rand(6)})
print (df)
W X Y Z
0 a 0.993329 c 0.641084
1 a 0.925746 d 0.428412
2 b 0.266772 d 0.460665
3 a 0.201974 d 0.261879
4 a 0.529505 c 0.503112
5 b 0.006231 c 0.849683
print (df.groupby( 'W').mean())
X Z
W
a 0.662639 0.458622
b 0.136502 0.655174
print (df.groupby([ 'W','Y']).mean())
X Z
W Y
a c 0.761417 0.572098
d 0.563860 0.345145
b c 0.006231 0.849683
d 0.266772 0.460665
To allow for multiple functions to be calculated at once, the aggmethod can be used.
It can take a list, dictionary, or string of functions.490 D.13. Scikit-learn
print (df.groupby( 'W').agg([ sum,np.mean]))
X Z
sum mean sum mean
W
a 2.650555 0.662639 1.834487 0.458622
b 0.273003 0.136502 1.310348 0.655174
D.12.4 Plotting
Theplot method of a DataFrame makes plots of a DataFrame using Matplotlib. Di erent
types of plot can be accessed via the kind = 'str'construction, where str is one of
line (default), bar,hist ,box,kde, and several more. Finer control, such as modifying
the font, is obtained by using matplotlib directly. The following code produces the line
and box plots in Figure D.4.
import numpy as np
import pandas as pd
import matplotlib
df = pd.DataFrame({ 'normal ':np.random.randn(100),
'Uniform ':np.random.uniform(0,1,100)})
font = { 'family ':'serif ','size ' : 14} #set font
matplotlib.rc( 'font ', **font) # change font
df.plot() # line plot (default)
df.plot(kind = 'box')# box plot
matplotlib.pyplot.show() #render plots
0
 20
 40
 60
 80
 100
2
0
2
Normal
Uniform
Normal
 Uniform
2
0
2
Figure D.4: A line and box plot using the plot method of DataFrame .
D.13 Scikit-learn
Scikit-learn is an open-source machine learning and data science library for Python. The
library includes a range of algorithms relating to the chapters in this book. It is widely
used due to its simplicity and its breadth. The module name is sklearn . Below is a brief
introduction into modeling the data with sklearn . The full documentation can be found
atAppendix D. Python Primer 491
https://scikit-learn.org/ .
D.13.1 Partitioning the Data
Randomly partitioning the data in order to test the model may be achieved easily with
sklearn ‚Äôs function train_test_split . For example, suppose that the training data is
described by the matrix Xof explanatory variables and the vector yof responses. Then the
following code splits the data set into training and testing sets, with the testing set being
half of the total set.
from sklearn.model_selection import train_test_split
X_train , X_test , y_train , y_test = train_test_split(X, y,
test_size = 0.5)
As an example, the following code generates a synthetic data set and splits it into
equally-sized training and test sets.
syndat.py
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
np.random.seed(1234)
X=np.pi*(2*np.random.random(size=(400,2))-1)
y=(np.cos(X[:,0])*np.sin(X[:,1]) >=0)
X_train , X_test , y_train , y_test = train_test_split(X, y,
test_size=0.5)
fig = plt.figure()
ax = fig.add_subplot (111)
ax.scatter(X_train[y_train==0,0],X_train[y_train==0,1], c= 'g',
marker= 'o',alpha=0.5)
ax.scatter(X_train[y_train==1,0],X_train[y_train==1,1], c= 'b',
marker= 'o',alpha=0.5)
ax.scatter(X_test[y_test==0,0],X_test[y_test==0,1], c= 'g',
marker= 's',alpha=0.5)
ax.scatter(X_test[y_test==1,0],X_test[y_test==1,1], c= 'b',
marker= 's',alpha=0.5)
plt.savefig( 'sklearntraintest.pdf ',format ='pdf')
plt.show()
D.13.2 Standardization
In some instances it may be necessary to standardize the data. This may be done in
sklearn with scaling methods such as MinMaxScaler orStandardScaler . Scaling may
improve the convergence of gradient-based estimators and is useful when visualizing data
on vastly di erent scales. For example, suppose that Xis our explanatory data (e.g., stored
as anumpy array), and we wish to standardize such that each value lies between 0 and 1.492 D.13. Scikit-learn
3
 2
 1
 0
 1
 2
 3
3
2
1
0
1
2
3
Figure D.5: Example training (circles) and test (squares) set for two class classiÔ¨Åcation.
Explanatory variables are the ( x;y) coordinates, classes are zero (green) or one (blue).
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
x_scaled = min_max_scaler.fit_transform(X)
# equivalent to:
x_scaled = (X - X. min(axis=0)) / (X. max(axis=0) - X. min(axis=0))
D.13.3 Fitting and Prediction
Once the data has been partitioned and standardized if necessary, the data may be Ô¨Åtted to
a statistical model, e.g., a classiÔ¨Åcation or regression model. For example, continuing with
our data from above, the following Ô¨Åts a model to the data and predicts the responses for
the test set.
from sklearn.someSubpackage import someClassifier
clf = someClassifier() # choose appropriate classifier
clf.fit(X_train , y_train) # fit the data
y_prediction = clf.predict(X_test) # predict
SpeciÔ¨Åc classiÔ¨Åers for logistic regression, na√Øve Bayes, linear and quadratic discrimin-
ant analysis, K-nearest neighbors, and support vector machines are given in Section 7.8.
+277
D.13.4 Testing the Model
Once the model has made its prediction we may test its e ectiveness, using relevant met-
rics. For example, for classiÔ¨Åcation we may wish to produce the confusion matrix for theAppendix D. Python Primer 493
test data. The following code does this for the data shown in Figure D.5, using a support
vector machine classiÔ¨Åer.
from sklearn import svm
clf = svm.SVC(kernel = 'rbf')
clf.fit(X_train , y_train)
y_prediction = clf.predict(X_test)
from sklearn.metrics import confusion_matrix
print (confusion_matrix(y_test , y_prediction))
[[102 12]
[ 1 85]]
D.14 System Calls, URL Access, and Speed-Up
Operating system commands (whether in Windows, MacOS, or Linux) for creating dir-
ectories, copying or removing Ô¨Åles, or executing programs from the system shell can be
issued from within Python by using the package os. Another useful package is requests
which enables direct downloads of Ô¨Åles and webpages from URLs. The following Python
script uses both. It also illustrates a simple example of exception handling in Python.
misc.py
import os
import requests
for cin"123456":
try: # if it does not yet exist
os.mkdir("MyDir"+ c) # make a directory
except : # otherwise
pass # do nothing
uname = "https://github.com/DSML -book/Programs/tree/master/
Appendices/Python Primer/"
fname = "ataleof2cities.txt"
r = requests.get(uname + fname)
print (r.text)
open ('MyDir1/ato2c.txt ','wb').write(r.content) #write to a file
# bytes mode is important here
The package numba can signiÔ¨Åcantly speed up calculations via smart compilation. First
run the following code.
jitex.py
import timeit
import numpy as np
from numba import jit
n = 10**8
#@jit
def myfun(s,n):
for iin range (1,n):494 D.14. System Calls, URL Access, and Speed-Up
s = s+ 1/i
return s
start = timeit.time.clock()
print ("Euler 's constant is approximately {:9.8f}". format (
myfun(0,n) - np.log(n)))
end = timeit.time.clock()
print ("elapsed time: {:3.2f} seconds". format (end-start))
Euler 's constant is approximately 0.57721566
elapsed time: 5.72 seconds
Now remove the # character before the @ character in the code above, in order to
activate the ‚Äújust in time‚Äù compiler. This gives a 15-fold speedup:
Euler 's constant is approximately 0.57721566
elapsed time: 0.39 seconds
Further Reading
To learn Python, we recommend [82] and [110]. However, as Python is constantly evolving,
the most up-to-date references will be available from the Internet.BIBLIOGRAPHY
[1] S. C. Ahalt, A. K. Krishnamurthy, P. Chen, and D. E. Melton. Competitive learning
algorithms for vector quantization. Neural Networks , 3:277‚Äì290, 1990.
[2] H. Akaike. A new look at the statistical model identiÔ¨Åcation. IEEE Transactions on
Automatic Control , 19(6):716‚Äì723, 1974.
[3] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Math-
ematical Society , 68:337‚Äì404, 1950.
[4] D. Arthur and S. Vassilvitskii. K-means ++: The advantages of careful seeding.
InProceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Al-
gorithms , pages 1027‚Äì1035, Philadelphia, 2007. Society for Industrial and Applied
Mathematics.
[5] S. Asmussen and P. W. Glynn. Stochastic Simulation: Algorithms and Analysis .
Springer, New York, 2007.
[6] R. G. Bartle. The Elements of Integration and Lebesgue Measure . John Wiley &
Sons, Hoboken, 1995.
[7] D. Bates and D. Watts. Nonlinear Regression Analysis and Its Applications . John
Wiley & Sons, Hoboken, 1988.
[8] J. O. Berger. Statistical Decision Theory and Bayesian Analysis . Springer, New
York, second edition, 1985.
[9] J. Bezdek. Pattern Recognition with Fuzzy Objective Function Algorithms . Plenum
Press, New York, 1981.
[10] P. J. Bickel and K. A. Doksum. Mathematical Statistics , volume I. Pearson Prentice
Hall, Upper Saddle River, second edition, 2007.
495496 Bibliography
[11] P. Billingsley. Probability and Measure . John Wiley & Sons, New York, third
edition, 1995.
[12] C. M. Bishop. Pattern Recognition and Machine Learning . Springer, New York,
2006.
[13] P. T. Boggs and R. H. Byrd. Adaptive, limited-memory BFGS algorithms for un-
constrained optimization. SIAM Journal on Optimization , 29(2):1282‚Äì1299, 2019.
[14] Z. I. Botev, J. F. Grotowski, and D. P. Kroese. Kernel density estimation via di u-
sion. Annals of Statistics , 38(5):2916‚Äì2957, 2010.
[15] Z. I. Botev and D. P. Kroese. Global likelihood optimization via the cross-entropy
method, with an application to mixture models. In R. G. Ingalls, M. D. Rossetti,
J. S. Smith, and B. A. Peters, editors, Proceedings of the 2004 Winter Simulation
Conference , pages 529‚Äì535, Washington, DC, December 2004.
[16] Z. I. Botev, D. P. Kroese, R. Y . Rubinstein, and P. L‚ÄôEcuyer. The cross-entropy
method for optimization. In V . Govindaraju and C.R. Rao, editors, Machine Learn-
ing: Theory and Applications , volume 31 of Handbook of Statistics , pages 35‚Äì59.
Elsevier, 2013.
[17] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and
statistical learning via the alternating direction method of multipliers. Foundations
and Trends in Machine Learning , 3:1‚Äì122, 2010.
[18] S. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press,
Cambridge, 2004. Seventh printing with corrections, 2009.
[19] R. A. Boyles. On the convergence of the EM algorithm. Journal of the Royal
Statistical Society, Series B , 45(1):47‚Äì50, 1983.
[20] L. Breiman. ClassiÔ¨Åcation and Regression Trees . CRC Press, Boca Raton, 1987.
[21] L. Breiman. Bagging predictors. Machine Learning , 24(2):123‚Äì140, 1996.
[22] L. Breiman. Heuristics of instability and stabilization in model selection. Annals of
Statistics , 24(6):2350‚Äì2383, 12 1996.
[23] L. Breiman. Random forests. Machine Learning , 45(1):5‚Äì32, 2001.
[24] F. Cao, D.-Z. Du, B. Gao, P.-J. Wan, and P. M. Pardalos. Minimax problems in
combinatorial optimization. In D.-Z. Du and P. M. Pardalos, editors, Minimax and
Applications , pages 269‚Äì292. Kluwer, Dordrecht, 1995.
[25] G. Casella and R. L. Berger. Statistical Inference . Duxbury Press, PaciÔ¨Åc Grove,
second edition, 2001.
[26] K. L. Chung. A Course in Probability Theory . Academic Press, New York, second
edition, 1974.Bibliography 497
[27] E. Cinlar. Introduction to Stochastic Processes . Prentice Hall, Englewood Cli s,
1975.
[28] T. M. Cover and J. A. Thomas. Elements of Information Theory . John Wiley &
Sons, New York, 1991.
[29] J. W. Daniel, W. B. Gragg, L. Kaufman, and G. W. Stewart. Reorthogonalization and
stable algorithms for updating the Gram-Schmidt QR factorization. Mathematics of
Computation , 30(136):772‚Äì795, 1976.
[30] P.-T. de Boer, D. P. Kroese, S. Mannor, and R. Y . Rubinstein. A tutorial on the
cross-entropy method. Annals of Operations Research , 134(1):19‚Äì67, 2005.
[31] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of the Royal Statistical Society , 39(1):1 ‚Äì
38, 1977.
[32] L. Devroye. Non-Uniform Random Variate Generation . Springer, New York, 1986.
[33] N. R. Draper and H. Smith. Applied Regression Analysis . John Wiley & Sons, New
York, third edition, 1998.
[34] Q. Duan and D. P. Kroese. Splitting for optimization. Computers &Operations
Research , 73:119‚Äì131, 2016.
[35] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern ClassiÔ¨Åcation . John Wiley & Sons,
New York, 2001.
[36] B. Efron and T. J. Hastie. Computer Age Statistical Inference: Algorithms, Evidence,
and Data Science . Cambridge University Press, Cambridge, 2016.
[37] B. Efron and R. Tibshirani. An Introduction to the Bootstrap . Chapman & Hall,
New York, 1994.
[38] T. Fawcett. An introduction to ROC analysis. Pattern Recognition Letters ,
27(8):861‚Äì874, June 2006.
[39] W. Feller. An Introduction to Probability Theory and Its Applications , volume I.
John Wiley & Sons, Hoboken, second edition, 1970.
[40] J. C. Ferreira and V . A. Menegatto. Eigenvalues of integral operators deÔ¨Åned by
smooth positive deÔ¨Ånite kernels. Integral Equations and Operator Theory , 64:61‚Äì
81, 2009.
[41] N. I. Fisher and P. K. Sen, editors. The Collected Works of Wassily Hoe ding.
Springer, New York, 1994.
[42] G. S. Fishman. Monte Carlo: Concepts, Algorithms and Applications . Springer,
New York, 1996.
[43] R. Fletcher. Practical Methods of Optimization . John Wiley & Sons, New York,
1987.498 Bibliography
[44] Y . Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. J. Comput. Syst. Sci. , 55(1):119‚Äì139, 1997.
[45] J. H. Friedman. Greedy function approximation: A gradient boosting machine. An-
nals of Statistics , 29:1189‚Äì1232, 2000.
[46] A. Gelman. Bayesian Data Analysis . Chapman & Hall, New York, second edition,
2004.
[47] A. Gelman and J. Hall. Data Analysis Using Regression and Multilevel /Hierarchical
Models . Cambridge University Press, Cambridge, 2006.
[48] S. Geman and D. Geman. Stochastic relaxation, Gibbs distribution and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 6(6):721‚Äì741, 1984.
[49] J. E. Gentle. Random Number Generation and Monte Carlo Methods . Springer,
New York, second edition, 2003.
[50] W. R. Gilks, S. Richardson, and D. J. Spiegelhalter. Markov Chain Monte Carlo in
Practice . Chapman & Hall, New York, 1996.
[51] P. Glasserman. Monte Carlo Methods in Financial Engineering . Springer, New
York, 2004.
[52] G. H. Golub and C. F. Van Loan. Matrix Computations . Johns Hopkins University
Press, Baltimore, fourth edition, 2013.
[53] I. Goodfellow, Y . Bengio, and A. Courville. Deep Learning . MIT Press, Cambridge,
2016.
[54] G. R. Grimmett and D. R. Stirzaker. Probability and Random Processes . Oxford
University Press, third edition, 2001.
[55] T. J. Hastie, R. J. Tibshirani, and J. H. Friedman. The Elements of Statistical Learn-
ing: Data mining, Inference, and Prediction . Springer, New York, 2009.
[56] T. J. Hastie, R. J. Tibshirani, and M. Wainwright. Statistical Learning with Sparsity:
The Lasso and Generalizations . CRC Press, Boca Raton, 2015.
[57] J.-B. Hiriart-Urruty and C. Lemar√®chal. Fundamentals of Convex Analysis .
Springer, New York, 2001.
[58] W. Hock and K. Schittkowski. Test Examples for Nonlinear Programming Codes .
Springer, New York, 1981.
[59] J. E. Kelley, Jr. The cutting-plane method for solving convex programs. Journal of
the Society for Industrial and Applied Mathematics , 8(4):703‚Äì712, 1960.
[60] A. K. Jain. Fundamentals of Digital Image Processing . Prentice Hall, Englewood
Clis, 1989.Bibliography 499
[61] O. Kallenberg. Foundations of Modern Probability . Springer, New York, second
edition, 2002.
[62] A. Karalic. Linear regression in regression tree leaves. In Proceedings of ECAI-92 ,
pages 440‚Äì441, Hoboken, 1992. John Wiley & Sons.
[63] C. Kaynak. Methods of combining multiple classiÔ¨Åers and their applications to
handwritten digit recognition. Master‚Äôs thesis, Institute of Graduate Studies in Sci-
ence and Engineering, Bogazici University, 1995.
[64] T. Keilath and A. H. Sayed, editors. Fast Reliable Algorithms for Matrices with
Structure . SIAM, Pennsylvania, 1999.
[65] C. Nussbaumer KnaÔ¨Çic. Storytelling with Data: A Data Visualization Guide for
Business Professionals . John Wiley & Sons, Hoboken, 2015.
[66] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Tech-
niques - Adaptive Computation and Machine Learning . The MIT Press, Cambridge,
2009.
[67] A. N. Kolmogorov and S. V . Fomin. Elements of the Theory of Functions and
Functional Analysis . Dover Publications, Mineola, 1999.
[68] D. P. Kroese, T. Brereton, T. Taimre, and Z. I. Botev. Why the Monte Carlo method
is so important today. Wiley Interdisciplinary Reviews: Computational Statistics ,
6(6):386‚Äì392, 2014.
[69] D. P. Kroese and J. C. C. Chan. Statistical Modeling and Computation . Springer,
2014.
[70] D. P. Kroese, S. Porotsky, and R. Y . Rubinstein. The cross-entropy method for
continuous multi-extremal optimization. Methodology and Computing in Applied
Probability , 8(3):383‚Äì407, 2006.
[71] D. P. Kroese, T. Taimre, and Z. I. Botev. Handbook of Monte Carlo Methods . John
Wiley & Sons, New York, 2011.
[72] H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms
and Applications . Springer, New York, second edition, 2003.
[73] P. Lafaye de Micheaux, R. Drouilhet, and B. Liquet. The R Software: Fundamentals
of Programming and Statistical Analysis . Springer, New York, 2014.
[74] R. J. Larsen and M. L. Marx. An Introduction to Mathematical Statistics and Its
Applications . Prentice Hall, New York, third edition, 2001.
[75] A. M. Law and W. D. Kelton. Simulation Modeling and Analysis . McGraw-Hill,
New York, third edition, 2000.
[76] P. L‚ÄôEcuyer. A uniÔ¨Åed view of IPA, SF, and LR gradient estimation techniques.
Management Science , 36:1364‚Äì1383, 1990.500 Bibliography
[77] P. L‚ÄôEcuyer. Good parameters and implementations for combined multiple recursive
random number generators. Operations Research , 47(1):159 ‚Äì 164, 1999.
[78] E. L. Lehmann and G. Casella. Theory of Point Estimation . Springer, New York,
second edition, 1998.
[79] T. G. Lewis and W. H. Payne. Generalized feedback shift register pseudorandom
number algorithm. Journal of the ACM , 20(3):456‚Äì468, 1973.
[80] R. J. A. Little and D. B. Rubin. Statistical Analysis with Missing Data . John Wiley
& Sons, Hoboken, second edition, 2002.
[81] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale
optimization. Mathematical Programming , 45(1-3):503‚Äì528, 1989.
[82] M. Lutz. Learning Python . O‚ÄôReilly, Ô¨Åfth edition, 2013.
[83] M. Matsumoto and T. Nishimura. Mersenne twister: A 623-dimensionally
equidistributed uniform pseudo-random number generator. ACM Transactions on
Modeling and Computer Simulation , 8(1):3‚Äì30, 1998.
[84] W. McKinney. Python for Data Analysis . O‚ÄôReilly Media, Inc., second edition,
2017.
[85] G. J. McLachlan and T. Krishnan. The EM Algorithm and Extensions . John Wiley
& Sons, Hoboken, second edition, 2008.
[86] G. J. McLachlan and D. Peel. Finite Mixture Models . John Wiley & Sons, New
York, 2000.
[87] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller.
Equations of state calculations by fast computing machines. Journal of Chemical
Physics , 21(6):1087‚Äì1092, 1953.
[88] C. A. Micchelli, Y . Xu, and H. Zhang. Universal kernels. Journal of Machine
Learning Research , 7:2651‚Äì2667, 2006.
[89] Z. Michalewicz. Genetic Algorithms +Data Structures =Evolution Programs .
Springer, New York, third edition, 1996.
[90] J. F. Monahan. Numerical Methods of Statistics . Cambridge University Press, Lon-
don, 2010.
[91] T. A. Mroz. The sensitivity of an empirical model of married women‚Äôs hours of
work to economic and statistical assumptions. Econometrica , 55(4):765‚Äì799, 1987.
[92] K. P. Murphy. Machine Learning: A Probabilistic Perspective . The MIT Press,
Cambridge, 2012.
[93] J. Neyman and E. Pearson. On the problem of the most e cient tests of statistical
hypotheses. Philosophical Transactions of the Royal Society of London, Series A ,
231:289‚Äì337, 1933.Bibliography 501
[94] M. A. Nielsen. Neural Networks and Deep Learning , volume 25. Determination
Press, 2015.
[95] K. B. Petersen and M. S. Pedersen. The Matrix Cookbook. Technical University of
Denmark , 2008.
[96] J. R. Quinlan. Learning with continuous classes. In A. Adams and L. Sterling,
editors, Proceedings AI‚Äô92 , pages 343‚Äì348, Singapore, 1992. World ScientiÔ¨Åc.
[97] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning .
MIT Press, Cambridge, 2006.
[98] B. D. Ripley. Stochastic Simulation . John Wiley & Sons, New York, 1987.
[99] C. P. Robert and G. Casella. Monte Carlo Statistical Methods . Springer, New York,
second edition, 2004.
[100] S. M. Ross. Simulation . Academic Press, New York, third edition, 2002.
[101] S. M. Ross. A First Course in Probability . Prentice Hall, Englewood Cli s, seventh
edition, 2005.
[102] R. Y . Rubinstein. The cross-entropy method for combinatorial and continuous op-
timization. Methodology and Computing in Applied Probability , 2:127‚Äì190, 1999.
[103] R. Y . Rubinstein and D. P. Kroese. The Cross-Entropy Method: A UniÔ¨Åed Approach
to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning .
Springer, New York, 2004.
[104] R. Y . Rubinstein and D. P. Kroese. Simulation and the Monte Carlo Method . John
Wiley & Sons, New York, third edition, 2017.
[105] S. Ruder. An overview of gradient descent optimization algorithms. arXiv:
1609.04747 , 2016.
[106] W. Rudin. Functional Analysis . McGraw‚ÄìHill, Singapore, second edition, 1991.
[107] D. Salomon. Data Compression: The Complete Reference . Springer, New York,
2000.
[108] G. A. F. Seber and A. J. Lee. Linear Regression Analysis . John Wiley & Sons,
Hoboken, second edition, 2003.
[109] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From The-
ory to Algorithms . Cambridge University Press, Cambridge, 2014.
[110] Z. A. Shaw. Learning Python 3 the Hard Way . Addison‚ÄìWesley, Boston, 2017.
[111] Y . Shen, S. Kiatsupaibul, Z. B. Zabinsky, and R. L. Smith. An analytically de-
rived cooling schedule for simulated annealing. Journal of Global Optimization ,
38(2):333‚Äì365, 2007.502 Bibliography
[112] N. Z. Shor. Minimization Methods for Non-di erentiable Functions . Springer, Ber-
lin, 1985.
[113] B. W. Silverman. Density Estimation for Statistics and Data Analysis . Chapman &
Hall, New York, 1986.
[114] J. S. Simono .Smoothing Methods in Statistics . Springer, New York, 2012.
[115] I. Steinwart and A. Christmann. Support Vector Machines . Springer, New York,
2008.
[116] G. Strang. Introduction to Linear Algebra . Wellesley‚ÄìCambridge Press, Cambridge,
Ô¨Åfth edition, 2016.
[117] G. Strang. Linear Algebra and Learning from Data . Wellesley‚ÄìCambridge Press,
Cambridge, 2019.
[118] W. N. Street, W. H. Wolberg, and O. L. Mangasarian. Nuclear feature extraction for
breast tumor diagnosis. In IS&T/SPIE 1993 International Symposium on Electronic
Imaging: Science and Technology, San Jose, CA , pages 861‚Äì870, 1993.
[119] V . M. Tikhomirov. On the representation of continuous functions of several variables
as superpositions of continuous functions of one variable and addition. In Selected
Works of A. N. Kolmogorov , pages 383‚Äì387. Springer, Berlin, 1991.
[120] S. van Buuren. Flexible Imputation of Missing Data . CRC Press, Boca Raton,
second edition, 2018.
[121] V . N. Vapnik. The Nature of Statistical Learning Theory . Springer, New York, 1995.
[122] V . N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative fre-
quencies of events to their probabilities. Theory of Probability and Its Applications ,
16(2):264‚Äì280, 1971.
[123] G. Wahba. Spline Models for Observational Data . SIAM, Philadelphia, 1990.
[124] L. Wasserman. All of Statistics: A Concise Course in Statistical Inference . Springer,
2010.
[125] A. Webb. Statistical Pattern Recognition . Arnold, London, 1999.
[126] H. Wendland. Scattered Data Approximation . Cambridge University Press, Cam-
bridge, 2005.
[127] D. Williams. Probability with Martingales . Cambridge University Press, Cam-
bridge, 1991.
[128] C. F. J. Wu. On the convergence properties of the EM algorithm. The Annals of
Statistics , 11(1):95‚Äì103, 1983.INDEX
A
acceptance probability, 78‚Äì80, 97
acceptance‚Äìrejection method, 73, 78
accuracy (classiÔ¨Åcation‚Äì), 254
activation function, 204, 325
AdaBoost, 317‚Äì320
AdaGrad, 339
Adam method, 339, 346
adjoint operation, 361
ane transformation, 405, 435
agglomerative clustering, 147
Akaike information criterion, 126, 176,
177
algebraic multiplicity, 363
aligned arrays (Python), 481
almost sure convergence, 440
alternating direction method of
multipliers, 220, 416
alternative hypothesis, 458
anaconda (Python), 463
analysis of variance (ANOV A), 183, 184,
195, 208
annealing schedule, 97
approximation error, 32‚Äì34, 184
approximation‚Äìestimation tradeo , 32,
41, 323
Armijo inexact line search, 409
assignment operator (Python), 467
attributes (Python), 465auxiliary variable methods, 128
axioms of Kolmogorov, 421
B
back-propagation, 331
backward elimination, 201
backward substitution, 370
bagged estimator, 306
bagging, 305, 307, 310
balance equations (Markov chains), 78,
79, 452
bandwidth, 131, 134, 225
barplot, 9
barrier function, 417
Barzilai‚ÄìBorwein formulas, 334, 413
basis
of a vector space, 355
orthogonal ‚Äì, 361
Bayes
empirical, 241
error rate, 252
factor, 57
na√Øve ‚Äì, 258
optimal decision rule, 258
Bayes‚Äô rule, 47, 48, 428, 454
Bayesian information criterion, 54
Bayesian statistics, 47, 49, 454
Bernoulli distribution, 425, 457
Bessel distribution, 164, 226
503504 Index
beta distribution, 52, 425
bias of an estimator, 454
bias vector (deep learning), 326
bias‚Äìvariance tradeo , 35, 305
binomial distribution, 425
Boltzmann distribution, 96
bootstrap aggregation, seebagging
bootstrap method, 88, 306
bounded mapping, 389
boxplot, 10, 14
broadcasting (Python), 481
Broyden‚Äôs family, 411
Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno
(BFGS) updating, 267, 338, 411
burn-in period, 78
C
categorical variable, 3, 177, 178, 191,
192, 251, 299
Cauchy sequence, 245, 385
Cauchy‚ÄìSchwarz inequality, 223, 246,
389, 412
central di erence estimator, 106
central limit theorem, 447, 458
multivariate, 448
centroid, 144
chain rule for di erentiation, 401
characteristic function, 225‚Äì227, 246,
392, 441
characteristic polynomial, 363
Chebyshev‚Äôs inequality, 444
chi-squared distribution, 436, 439
Cholesky decomposition, 70, 154, 247,
264, 373
circulant matrix, 381, 393
class (Python), 473
classiÔ¨Åcation, 20, 251‚Äì286
hierarchical, 256
multilabel, 256
classiÔ¨Åer, 21, 251
coecient of determination, 181, 195
adjusted, 181
coecient proÔ¨Åles, 221
combinatorial optimization, 402
comma separated values (CSV), 2
common random numbers, 106, 119complete Hilbert space, 224, 384
complete vector space, 216
complete convergence, 443
complete-data
likelihood, 128
log-likelihood, 138
composition of functions, 400
concave function, 404, 407
conditional
distribution, 431
expectation, 431
pdf, 74, 431
probability, 428
conÔ¨Ådence interval, 85, 89, 94, 186, 457
Bayesian, 51
bootstrap, 89
conÔ¨Ådence region, 457
confusion matrix, 253, 254
constrained optimization, 403
context management (Python), 477
continuous mapping, 389
continuous optimization, 402
control variable, 92
convergence
almost sure, 440
inLpnorm, 442
in distribution, 440
in probability, 439
sure, 439
convex
function, 62, 220, 403
program, 405‚Äì408
set, 42, 403
convolution, 380, 392
convolution neural network, 329
Cook‚Äôs distance, 212
cooling factor, 97
correlation coe cient, 71, 429
cost-complexity
measure, 303
pruning, 303
countable sample space, 422
covariance, 429
matrix, 45, 70, 430‚Äì432, 435, 436
properties, 429
coverage probability, 457Index 505
credible
interval, 51
region, 51
critical
region, 458
value, 458
cross tabulate, 7
cross-entropy
method, 100, 110
risk, 53, 122, 125
in-sample, 176
training loss, 123
cross-validation, 37, 38
leave-one-out, 40, 173
linear model, 174
crude Monte Carlo, 85
cubic spline, 236
cumulative distribution function (cdf),
72, 423
joint, 427
cycle, 81
D
Davidon‚ÄìFletcher‚ÄìPowell updating, 352,
412
decision tree, 288
deep learning, 330
degrees of freedom, 181
dendrogram, 147
density, 385
dependent variable, 168
derivatives
multidimensional, 398
partial, 397
design matrix, 179
detailed balance equations, 453
determinant of a matrix, 357
diagonal matrix, 357
diagonalizable, 364
dictionary (Python), 473
digamma function, 127, 162
dimension, 355
direct sum, 217
directional derivative, 404
discrete
distribution, 423Fourier transform, 392
optimization, 402
probability space, 422
sample space, 422
uniform distribution, 425
discriminant analysis, 259
distribution
Bernoulli, 425
Bessel, 226
beta, 52, 425
binomial, 425
chi-squared, 436, 439
discrete, 423
discrete uniform, 425
exponential, 425
extreme value, 114
F, 439
gamma, 425
Gaussian, seenormal
geometric, 425
inverse-gamma, 50, 83
joint, 427
multivariate normal, 45, 435
noncentral2, 437
normal, 44, 425, 434
Pareto, 425
Poisson, 425
probability, 422, 427
Student‚Äôs t, 439
uniform, 425
Weibull, 425
divisive clustering, 147
dot notation (Python), 466
dual optimization problem, 407‚Äì408
E
early stopping, 49, 250
eciency
of estimators, 454
of acceptance‚Äìrejection, 72
eigen-decomposition, 364
eigenvalue, 363
eigenvector, 363
elementary event, 422
elite sample, 100
empirical506 Index
Bayes, 241
cdf, 11, 76
distribution, 131
entropy impurity, 292
epoch (deep learning), 349
equilikely principle, 422
ergodic Markov chain, 452
error of the Ô¨Årst and second kind, 459
estimate, 454
estimator, 454
bias of, 454
control variable, 92
eciency of, 454
unbiased, 454
Euclidean norm, 360
evaluation functional, 223, 245
event, 421
elementary, 422
independent, 428
exact match ratio, 256
exchangeable variables, 40
expectation, 426
conditional, 431
properties, 429, 431
vector, 45, 430, 432, 435
expectation‚Äìmaximization (EM)
algorithm, 128, 137, 209
expected generalization risk, 24
expected optimism, 36
explanatory variable, 22, 168
exponential distribution, 425
extreme value distribution, 114
F
factor, 3, 178
false negative, 254
false positive, 254
fast Fourier transform, 394
Fscore, 255
Fdistribution, 183, 197, 424, 439
feasible region, 403
feature, 1, 20
importance, 311
map, 189, 216, 224, 230, 243, 274
feed-forward network, 326
feedback shift register, 69Ô¨Ånite di erence method, 107, 113
Ô¨Ånite-dimensional distributions, 427
Fisher information matrix, 124
Fisher‚Äôs scoring method, 127
folds (cross-validation), 38
forward selection, 200
forward substitution, 370
Fourier expansion, 386
Fourier transform, 391
discrete, 392
frequentist statistics, 453
full rank matrix, 28
function (Python), 468
function space, 384
function, Ck, 403
functional, 389
functions of random variables, 431
G
gamma
distribution, 425
function, 424
Gauss‚ÄìMarkov inequality, 59
Gauss‚ÄìNewton search direction, 414
Gaussian distribution, seenormal
distribution
Gaussian kernel, 225
Gaussian kernel density estimate, 131
Gaussian process, 71, 238
Gaussian rule of thumb, 134
generalization risk, 23, 86
generalized inverse-gamma distribution,
163
generalized linear model, 204
geometric cooling, 97
geometric distribution, 425
geometric multiplicity, 363
Gibbs pdf, 97
Gibbs sampler, 81, 83, 84
random, 82
random order, 82
reversible, 82
Gini impurity, 292
global balance equations, 452
global minimizer, 402
gradient, 397, 403Index 507
boosting, 316
descent, 412
Gram matrix, 218, 222, 270
Gram‚ÄìSchmidt procedure, 375
H
Hamming distance, 142
Hermite polynomials, 389
Hermitian matrix, 362, 365
Hessian matrix, 124, 398, 403, 404
hidden layer, 325
hierarchical classiÔ¨Åcation, 256
Hilbert matrix, 33
inverse, 60
Hilbert space, 215, 385
isomorphism, 246
hinge loss, 269
histogram, 10
Hoeding‚Äôs inequality, 62
homotopy paths, 221
hyperparameters, 50, 241
hypothesis testing, 458
I
immutable (Python), 464
importance sampling, 93‚Äì96
improper prior, 50, 83
in-sample risk, 35
incremental e ects, 179
independence
of event, 428
of random variables, 429
independence sampler, 79
independent and identically distributed
(iid), 429, 446, 454
indicator, 11
indicator feature, 178
indicator loss, 251
inÔ¨Ånitesimal perturbation analysis, 113
information matrix equality, 124
inheritance (Python), 474
initial distribution (Markov chain), 452
inner product, 360
instance (Python), 474
integration
Monte Carlo, 86
interaction, 179, 193interior-point method, 418
interval estimate, seeconÔ¨Ådence interval
inverse
discrete Fourier transform, 393
Fourier transform, 391
matrix, 370
inverse-gamma distribution, 50, 83
inverse-transform method, 72
irreducible risk, 32
iterable (Python), 472
iterative reweighted least squares, 213,
349
iterator (Python), 472
J
Jacobi
matrix of, 409, 433
Jensen‚Äôs inequality, 62
joint
cdf, 427
pdf, 427
jointly normal, seemultivariate normal
jointly normal distribution, see
multivariate normal distribution
K
Karush‚ÄìKuhn‚ÄìTucker (KKT) conditions,
407, 408
kernel density estimation, 131, 135, 226,
329
kernel trick, 231
Kiefer‚ÄìWolfowitz algorithm, 107
K-nearest neighbors method, 268
Kolmogorov axioms, 421
Kullback‚ÄìLeibler divergence, 42, 100,
128, 350
L
Lagrange
dual program, 407
function, 406
method, 406‚Äì407
multiplier, 406
Lagrangian, 406, 416
penalty, 416
Laguerre polynomials, 388
Lance‚ÄìWilliams update, 149508 Index
Laplace‚Äôs approximation, 450
lasso (regression), 220
latent variable methods, seeauxiliary
variable methods
law of large numbers, 67, 446, 458
law of total probability, 428
learner, 22, 168
learning rate, 334, 409
least-squares
iterative reweighted, 213
nonlinear, 190, 335, 414
ordinary, 27, 46, 171, 191, 211, 378
regularized, 172, 235, 376
leave-one-out cross-validation, 40, 173
left pseudo-inverse, 360
left-eigenvector, 365
Legendre polynomials, 387
length preserving transformation, 361
length of a vector, 360
level set, 103
Levenberg‚ÄìMarquardt search direction,
415
leverage, 173
Levinson‚ÄìDurbin, 71, 382
likelihood, 42, 48, 123, 456
complete-data, 128
log-, 136, 456
optimization, 137
ratio, 93
limited memory BFGS, 336
limiting pdf, 452
limiting pdf (Markov chain), 452
line search, 409
linear
discriminant function, 260
kernel, 224, 271
mapping, 389
model, 43, 211
program, 406
subspace, 362
transformation, 356, 431
linearly independent, 355
link function, 204
linkage, 148
matrix, 150
list comprehension (Python), 473local balance equations, seedetailed
balance equations
local minimizer, 402
local/global minimum, 402
log-likelihood, 456
log-odds ratio, 266
logarithmic e ciency, 117
logistic distribution, 204
logistic regression, 204
long-run average reward, 89
loss function, 20
loss matrix, 253
M
M-estimator, 448
Manhattan distance, 142
marginal distribution, 427, 436
Markov chain, 74, 78, 80, 83, 451
ergodic, 452
reversible, 452
simulation of, 75
Markov chain Monte Carlo, 78
Markov property, 74, 451
Mat√©rn kernel, 226
matplotlib (Python), 483‚Äì485
matrix, 356
blockwise inverse, 370
covariance, 70, 436
determinant, 357
diagonal ‚Äî, 357
inverse, 357
of Jacobi, 398, 409, 414, 433
pseudo-inverse, 360
sparse, 379
Toeplitz, 379
trace, 357
transpose, 357
matrix multiplication (Python), 481
max-cut problem, 151
maximum a posteriori, 52
maximum distance, 142
maximum likelihood estimation, 42, 46,
100, 127, 136, 137, 456
mean integrated squared error, 133
mean squared error, 32, 88, 454
measure, 385Index 509
Mersenne twister, 69
method (Python), 466
method of moments, 455
Metropolis‚ÄìHastings algorithm, 78, 81
minibatch, 335
minimax
equality, 408
problem, 408
minimization, 411
minimizer, 402
minimum
global, 402
local, 402
misclassiÔ¨Åcation error, 253
misclassiÔ¨Åcation impurity, 292
mixture density, 135
model, 40
evidence, 54
linear, 211
matrix, 43, 170, 174
multiple linear regression, 169
normal linear, 174, 182, 183, 438
regression, 191
response surface, 189
simple linear regression, 187
modiÔ¨Åed Bessel function of the second
kind, 163, 226
module (Python), 469
modulo 2 generators, 69
modulus, 69
moment
generating function, 427, 436
sample-, 455
momentum method, 340
Monte Carlo
integration, 86
sampling, 68‚Äì85
simulation, 67
Moore‚ÄìPenrose pseudo-inverse, 360
multi-logit, 266
multi-output linear regression, 213
nonlinear, 328
multilabel classiÔ¨Åcation, 256
multiple linear regression, 169
multiple-recursive generator, 69
multiplierLagrange, 406
multivariate
central limit theorem, 448
normal distribution, 44‚Äì46, 435
mutable (Python), 464
N
na√Øve Bayes, 258
namespace (Python), 470
nested models, 58, 180
network architecture, 329
network depth, 327
network width, 327
neural networks, 323
Newton‚Äôs method, 127, 205, 213, 336,
409
‚Äî for root-Ô¨Ånding, 409
quasi ‚Äî, 336
Neyman‚ÄìPearson approach, 459
noisy optimization, 105
nominal distribution, 93
noncentral2distribution, 437
norm, 384, 389
normal distribution, 45, 425, 434, 435
normal equations, 28
normal linear model, 46, 174, 182, 183,
438
normal matrix, 365
normal method (bootstrap), 89
normal model, 44
Bayesian, 49, 50, 83
normal updating (cross-entropy), 101
null hypothesis, 458
null space, 363
O
object (Python), 464
objective function, 402, 403, 407, 415
Occam‚Äôs razor, 173
operator, 389
operator (Python), 465
optimal decision boundary, 270
optimization
combinatorial, 402
constrained, 403
continuous, 402
unconstrained, 403510 Index
ordinary least-squares, 27
orthogonal
basis, 361
complement, 362
matrix, 361, 382
polynomial, 388
projection, 362
vector, 360
orthonormal, 361
basis, 386
system, 385
out-of-bag, 307
overÔ¨Åtting, 23, 35, 141, 172, 216, 237,
289, 293, 300, 314
overloading (Python), 468
P
p-norm, 220, 408
P-value, 195, 459
pandas (Python), 2, 485‚Äì490
Pareto distribution, 425
Parseval‚Äôs formula, 392
partial derivative, 397
partition, 428
peaks function, 233
Pearson‚Äôs height data, 207
penalty function, 415, 419
exact, 416
percentile, 7
percentile method (bootstrap), 89, 91
permutation matrix, 368
Plancherel‚Äôs theorem, 392
PLU decomposition, 368
pointwise squared bias, 35
pointwise variance, 35
Poisson distribution, 425
polynomial kernel, 230
polynomial regression model, 26
positive deÔ¨Ånite
matrix, 403
positive semideÔ¨Ånite
function, 222
matrix, 367, 404
posterior
pdf, 48
predictive density, 49precision, 255
predicted residual, 173
‚Äî sum of squares (PRESS), 173
prediction function, 20
prediction interval, 186
predictive mean, 240
predictor, 168
primal optimization problem, 407
principal axes, 154
principal component analysis (PCA),
153, 155
principal components, 154
prior
improper, 83
pdf, 48
predictive density, 49
uninformative, 49
probability
density function (pdf), 424
density function (pdf), joint, 427
distribution, 422, 427
mass function, 424
measure, 421
space, 422
product rule, 74, 428, 452
projected subgradient method, 106
projection matrix, 27, 173, 211, 265,
362, 438
projection pursuit, 349
proposal (MCMC), 78
pseudo-inverse, 28, 211, 360, 378
Pythagoras‚Äô theorem, 180, 181, 183, 231,
361
Q
quadratic discriminant function, 260
quadratic program, 406
qualitative variable, 3
quantile, 51, 85
quantile‚Äìquantile plot, 199
quantitative variable, 3
quartile, 7
quasi-Newton method, 336, 411
quasi-random point set, 233
quotient rule for di erentiation, 160Index 511
R
radial basis function (rbf) kernel, 225,
276
random
experiment, 421
number generator, 68
numbers (Python), 482
sample
see iid sample, 454
variable, 422
vector, 427, 431
covariance of, 430
expectation of, 430
walk sampler, 80
range (Python), 472
rank, 28, 356
rarity parameter (cross-entropy), 100
ratio estimator, 89
read_csv (Python), 2
recall, 255
reference (Python), 467
regional prediction functions, 288
regression, 20, 167
function, 21
line, 169
model, 191
simple linear, 181
regularization, 216, 217
paths, 221
regularization parameter, 217
regularizer, 217
relative error (estimated), 85
relative time variance product, 454
renewal reward process, 89
representational capacity, 323
representer of evaluation, 222
reproducing kernel Hilbert space
(RKHS), 222
reproducing property, 222
resampling, 76, 88
residual squared error, 171
residual sum of squares, 171
residuals, 171, 173
response surface model, 189
response variable, 20, 168
reverse Markov chain, 452reversibility, 452
reversible Gibbs sampler, 82
ridge regression, 216, 217
Riemann‚ÄìLebesgue lemma, 391
right pseudo-inverse, 360
risk, 20, 167
Robbins‚ÄìMonro algorithm, 106
root Ô¨Ånding, 408
R2,seecoecient of determination
S
saddle point, 403
problem, 408
sample
mean, 7, 85, 455
median, 7
quantile, 7
range, 8
space, 421
countable, 422
discrete, 422
standard deviation, 8, 455
variance, 8, 89, 455
saturation, 332
Savage‚ÄìDickey density ratio, 58
scale-mixture, 164
scatterplot, 13
scikit-learn (Python), 490‚Äì493
score function, 42, 123
method, 113
secant condition, 411
semi-simple matrix, 364
sequence object (Python), 472
set (Python), 472
shear operation, 359
Sherman‚ÄìMorrison
formula, 174, 247, 371
recursion, 372, 373, 414
signiÔ¨Åcance level, 459
simple linear regression, 169, 187
simulated annealing, 96, 97
sinc kernel, 226
singular value, 377, 378
singular value decomposition, 154, 376
slack variable, 417
Slater‚Äôs condition, 408512 Index
slice (Python), 3, 464
smoothing parameter, 100
softmax function, 267, 328
source vectors, 143
sparse matrix, 379
speciÔ¨Åcity, 255
spectral representation, 377
sphere the data, 264
splitting for continuous optimization,
103
splitting rule, 289
squared-error loss, 167
standard basis, 356
standard deviation, 426
sample-, 455
standard error (estimated), 85
standard normal distribution, 434
standardization, 435
stationary point, 403
statistical (estimation) error, 32, 95
statistical test
one-sided ‚Äì, 458
two-sided ‚Äì, 458
statistics
Bayesian, 454
frequentist, 453
steepest descent, 330, 412
step-size parameter , 314
stochastic approximation, 106, 335
stochastic conÔ¨Ådence interval, 457
stochastic counterpart, 107
stochastic gradient descent, 335, 349
stochastic process, 427
strict feasibility, 408
strong duality, 408
Student‚Äôs tdistribution, 183, 424, 439
multivariate, 162, 164, 226
studentized residual, 212
stumps, 319
subgradient, 404
subgradient method, 106
sum rule, 422
supervised learning, 22
support vectors, 271
Sylvester equation, 379
systematic Gibbs sampler, 82T
tables
counts, 6
frequency, 6
margins, 7
target distribution, 78
Taylor‚Äôs theorem
multidimensional, 400
test
loss, 24
sample, 24
statistic, 458
theta KDE, 134
time-homogeneous, 452
Tobit regression, 209
Toeplitz matrix, 379
total sum of squares, 181
tower property of expectation, 431
trace of a matrix, 357
training loss, 23
training set, 21
transformation
of random variables, 431, 433
rule, 95, 432
transition
density, 74, 452
graph, 75
transpose of a matrix, 356, 357
tree branch, 301
true negative, 254
true positive, 254
trust region, 409
type (Python), 466
type I and type II errors, 459
U
unbiased, 59
unbiased estimator, 454
unconstrained optimization, 403
uniform distribution, 425
union bound, 422
unitary matrix, 362
universal approximation property, 226
unsupervised learning, 22
V
validation set, 25, 303Index 513
Vandermonde matrix, 29, 393
Vapnik‚ÄìChernovenkis bound, 62
variance, 426, 430
properties, 429
sample, 89, 455
sample-, 455
vector quantization, 143
vector space, 355
basis, 355
dimension, 355V oronoi tessellation, 143
W
weak derivative, 113
weak duality, 407
weak learners, 313
Weibull distribution, 425
weight matrix (deep learning), 326
Wolfe dual program, 407
Woodbury identity, 248, 351, 371, 399