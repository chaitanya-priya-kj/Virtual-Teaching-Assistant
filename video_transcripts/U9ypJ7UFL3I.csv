Image Name,Transcript
000000_182982.8.jpg,at any point once it trees built you'd predict the the test observation by passing it down the tree so bein each of the splits it'll end up in a terminal node and then you'll you'll use the mean of the training observations in that region to make the prediction it's looking at a slightly bigger example a cartoon example in in the next slide first of all in the slide we show in the in the left panel here set of regions that couldn't be achieved by this process so this this one you could not be achieved because these aren't boxes that were obtained by sec sequentially splitting the data repeatedly whereas the one on the right can if you look at the one on the right in conjunction with the tree on the bottom left here you will see that the first split was made at t 1 so there's t 1 over there and that was a vertical split right that broke this region we've got two variables X 1 and X 2 you into a region to the left and to the right so that's a partition then we have a split on X 2 at T 2 so that is a splitter via where we divided that box into two and so that takes care of the left we got region 1 and region 2 then on the right hand partition that we had by splitting T 1 we split the X 1 again at t 3 so we make another vertical split here that partitions store these in and so that's a split up at this internal node over here and so that makes region 3 on the left and on the right we're going to do one more split and that's a t4 on variable X 2 that divides that into into two so you can see in the sequential greedy fashion we've subdivided the the space which is just two variables here into five different regions now at each of the terminal nodes we are going to approximate the regression function in this case by the mean of the training observations that end up in these terminal nodes so what that means is if you have a test observation you want to come and predicted you're going to start at the top and you're going to query what's its value for x1 if it's less than t1 you'll go to the left otherwise you'll go to the right likewise you answer each of the questions at each of the internal nodes and you're going to end up in one of these regions and then your prediction is going to be the mean in that region so as a function you can actually represent it nicely because it's just a two-dimensional function by this picture that we got on the right here and so it's a piecewise constant function that's its piecewise constant you can see here the slices let's see this will be this will be t1 is this slice over U and this will be t2 and this function is constant in each of the regions and that's how we're going to represent our function at the end of the day ok so we
000001_229562.7.jpg,see we've seen how we grow a tree the question one question arises is how large should the tree be when should we stop growing the tree well one possibility would just be to growth largest trees is possible right at the extreme we could for example have a tree that had one observation each terminal node or leaf but that's that's like an overfit the data producing poor test set performance I say why well I guess it's kind of obvious right if you have a tree that so large that each observation has its own terminal node it's going to have training error of 0 but it's going to be adapted to that data very much it's gonna over fit I'm going to give a new test data you probably going to have a quite high high error as a result so not a good idea to build as large as tree as possible another possibility would be to
000002_234901.3.jpg,stop early in other words just keep
000003_240240.0.jpg,splitting until it looks like no split helps in some sense in terms of reducing
000004_256256.0.jpg,through through zero sum of squares the criterion we saw before but it turns out this strategy can be too short-sighted no it's very often stop too early because it looks like no good split is available but actually if we kept splitting we'd see a better split farther down the tree so it turns out
000005_298965.3.jpg,that a better strategy is actually to grow a large tree stopping only once rule like for example several women's uses is a terminal node has to have it have no fewer than five observations say and then with this large tree which is probably too big we prune it from the bottom to produce say a tree which a midsize it's got it tries to balance bias and variance and and hence have lower prediction error than either the single term single node tree or the extreme tree that has one observation one note per observation so very bushy tree is got high variance in alright it's got high variance over fitting the data will bias but is overfitting and
000006_302168.5.jpg,probably not going to predict well so
000007_302402.1.jpg,
000008_304304.0.jpg,actually so what's done in in the cart
000009_309642.7.jpg,software for example or the tree library in R which will will will see is this
000010_405738.7.jpg,idea of building a large tree and then pruning it back and the the framework for that is called cost complexity pruning or weakest link pruning so for that we do something actually very much like the the lasso we saw for regression we have an objective function which is the fit and remember the fit we're going to measure in terms of the sum of squares of observations are the response around their assigned terminal nodes so our terminals are r1 through our M and I'm sorry our T the number of terminals we're going to know by this notation looks like an absolute value but it means the number of terminal nodes in the tree T so this is the sum of squares of observations around the mean of of region M which is one of the term o nodes and we add them up over all the term o notes so we want a tree which has small variance and but we also want to counterbalance the side the size of the tree so we put a penalty on the the total number of nodes in the tree with a penalty parameter which will will last Nate the cross-validation again this is very much of a benefit that the last so where we had a sum of squares of regression and we penalize the some of the apps advised of the coefficient which is complexity so you want the coefficients to be small to avoid overfitting here we want the size the tree to be small to avoid avoid overfitting yeah so cost complexity pruning is going to kind of find the best value of alpha by cross-validation and then we're going to pick the subtree in the tree the large tree we've grown that has a smallest value of this criterion so summarized in
000011_408908.5.jpg,the next slide
000012_417283.5.jpg,well sorry this actually just says what I just said and again yes okay so here's
000013_417517.1.jpg,
000014_421754.7.jpg,the summary of the tree growing algorithm again this is not something that you would normally have to program
000015_437770.7.jpg,yourself because software which is available for example cart or the tree software in in are it does all this cross validation process for you but it's good to see at least once what it's doing so we build a large tree it's just with a very simple stocking rule again
000016_480480.0.jpg,for example the for example we stop it when the number number of observations instrument note is some minimum number like five and then we we prune the tree to find the best subtree and for that we need to get an idea of the cutlet cost complexity parameter that the best trade-off between fit and tree size so for that we use cross-validation in this similar way that we've used it before for regression we divide the data into K parts KB maybe 5 or 10 we set aside one part we fit trees of various sizes on the K minus 1 parts and then evaluate the prediction error on the part we've left out so that's clear but the whole thing is controlled by elephant an
000017_496496.0.jpg,elephant decides how big the trees and you use cross-validation just to pick alpha exactly right and so we choose day off and we'll see cross-validation curves in a few slides but it's going to tell us a good idea of that the best value of alpha to trade-off the fit with the size of the tree having chosen alpha
000018_507173.3.jpg,we then go back to the full tree and find the subtree that has the smallest you mean the tree growing and all the training data exactly so let's see what
000019_508107.6.jpg,
